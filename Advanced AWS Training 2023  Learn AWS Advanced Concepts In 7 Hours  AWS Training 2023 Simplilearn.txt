hello everyone kaushal this side and
welcome to Advanced AWS training session
by simply learn
in this video we'll start this session
with an introduction to Amazon web
services and move forward by covering
topics like yaws what AWS is and AWS
architecture after covering all these
topics we'll see how to set up an AWS
account after that we'll cover AWS
services like AWS ec2 AWS cloud
formation AWS IAM AWS S3 AWS virtual
private Cloud AWS Lambda and AWS ECS
after covering all these Services we'll
move forward and see the difference
between Amazon web services and
Microsoft Azure not just that but we'll
also take you through the difference
between AWS and Google Cloud platform
and at the very end we'll cover all
three Cloud providers these are the
three Giants of cloud computing today
Amazon Microsoft and Google after that
we'll see some of the best
certifications and why should you choose
the simply learn AWS course
at the end we'll wrap up this Advanced
course with AWS interview questions and
let me tell you guys that we have
regular updates on multiple Technologies
so if you are a tech gig in a continuous
hunt for the latest technological Trends
then consider getting subscribed to our
YouTube channel and press that Bell icon
to never miss any updates from Simply
loan also if you want to master the
field of cloud computing the
postgraduate program in cloud computing
designed in collaboration with Caltech
ctme helps you become an Azure AWS and
gcp expert this in-depth cloud computing
certification course lets you master key
architectural principles and develop the
skills needed to become a cloud expert
benefits of this postgraduate program
includes Caltech cdme postgraduate
certificate enrollment and simply learns
job assist you will receive up to 30
CEUs from Caltech ctme you will attend
master classes from Caltech ctm
instructors live virtual classes led by
industry experts Hands-On projects and
integrated Labs online convocation by
Caltech ctme program director 40 plus
Hands-On projects and integrated Labs
Capstone project in four different
domains and Caltech ctma Circle
membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms and services such as AWS
Lambda Amazon S3 Azure app services and
many more so this program also covers
the essential skills to become an expert
in cloud computing and you can consider
this program because it provides you
with a certification from Caltech ctme a
well-reputed university in the United
States so what are you waiting for check
out the link mentioned in the
description box below and start your
cloud computing career today so without
any further delay let's get started
meet Rob he runs an online shopping
portal the portal started with a modest
number of users but has recently been
seeing a surge in the number of visitors
on Black Friday and other holidays the
portal saw so many visitors that the
servers were unable to handle the
traffic and crashed is there a way to
improve performance without having to
invest in a new server wondered rob a
way to upscale or downscale capacity
depending on the number of users
visiting the website at any given point
well there is Amazon web services one of
the leaders in the cloud computing
Market before we see how AWS can solve
Rob's problem let's have a look at how
AWS reached the position it is at now
AWS was first introduced in 2002 as a
means to provide tools and services to
developers to incorporate features of
amazon.com to their website in 2006 its
first Cloud Services offering was
introduced in 2016 AWS surpassed its 10
billion Revenue Target
and now AWS offers more than 100 cloud
services that span a wide range of
domains thanks to this the AWS cloud
service platform is now used by more
than 45 percent of the global market now
let's talk about what is AWS AWS or
Amazon web service is a secure cloud
computing platform that provides
computing power database networking
content storage and much more
the platform also works with a PSU go
pricing model which means you only pay
for how much of the service is offered
by AWS you use
some of the other advantages of AWS are
security AWS provides a secure and
durable platform that offers end-to-end
privacy and security
experience you can benefit from the
infrastructure management practices born
from Amazon's years of experience
flexible it allows users to select the
OS language database and other services
easy to use users can host applications
quickly and securely
scalable depending on user requirements
applications can be scaled up or down
AWS provides a wide range of services
across various domains what if Rob
wanted to create an application for his
online portal AWS provides compute
services that can support the app
development process from start to finish
from developing deploying running to
scaling the application up or down based
on the requirements
the popular Services include ec2 AWS
Lambda Amazon light cell and elastic
Beanstalk for storing website data Rob
could use AWS storage services that
would enable him to store access govern
and analyze data to ensure that costs
are reduced agility is improved and
Innovation accelerated
popular services within this domain
include Amazon S3 EBS S3 Glacier and
elastic file storage
Rob can also store the user data in a
database with aw Services which he can
then optimize and manage popular
services in this domain include Amazon
RDS dynamodb and redshift if Rob's
businesses took off and he wanted to
separate his Cloud infrastructure or
scale up his work requests and much more
he would be able to do so with the
networking Services provided by AWS some
of the popular networking Services
include Amazon VPC Amazon Route 53 and
elastic load balancing other domains
that AWS provide services in are
analytics blockchain containers machine
learning internet of things and so on
and there you go
that's AWS for you in a nutshell
Amazon uses decentralized or distributed
it infrastructure to make several it
resources available on demand
since the 1990s it has invested millions
of dollars in building and managing
large-scale distributed and efficient it
infrastructure
Amazon launched AWS a cloud computing
platform to allow other organizations to
take advantage of this reliable it
infrastructure
there are many scenarios in which AWS is
an efficient option for running web
applications or organization portals
let's explore a few AWS use cases
a small manufacturing organization can
use its expertise in expanding its
business through quality production
while leaving it management to AWS
a large Enterprise spread across the
globe can utilize AWS to deliver
training to its distributed Workforce
in architecture consulting company can
use AWS to get high compute rendering of
its construction prototypes
a media company can use AWS to provide
different types of content such as
videos ebooks and audio files to its
worldwide customers
based on the concept of pay as you go
AWS provides a suite of services that
customers can use when required and
without any long-term commitments or
upfront expenditure
the platform enables customers to
procure service such as Computing
programming models database storage
networking and development Platforms in
minutes
this allows the customers to enjoy the
benefit of low operational overheads
the distributed it infrastructure
provided through AWS has evolved with
time through the lessons learned from
over 16 years of experience AWS
diligently listens to its customer
feedback this enables the AWS team to
efficiently deliver creative features
and services
even today AWS continues to hone its
operational expertise continually to
retain lasting reliability by employing
its own advances and Industry best
practices into its Cloud infrastructure
as a result the customers tend to
benefit significantly from AWS
these benefits include the following
flexibility cost effectiveness
scalability and elasticity
security
flexibility is the major benefit of
using AWS when compared with the
traditional it infrastructure this
benefit is realized in different ways
first you get more time for Core
Business tasks through the instant
availability of new features and
services
the traditional it infrastructure
requires a lot of time in planning
designing procuring hiring and managing
resources
second you enjoy effortless hosting of
Legacy applications
AWS does not require rewriting Legacy
applications or learning new
technologies
migrating Legacy applications to AWS
gives the benefits of advanced Computing
as well as efficient storage
last you get a choice in running
services and applications
AWS is flexible enough to run services
and applications separately or together
as a platform
you can even choose to run a part of
your ite infrastructure in AWS and the
remaining part in your data centers
large organizations usually prefer this
method of operation
AWS requires no upfront investment
long-term commitment or minimum expense
this is its significant Advantage when
compared to the traditional it
infrastructure owning and operating
one's own infrastructure requires huge
investments in it Hardware power
bandwidth and human resources
through AWS techniques such as Auto
scaling and elastic load balancing can
automatically scale resources
they scale up the required resources to
fulfill a sudden demand and then scale
them down when the demand Falls without
affecting Speed and Performance
the AWS techniques are ideal for dealing
with unpredictable and variable loads
as a result the organization enjoys the
benefits of reduced cost and increased
user satisfaction
AWS delivers end-to-end security and
privacy to its customers its virtual
infrastructure offers Optimum
availability while managing full privacy
for customers and isolation of their
operations
customers can expect High physical
security and this is due to Amazon's
several years of experience in designing
developing and running large-scale it
operation centers
AWS ensures confidentiality integrity
and availability of the user's data
the purpose of AWS compliance is to
enable you to understand its powerful
controls in action and maintain security
and data protection
AWS provides security to their Global
infrastructure along with a variety of
features for securing critical data in
the cloud
let's explore some of the security
highlights
AWS sternly controls supervises and
audits physical access to their data
centers and Network
as a customer you can manage credentials
allowing access to your AWS account
Access Control list or ACL type
permissions on critical data as well as
encrypted while in storage
configure a virtual private cloud or VPC
a virtual Network logically isolated in
AWS from other virtual Networks
configure and control the operating
system on your own Amazon web services
virtual server
configure a security group as a virtual
firewall to keep an eye on the incoming
and outgoing traffic for your virtual
servers in AWS
State a key pair at the time of
introducing your virtual server in AWS
this key pair is then used to encrypt
login information
the AWS it infrastructure is designed
and managed according to the best
security practices certifications and it
security standards
these certifications include service
organization control or soc1 soc2 and
SOC 3. federal information security
management act or fisma
Department of Defense or DOD information
assurance certification and
accreditation process or diacap
Federal risk and authorization
Management program or fedramp
DOD cloud computing security
requirements guide or srg criminal
justice Information Services or cjis
National Institute of Standards and
technology or nist payment card industry
or PCI Data security standard DSS level
1.
International Organization for
standardization or ISO 9001 and ISO
27001 U.S International traffic and arms
regulations or itar section 508
voluntary product accessibility template
or vpat
federal information processing standard
or fips publication 140-2
the industry-specific standards for
customers to deploy their Solutions
include health insurance portability and
accountability act or HIPAA
the family educational rights and
privacy act or FERPA
Cloud security Alliance or CSA
Motion Picture Association of America or
npaa
today we're going to talk about AWS in
10 minutes let's look at the agenda here
let's talk about what AWS is
why it's such a big hit
we'll have a look at the overview of the
services we'll see how much it costs
how big it is and what the future of AWS
is
so what is AWS everyone is talking about
Amazon web services but let's have a
look at what it is
basically a global Cloud platform which
allows you to host and manage his
services on the internet
it's used by almost 80 percent of
Fortune 500 companies to host their
infrastructure and it has a lot of
services which it provides to its
customers there's infrastructure service
which means they provide bear servers as
a service so you don't need to manage
the backup and the power supply of the
service
they provide platform of the service
you can get Java Ruby PHP as a service
so that you don't have to manage the
binaries of these applications
you get software as a service wherein
you get email sending capabilities like
SES
you get queuing services like sqs and
it's a cloud storage platform wherein
you have a lot of storage options
including EBS and S3 so all in all AWS
is a hosting provider which gives you a
lot of services wherein you can run your
applications on the cloud
now let's take a look at why it's such a
big hit
so everyone is trying to use AWS
everyone is trying to put their
applications on the cloud
so what's the reason that AWS is the top
provider and the top choice for doing
anything on the cloud
one of the biggest reasons is the
billing so the billing is very clear you
get a per hour billing
every instance or every service has a
micro billing so be it instances on ec2
you get per hour billing rate which is
very transparent
even S3 buckets are charged on a per GB
basis although it is a storage service
but still there is micro billing
available there
the sign up process is easy you don't
need to sign any agreement nothing you
just go sign up with an email ID add a
credit card and you're good to go
you can go from 0 to 100 in just two
minutes
you can launch your servers big machines
without buying Hardware without
procuring any hardware you can just be
up and running in minutes
so their billing dashboard is also very
simple
they give you an integrated billing
dashboard which gives you reports
you can pull out reports every month you
can pull out reports based on Services
based on various parameters
[Music]
for the cloud provider to be a hit you
need it to be stable it has to be a
trusted thing so their services are
quite stable
in the last seven or eight years they
have seen some three or four major
outages but those have been only region
specific
so that means out of the 12 or 13
regions in which they operate the
outages have been region specific in a
particular country or continent and
those also have not been more than two
or three hours and have not affected all
of their customers
it's a trusted vendor so when we talk
about AWS it comes up as something which
is used by everyone in the industry
from small startups to Big Enterprises
everyone sees Amazon as a trusted
advisor
now let's have an overview of the most
commonly used services
so the first and the most commonly used
service is ec2 which is elastic compute
cloud
this is the service which gives you bear
servers so this service will give you a
machine which you can launch and you can
run your software on those
you can get small or big machines based
on your requirements
the second choice is VPC so Amazon will
not allow you full control of their
Cloud instead they give you chunks of
their Cloud which is VPC or the virtual
private cloud
so VPC lets you create networks in the
cloud and then run your servers in those
Networks
the next one is S3 which is simple
storage service
so S3 gives you the opportunity to
upload and share files so it is mostly a
file storage and sharing service
then you've got RDS which is relational
database service
so this RDS allows you to run and manage
databases on the cloud so they've got
almost all the major flavors of
databases right from SQL Server to
Oracle and MySQL post gray SQL
they have recently launched another one
which is called Aurora which claims to
be a very high performance database then
Route 53 is there for DNS so they've got
a managed DNS service wherein you can
point your DNS to Amazon and they take
care of the stuff
so it's a global DNS service it's a
scalable DNS service so it scales
according to demand
there's also elastic load balancer the
elb is a service which gives you the
opportunity to load balance incoming
traffic to multiple machines so this way
you can scale up your web applications
to any number of users
you've got Auto scaling which adds
capacity on the fly to elastic load
balancers so that your website or your
application is never down due to a load
how much does it cost this per our
billing is already mentioned for
everything if it's something like a
storage thing again there's a per hour
or per GB month storage so I think there
is region specific pricing so Virginia
is the cheapest region out of all of
them so the region-specific pricing is
because they have got some regions which
they have got good hold of and they are
the headquarters so Oregon and Virginia
are the cheapest actually
they also give you Services based on the
term so if you sign up for something for
a year it would be cheaper for you
rather than signing up for something on
an on-demand basis so they've got
reserved instances which are very cheap
as compared to the on-demand ones you
can get discounts from 20 to almost 60
percent if you sign up for a three year
term
they are spot resources examples of
these are spot instances so this is like
a bidding Market base where you can bid
for a price the only downside of this
kind of pricing is that your machine
might be terminated or your resources
might be terminated if someone bids
higher people use these kind of things
for doing some ad hoc in search or some
ad hoc tasks which are really not
critical
how big is it they have got 15 regions
across major countries of the world
they've got regions in the U.S Europe
Asia Pacific they've got a global
footprint so in today's world if you are
anywhere in the world you would have a
region within 1 000 miles of your
location they've got massive data
centers so each of the region has got
multiple availability zones so one
availability Zone can be thought of as a
big data center
the data centers have anywhere from
three hundred thousand to five hundred
thousand servers
what is the future of AWS
so they currently have 64 Services which
span across infrastructural service
software as a service platform as a
service
they are launching new services in all
demands every day
right now they are focusing on machine
learning so recently they've launched a
couple of services which Focus
exclusively on machine learning and they
are focusing on software as a service
product wherein they want to take
control of the service you want to
utilize they don't want you to do it
they want you to upload it to them
and every now and then they keep on
reducing the costs so you would hear it
in the blog that okay the price of ec2
machines has been reduced
and this is because of their scale so
they scale up and they give the cost
benefit to the customer
that's all for today guys with just a
quick intro of the Amazon web services
Cloud also if you want to master the
field of cloud computing the
postgraduate program in cloud computing
designed in collaboration with Caltech
ctme helps you become an Azure AWS and
gcp expert this in-depth cloud computing
certification course lets you master key
architectural principles and develop the
skills needed to become a cloud expert
benefits of this postgraduate program
includes Caltech ctme postgraduate
certificate enrollment and simply learns
job assist you will receive up to 30
CEUs from Caltech ctme you'll attend
master classes from Caltech ctme
instructors live virtual classes led by
industry experts Hands-On projects and
integrated Labs online convocation by
Caltech ctme program director 40 plus
Hands-On projects and integrated Labs
Capstone project in four different
domains and Caltech ctma Circle
membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms
and services such as AWS Lambda Amazon
S3 Azure app services and many more so
this program also covers the essential
skills to become an expert in cloud
computing and you can consider this
program because it provides you with a
certification from Caltech ctme well
reputed university in the United States
so what are you waiting for check out
the link mentioned in the description
box below and start your cloud computing
career today so without any further
delay let's get started
in the topic how to design cloud
services
we will take a look at the AWS well
architected framework
and we'll take a deeper look at the four
pillars which make up the framework and
these are security
reliability
performance efficiency and cost
optimization
we'll then take a look at where you can
find more information about designing
cloud services for Amazon
and we will look at the AWS quick start
reference deployments
and where you can find out real life
examples about how real businesses are
using Amazon web services
so let's start by looking at the AWS
well architected framework
now this was introduced because building
Cloud infrastructure Services is vastly
different to what you're used to with
on-premise infrastructure so AWS created
this framework to help you understand
the pros and cons of decisions you make
while Building Systems on AWS
the AWS framework documents a set of
foundation questions that allow you to
understand if a specific architecture
aligns well with bitcloud best practices
and there's five principles it helps you
with the first of which is stop guessing
your capacity needs
test systems at production scale
lower the risk of architecture change
automate to make architecture
experimentation easier
and allow for evolutionary architectures
and we'll take a look at each of these
in Greater detail
so firstly let's take a look at stop
guessing your capacity needs now imagine
you're designing a brand new application
for your business
all the infrastructure decisions you
will make are made using estimates which
are basically guesses
if you overestimate you might end up
buying too much hardware and have a
fleet of expensive idle resources
conversely you might underestimate and
have to deal with the performance
implications of limited capacity
with cloud computing these problems go
away as AWS helps you eliminate the
guesswork in your infrastructure
capacity needs you can use as much or as
little capacity as you need and scale up
and down automatically depending on your
demand
the cloud allows you to test systems of
production scale
traditionally in a non-cloud environment
it's usually very difficult to fully
Test new productsal services in
development as it's cost prohibitive to
have like the like environments or you
just don't have the resources available
personally I've lost count how many
projects I've worked on when things have
gone wrong as soon as the product's Gone
live due to insufficient testing
with the cloud you can duplicate
environments on demand and when you've
completed your testing just shut them
down and you only have to pay for the
time the test environment was up and
running so you can test fully before you
go live in production
AWS allows you to lower the risk of
architecture change because you can
automate the creation of test
environments
so you can emulate your production
configurations and Carry Out testing
against comparable environments
you can also remove testing bottlenecks
where various teams are lining up to use
your test environments as with AWS you
can cheaply spin up as many test
environments as you need and ensure that
all the production changes have been
sufficiently tested
you can automate to make architecture
experimentation easier
if you need to make a change to
production but not sure what the impact
will be well you can automate the
creation and replication of your systems
at low cost and low effort to test these
changes you can then order the impact
and then if necessary you can revert the
changes and try again
with AWS you can allow for evolutionary
architectures
in a traditional environment
architecture decisions are often
implemented as a static one-time event
you have to live with during the
lifetime of a system but with a cloud
the capability exists to automate and
test on demand to lower the risk of
design changes
so you can Implement new Innovations and
features easily
what this means is you aren't stuck with
last year's technology and you can
Implement new technology as soon as it
comes out
as mentioned earlier the well
architected framework is based on four
pillars security reliability performance
efficiency and cost optimization
so in the coming slides we're going to
take a further look at these four
pillars starting with security
so Amazon defines security as the
ability to protect information systems
and assets while delivering businesses
value through risk assessments and
mitigation strategies
what this means is you can apply
security at all layers
so rather than just running minimal
security like firewalls at the edge of
your infrastructure
you can use firewalls and other security
controls on all of your resources for
example security groups on ec2 instances
allow you to Define who and what can
access that specific resource rather
than just defining it at a global
infrastructure level
you can enable traceability so you can
log and order all changes to your
environment
you can automate responses to security
events
so you can monitor and automatically
trigger alerts depending on your needs
you can focus on sharing your system the
AWS shared responsibility model allows
you to focus on securing your
application data and operating systems
whilst AWS secures your infrastructure
and services
and you can automate security best
practices AWS allows you to create and
save a custom Baseline image of a
virtual server and then you can use that
image to automatically launch new
servers
this way you can ensure that all new
resources adhere to your security
standards
so I just briefly mentioned the AWS
shared responsibility model now we take
a look at this in more depth in a later
lesson
but what it means is that the AWS shared
responsibility model allows AWS
customers to focus on using services to
accomplish their security and compliance
goals because AWS physically secures the
infrastructure that supports your cloud
services so if you look at this diagram
you can see it's split into two
security measures that the cloud service
provider implements and operates is the
security of the cloud and that's a
responsibility of AWS
then this is security measures that the
customer inputs and operates and this is
related to the security of customer
content and applications that make use
of the AWS services and this is Security
in the cloud and this is the
responsibility of you
so let's take a look at security in the
cloud and this is composed of four areas
data protection privilege management
infrastructure protection and detective
controls
let's start with data protection
before architecting any system practices
that ensure security should be in place
with data protection these are
categorizing data based on levels of
sensitivity
granting least privilege while still
allowing users to perform their work and
encryption to protect your sensitive
data
if you're storing customer credit card
data obviously this needs to be
categorized as sensitive and it needs to
be encrypted but log files from a daily
maintenance task wouldn't need this
level of security
now there's a number of tools AWS offers
to assist you with this
AWS makes it easy to encrypt data and
manage keys and key rotation using I am
and Key Management Service
you can perform detailed logging using
cloudtrail
and AWS offers resilient storage systems
like Amazon S3 which has 11 nines of
durability
now other lines of durability means that
if you store 10 000 objects with Amazon
S3 on average you can expect to incur a
loss of a single object once every 10
million years so in other words you're
not going to lose many files and finally
AWS offers versioning and lifecycle
management again with S3 so you can
predict again accidental deletes or file
over rights
now a central part of any information
security program is privilege management
so that you can ensure that only
authorized and authenticated users are
able to access your resources
and that they can only access them in a
way that is acceptable
for example if a user needs access to a
resource don't just give them admin or
read write permissions by default
because they might actually only need
read-only access
Now to control your privilege management
you need to have a few things in place
the first of which is an access control
list and this is a document that needs
to be maintained which lists access
permissions attached to an object so if
you have some particular documents you
need to Define what access is allowed to
it
then you have role-based access controls
and this defines the permissions for a
particular end user's role or function
so an administrator would obviously need
administrator access to various
resources but you know a date a regular
end user might only need read-only
access so you need to Define which roles
have which permissions
then there's password management
and this includes complexity
requirements and change intervals so you
define how often people need to change
their passwords and how complex they
need to be
now Amazon helps you with all this with
the identity and access management or IM
service and this is the primary service
you use to control access to AWS
services and resources
we cover I am in huge detail in a later
lesson but the briefly it allows you to
apply granular policies which assign
permissions to users groups roles or
resources and you can also control the
password strength using this as well as
Federation with your existing directory
services such as Microsoft active
directory
we also need to protect your
infrastructure and to do this it's
recommended to have multiple layers of
defense and also multi-factor
authentication to meet best practices
with industry or regulatory obligations
now Amazon allows you to do this
by implementing stateful or stateless
packet inspection EVS in AWS native
Technologies or by using partner
products and services available through
the AWS Marketplace
Amazon virtual Cloud VPC enforces
boundary protection and monitoring
points of Ingress and egress and then
there's cloudtrail and cloudwatch which
provides comprehensive logging
monitoring and alerting
you should always have detective
controls in place so you can detect or
identify security breaches
this needs to happen so you can provide
both a quality support process and meet
compliance obligation
and buy a quality support process I mean
one in which when you have a problem
you're committed to finding out the root
cause and actually resolving the issue
rather than just saying oh we think it
was that let's hope it doesn't happen
again
I mean I know I've worked for some
places like that and it's certainly not
a quality support process
AWS helps you with your detective
controls by providing these Services AWS
cloudtrail is a service that logs API
calls so you can find out the identity
of callers the time of calls Source IP
addresses and things like that so you
can find out who is doing what and when
Amazon cloudwatch is a monitoring
service for AWS resources so you can log
ec2 CPU usage or network activity and
you can use cloudwatch with many
different services like RDS EBS and more
you can also set up alarms so you can be
notified when certain things happen
AWS config is an inventory and
configuration history service that
provides information about the
configurations and changes in
infrastructure over time
with Amazon S3 you can see to access all
the ting so you can find out who's been
accessing your S3 data and who when they
did it and when
and Amazon Glacier you can use the Vault
lock feature to reserve Mission critical
data with compliance controls that are
designed to be auditable for long-term
retention
the next pillar is reliability and
Amazon defines this as the ability of a
system to recover from infrastructure or
service failures
dynamically acquire Computing resources
to meet demand and mitigate disruptions
such as misconfigurations or transient
network issues
with reliability in the cloud there's a
number of things to look at firstly you
can test recovery procedures
Cloud infrastructure means you can test
how your system fails and you can
validate your recovery strategy
places I've worked at in the past
Disaster Recovery was a big event it was
only done annually it took months of
preparation it involved working a whole
weekend and then some and was also
incredibly disruptive to the business
imagine being able to duplicate your
production environment and test failure
and failover whenever you want
you can automatically recover from
failure with the cloud monitoring a
system for a key performance indicator
or kpi you can automatically trigger
automated recovery processes when a
threshold is breached
you can scale horizontally to increase
aggregate system availability
using multiple small resources instead
of one large resource will reduce your
single points of failure
and also you can stop guessing capacity
gone are the days of using lack of
memory or not enough CPU is a failure
for a resource with the cloud you can
scale as and when you need to so you
can't blame badly estimated capacity as
the reason for reliability issues
now reliability in the cloud is composed
of three areas these are foundations
change management and failure management
to achieve reliability a system must
have a well-planned foundation and
monitoring in place
plus systems for handling changes in
demand or requirements is also required
in an Ideal World your system would be
designed to detect failure and
automatically heal itself
before architecting any system you
should have foundations in place so that
your reliability is not impacted you
should have sufficient Network bandwidth
to your data center you should have
sufficient compute capacity
your staff should be trained in the
areas they need to be and you should
have support contracts with your vendors
with AWS Network bandwidth and compute
capacity is already taken care of
AWS also offers a range of support
contracts to help you with issues and a
range of training courses for your staff
change management is a critical part
being aware of how a change affects the
system allows you to plan proactive and
monitoring allows you to quickly
identify trends that could lead to
capacity issues or SLA breaches
with AWS you can monitor the behavior of
a system and automate the response to
kpis for example you can add additional
servers as a system gains more users so
you can control who has permissions to
make system changes and order the
history of these changes
unfortunately it's a given that failures
will occur so you need to have some
failure Management in place
it's good practice to understand why
things failed so you can prevent it from
happening again no one wants to work for
a company that when things go wrong they
just get it working again with no
understanding of why
a key to managing failure is frequent
and automated testing of systems to
failure and through recovery ideally you
do this on a regular schedule and also
after you've made significant system
changes
now Amazon allows you to do this with a
few products firstly AWS cloud formation
this allows you to launch temporary
copies of whole systems at very low cost
so you can use automated testing to
verify your recovery processes
you can use cloudwatch to set up
automation to react to monitoring data
that indicates a failure
and you can store all your data on
Amazon S3 buckets for future use
the next pillar is performance
efficiency and Amazon defines this as
the ability to use Computing resources
efficiently to meet system requirements
and to maintain that efficiency as
demand changes and Technologies evolve
so what does this mean well performance
efficiency in the cloud means you can
democratize Advanced Technologies
AWS provides products such as nosql or
Media transcoding or even machine
learning as a service so rather than
your it team having to learn how to host
and run these new technologies AWS does
it for you
you can go Global in minutes AWS allows
you to easily deploy your systems in
multiple regions around the world with
just a few clicks of your mouse so this
leads to lower latency for your
customers
you can use serverless architectures in
the cloud which remove the need for you
to run and maintain servers to carry out
traditional compute activities
for example storage Services can act as
static websites which remove the need
for web servers and you can also use
serverless Technologies such as Lambda
or elastic Beanstalk
and Euro can also experiment more often
with virtual and automatable resources
you can quickly carry comparative
testing using different types of
instances storage or configuration
so performance efficiency in the cloud
is composed of four areas compute
storage database and something called
space time trade-off and let's take a
look at each of these
with the cloud finding the optimal
server configuration for a product will
vary based on application design usage
patterns and configuration settings
traditional it infrastructure requires
making estimates in advance which can
lead to incorrect server configurations
and lower performance efficiency
AWS is virtualized so you can quickly
change the ec2 server configuration to
increase its performance efficiency
it may be optimal to run serverless
Computing for example AWS Lambda allows
you to execute code without running an
instance an elastic Beanstalk allows you
to run web applications in a similar
manner
from an operational standpoint you
should have monitoring in place to
notify you of any degradation in
performance
the optimal storage solution for a
particular system will vary based on
whether you need block file or object
storage the type of throughput required
frequency of access and availability and
durability constraints
well architected systems use multiple
Storage Solutions and enable different
features to improve performance
in AWS storage is virtualized and is
available in a number of different types
this makes it easier to match your
storage methods more closely with your
needs and also offers storage options
that are not easily achievable with
on-premise infrastructure for example
Amazon S3 is timed for 11 9's durability
and provides a variety of storage
classes for your different data
categories like Glacier for archive data
EBS and EFS offer numerous options for
your ec2 instances based on the level of
performance you require from your
storage
the optimal database solution for a
particular system can vary based on
requirements for consistency
availability partition tolerance and
latency
many systems use different database
solutions for various subsystems and
enable different features to improve
performance selecting the wrong database
solution and features for a system can
lead to much lower performance
efficiency now for database usage you
need to be able to monitor them test
them and also you need to have some
database platform knowledge but Amazon
makes this much easier for you as it
provides a suite of Amazon relational
database Services which are fully
managed relational databases
for example Amazon dynamodb is a fully
managed nosql database that provides
single digit millisecond latency at any
scale Amazon redshift is a managed
petabyte scale data warehouse that
allows you to change the number or type
of nodes is your performance or capacity
needs change you can also make use of
Aurora or Amis where you install your
own databases from templates
so what is the space-time trade-off well
you can think of space as memory or
storage and you can think of time as the
processing time or the compute time to
complete a task
so when you're architecting Solutions
there's always a series of trade-offs
where it might be better to have more
memory or storage to reduce the
processing time or maybe you're happy to
sacrifice the processing time in order
to reduce your memory or storage
requirements AWS gives you the option to
maximize one or the other
for example you could launch your
systems globally so you're increasing
space and memory so that they're closer
to the end users in order to reduce
latency all the time
but whatever resource you use you need
to have monitoring in place to notify
you of any degradation in performance
and the final pillar of the well
architected framework is cost
optimization this is defined as the
ability to avoid or eliminate unneeded
cost or sub-optimal resources
AWS cloud has a number of ways you can
provide cost optimization
firstly you can transparently attribute
expenditure as the cloud makes it easy
to identify the costs of a system an
attribute it costs of individual
business owners so those owners will
have an incentive to optimize their
resources and reduce costs
you can use managed services to remove
the operational burden of maintaining
servers for tasks such as sending email
or managing databases
you can trade Capital expense for
operating expense instead of investing
heavily in data centers and servers
before you know how you're going to use
them with AWS you pay only for the
Computing resources you consume when you
consume them
also AWS allows you to achieve higher
economies of scale because they can buy
way more servers and Hardware than you
could ever imagine so it's much cheaper
to use their Hardware than buy your own
you should also stop spending money on
data center operations AWS is the heavy
lifting of racking stacking and powering
servers so you can focus on your
customers and business projects rather
than on your it infrastructure
cost optimization in the cloud is
composed of four areas
match supply and demand cost effective
resources expenditure awareness and
optimizing over time
matching Supply to demand will deliver
the lowest costs for a system
but you'll need to be able to provide
sufficient extra capacity to cope with
demand and failures
in AWS you can automatically provision
resources to match demand
Auto scaling time-based and event-driven
and cue based approaches allow you to
add and remove resources as needed
monitoring tools and regular
benchmarking can help you achieve much
greater utilization of your resources
using the appropriate instances and
resources for your system is key to cost
savings for example a reporting process
might take five hours to run on a
smaller server but a larger server that
is twice as expensive might better do it
in just one hour
both jobs give you the same outcome but
the smaller server will incur more costs
over time
but to be able to find out these things
you need to have monitoring of
expenditure in place and also regular
benchmarking
AWS helps you with this AWS trusted
advisor goes through your resources and
tells you where you can make savings
you can use on-demand instances so you
only pay for compute capacity by the
hour with no minimum commitments
required or you could use reserved
instances which allow you to reserve
capacity and offer Savings of up to 75
percent off on-demand pricing
and with spot instances you can bid on
unused ec2 capacity at significant
discounts you can also take advantage of
managed AWS services such as RDS or
dynamodb which can also lower your costs
AWS and its virtually Unlimited on
demand capacity requires a new way of
thinking about expenditures
so you need to have a different way of
budgeting and you need to understand
your business units usage
to do this you can use cost allocation
tags to categorize and track your AWS
costs for AWS resources such as ec2 or
Amazon S3
an AWS generates a cost allocation
Report with your usage and costs
aggregated by your tags
so you can set up tags for each of your
business categories and units
this increased visibility of costs makes
it easier to identify resources or
projects that are no longer generating
value and should be decommissioned
billing alerts can be set up to notify
you of predicted overspend and the AWS
simple monthly calculator allows you to
calculate data transfer costs and you
can set up SNS alerts so you can be
notified when certain resources or cost
breaches occur
optimizing over time is an important
strategy to reduce your costs associated
with your Cloud environment
AWS is always releasing new Production
Services as such you need to reassess
your existing setup to see if it's the
most cost effective for example rather
than running a database on an ec2
instance it might be cheaper to run an
Amazon RDS database
or Lambda might be more cost effective
than ec2
but where did you find out more
information about these new products and
services from Amazon
well the first place to look for any new
architecture guide is the AWS
architecture Center
this is a resource that provides you
with the guidance and application
architecture best practices to build
highly scalable and reliable
applications in the cloud
here you'll find the AWS reference
architectures
the AWS reference architecture data
sheets provide architectural guidance to
help you build an application on the AWS
Cloud you'll find data sheets that
include a description of how each
service is used plus visual
representations of the application
architecture for example web application
hosting or large-scale processing and
huge data sets or even building elastic
web front ends for an e-commerce website
AWS white papers provide a comprehensive
list of technical AWS white papers that
cover all Amazon related topics such as
architecture security and economics
there are new white papers being
released all the time and are written by
some of the most knowledgeable AWS
people around
AWS quick start reference deployments
use AWS cloud formation templates to
rapidly deploy a fully functional
environment for a variety of enterprise
software applications
supplementary deployment guides describe
the architecture and implementation in
detail
in this course we'll deploy things
manually step by step so you can see how
it works but with cloud formation
templates you can do several hours work
with just the click of a button
examples of cloud formation templates
are SharePoint Rd Gateway or even SQL
server on Windows Server failover
clusters
and finally case studies AWS maintains a
large list of case studies and success
stories from their clients you can check
to see how some of the largest and most
successful companies on the planet use
AWS for their business you'll learn how
the AWS well architected framework is
used in planning and designing
we'll take a look at scaling and all the
different versions
we'll look at the importance of loose
coupling
how you can build redundancy into your
Cloud infrastructure how you can save
money by cost optimization
we'll look at Automation and how it can
make management of an IT environment
easier and also we'll take a look at
security and how you can secure your
Cloud resources
so let's start with scalability
in the past you would have to estimate
the peak load of your systems and
purchase your Hardware accordingly so
perhaps you were setting up a new
website and you are estimating that
you're going to get a million people per
month hitting your website so you buy
the hardware accordingly but then you
find out that only 50 000 people are
hitting it or 10 million people are
hitting it so your Hardware is
inappropriate for the work
cloud computing provides virtually
unlimited on-demand capacity so you can
scale whenever you need to to do this
your designs need to be able to
seamlessly take advantage of these
resources and there are two ways to
scale vertically and horizontally
vertical scaling means increasing the
specifications of an individual resource
for example increasing the memory or the
number of CPUs on the server
with AWS this is easily achieved with a
restart of your virtual server so that
it resizes to a larger or smaller
instance type the advantage of this
approach is that it's very easy and
requires little for in the short term
however this type of scaling can
eventually hit a limit and sometimes be
cost inefficient especially if these
scale up to some very very large
instances
horizontal scaling means increasing the
number of resources rather than the
specifications of each individual
resource
for example adding additional web
servers to help spread the load of
traffic hitting your application
this is a great way to build
applications that Leverage The
elasticity of cloud computing however
not all architectures can distribute
their workload to multiple resources
stateless application is one that needs
no knowledge of previous interaction and
stores no session information
an example of this would be a web server
that provides the same web page to any
end user
a stateless application can scale
horizontally as any request from any end
user can be serviced by any of the
available compute resources
you can add more resources as required
and then remove them when the capacity
is no longer available
stateless applications do not need to be
aware of their peers all that is
required is a way to distribute the end
user sessions to them
and there are two ways to distribute
load to multiple nodes the first of
these is the push model
a load balancer such as the AWS elastic
load balancer is a popular way to
distribute a workload across multiple
resources
an alternative but less recommended
approach is to implement a DNS round
robin using Amazon Route 53 where DNS
responses return an IP address from a
list of valid hosts in a round robin
version
this is an easy to set up or option but
it can cause issues if DNS records are
cached
the alternative to the push model is the
pull model
tasks that need to be performed can be
stored as messages in a queue and
multiple compute resources can pull and
process the messages in a distributed
fashion
examples of this are big data processing
scenarios where many servers work to
analyze data and return results or media
file conversion processes where multiple
servers convert the files as they arrive
Amazon sqls or Amazon Kinesis are
services that can provide pool model
load balancing
most applications need to maintain some
kind of state information for example an
automated multi-step process will also
need to track previous activity to
decide what its next action should be
you can make components in your
architecture stateless by not storing
anything on the local file system and
instead storing user or session-based
information in a database like dynamodb
or MySQL or on shared storage like
Amazon S3 or EFS
additionally Amazon swf can be used to
centrally store execution history and
make your workload stateless
so the opposite stateless applications
are stateful applications as it's not
possible to turn some layers of your
architecture into stateless components
for example databases which are stateful
by definition or applications that were
designed to run on a single server
providing multiple users a consistent
view of the database or application is
much simpler when your components are
not distributed
Distributing the load is possible low
through session Affinity this means that
all transactions of a session are bound
to a specific compute resource however
this means that existing sessions would
not be able to utilize newly introduced
compute nodes and also if a node is shut
down or becomes unavailable users bound
to that session will be disconnected
for situations when you need to process
large amounts of data it's always
beneficial to see if a distributed
processing approach can be used
imagine a task that requires a huge
amount of data processing if it was to
run on a single compute resource it
would max out the resources and take a
long time to complete
you can divide the task into smaller
fragments of work then each of the tasks
can be executed across a larger set of
compute resources
examples of this are the AWS elastic map
reduced service which allows you to run
Hadoop workloads across multiple ec2
instances or Amazon Kinesis allows you
to run multiple shards of data on ec2 or
Lambda resources for real-time
processing of streaming data
the concept of disposable resources is
completely new with cloud computing
as cloud computing completely changes
the mindset of an I.T infrastructure
environment traditional environments
involve purchasing Hardware up front and
required manual configuration of the
software Network Etc
whereas with cloud computing all
infrastructure is temporary or
disposable
you can launch new instances when you
need them and get rid of them when you
are done
automate compute resource initiation is
a way to speed up the creation of new
environments AWS has plenty of features
to make new environment creation and
automated and repeatable process and
there's three options available which
we'll look at in the coming slides
bootstrapping golden images and a hybrid
approach
bootstrapping means you can take a newly
launched AWS resource with its default
configuration and then execute automated
scripts to install software or copy data
to bring that resource to a required
state
so you could launch an ec2 instance and
copy data on so that it becomes a web
server
scripts can be parameterized so that
different environments production or
development can be initiated easily with
AWS bootstrapping can be achieved with
your own scripts Chef or puppet Ops work
lifecycle events or cloud formation
golden images mean you take snapshots of
your ec2 instances or RDS instances or
even EBS volumes and they can be used to
launch new instances
these ec2 instances can also be
customized and saved as Amis Amazon
machine images and then you can launch
as many instances as you want from this
template
if you have an on-premise virtualized
environment you can also use AWS VM
import export to create your own AWS
Amis
the hybrid approach makes use of both
bootstrapping and golden images
you can launch new instances from a
golden image but then bootstrap to
configure other actions
basically with AWS you are only
restricted by your own imagination or
coding skills
AWS assets are programmable so you can
apply all these techniques and
principles that we've just talked about
to entire environments and not just
individual resources so you can
completely rethink the way you work your
it infrastructure
automation is a big part of planning and
designing Cloud infrastructure AWS
allows you to reduce the level of manual
interaction in your environment using
automation you can react to a variety of
events without having to do anything
let's take a look at some examples AWS
elastic Beanstalk developers can upload
their code and the Beanstalk service
automatically handles Resource
provisioning Auto scaling load balancing
and monitoring
ec2 Auto Recovery in some situations you
can use auto recovery to automatically
recover an ec2 instance if it becomes
impaired the recovered instance is
identical to the original instance name
IP address metadata Etc
Auto scaling you can automatically add
resources to cope with demand spikes and
decrease again during quiet times
cloudwatch alarms and events cloudwatch
can send SMS notifications to alert when
a particular event occurs for example
CPU usage is too high the notifications
can be used to perform follow-up actions
like run a Lambda function
Ops works like cycle events you can
continually update your instances
configuration to adapt to environment
changes for example if a new database
instance is added to your environment
then an event can automatically trigger
a chef reppi that points existing
applications to the new server
loose coupling is an important concept
ion should be designed so that they're
broken into smaller Loosely coupled
components
the desired outcome is that a failure in
one component should not cause other
components to fail
this is achieved for a variety of
measures firstly well-defined interfaces
by ensuring that all components only
interact with each other through
specific technology agnostic interfaces
for example restful apis will result in
being able to modify resources without
affecting other components
Amazon API Gateway is a fully managed
service for apis which you can do this
with
loose coupling needs services to be able
to interact with each other without
having any prior knowledge of their
existence for example imagine two
servers communicating on hard-coded IP
addresses then the addition or
modification of resources will result in
manual interaction to update the
configuration however Loosely coupled
infrastructure is an essential
ingredient in making the most of cloud
computing elasticity the easiest way to
achieve this is to use an elastic load
balancer service to provide a stable
endpoint for all your services this way
they don't have to worry about each
other the elastic load balancer does
that for them
rather than use synchronous integration
where server a completes an action and
passes it to server B which then passes
it to server C asynchronous integration
involves the use of an intermediate
storage area like an sqsq
this approach means that when a server a
completes its action it sends a
notification to sqs this way the compute
resources are decoupled and not directly
linked to each other this means you can
easily add or remove resources without
having to update the configuration of
the existing compute resources
designing applications to fail
gracefully allows other resources to
continue a service without causing a
complete outage
examples of this would be if your
primary website fails the Route 53 DNS
failover feature points your users to a
backup site hosted on a static website
on Amazon S3
the idea is to continue to provide a
service even if it isn't the full
experience
traditional it infrastructure involves
developing managing and operating
applications with a wide variety of
underlying technology components
AWS isn't just about ec2 instances there
are Suite of products to do everything
mentioned and a lot more besides to help
you lower your it costs and allow you to
move faster as an organization
for example manage services AWS provides
many services that are fully managed
databases machine learning analytics
search email
Etc
this lessens the burden on your it
infrastructure Department
RDS databases mean you don't have to
worry about backups fault tolerance for
your simple databases
S3 allows you to store as much data as
you need without having to concern
yourself with capacity management
serverless architectures you cannot
reduce operational complexity of your
applications through the use of
serverless architectures
tools such as the Internet of Things
Lambda S3 Beanstalk all allow you to run
your applications without paying for any
server infrastructure
in a traditional it environment the type
of database in use is typically
restricted by constraints on licensing
support capabilities Hardware
availability and things like that
AWS provides you with multiple managed
database server options that remove
these constraints meaning that you can
choose exactly the right database for
your needs rather than making use of
what is available to you AWS offers
fully managed easy scalable services for
relational databases nosql data
warehouses and search functions that
provide fully managed easily scalable
services
that can create synchronously replicated
standby instances in different
availability zones
and they'll also automatically fail over
without the need for manual intervention
we'll cover databases in a whole lesson
later on in this course
now we'll take a look at redundancy
introducing redundancy will ensure your
systems are highly available so they can
withstand the failure of individual or
multiple components for example hard
disks servers or network links
introducing redundancy will remove
single points of failure from your
systems this is achieved for having
multiple resources for the same tasks in
either standby or active mode
standby redundancy is often used for
stateful components like databases the
standby resource becomes the primary
resource by failing over to it the
standby resource can be brought online
only when required to save cost or it
can already be running to speed up the
failover process and minimize the outage
active redundancy is where multiple
redundant compute resources share
requests and are able to absorb the loss
of one or more of the instances failing
examples of this would be multiple web
servers sitting behind the load balancer
automatic failure detection allows you
to react to an outage automatically
without the need for manual intervention
services like elastic load balancer and
Route 53 let you configure health checks
so you can automatically root traffic to
healthy resources
for example setting up a health check to
test for a HTTP 200 response which means
okay would allow AWS to know if a node
is healthy or not services such as Ops
Works elastic load balancer and ec2 auto
recovery will let you replace unhealthy
Resources with brand new healthy ones
durable data storage is an important
part of planning your Cloud resources
replicating your data to other sites or
resources will protect its availability
and integrity
there's three ways you can replicate
your data synchronously asynchronously
or Quorum based
synchronous replication ensures that
your data has been durably stored on
both the primary and replication
locations any write operation will only
be acknowledged as complete when this
has taken place
this should only be used on low latency
network connections where absolutely
necessary to try to ensure maximum
performance
asynchronous replication decouples the
primary node from the replica so that a
write operation doesn't have to wait for
any acknowledgment this can introduce a
replication lag but does not impact
performance and from personal experience
I've seen the difference between
synchronous and asynchronous replication
changing performance as much as a
hundred percent
Quorum based replication is a mix of
both synchronous and asynchronous
replication replication to multiple
nodes is controlled by defining the
minimum number of nodes that must
participate in a successful right
operation so if you have a cluster of
four replicas you can say I need
confirmation from at least two of them
before the acknowledgment is sent back
to the end user
a traditional data center failover
scenario involves failing over all your
resources to a secondary distant Data
Center
because of the long distances between
the two data centers synchronous
replication is often impractical slow
and usually involves data loss and isn't
tested very often however it does
protect against low probability
scenarios such as a natural disaster so
if your main data center is in New York
and your secondary was in say Washington
if New York had a massive outage you
could fail over to Washington but it's
going to be slow and practical and take
some time
a more likely scenario is a shorter
interruption in your data center which
isn't predicted to be too long in this
situation your choice is to sit it out
and wait or initiate the complex
failover procedure
AWS data centers are configured to
provide multiple availability zones in a
region with low latency network
connectivity this means you can
replicate your data across data centers
in a synchronous Manner and as a result
failover becomes much simpler also this
concept is baked into many AWS services
such as RDS and S3
active redundancy is great for Disaster
Recovery situations or balancing traffic
but what if all the requests to your
resources were harmful
for example if a particular request
caused a bug and it resulted in multiple
instances crashing
a practice called Shuffle sharding means
only sending some of the requests to
some of your resources this way of one
Shard of resources is infected or down
the other Sharda resources will be up
and running
in the example shown here if request 2
is bad then at least one set of
resources is still protected
AWS economies of scale offer
organizations huge opportunities to make
cost savings by making further use of
AWS capabilities there's even more scope
to create cost optimized cloud
architectures
right sizing is one such approach in
stark contrast to a traditional I.T
infrastructure cloud computing means you
can select the most cost-effective
resource and configuration to fit your
requirements ec2 RDS redshift and
elasticsearch give you a wide variety of
instant types to choose from
through benchmarking you can determine
the optimal configuration for your
workload in some cases it might be
optimal to select many of the cheapest
instant type available in others it
might be more beneficial to select fewer
instances but of a larger instance type
AWS storage offers the same level of
right sizing opportunities Amazon S3
offers many different storage classes to
suit your needs EBS also comes in a
variety of different volume types all of
which will be covered in later lessons
AWS offers many elasticity options to
help you save money you can Auto scale
your ec2 workloads horizontally or
vertically you can turn off production
workloads when you don't need them and
where possible you can use Lambda
compute workloads as you never have to
pay for idle resources
ec2 on-demand instance pricing means you
only pay for what you use with no
long-term commitments however there are
two more ways to pay for ec2 instances
one of these is reserve capacity by
committing to a defined period of
between 12 to 36 months you can receive
significantly discounted hourly rates
compared to on-demand pricing
AWS trusted advisor or AWS ec2 usage
reports identify the resources that
would benefit from Reserve capacity
there is technically no difference
between on-demand and reserved instances
the only difference is the way you pay
for it this concept also exists for
redshift RDS dynamodb and cloudfront
ec2 spot instances are ideal for
workloads that have flexible start and
end times as you are allowed to bid on
spare ec2 Computing capacity and spot
instances are often available at
significant discounts compared to
on-demand pricing
your ec2 spot instance is launched when
your bid exceeds the current spot market
price and it will continue to run until
evu terminate it or the stock market
price exceeds your bid when the latter
happens your instance is terminated and
you'll not be charged for the partial
Outlet it was running
so there's three strategies to use for
spot instances there's a form we've just
discussed which is bidding you could bid
much higher than the spot market price
for as long as the instant runs this way
even if the market price spikes
occasionally you'll still make a saving
overall
the mix strategy AWS recommends
designing applications that use a
mixture of reserved on-demand and spot
instances this way you can combine
predictive capacity with opportunistic
access to cheaper additional compute
resources
and then finally there's spot blocks AWS
allows you to bid for fixed duration
spot instances these have a different
hourly pricing but allow you to define a
duration requirement if your bid is
accepted your instance will run until
you terminate it or the duration ends
now let's take a look at caching caching
data means storing previously calculated
data for future use so you don't have to
recalculate it again
there's two approaches application
caching you can design applications to
store and retrieve information from Fast
managed in-memory caches
this way an application can look for
results in the cache first and if the
data isn't there it can then calculate
it or retrieve the data and store it in
the cache for subsequent requests
this approach approves user response
times and reduces load on your resources
an example of this is Amazon elasticash
which is a service that provides an
in-memory cache in the cloud suitable
for use with web services
Edge caching uses Amazon cloudfront so
you can cache static content such as
images or video and dynamic content such
as live video around the world using
Edge locations this way users can be
served to content that is closest to
them and resulting in low latency
response times
the principle applies to both download
and upload with Edge caching
and finally security it's important to
have security built into your plans and
designs for AWS infrastructure
AWS allows you to take most of the
security tools and techniques that you
already know and improve them by
formalizing security control design into
your AWS platform this simplifies the
system for administrators and Auditors
alike now we're not going to cover too
much here because this is all covered in
the I am lesson but there's things like
defense in depth which allows you to add
multiple layers of protection to your
resources reduced privileged access to
only give people the access they require
security as a code lets you capture
security requirements in one script that
you can use to deploy new environments
and real-time auditing AWS has multiple
services to ensure you can test in order
that your environment in real time
AWS offers a wide and deep set of
functionality that allows you to
integrate your existing network security
data lifecycle management and Resource
Management capabilities with the cloud
AWS allows you to extend your existing
on-premise network configuration onto
your AWS virtual private clouds
so that your AWS resources will operate
as if they're part of your existing
Network
you can do this using Amazon virtual
private Cloud VPC which is a
logistically isolated Network in the
Amazon Cloud that gives you complete
control over your virtual networking
environment you can choose your own IP
range subnets route tables Network
Etc then there's Direct Connect
rather than use internet-based
connections between your on-site
resources and the Amazon Cloud Direct
Connect lets you establish a dedicated
network connection between the two to
provide lower costs and a higher level
of service
you can use AWS to reliably and cost
effectively back up and secure your data
AWS allows you to replicate your data
across geographical regions manage the
life cycle of the data or even
synchronously replicate your data to a
local AWS Data Center
AWS storage Gateway is a VM that you
install in your data center and
configure to be associated with your AWS
account
then you can either use Gateway cached
which lets you use Amazon S3 storage as
your primary data storage while
retaining frequently accessed data
locally in your storage Gateway or
Gateway stored
Gateway stored volumes let you store
your primary data locally and
asynchronously back that up to AWS
Amazon simple storage service S3 offers
a practically unlimited storage that's
available for backing up And archiving
your critical data and Amazon Glacier is
extremely low-cost storage for
infrequently accessed data for which
recovery intervals of several hours are
suitable
AWS security and access control is
easily managed using I am or Microsoft
active directory services
AWS identity and access management is
the service that enables you to securely
control user access to all your AWS
services and resources and AWS directory
service is a managed service that allows
you to connect your AWS Resources with
an existing on-premise Microsoft active
directory or you can set up a new
Standalone directory in the AWS cloud
AWS security and access control is
easily managed using I am or Microsoft
active directory services
AWS identity and access management is
the service that enables you to securely
control user access to all your AWS
services and resources and AWS directory
service is a managed service that allows
you to connect your AWS Resources with
an existing on-premise Microsoft active
directory or you can set up a new
Standalone directory in the AWS cloud
and there's three flavors of active
directory there's Microsoft ad which is
a fully blown managed Microsoft active
directory service that supports up to 50
000 users the simple ad which is a
standalone manage directory that's
available in two sizes small and large
small list for up to 500 users and large
supports up to 5000 users
all this ad connector and this is a
directory Gateway that allows you to
proxy directory requests to your
on-premise Microsoft active directory
and this also comes in two sizes small
and large a small ad connector is
designed for small organizations of up
to 500 users and a large ad connector is
designed for larger organizations for up
to 5000 users
you can also use integrated resource and
deployment management Tools in your
hybrid environment AWS provides
monitoring and management tools with
robust apis so you can easily integrate
your AWS resources of on-site tools and
most major software vendors already
support AWS like Microsoft VMware and
BMC
tools such as AWS Ops Works allow you to
deploy and operate applications in the
AWS cloud or in your own Data Center
you can use templates or build your own
tasks to specify an application's
architecture
AWS code deploy automates code
deployments to instances in your
existing data center or in the AWS cloud
AWS code deploy simplifies the code
deployment process to your instances
also if you want to master the field of
cloud computing the postgraduate program
in cloud computing design in
collaboration with Caltech ctme helps
you become an Azure AWS and gcp expert
this in-depth cloud computing
certification course lets you master key
architectural principles and develop the
skills needed to become a cloud expert
benefits of this postgraduate program
includes Caltech ctme post graduate
certificate enrollment and simple learns
job assist you will receive up to 30
CEUs from Caltech ctme you'll attend
master classes from Caltech ctme
instructors live virtual classes led by
industry experts Hands-On projects and
integrated Labs online convocation by
Caltech ctme program director 40 plus
Hands-On projects and integrated Labs
Capstone project in four different
domains and Caltech ctma Circle
membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms
and services such as AWS Lambda Amazon
S3 Azure app services and many more so
this program also covers the essential
skills to become an expert in cloud
computing and you can consider this
program because it provides you with a
certification from Caltech ctme well
reputed university in the United States
so what are you waiting for check out
the link mentioned in the description
box below and start your cloud computing
career today so without any further
delay let's get started
let's see the steps to create in AWS
account the first step is to open the
AWS webpage
then click create a free account to
display the sign in or create an AWS
account page
in this page click I am a new user
enter your email ID or mobile number in
the email or mobile number box and then
click sign in using secure server
the AWS servers verify if an account
already exists with the entered email ID
if yes it displays an error message else
displays the login credentials page
in the login credentials page enter the
name
re-enter the previously entered email ID
and the password for your account
then click create account to display the
contact information page
this page gives you the option to create
a company account or a personal AWS
account
for this demonstration we will create
personal AWS account
then enter your full name
and address in the displayed fields
next enter the captcha characters from
the displayed security image
an important step is to open the AWS
agreement and read it thoroughly then
select the AWS customer agreement
checkbox
selecting the check box indicates that
you have read the AWS agreement and you
agree to it then click create account
and continue to display payment
information page
in this page enter the credit card
details and then click verify card and
continue to display the identity
verification page
before displaying the identity
verification page the AWS servers verify
the entered credit or debit card details
by performing a reversible transaction
of a small amount
this page displays your entered mobile
number and is used to verify and confirm
your identity
as you click call me now the AWS system
calls you on the displayed number and
your computer screen displays a four
digit verification pin
enter the displayed four-digit pin in
your phone using the keypad
once the verification is successful the
page displays a message
identity verification complete simply
click continue to select your support
plan to display the support plan page
page displays and explains about the
three support plans basic developer and
business
for this demonstration we will select
the free or basic plan and then click
continue to display the welcome page of
the AWS services
congratulations you have successfully
created an AWS account AWS ec2 a compute
servers in the cloud through this video
somewhere far in the country a scientist
has a lab far in the woods and at one
day he stumbled on a calculation that he
had trouble solving and he was very
certain that his computer can handle it
and he asked his computer to do the math
or the calculation for him and the
computer thought about it for a while
then took couple of hours in trying to
process the data and eventually at one
point it gave up and died due to low
memory and this old scientist is now
clueless about how he was going to
finish the task and complete his
invention and let the whole world know
about it and that's when he was
introduced to this new friend AWS as you
see on the screen AWS was very handy in
replacing his old machines and it was
able to give him the service that he
wanted at that moment AWS has been
around for a while helping people with
their computer needs and now this new
scientist friend in need is welcomed by
the new and trendy ID technology AWS and
sure enough his new AWS friend welcomed
and walked him through the new way of
computing and this new Happy faced AWS
friend talked to this scientist about
AWS ec2 in specific and walked him
through about all the new Innovations in
it and in Cloud that he had been missing
all these days so his new friend AWS
started the conversation about
explaining how there is no need for any
hardware units no managing Hardware or
provisioning in AWS and secondly it
explained increasing and decreasing the
capacity as per the demand is just a
click away and the best part here is
that there is no upfront commitment and
we only pay for what we have used just
like we pay electricity and water bills
and all this comes with complete control
from our side other words the whole key
for this info infrastructure is with us
and as if this was not enough all this
comes with Enterprise grade security
that's beyond imagination and
on-premises and if this tool does not
excite you then this definitely would
you can work from anywhere in the world
now this really made the scientist to
get excited especially when he thought
about working from home working from
home there is nothing like it isn't it
now this person the scientist is not
tied up to a particular environment he
can work from home work on the Fly work
from anywhere still the data and
everything else in his ID environment is
secure and safe now let's move to the
next level of discussion let's talk
about what is ec2 and some of the use
cases of ec2 and this use case we're
going to talk about are this
architecture we're going to talk about
it notifies a group of users about a new
letter and the components are resources
this architecture would need be first
ec2 instance then and SNS a simple
notification service and coupling ec2
and S3 service together that's all it
would require to get an architecture
that notifies a group of users about a
new newsletter and now I think it would
be a good time to talk about what ec2 is
AWS offers plenty of services offered
under different domains like you know
compute storage database migration
Network management tools Media Services
security business productivity
application integration a machine
learning game development and lot more
coming up out of which ec2 falls under
the compute capacity so what is ec2 ec2
is a web service which aims to make life
easier for Developers for providing
secure and resizable compute capacity in
the cloud with ec2 it is very easy to
scale up or scale down our
infrastructure based on the demand and
not only that this ec2 service can be
integrated well with almost all the
services in Amazon and out of all this
the best part could be we only pay for
what we use all right let's talk about
this use case a use case here is that a
successful business owner has a bunch of
users and a successful product that's
running and now he has developed few
more products that he thinks will be
very successful that he thinks his
customers are gonna like now how can he
advertise his product to his new and
prospective customers or the solution
for this question can be addressed by
AWS in AWS we can use services like
simple notification service SNS and ec2
for compute and S3 for storage we can in
a way integrate them all and achieve
this business use case and that sort of
got this business owner very chewed up
and now he wants to use the service and
he wants to get benefited from the
service he wants to advertise or he
wants to notify his use users every time
the company creates a newsletter alright
so let's talk about what it would take
to get this environment up and running
or what it would take to connect the
environment and put the applications on
top of it firstly we would require an
AWS account and then for compute
capacity we would require an ec2
instance and here's how we go about
doing it the first thing is to create an
Ami which is Amazon Mission image that's
really the softwares and the application
packages we would need to run our
application and the second is to choose
the hardware in here it's the instance
type depending on the workload we would
be choosing the hardware and depending
on the intents of the workload we will
be choosing the size of the hardware and
finally we would configure the instances
and how many instance do I want you know
which subnet do I want them in and
what's going to be the in a stop or
terminate a behavior of the instance and
do I want to update any patches when the
instance starts running all those speeds
of information go in here when we
configure the instance and then the
first three steps is really about the OS
volume and the Basic Hardware now it's
time to add additional storage to the
ec2 instance that would be step four
here we add additional storage to the
ec2 instance and then tags we use tags
or we would configure tags to easily
identify an ec2 instance at a later
point you know we give it some
meaningful names so we can identify like
you know which team it belongs to which
billing department it belongs to what's
the purpose behind launching this
instance stuff like that in an
environment where we run 700 to 800 or
even more instances identifying an
instance and trying to understand you
know who owns the resource for what
purpose we created it could be an
full-time work so tagging comes to a
rescue at that time after tagging as
step 6 we would configure the firewall
which which is also called Security
Group for the ec2 instant and this is
where we would allow or deny connection
from external world to this particular
ec2 instance well it works both ways
from outside and from inside out this
firewall blocks the connection based on
port number and IP address and finally
as step 7 we review all the
configurations that you have done and we
make sure that the configurations is
what we wanted and finally click on
submit that's going to launch an ec2
instance alright this was just an
overview of how to create an ec2
instance now let's talk about each and
every step in detail so to begin with
let's talk about how to create an Ami
well the Ami is just a template a
template that's used to create a new
instance or an a new computer or a new
VM or a new machine based on the user
requirement the things that go into an
Ami are the software the operating
system system the additional
applications that get installed in it
stuff like that the Ami will also
contain software information you know
information about operating system
information about access permission
information about volumes they all
compact in the Ami again the Ami is of
two types one is predefined Amis are
called Amazon provided Amis the other
one would be custom Amis the Amis that
we create and if you're looking at a
particular Ami that you don't want to
create but still want to get it from
Amazon there is a place or a portal
called Ami Marketplace there we get like
thousands of Amis in there available for
us to shop and use them on a
pay-as-you-go business model and use
them as pay as you go billing so there
you can search Ami that you're looking
for most probably you'll be able to get
it there now let's talk about choosing
the instance type the instance type is
basically the hard Hardware
specification that's required for a
machine that we're trying to build and
the instant types is categorized into
five main families they are to begin
with it's computer optimized now
computer optimize gives us lots of
compute power a lot of processing power
so if my application is going to require
a lot of processing power I should be
picking compute optimized instance and
the second one is memory optimized now
this is very good for application that
require in-memory caching you know there
are some application that performs well
with cash or through cash or the
application would create a lot of data
that it wants to keep in cash for
re-reading or for processing you know
for lengthy processing stuff like that
for those type of application this
memory optimized instance that comes
within memory cache is a very good use
case and the third one is the instant
that comes with the GPU otherwise called
GPU optimized gpus stands for graphical
process unit and this is very good for
application that deals with gaming this
is very good for application that's
going to require large graphical
requirements and storage optimized is
the fourth option just like the name
says this is a very good use case for
storage servers and the fifth family
type is general purpose just like the
name says it's for general purpose if
you're not particular about the family
then you generally would end up picking
the general purpose because here the
services are sort of equally balanced
you know you'll find a balance between
the virtual CPU and the memory and the
storage and the network performance it's
sort of balanced all the components all
the features that needs to go on a
computer are sort of balanced in general
purpose now these instant types are
fixed and they cannot be altered because
it's Hardware based we buy Hardware we
do not have much control on the hardware
that's being used well we have options
but we do not have control on the
hardware and these instant types are
divided into five main families they are
computer optimized memory optimized GPU
enabled storage optimized and general
purpose then as third thing we have to
configure the instance now here is where
I have a lot of options about purchasing
you know what type of purchasing do I
want to do do I want to go for a spot
instance do I want to go for a reserved
instance do I want to go for an
on-demand instance these are different
billing options available and that's
available under configure instance not
only that here's where I'm going to put
the ec2 instance do I want an public IP
address assigned to it do I want an IAM
role attached to it IM role is
authentication what kind of
authentication am I going to provide and
the shutout Behavior the Sharon
behaviors include do I want to stop the
instance when the user shuts down the
machine from the desktop or do I want to
Simply terminate this instance when the
user shuts down the instance from the
desktop so those things go in here just
like the name says configure instance a
lot of instance configuration options
comes in here that's the third step and
not only that under the advanced details
or Advanced tab under configure instance
I can bootstrap the instance with some
scripts now bootstrap is nothing but the
scripts that you want to be run in the
instance before it actually comes online
let's say you're provisioning the
instance for somebody else you know
instead of you launching the instance
and then logging in and running some
commands and then handing it over to the
other person you can create bootstrap
shell scripts and you know paste it in a
console option available under configure
instance and Amazon is going to take
those commands run it on the instance
before it hands over to the user that
initially requested for that instance
now it could be a different user or just
you it sort of automates you know
software installation procedures in the
instance that we will be launching and
not only that there are multiple payment
options available under configure
instance the user can pick an instance
under normal price and that instance
would apply normal rates applied to it
and there are also options like reserved
instance where the user can pay for an
instance upfront before a year or before
months you know for a span of year or a
span of months and that way they can pay
less per hour for using that instance
not only that you can also go for spot
instance like bidding for those
instances whoever bids more they get the
instance for that particular time well
these instances are a lot cheaper than
on-demand instances and through bidding
and buying you can keep the instance as
long as your bid price doesn't exceed
the price that Amazon is proposing and
as the fourth step we will have to add
storage to the instance that we are
about to launch and here we have bunch
of storage options I can go for a
permanent storage which is free or I can
go for an external elastic block storage
also called EBS which is paid and it's a
permanent storage or else I can
integrate my ec2 instance with S3 for
its storage needs and the best part
about storage is a free subscription
users they get to use 30 gigabit of SSD
storage or magnetic storage for the
whole year in this page where we are
ready to add storage we will have to
mention or provide the size in gigabit
and the volume type is it going to be an
provisioned volume is it going to be an
general purpose volume is it going to be
a magnetic volume stuff like that there
are volume types and we also need to
give inputs about where the disk will be
mounted and whether this volume needs to
be encrypted or not so all these options
are all these inputs are received from
us under the adding storage section and
then the fifth option would be adding
tags like we discussed sometime back
tags are very helpful to identify a
machine in an environment where we have
700 or 1000 VMS running and security
groups are the actual firewall that sits
in front of ec2 instance and it protects
that ec2 instance from unintended
inbound and outbound traffic now here is
where I can fine tune the access to my
ec2 instance based on port numbers and
based on IP address from which it can be
accessed and finally we get to review
the whole changes or the whole
configurations that we have made to find
out whether they are intact with the
requirement and then click on submit
that's going to launch an ec2 instance
but hold on we're not done yet when
we're about to launch or before the
Amazon console actually launches the ec2
instance it's going to give us an option
to create a keypad remember I said it's
key pair you know key pair is two things
one is public and private the private
key is downloaded by the user and is
kept with the user and the public key is
used by Amazon to confirm the identity
of the user so just go ahead and
download the private key and keep it for
yourself and this private key gets
downloaded as a DOT pem file it's a
format of the file and it gets
downloaded as dot pimp file and our next
step is to access the ec2 instance and
because the instance that we have have
launched in this example let's assume
its Linux instance and that's going to
require a tool called putty to be able
to access it and this putty tool is
really needed when we are trying to
access a Linux instance from Windows
instance most of the time Windows
instance will have put the install in
them but in some rare cases they do not
come with putty in those cases we can go
ahead and download putty and putty
generator and we can start using it to
access the Linux instance now you might
ask well I understand putty what's putty
generator now the file that we have
downloaded is in dot pem format but
unfortunately putty does not accept dot
pem format as input you know it has to
be converted into a different format
called PPK and puttygen is a tool that
helps us to convert the dot pem file
into PPK file so the quick way to do it
is download generator open it up click
on conversion and insert the dot pem key
that we have downloaded and save the
private key and this when we save the
private key it gets saved as a DOT PPK
type private key and when that's done
the next very step is to open putty and
try to log in and the way to log in is
to open putty put that IP address here
and then click on auth you know this is
where we would input the file that we
have created so click on op and then
click on browse and find the dot PPK
file that we have converted and stored
browse it and uploaded and then click on
open now that's going to open up a login
screen for us and the Amis comes with a
default username depending on the Ami
that we have picked the username might
differ in our case we have picked an Ami
for which the username is ec2 hyphen
user and this is the default by the way
let's put the username ec2 hyphen user
and hit enter and that's going to open
up the Linux instance using CLI there
are a few other things that we can do
with the terminal that will explain it a
little later alright so we have
successfully launched an ec2 instance
and yeah give yourself a pat on your
back launching an instance was just one
part of the solution so let's actually
talk about how we can notify our
customers SNS or simple notification
service is a service or a product in
Amazon that helps us to notify customers
through email so navigate to SNS in your
account and create a topic and we're
going to use this topic for public
notification so let's make it public and
then add subscribers to it now these
subscribers these subscribers are the
people who you want to be notified about
the newsletter so we already have the
email database in there add them to the
subscribers list and then they will
start getting new newsletters as and
when we post them to the topic and as
Next Step create a bucket in S3 where
you can store content and in that bucket
create an event that triggers a
notification to simple notification
service so this is how it will be set up
and notification will be sent to our
subscribers anytime we put something in
our S3 bucket so S3 bucket is going to
create an event for the notification and
the notification is going to make sure
that it's delivered to your end Customer
because they're already subscribed to
the topic as subscribers and finally
let's connect the S3 with ec2 so the
bucket and the AWS instance are in sync
so we put some content in the S3 bucket
our email system notifies our customers
and the customers can go online to a
website that's hosted in ec2 and because
S3 and ec2 are in sync the items that we
put in S3 will also show up in easy to
see how this is all connected and it's
working together and once this is all
connected our subscribers will regularly
be informed anytime we put new content
in the S3 bucket and the same content
will be made available in ec2 instance
through the website in this section
we're going to learn about why we need a
solution called cloud formation or why
we need a product called cloud formation
and we're going to talk in detail about
what is cloud formation and then how
cloud formation works and the concepts
involved in cloud formation like I know
templates and stacks there's some
Concepts involved in cloud formation
we're going to talk about that and how
do we maintain access control over cloud
formation we're going to talk about that
as well in this video and then we're
going to go through a demo or a lab
where we create an lamp stack
environment from a template through
cloud formation and then we're going to
talk about some use cases one of which
include being able to create and
redeployable template using cloud
formation that's a very good use case we
going to talk about that in this video
so let's talk about why cloud formation
so when the number of products and
services grow more and managing them and
maintaining them becomes an tedious work
and cloud formation is sort of eases
that environment now cloud formation is
really an infrastructure as the code and
it sort of solves all the problem that
we discussed sometime back and with
cloud formation we can actually create a
perfect clone of the server
configuration at any point in time and
we can also manage the configuration
changes and configuration changes with
cloud formation we can manage the
configuration changes across the
environment and not only that we can
easily embed in the ad hoc changes to
our existing environment so one might
ask why use cloud formation let's look
at the current problem that we have
creating and managing multiple AWS
resource in the portal is a big task
especially when you need to replace some
Services it becomes an bigger task now
cloud formation eases that word let me
explain it in detail now at the moment
without the cloud formation or
environments that don't use cloud
formation have this problem that is lot
of time is being spent on building the
infrastructure and less focus and less
time at the moment is being given on
developing that application that runs on
that environment that's because majority
of the time is consumed by building the
environment and if we need to redeploy
the same environment again and again the
same happens in a cycle again in a new
environment we start from the scratch
build environment and then put
application on top of it now that can be
avoided using cloud formation now
without cloud formation it still is a
problem because the the resource or the
bandwidth that is needed for the
application development is not provided
because that's taken by developing the
infrastructure and to solve that problem
we can use cloud formation now that
leads to a discussion about what is
cloud Farm Mission so let's talk about
or let's try understanding what cloud
formation is cloud formation is a
service that provides users a simple way
to create and manage collection of AWS
Resources by provisioning and updating
them in a very orderly and in a
predictable way if I need to expand the
explanation of cloud formation cloud
formation is an server that really helps
us to model and set up our Amazon web
services or resources so we can spend
less time on managing the resources and
show more focus on the application that
runs on top of it we can simply create a
template for an ec2 instance or for an
RDS instance and upload it in the cloud
formation portal and cloud formation
will provision and configure those
resources based on the template that we
Define we really don't have to sort out
the complexities in dependency between
those applications once the template is
vetted and validated cloud formation
takes the responsibility of handling the
dependencies between the application in
an orderly and a predictable way it
creates those services and makes it
available for us once the template is
fully run so in short the cloud
formation it allows us to create and
model our infrastructure and
applications without we having to
perform the manual action for example
the well-known company called Expedia is
able to easily manage and run its entire
front and back-end resources they run on
AWS in cloudfront what does that mean
they are spending less time on managing
infrastructure and More Time On The Core
Business and application that runs on
top of it and we're still talking about
what plot formation is with cloud
formation it enables us to manage our
complete infrastructure or AWS resource
in a text file now we're going to see
that in a moment you know it's either a
Json or an yaml file and that file is
called as the template to begin with the
resources the template provision is
called the stack all right so the
abstract of the code is called the
template and the resource that it
Provisions is called the stack and this
stack can be I mean it's going to run
resources obviously and those resources
can be updated as well it's not like
once you create a stack it's done if you
need a change you need to go create
another resource that's not needed this
stack is an updatable stack let's say if
I'm including two more servers if I'm
sort of extending my environment it's
branching out to another application now
I'm including another functionality in
my environment all those can be embedded
in the update and can be updated so like
I said the stack is an updatable one now
let's talk about what the template can
do or some features and functionality of
the template Now using a template we can
almost include all the application and
resources that a user might require
let's say you know after the ec2
instance is provision running an
application on top of it and you want to
templatize that and include that in the
template that's possible as well and
these templates are portable ones
meaning I can use the template in one
region and I can use the same template
in another region and that's going to
build a very similar environment in
another region I can also share that
with the customer and when they run it
it's going to run an environment from
the portable template in their account
in their environment of course all the
security is taken care I'm just saying
that if we need to build a resource if
you need to build an architecture if you
need to build an environment which looks
very similar to the other environment
that is possible and and these templates
are usable by the use of some sections
in the template like the parameter
section the mapping section the
condition section within the template is
what makes the templates reusable and we
will talk about parameters mapping and
conditions down the line we're still
talking about AWS how AWS cloud
formation works and we're going to talk
about how a template becomes an
infrastructure check right so to begin
with one requisite template and the
template would be in Json or yml format
so one would develop a template based on
the requirement based on number of ec2
instances based on whether they want a
load balancer based on whether they want
and a Windows Server based on whether
they want an database server in that
environment and then based on the
applications that will be running on top
of it that can be templatized in the
code itself so one creates a code and
then the code is saved locally or they
can be put in an S3 Bucket from which
the cloud formation will call the
template and provision the resource
right so we use cloud formation to
create a stack or create the resource
defined in the template and
cloudformation analyzes the template
first validates template and then it
identifies the dependencies between them
and then it starts provisioning the
resource one after the other for example
if you need a 57 servers in a new VPC
obviously it's going to create the VPC
first and servers second and stack all
right let's understand the two major
components in cloud formation by now you
might have guessed it the two major
components are the template and the
stack together the aid or they
complement cloud formation and that's
how the resource of provision that's how
the resources are managed and that eases
the job of the administrator who's
managing the environment so let's talk
about template first a template in cloud
formation is an text file or a formatted
text file in Json or yaml language and
that the code or the template or the
text file that actually describes how
your infrastructure looks like or will
look like once provision and for us to
create the template or for us to you
know modify an existing template we can
use a tool available called cloud
formation designer from the console or
any text editing tool that's available
but I would recommend the cloud
formation designer though that has a
rich Graphics in it and these templates
are built off or they consist of nine
main objects the first of which is the
format version and this really
identifies the capabilities of the
template based on the version number and
then the second object in a template
would be description and this helps us
to include any arbitrary comments about
the template for what reason are we
building this template what's the
purpose of it stuff like that and then
we have the object called metadata in a
template and that includes a details
about the template details of the
resources in the template and then we
have the optional parameter section this
really helps us to customize our
template now parameters enable us to
input custom values to our template each
time we create or update and stack and
the mapping object it helps us to match
a key to a corresponding set of named
values for example if you want to set a
value based on a region then we get to
create a mapping that uses region name
as the key and then the value that we
want to specify for each specific
regions and then we have conditions
object this is again an optional object
with this we can include statements that
Define when a resource is created or
when a property is defined for example
we can compare whether a value is equal
to another value and based on the result
of that condition we can conditionally
create additional resources and then the
object called transform it's it's an
optional object as well and this
transform section specifies one or more
transforms that we can use in our cloud
formation template for example AWS cloud
formation transforms helps simplify
template authoring by condensing the
expression of the AWS infrastructure as
a code are the cloud formation template
and it enables reuse of template
components for example we can condense
multiple line resources declaring it
into a single line in our template now
we will talk about all these examples in
the upcoming slides and then we have the
required section called resource that
declares the AWS resource that you want
to include in the stack for example if
you want to include an ec2 instance if
you want to include an S3 bucket we
would be using the resource object in
our stack and then we have object call
output this output object it's an
optional object in the template and it
helps us to sort of declare the outputs
that we can import into other Stacks or
we can actually show it on the cloud
formation console for example if you
want the output of the or the name of
the S3 bucket that got created in
response to a cloud formation temp plate
now we can have that outputted into an
output section in the cloud formation
console itself so it's sort of easy for
us to identify the resources that the
template provisioned so with all of
those objects put together this is how a
full-blown plot formation template with
all the objects included is going to
look like now this includes optional and
mandatory object let's talk about those
objects in detail in the upcoming
section all right let's now discuss each
and every object of this template
structure and let's begin our discussion
with the format version The Format
version it really defines the
capabilities of the template the latest
version format version as of now is the
one that was updated on 2010. now the
value of the template format version it
has to be an string and if we really
don't specify the format version cloud
formation automatically assumes the
latest version talking about description
descriptions are really any comments
that help ourselves that we want to
embed in the template itself that can be
a description and this description in a
template it has to be an string the
length of a string can be between 0 to
124 bytes in length the metadatars are
really used in the template to provide
any additional information using the
Json and yaml objects and metadata holds
some implementation details of a
specific resource and parameters are
really a way to customize the template
each time we create or update our stack
these parameters are the ones that helps
us to give some custom value to the
template during runtime in the example
shown below the parameter named instance
type parameter it lets us to specify the
ec2 instance type for the stacks to use
when creating or updating the stack by
default as we see it's picking T2 micro
and this is the value that will get
picked if we don't provision another
value to it and mapping mapping really
enables us to map a key to a
corresponding named value that we
specify in a conditional parameter and
also we can retrieve the map or to try
the values in the map using the function
find in map in strict function let's
take this example let's say based on a
region if you want to set values in a
template we can create a mapping that
uses a key and holds the value that you
want to specify for each region and the
object conditions can be used when we
want to reuse the templates by creating
resources in a totally different context
for example in a template during stack
creation all the conditions that we
mentioned in the template are evaluated
and any resources that are associated
and that come up to be true are created
and any condition that fails or invalid
are kind of ignored automatically and
the check moves forward to the next
condition and conditional is an optional
object and it includes statement that
Define when a resource is created or
when a property is actually defined for
example you can create whether for
example you can compare whether a value
is equal to another value and based on
the result of that condition we can
conditionally create resources if you
have multiple conditions then they are
generally separated with commas
sometimes conditions are very helpful
when we want to create templates and we
want to reuse those templates which was
created for one environment to another
environment for example if you want to
use a template that was created in a
test environment in a Dev environment
conditions would be helpful for example
in a production environment we might
want to include instances with certain
capabilities and in a test environment
we might want to include instances with
some reduced capability in them for
saving money and condition helps us to
sort of create resources based done the
environment it will be used in we can
also use intrinsic functions to Define
conditions like equals we can also use R
conditions we can use not functions
intrinsic functions in a condition and
when put together the syntax is going to
look like the one that you see on the
screen now the optional object called
transform it specifies one or more
transforms that the cloud formation uses
to process our template now these
transform sections are built on a simple
declarative language of AWS cloud
formation with a powerful macro system
that helps us to reuse the templates or
the template components and they really
help us to condense or simplify our
codes by enabling reuse of the template
components for example condense a
multiple line resource declaration into
a single line in our template the
resource section it really declares the
AWS resource that you want to include in
their stack such as as ec2 instance such
as S3 bucket and some of the resource
fields are The Logical ID the resource
type and the properties let's talk about
them in brief The Logical ID is unique
within the template and The Logical ID
name it actually reference the resource
in other parts of the template in this
case it's my bucket for example I know
if we want to map an elastic Block store
volume or if we want to map and name of
the bucket in another place in the
template now then we would use the
logical ID and again in this case it is
my bucket a logical name has the
physical ID with which we actually
assign the resource for example in this
case my S3 bucket was named as my bucket
to begin with and the resource type in
the template it identifies the type of
the resource is an S3 bucket and the
properties are the additional optional
information that we can specify 5 to our
resource for example if it is an ec2
instance let's say we would like to know
which Ami it used then property section
really defines the Ami that was used or
Ami as the property of the instance so
this section describes the output value
that needs to be imported to other
Stacks or the output value that needs to
be shown in the console for the user to
easily navigate to the resource that are
being created for example for a S3
bucket name we can declare an output and
once the bucket is created we can
actually declare the output and sort of
import the name of the bucket in a
different place in the console so it's
it's easy to view instead of you know
the user getting confused and going to
S3 and searching for the bucket name
this can be made available in the cloud
formation console itself and the output
Fields it can include some of these
followings like The Logical ID it's
actually the identifier of the current
output and it's very unique within the
template and it can also include a
description description is really a
string type that describes the output
value and it can also include value and
value is the property that was returned
by the AWS cloud formation describe
Stacks command all right let's talk
about the template resource attribute
now in this section we're going to talk
about attributes that you can add to a
resource to control additional behavior
and relationships so let's talk there
are many resource attributes available
out of which we have I mean if we need
to name them we have a creation policy
and then we have deletion policy it
depends on metadata attribute update
policy attribute let's talk about each
of them separately when we associate the
create policy attribute with the
resource it actually delays the resource
configuration actions before proceeding
with the stack creation I mean the
attribute stack creates is delayed till
the time the cloud formation receives
some number of success signals back I
mean this is a very good use case when
we do auto scaling and this creation
policy can be used only with a limited
number of services like Auto scaling and
ec2 instances
some of the use cases would be if you
want to install and software application
on an ec2 instance you might want the
applications to be running before it
proceeds further so in this case we
would use the creation policy attribute
to the instance and then send the
success signal after the applications
are installed and configured then cloud
formation goes to the next step here the
syntax that we're looking at the auto
scaling creation policy and the minimum
successful instance percentage it's
actually a number of signals that it
will have to reply back before it can
move forward using deletion policy it
preserves backing up of resources when
the stack is getting deleted and by
default if you don't have a delete
policy the data gets deleted just like
that but if we use an deletion policy we
would be able to preserve the data by
taking a snapshot of it or just letting
the resource and moving on with deleting
the other resources in the stack and we
specify the delete policy attribute for
each resource specifically that we want
to control and if there are no deletion
policy mentioned for a particular
resource the cloud formation simply
deletes the resources as that's its
default behavior and if you want to keep
the results when the stack is deleted we
use we we mention V10 in the delete
policy and that simply retains the
resource from getting deleted so in this
example we're actually retaining an
dynamodb table rest of the resources in
the stack will get deleted and using the
depends on attribute we actually create
an order in which the resource gets
deleted for example if I map one
resource to another resource and say
that this resource depends on the other
resource the other resource gets created
first Then followed by this instance or
this resource so with the depends on
attribute we can specify that the
creation of specific resource is
actually followed by and another when we
add the depends on attribute to a result
Source the resources created only after
the creation of the resource specified
by the depends on attribute let's take
this example in this example we're
saying that the resource X depends on y
we've said that X depends on y so in
this case when cloud formation is
executing it's going to deploy y first
because it has some resources that
depends on it so it's going to create y
first and then it's going to create the
resource called X on the same context
let's say in the template we say that an
ec2 is going to depend on S3 and when
the cloud formation is executing it's
going to create an S3 bucket first and
then the ec2 instant that's probably
because the codes needed for the ec2
instance are stored in the S3 bucket and
and what we're looking at is the syntax
of how that depends on template resource
attribute will look like this is how it
actually will look like when we put it
in the cloud formation the instance
depends on the buckets the other
template resource attribute is metadata
metadatars are the one that really helps
us to associate our Resources with the
the structured data and by including
this attribute in our template or by
including this attribute to a resource
we can specify the data in Json or in
yaml language for example in this
section of the template we're creating a
VPC and the metadata explains our
metadata defines that the VPC is part of
the region U.S east one and the update
policy template resource attribute and
cloud formation we can manage and
replace the updates of the instance in
an auto scaling room for example you're
looking at this sample where it says
that the update policy will it replace
the instance in an auto scaling group or
not so depending on what the value is
for the will replace section whether
it's true or false depending on that at
the update policy will replace the
instance or will replace the auto
scaling group itself when it's doing
enrolling deployment now that we've
discussed about templates let's talk
about AWS Stacks as well now we know
that AWS Stacks are the actual resources
that gets generated or that gets created
because we ran a template right so stack
is a collection of AWS resource that we
can manage in a single unit in other
words you know we can create a stack
update a stack or delete a stack and
that would actually create update delete
the collection of resources within the
stack and all the resources in the stack
are really defined by the cloud
formation template and a stack for
instance can include all the resources
required to run a web application such
as a web server a database server and
some networking rules that come along
with it all that gets embedded in a
stack and let's say if we want to delete
all the resources that were created by
template that is in a separate stack we
can simply delete the stack and that
would sort of delete all the resources
that is in the stack so it sort of helps
us to manage all the resources in a
single unit or as a single unit so to
summarize stack is a collection of AWS
resource and it can be managed as a
single unit and the cloud formations
template defines a stack in which the
resource can be created deleted or
updated in a very predictable way and a
stack can have all the resources like
web server database server VPC the
network and the security configurations
in a VPC all that is required to run a
web or a database application now stack
could be nested as well with cloud
formation we can actually Nest a stack
with another stack and that's going to
create an hierarchy of the stacks for
example I mean this is a very good
picture that explains how the stacks are
nested and hierarchical way let's say
stack p is the root stack and that could
should be apparent stack for the stack
called R and the stack all Cube and the
same way the the stack call R is a
parent stack for the stack s and that
the stack s is the Varan stack for D and
Q you get that right so Stacks can
always be nested it's possible cloud
formation allows us to create a
Microsoft Windows stack based on Amazon
ec2 Windows Amis and provide us with the
ability to install software and use a
remote desktop to access our stack and
update the configuration of our stack
and these Stacks can be run on Windows
Server instances there are a number of
pre-configured templates available and
it's directly available to use from the
AWS website itself to name a few we have
templates available for SharePoint
running on and a Windows Server 2008 R2
we also have templates that create a
single server installation of active
directory running on a Windows server
and in fact we have templates available
for elastic Beanstalk and some sample
applications that runs from an Windows
Server 2008 R2 let's talk about stack
sets stack sets actually has taken the
attack level implementation to the next
level using a the AWS cloud formation
template we can define a stack set that
lets us to create stack in AWS account
across the globe by using just a single
template and after we create the stack
or after the stack set is defined
creating updating or deleting the stack
in the Target account and the regions
can also be specified by us so the stack
said it really extends the functionality
of the stacks by allowing us to create
update and delete stack across multiple
accounts regions in just a single
operation and using an administrator
account we can Define and manage an AWS
cloud formation template and use the
template as the basis for provisioning
Stacks into you know some selected
Target accounts across a specified
region let's talk about cloud formation
access control with IAM I can specify
the list of users or list of privileged
users who would have access to the cloud
formation template who can create update
and delete Stacks so that's how IM helps
me with having or putting a layer of
control over the cloud formation service
and we can also use service roles
service roles are the one that allows
AWS cloud formation to make a call to a
resource in a stack on the user's behalf
and I can also have stack policies and
these stack policies are applied to the
users who already have access to the
cloud formation service and these
policies are the ones that help the
users who attempt to update the stack
all right let's do a quick lab on
creating and lamp stack on an ec2
instance with a local mySQL database for
storage now we can do the other way as
well I can pick an ec2 instance and
install the servers one top of the other
and that can be done as well but that's
going to take a lot of time now we're
talking about automating it we're
talking about you know provisioning and
infrastructure using a code and platform
machine helps us to do that so this is
what we're going to do we're going to
create or we have in cloud formation
template already and we're going to use
that template the provision web instance
and that is going to have look at that
that's going to have MySQL installed in
it that's going to have PHP that's going
to have httpd service installed in it
and running and it's also going to have
an a PHP website running from it so
that's going to receive information from
the website and that's going to store
that information in the database that's
very local to it
this is the template that defines that
environment
all right this is a very simple
provisioning so let's provision this
environment let's create a stack
we're going to give a stack name
and a database password
right and
which region is it this is in California
is that
all right
now let me do one thing let me switch to
the Mumbai region or let me switch to
North Virginia region
Al from here actually let me switch to
Mumbai region
all right just to be sure it's the same
template that we're provisioning
right that's the same template that we
are provisioning
all right let's give this a name so it
can be great this could be simply learn
simply learn lamb stack and what could
be the DB password
and DB root password
and what could be the DB user
I want to keep the rest as defaults
and I'm going to launch
all right so there's an issue with the
password
[Music]
password should only have Alpha numeric
characters get that
so let's go back and create a new
password
right it also needs to have eight
characters
so what can be your password
look at that the template is actually
creating the resources
and here on the screen we can see the
template that we used to deploy the
environment
if you remember we have we discussed
about instant types in parameters right
the default instance type is T2 small
but these are also allowed
to run
all right parameter section is the one
that was used to receive information
from us like the DB name
the DB password the DB root password DB
user instance type key name and the SSH
locations
all right now this resource section is
is showing us the resource that it
actually uh created
right looks like it's rolling back why
would that be give me a moment
[Music]
all right this is one live example
anytime it could not provision or any if
there is an interruption that it cannot
proceed it would simply roll back and
delete all the resources that it had
provisioned and that's what it did
now let me fix this
oh as a fix I have switched the regen
because I had some issue with the VPC
that's going to take time for me to
analyze and fix it so here we're going
to understand about cloud formation so I
thought I'll run the cloud formation
from another VPC that I know is working
fine that's not Virginia
all right let's provision this lamp
server which is running out of Linux
which has Apache installed on it which
has PHP and MySQL already installed in
it so one server gonna act as web and DB
server
all right so I'm going to create a stack
and from here I'm going to pick a sample
stack if we look at if you look at it in
the designer
it's going to provision one lamp server
and a security attack Security Group
attached to it there you go right so
let's go ahead and create a stack
all right
parameters are used to receive
input from us what could be the DB
password
and the DB user
and
let's select the key name
right I'm going to keep the rest as
defaults and I'm going to create
a stack
all right so our stack
that we run from the template
at the moment it's getting created it's
in progress
from here I can watch the events as to
what is getting created at the moment
like I said in a lot of these would have
already sorted out their dependencies
you know which one needs to get created
first which one needs to get created
second all that is sorted out by cloud
formation itself
and parameters are the values that we
have inputted and resource
tab is section where we can find what
resources are being provisioned at the
moment so to begin with it created a
security group at the moment it's
creating an instance and output uh it's
empty as of now we're going to come back
and see it now output is where we can
get outputs of a finished job let's say
here I wanted an URL
of the server that got created so
instead of me uh you know going to a
couple of other places within the
console and find the output of the URL
or the the output of the resource that
was created I can simply get it from
here well this needs to be defined in
the templates First Once We Define it in
the template it's going to be made
available
like here
right so I can open it up
and there you go so it has it already
has
a database locally installed and it's
connected over if I launch an
application on top of it that will get
connected to the database which is very
local and my single
server with web and database in it is
ready for production
now while we're talking about a lamp
server a single lamp server let's talk
about
creating an environment like this which
has elastic load balancer a VPC
server running in an auto Skilling group
and putting some restrictions that
nobody will be able to access the ECU
instance directly but they should be
able to access the ec2 instance or the
data only through the elastic load
balancer this is a bit complicated than
the environment that we've already
created let's see how this is done all
right in this lab we're going to create
elastic load balance to Auto scaling
Group which is going to create a load
balancer and an auto scaling group and
that auto scaling group receives traffic
only from the load balancer so here's
the template
and here's the architecture that we're
going to provision
let me call it all right and the key
pair it's going to use is uh simply
learn key pair
now this is where the problem was now we
were selecting few subnets
so now if I provision it
now this is going to take its short time
to provision the resources
so through parameters we can see the
values that we have inputted
and the events look good no roll back no
no delete and the resource section shows
the resource that it is creating so
previously created a Target group now
it's creating the application load
balancer itself
all right so it created and
load balancer and then it create a
security group it creates a launch
configuration it at the moment it's
creating a web server group
and the events look good with all green
and yellow green ones are completed
yellow ones are in progress we don't
have any red delete or rollback errors
so it looks good all right it took a
while and
the resources have been created some of
them show that they are in progress but
the status of the stack is complete
all resources have been created and we
do have an output to access the URL
there you go
[Music]
and remember there was another feature
of the cloud formation that was it
should restrict access I mean direct
access to ec2 instance let's see if that
has been done
if I try to access using the load
balancer it lets me access but if I try
to access it using
the ec2 instance IP or the DNS name my
connection won't go through because it's
blocked through the security group but
direct access through the load balancer
is allowed and that's what we wanted to
achieve through that template also if
you want to master the field of cloud
computing the postgraduate program in
cloud computing designed in
collaboration with Caltech ctme helps
you become an Azure AWS and gcp expert
this in-depth cloud computing
certification course lets you master key
architectural principles and develop the
skills needed to become a cloud expert
benefits of this postgraduate program
includes Caltech ctme postgraduate
certificate enrollment and simply learns
job assist you will receive up to 30
CEUs from Caltech ctme you'll attend
master classes from Caltech ctme
instructors live virtual classes led by
industry experts Hands-On projects and
integrated Labs online convocation by
Caltech ctme program director 40 plus
Hands-On projects and integrated Labs
Capstone project in four different
domains and Caltech ctma Circle
membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms and services such as AWS
Lambda Amazon S3 Azure app services and
many more so this program also covers
the essential skills to become an expert
in cloud computing and you can consider
this program because it provides you
with a certification from Caltech ctme
well reputed university in the United
States so what are you waiting for check
out the link mentioned in the
description box below and start your
cloud computing career today so without
any further delay let's get started
here in this section we're going to talk
about what is Security in AWS and then
we're going to slowly or gradually move
on to the other topics like types of
security available in AWS out of all the
services why IM is the most preferred
one and then we're going to talk about
what is IM in general how it works the
building blocks or components in IM and
the features it the the attractive
features that it provides that makes IAM
stand out from the rest of the services
available we're going to talk about that
and then we're going to end today's
session with a demo about how IAM gets
well interacted with other services in
Amazon and help create a secure
environment in the cloud let's talk
about AWS security Now Cloud security is
the highest priority in AWS and when we
host our environment in the cloud we can
be rest assured that we are hosting our
environment in a data center or in
Network architecture that's really built
to meet the requirement of the most
security sensitive organization and this
high level of security is available to
us on a pay-as-you-go type meaning there
is really no upfront cost that we need
to pay and the cost for using the
service is a lot lot cheaper than what
it is in an on-premises environment so
AWS Cloud provides an secure virtual
platform where we can deploy our
application now compared to the
on-premises with AWS the security level
is very high and the cost involved in
using or attaching that type of security
to our application is very low compared
to on-premises there's really no upfront
cost and the cost is very lower whatever
cost that we pay is very lower than what
it would be in on-premises and all this
contributes to the secure environment we
can get through IM in AWS Cloud there
are really many types of security
services available in AWS to name a few
are to name the familiar ones IM stands
first followed by a key management
system KMS and then we have Cognito
that's another service and web access
firewall Waf is another security service
in AWS now let's start our discussion by
talking about IM and that's what this
whole series is about now I am or
identity and access management it really
enables us to manage access to AWS
services and AWS resources in a very
secure manner with IM we can create
groups we can actually allow those users
or groups to access some service or we
can sort of deny them to access the
service whichever we want that's all
possible through identity and access
management and this feature comes with
no additional charge now let's talk
about how things were before AWS before
AWS or before IM in general it was not
that safe in a corporate to share the
password over the phone or through the
email that was the practice that was
existed at that time now we all can
remember the days when we need to switch
to a different account or we had just
one admin password commonly stored in a
location or one person would reset it
and maintain it and anytime or we need
to log in we call the person ask for the
admin password over the phone and then
we try logging in now that was not
secure at all back then no somebody
could walk by and sort of eavesdrop and
get the password now they walk away with
the password with them so they're all
possible when we share the password over
the phone and through the internet or
email now with AWS a lot of options are
available that I'm not sharing password
over the unsecure medium now a slack is
a third-party product available with AWS
it's not an AWS product but slack is and
third-party application that's hosted on
AWS and it's really a communication tool
that helps people to share a documents
so now we're not sharing the password
over the phone rather through the
applications and no eavesdrop person can
really catch the password and you know
try it in their system to access our
environment that's not possible so you
see the difference now back then sharing
password was only through phone and
email and you would write it on a paper
and you give it to somebody but now
technology is providing Provisions
enough for us to share the password in a
secure way we're still talking about I
am I am is an web servers for securely
controlling access to the AWS resource
now it really helps us to authenticate
or sort of limit access to a certain set
of users accessing the AWS account or
certain set of users accessing a certain
set of resources in AWS account now you
see this picture we have an IM
administrator who's trying to allocate
permissions to different group of people
so we have group one on the top and
group two in the middle and group three
towards the end the administrator using
IM or IAM empowers the administrator
allow access to certain group to certain
resources that's what IAM is all about
now let's talk about the building blocks
or let's talk about the elements that
make and complete I am so the elements
are categorized into six different
elements we have the principle in IM the
authentication the request the actual
request coming in the authorization
allowing denying access actions what are
the scope of actions can be taken and on
what resource is it easy to visit rdas
is it S3 bucket so what resource these
actions are applied to so they are the
basic elements of an IM workflow let's
talk about principle for example an
action on AWS resource can only be
formed by a principal an individual user
they can be a principal a role in AWS IM
it can be a principle as well in other
words a principle is an entity that can
take an action on an AWS resource you
know a principle can be an user a
principle can be a role a principle can
be an application that's trying to
access the AWS environment secondly
authentication now authentication is a
process of confirming the identity of
the principle trying to access the AWS
environment now how do we get
authenticated or how does a user get
authenticated by providing credentials
or the required keys to validate that
this is what he or she is as per words
in the record so all principle must be
authenticated and we get authenticated
using the username and password to log
into the console if it is CLI or we get
authenticated using the access key and
the secret access key the other
component that makes up IM is request
test when a principal tries to use the
AWS Management console what the
principal is doing or what the user is
doing is sending a request to the AWS
environment and the request will have
the principal actually wants to perform
is it an put request or is it a get
request or is it a delete request stuff
like that and it's also going to carry
information about on which resource the
action needs to be done is it done easy
to one is it on S3 bucket
simplylearn.com stuff like that so it
has a specific action that it wants to
perform and the resource on which it
wants to perform and also some
information about from where the request
is originating from is it within an AWS
environment or is it with another cloud
service provider or is it with
on-premises stuff like that so these
information will be present in the
request authorization now this IM uses
information from the request context to
check for matching policies and it
determines it takes a decision whether
to allow allow or to deny the request
now here it has a logic by default all
the resources are denied and the IM
authorizes your request only if every
part of the request is allowed by a
matching policy so there is an
evaluation logic and that says by
default all requests are denied and if
there is an explicit allow then it
overrides the default deny and if you
are explicitly denying a service then
all the allow statements gets overridden
and all the other statements gets
overwritten and this explicit deny
stands by default only the AWS account
root user has access to all the
resources in that account so if you're
not signed in as a root user then we
must be specifically granted permission
through a policy in other words we need
to be authorized through the policy now
let's talk about action now after
authenticating and authorizing the
request AWS approves the action that
that we are about to take Now using
action we can view the content we can
create a Content or create a bucket or
create an ec2 instance I can edit
content or even I can delete a resource
that's all based on the action that I'm
allowed to do now that was about actions
now where is the action performed the
action is performed on the resources a
set of actions can be performed on a
resource and anything that's not
mentioned in the action or anything
that's not tagged with the action and
the resource they don't get to complete
for example let's say a few attempt to
delete an IM role and you also request
to access the ec2 instance for that role
then the request gets denied now let's
talk about some of the other components
of Im so the other components are IM are
the basic building block is in user and
then many users together they form a
group and then policies are the engines
that allows or denies a connection you
know based on policy one one gets to
access or gets no access to a resource
and then roles roles are another
component roles are temporary
credentials I would say that can get
assumed to an instance as and when
needed right let's talk about this
component called user in an IM with IM
we can securely manage access to AWS
services and we can create an IM user
anytime there is a new employee joining
our corporate so it's a one-to-one
mapping in a one user or one employee
get a username directly attached to them
and a username or a user is an account
specific thing you know it's very local
to that AWS account that we are using so
for in detail and IM user is an entity
that we create in an AWS environment to
represent a person or a service that
interacts with AWS environment and this
user is going to have some credentials
attached to them and by default they do
not have have a password and they do not
have any access key or secret access key
attached to them in fact literally no
credentials of any kind now we must
create a user and we must create the
type of credential that gets attached to
the user do they want to access a CLI So
based on that we would be adding
credentials to them a user by default is
not authorized to perform any action in
AWS and the advantage of having
one-to-one user is that we can assign
permissions individually to that user
for example we might want to assign
administrative permissions to few users
who can administer our AWS environment
and users are entity within an AWS
account users don't have to pay
separately you know they start using the
servers and that get bills under the
account they are attached to so separate
users doesn't mean that separate payment
for those users they are just users in
an account and you know the whole
account gets to pay for for the
resources that they provision again the
IM user does not always represent a
person an IM user is really just an
identity with Associated credential and
permissions attached to it it could be
an actual person who is a user and it
could be an application also a who's a
user next we'll talk about groups
talking about groups or understanding
group is very easy a collection of user
forms an IM group and we can use IM
group to specify permission for multiple
users so that any permission that's
applied to the group is also applied to
the users who are at ad to that group
now there are a few things you might
want to know about group a group can
have many users and a user they can
belong to many groups but groups can't
be nested meaning group can only have
users you know a group cannot have a
group inside a group that's not possible
and there's nothing called a default
group that sort of automatically
includes all users in the AWS account if
if you want to have a group then we will
have to create a group now let's take
this diagram for example now this is a
diagram of an small company now the
company owner creates an admin group for
users to create and manage other users
as the company expands every day and the
admin group creates a developer group
and a test group each of these group
consists of users users who are people
users who are application that interact
with AWS environment so the different
types of users we have are gym is a user
Brad is a user and Dev app one is a user
and and so on so each users have an
individual set of security credential
like I said it's one to one and the 10
users that we see on screen are shared
between or they belong to multiple
groups in the account and managing group
is quite easy we set permission to the
group and that permission gets applied
to all the the users automatically in
the group and after we applied the
policy if I add another user to the
group the new user will automatically
inherit all the policies all the
permissions that's already mentioned in
that group so administrative burden is
sort of taken away administrative button
about user a privilege or user
permission management is sort of taken
out with IM all right the next thing we
would want to discuss is policy a policy
sets permission and controls the access
to AWS resource and the policies this
permissions and controls are stored in
Json format a policy is a very clear
document it defines who has access to a
resource and what are all the actions
they can perform so in other words
policies and entity in AWS that when a
policy gets attached to an identity or
resource you know it really defines the
permission what they can and cannot do
and AWS it evaluates these policy is
when a principal such as a user makes an
request and the permission statement and
the policy it really defines whether the
usage is allowed to access a service or
is he denied to access a service all
right let's take this example now we
have a task the task is to give Paul who
is a developer access to Amazon S3
environment and what you're looking at
is a snippet of what a policy would look
like so these are some questions we will
have to answer or these are some
question the policy tries to answer and
once it found an answer then it becomes
a complete policy so who wants access to
the service it's Paul who wants access
to the service and what action do Paul
want now Paul wants to download or Paul
wants to upload objects in S3 in Json
language it's otherwise called Paul
wants to get information Paul wants to
put information otherwise Paul wants to
get object Paul wants to put object in
S3 and which results does he want now in
this case every bucket the star symbol
represents all buckets in S3 so which
results it's all bucket in S3 so we can
start reading already from the data
present here so who wants access Paul
wants access what action does he want he
wants to get he wants to put and what's
the resource that he's trying to access
or you want to give him access it's all
the bucket in AWS and when does he want
these are additional statement that goes
in a policy so since when does he want
that access he wants access till March
2nd 2019. so all this who all this
action all this resource you know till
the time all this gets allowed so this
is the statement that really defines now
this is all refer to an allow statement
or does this all refer to a deny
statement if we had deny here it's the
direct opposite of what we discussed but
we have allow that means Paul is allowed
to get and put access in an S3 bucket
and all the buckets in S3 till March 2nd
2019 he's allowed to access the
resources so if we put together put all
the information that we have gathered or
all the answers that we have so far if
we put together in a Json format this is
how it's gonna look like so the effect
is allow or deny here it's allowing and
the principle is any now who can access
it it's any an action it's any bucket in
S3 any action in S3 put get request and
the resource is any bucket in S3 so
that's how a basic policy would look
like or that's how a basic policy is
formed by answering those basic
questions so let's talk about the types
of policies so we have two broad
categories manage policy and inline
policy now manage policy it's the
default policy that we attach to
multiple entries be users beat groups
and beat roles in our AWS account on the
other hand inline policy are the
policies that we create in manage them
and these policies are embedded directly
into a single user group or a role so if
I need to brief it a little bit more
then a managed policies are Standalone
identity based policies that we can
attach to multiple users groups and
roles in our AWS account we can use two
types of managed policies in other words
manage policies they themselves have two
categories one is AWS managed policies
which is created and managed by AWS and
the other one is customer managed
policies just like the name says it's
managed by the customer now we can
create and edit an IM policy in visual
editor or simply create a Json policy
document directly and upload it in the
console and start using them so that's
manage policy on the other hand inline
policies are policies that we create and
manage and that are inverted directly
into a single user group or role let's
talk about this this next component
called roll now this role is very
similar to everything that we discussed
so far it's a set of permission that
defines what actions are allowed and
denied by an entity in an AWS console
the role is very similar to an user I
said very similar I didn't say it's the
same as the user but I said it's very
similar to the user and the difference
is that user is hard-coded user
permissions are hard-coded but the role
permissions are accessed by any entity
in a beta user or a beaten AWS service
and moreover the role permissions are
temporary on the other hand user
permissions are permanent and the role
permissions are temporary
so in detail a role is very similar to a
user in that it's an AWS entity with
permission policies that determines what
the identity can and cannot do in AWS
now instead of it being uniquely
associated with one person a role is
intended to be assumable by anyone who
needs it roles generally do not have a
long-term credential you know password
or access Keys associated with it you
know it does not have long-term
credentials it only has temporary
credentials created dynamically and
provided to the user as in when the user
assumes a role and wants to access any
service roles can be assumed by users
roles can be assumed by applications
roles can be assumed by services that
normally we don't have access to AWS
resources for example you might want to
Grant a user in your AWS account access
to your results that they do not
normally have or Grant users in one AWS
account access to resource in another
AWS account now this is something that
we don't have by default and this is
something we may not need all the time
or you might want to allow a mobile app
to use AWS resource but you do not want
to save the key or save the credential
or save the passwords in the mobile app
or sometimes you might want to give
access to resources who already have
identities defined outside of AWS like a
user who's already and Google or
Facebook authenticator if you want to
give them some service or if you want to
let them to access some of the resources
in your account we can use roles for
that purpose or you also might want to
Grant access to your account to a third
party a consultant or an auditor so they
can for some time get some temporary
access to audit our AWS resources
remember they're not permanent users
such as Templar users they want some
temporary access to our environmental
the audit is over so for those cases row
goals are a very good use case so let's
take this example let's see where role
sets and let's see how roles give
permission to other services so here is
a scenario where where an easy to
instance wants access to an S3 bucket by
default though both of them were created
by the same username are the same admin
they do not have access by default so by
default they don't have access and I
also do not want to give permanent
access in a one-on-one hard-coded
permanent access instead I would like to
create and share access so that's
possible through role so coming back to
the discussion uh the scenario here is
that an ac2 instance wants to access
data in an S3 bucket so the first thing
is to create a role or a permission we
saw how that's done right using policies
roles use policies policy is the actual
working engine for any permission
related actions right so create a role
with an appropriate policy that that
gives access to S3's bucket and then
launch an ec2 instance with the role
attached to it we said rules are assumed
by any entity right it could be user it
could be an application it could be a
service it could be another resource in
AWS account so here it's another
resource in AWS account right so we
create a role and we attach it to an ec2
instance so now the ec2 instance can
assume the role now the same role can be
attached to another ec2 instance the
same role can be attached to another
user the same role can be attached to
another database service we get the idea
right roles can be assumed by anyone so
when the ec2 instance wants to
communicate with S3 it contacts the role
and then the role gives this ec2
instance some temporary username and
password that expires after one hour
let's say so it's a temporary username
and password now with that temporary
username and password the ec2 instance
can access the S3 bucket and access the
file with the credential that it already
has and that credential is a temporary
one and the credential will expire after
one hour or how much over time the admin
sets it to be and then the next time
when is when an ec2 instance wants to
contact S3 it will have to contact the
role and the role will propose a new
access key and secret access key in
other words a new credential with that
new credential the second time the ec2
instance will be able to contact the S3
bucket and get the information so the
password is not a permanent or a
one-time or a hard-coded password it
keeps rotating it's a token it's a
secure token that an ec2 instance or any
application that uses the role gets to
access the resources on the other end so
let's talk about the features of IM the
main feature of IM is that I can create
separate username and password for
individual users or resources and I can
delegate access now those days where we
had just one username and password for
admin and we need to call the other
person to get the username and password
over the phone uh if it gets compromised
you know the whole account gets
compromised stuff like that those issues
are gone I can create separate username
and password for all thousand users in
my account no more username and password
sharing I can manage access to all those
thousand accounts in a very simple and
an easy way so IM provides share access
for all the Thousand users in my account
it provides share access to the AWS
environment and the permissions that can
be given through IM are very granular
now we saw how restrictions can be put
on get request you know we can allow a
user to download information and not
update an information so that's possible
through the IM policies that we looked
at some time back and with the use of
roles you know we're authenticating an
ec2 instance answer of the application
running on top of it so if there are
like 10 or 20 application is running on
top of an ec2 instance instead of
creating a separate username and
password for all those 20 instances I
can create a role and assign that role
to the ec2 instance and all the
applications running on the ec2 instance
get a secure access to the other
resources use the role credential that's
provided to that ec2 instance now
besides username and password IM also
supports multi-factor authentication
where we provide a credential the
username and password plus and OTP from
our phone or an randomly generated
number that gets displayed in our phone
before Amazon console or before Amazon
CLI would authenticate and lets us
access some of the servers in the
account so it's called multi-factor
authentication it's username and
password plus something else from what
you have I am also provides identity
Federation if a user is already
authenticated with account corporate ad
or Facebook or Google you know IM can be
made to trust that authentication method
and then allow access based on that
authentication now without Federation
what would happen is the users will have
to remember two passwords you know one
for on-premises and one for cloud
anytime they work on on premises they'll
have to use one set of password and
anytime they switch to the cloud which
is going to be very frequent in uh every
day so anytime they switch to the cloud
they will have to again type in the
cloud username and password now with
identity Federation we can sort of
Federate or we can connect both the
authentication systems both on-premises
and Im so users can just use one
username and password for both
on-premises and Cloud environments and
all that we saw so far it's free to use
there is no additional charge for IM
security there is no additional charge
for creating additional users groups or
policies it's a free service comes along
with any account IMS and PCI DSS
compliant product now IM comes with
password policy where we can reset the
password remotely or rotate the
passwords remotely and we can set rules
like and how a user should pick an
password it should not be the one that
was used in the past any of the past
three times stuff like that so you get
the idea right how the features of IM
help us to build and a secure
authentication and authorization system
in or for our AWS account all right it's
time to do a quick lab let's let's put
all the knowledge that we have earned so
far together and apply it and try a lab
and solve the problem so here is a
problem statement we need to create an
S3 bucket for a company where users can
read or write the data only if they are
multi-factor authenticated so that could
be like 20 users in an account but a
user is allowed to read or write a data
in an S3 bucket only if the user is
multi-factor authenticated now this is a
very good use case if we have a
sensitive data in an S3 bucket and you
only want privileged or MFA
authenticated users to do changes to
those buckets and for those privileged
users you would enable multi-factor
authentication so let's see how that's
done before that let me talk to you a
bit more about multi-factor
authentication a multi-factor
authentication is that additional level
of security process is provided by AWS
here the user's identity is confirmed
only after the user proposes or the user
passes two level of verification now one
being the username and password and
another being an OTP that gets generated
in this case it's the mobile phone it
could also be an RSA token key that
generates the one-time password that's
possible as well but in this case it's
the mobile phone now that's going to be
very similar to how we login to our
Gmail account sometimes when we log into
a Gmail account and I mean if you have
the proper setting enabled it's going to
send an 110 password to mobile phone and
only after we put that information it's
going to let us log in let's see how all
this is done in an AWS account so the
first thing is to log in to the AWS
account create a user and attach the
user to the virtual MFA device now we're
going to use Google Authenticator here
and we're going to attach the user to
Google Authenticator and make the user
use the Google Authenticator one-time
password every time he logs in to the
account now this is an addition to the
username and password that he or she
already has right so the first thing is
to log into the account connect or sync
the virtual Appliance with the AWS
username and password now when we sync
there will be an one-time password that
gets shown in the phone type in the
password in the AWS console and then the
phone and the username comes to sync and
from that point onwards the user anytime
they log in they will have to propose
their username and password which is the
first step of security and then once
they typed in the username and password
type in the MFA code that gets generated
on the phone that would be the second
step of security or the last step of
security once that is done the account
is going to let them log in and access
the AWS resources all right so in order
to test IM MFA S3 together this is what
we're gonna do so we're gonna create a
bucket and we're gonna have two users
and we're gonna allow or deny access to
the S3 bucket to those two users based
on whether they are MFA authenticated or
not and on purpose we're going to assign
MFA to one user and no MFA to the other
user like you might have guessed by now
the user with MFA will have access to
the bucket and the user with no MFA
attached will not have access to the
bucket of them are going to have full S3
services but MFA like I said is one
layer above the other permissions MFA
stands at all time so irrespective of
whether the underlying policy says but
they have full access to the S3 bucket
they are not going to because MFA stands
on top of it let's see how that's done
to begin with firstly we need a bucket
so let's create a bucket
so I'm in my AWS console so let me go to
S3
and in here create a bucket call that
bucket as simply learn MFA bucket and
let me put it in North Virginia and
create
all right so that's done
now I'm going to put a separate folder
in here
I'm going to create another folder in
here and the name of the folder is tax
documents now this is where I'm planning
to keep my tax files
and I only want my privileged users to
be able to upload and download
information from the tax document so on
the bucket side it's partially done or
on the S3 side it's partially done so
let's go back to IM and here we're going
to create two users one a junior
employee and another one and a senior
employee the senior employee is the one
who's going to have MFA access and more
access to the S3 bucket and the junior
employee is the one who's not going to
have access to that particular bucket so
let's get started let's create an user
under IM
click on users and let's create a user
called as user
Junior
right so he's a junior user
and for the password
set a password
the user does not need to create a new
password at sign in because it's a test
environment and the user who creates the
username and the user are the same
person so let's keep it simple username
password Management console access
and this user is going to have full S3
bucket access all right this is really
the policy
we should be familiar with this jsr
document by now it says effect is
allowed and what action any action in S3
and what resource anybody who's attached
to it review
and create a user
so this user is having S3 full access
now let's create another user call them
as senior employee
all right attach
a policy to the user
you know from the surface level both of
them have the same permissions both of
them have S3 full access
all right so both of them have S3 full
access as we see
there's one thing different we're going
to do to this senior employee now we're
going to attach MFA to this employee so
the way we would do it is under the
summary section of that particular user
click on security credential and see
here is a place we can attach an MFA to
this user so let's attach MFA to this
user for MFA I can use both virtual MFA
or Hardware MFA this being in lab
environment we're going to use Virtual
MFA now that is an application there are
a lot of applications available but
there is one application that I'm using
at the moment called Google
Authenticator
so once we have installed the
application and once it is running
so I can go to Virtual MFA device and
once it is running I can scan the code
using the Google Authenticator in my
mobile phone once you turn on the
application and once you do scan the
barcode that is showing right here now
that's going to give us codes that we
can use and we will have to put in those
codes here twice we'll have to wait for
some time and it will generate a second
set of key that we can put so my first
set of key is four zero two three five
one right I'll have to wait for some
time and then it would throw me a second
set of key it's like validating twice
right here so let me put in my second
set of key
so this user is now MFA authenticated
user so anytime this user logs in it's
going to ask for MFA anytime this user
proposes something it's going to attach
the MFA key along with it all right so
we're done with the IM what have you
done with IM we have created two users
both of them have s34 access but one of
them have MFA access and the other one
does not have MFA access and based on
whether the user is having MFA or not
you know we're identifying privileged
users by this MFA right so based on
whether the user has MFA or not are we
going to allow deny access in the S3
bucket
so we already have a bucket we already
have a folder and this is a very
privileged folder path here I would like
to restrict access based on whether the
user is having MFA or not
so let me create a bucket policy here
now here's a policy that I have written
the policy says that you know under tax
document folder give access only if the
user is MFA authenticated right if the
user is not MFA authenticated simply
deny the access to that user so as you
might guess right now the user 1 will
not have access or the junior employee
you will not have access because it's a
privilege and you know Junior employee
might tend to delete them unknowingly
and only privileged users get access to
the tax document because it's
confidential because it's privileged
because if it gets deleted we won't be
able to get it back for those reasons
right so apply the bucket policy
now try logging in as
the different users and see how the
permissions are applied on both the
users all right so let's make a note of
the IM URL
and then the two usernames and password
let's login
so let me log in as the privileged user
sure enough I'm being asked for an MFA
code let me put my MFA code
so let's directly navigate to the S3
bucket
right here's my privileged path let me
try uploading some content
claim forms
sure enough I'm able to upload content
into it will I be able to delete
yes I'm able to delete as well now let's
see what's the situation for the other
user
so let's login as the other user
navigate to the SC bucket
if you noticed when logging in it did
not ask for an MFA code because the user
is not set up for MFA
so navigate to the folder which we want
to test
now try adding some files
as you see it's failing to add content
to this bucket it shows error because
the bucket policy is requiring MFA and
this user is not MFA and we did that in
IEM or under iam
but still as you see you know they will
be able to view the content in the
bucket they can do all that we have
specifically restricted access to upload
and download content from the bucket and
that's exactly this user is now denied
with so what are we going to learn are
we going to learn about what is cloud
storage in general and then the types of
storage available in general in the
cloud and how things were before anybody
adopted using S3 that's something are we
going to learn and then we're going to
immediately dive into what is S3 and
then the benefits of using S3 over other
storage and then we're gonna do a couple
of labs or console walkthroughs about
what is object and how to add an object
what is bucket how to create a bucket
stuff like that and then we're going to
talk about how Amazon S3 generally works
now it comes with a lot of features it
comes with a lot of Promise um how does
this all work how does Amazon able to
keep up with the promise we're going to
talk about that and then we will talk
about some features add-on features that
comes along with the Amazon S3 so what
is cloud storage in general cloud
storage is a service that provides web
services where we can store our data and
not only that that data can be easily
accessed and the data can be easily
backed up everything over the internet
in chart if you could store your data if
you could access the data if you could
backup your data everything you do
through the internet then that's a good
definition for cloud storage and
additional definitions are in cloud
storage we only pay for what we use you
know no commitment no pre-proitioning is
you know pay as you go type subscription
and the best part is we pay on a monthly
basis you know we don't rent a hardware
for the year or we don't give commitment
for the whole year it's pay as you go
and pay on a monthly basis and these
Cloud storages are very reliable meaning
once you put the data in it it's never
going to get lost and these Cloud
storages are scalable assuming I have a
requirement to store 100 times of what
my actual data size is and I want it now
it's available in the cloud and these
storages are secure as well because
we're talking about data data virginity
they need to be secure and Amazon
provides tools and Technologies through
which we can secure our data and these
are generally not found in the
on-premises storage system so let's talk
about the different types of storage in
general so S3 is cloud storage in AWS
and then we have elastic Block store now
elastic Block store is actually the SSD
hard drives that gets attached to our
ec2 instance you know it's like the C
drive it's like the D drive it's like
the e Drive that gets attached to our
instances now EFS is elastic file system
the underlying technology is kind of the
same but it differs from EVS in a way
that EBS can be accessed only by the
system that's attached to it meaning the
e- volumes and the D volumes we spoke
about they can be accessed only if there
is an instance connected to it but these
EFS are actually shared file systems
elastic file system all right they are
shared systems they can be accessed by
multiple systems they can be accessed
from inside the Amazon environment it
can be accessed from on-premises
equipment as well a glacier is actually
the archiving solution in the cloud if
you want to dump a data and try to keep
them in the low cost as possible then
Glacier is the product we should be
using and then we have storage Gateway
if I want to safely move my data from my
local environment to the cloud
environment and also want to keep a copy
of the data locally so users locally can
access them and in a cloud users or
Internet users can access the data from
the cloud if that's your requirement
then storage Gateway is the one we would
be choosing and then we have snowball
snowball is really an data Import and
Export a system but it is actually an
Hardware that gets shipped to our
premises where we can copy a data into
it and then I ship it back to Amazon and
Amazon would copy the data into whatever
destination we give them in our account
and if the data is really huge then I
can call for a snowmobile which is
actually a data center on a truck where
Amazon would send me a truck loaded with
the data center as you see that has
compute capacity lots and lots of
storage capacity and electricity AC and
lot more so they get comparked near our
data center and cables run into our data
center we can copy data into it send it
back to Amazon and they would copy it to
our account in whatever storage that we
advise them so if the data is really
really huge then I would be calling
snowmobiles no more bill is not
available in all the regions all right
let's take this example how things were
before S3 was introduced you know two
professionals are having a conversation
and one of them is finding it very
difficult to sort of manage all the data
in the organization well if the data is
small it can be easily managed but as
company grow and we are living in an era
where data is everything we want data to
backup every idea we want data to backup
every proof of concept that we provide
so data is everything so in this era
it's all about collecting data analyzing
them and saving you know not losing logs
you know saving them analyzing stuff
like that so coming back to our
discussion here one person finds it very
difficult to store and manage all the
data that they have so some of the datas
that this person is having problem
storing is data stat application used to
running and then datas that gets sent to
the the customers and data that the
websites require the data that are
because of the email backups and a lot
more other storages that an Enterprise
can have and this person is having
problem backing up all those data and
even if we think of increasing the local
storage capacity that's going to cost
the fortune and few things that make it
sometimes impossible to increase the
storage capacity in-house is you know we
will have to go and pay heavy to buy
hardware and software to run these
storages and we need to hire a team of
experts for maintaining them the
hardware and the software and anytime if
there is a dynamic increase in the
storage capacity the on-premises or
in-house Hardwares won't be able to
scale and data security data security is
very costly when it comes to building
our own storage environment in-house and
adding data security on top of it so the
other a guy in the conversation was sort
of quietly listening everything the
manager was saying and then he slowly
introduced him to S3 because he knew
that all the problem that this manager
was worried about can be solved to S3 in
other words all the scalability all the
data security all the not being able to
provision hardware and software
components are all available with S3 so
that actually brings us to the
discussion about what S3 is S3 is simple
storage service it actually provides an
object storage service let me talk to
you about object and block storage
object storage is where you can store
things in the drive all right you can't
install anything in it and this object
storage can be accessed directly from
the internet whereas block storage is
something that needs to be attached to
an instance and we can't directly access
it but we can install software in it so
that's high level difference between
object storage and block storage and S3
is an object storage what does that mean
we can store data from the internet we
can retrieve from the internet but we
can't install anything in S3 all right
so so S3 is an object based storage and
it's it's really built for storing and
recovering or retrieving any amount of
data or information from anywhere in the
internet few other things you need to
know about S3 is that the Z3 is
accessible through the web interface the
storage one type of accessing or one way
of accessing S3 is by dragging dropping
content into it and another way of
retrieving data from S3 is go to a
browser click on download that's going
to let you download any content and the
data can be five terabytes in size now
we're talking about one file you know
you can have hundreds or thousands of
files like that one file can be as big
as 5 terabytes in size and S3 is
basically designed for developers where
they they can push logs into S3 or drive
logs anytime they want instead of
storing locally in the server they can
use S3 as code repositories you know
where they can save the code and have
the applications read the code from
there and lot more if they want to
safely share the code with another
person with a lot of encryption and
security added on top of it that's
possible as well so there are a few
things about S3 and on top of all this
S3 provides 11 9 durability and four
nine availability meaning durability is
if I store the data will it get lost
Amazon is like no it's not going to get
lost you're gonna have the data because
we provide level 9 durability for the
data and availability is if you want the
data now will you be able to show it
Amazon is like yes we will be able to
show it we have 99.99 availability and
when you request the data we will be
able to show the data to you all right
so let's talk about the benefits of S3
S3 is durable as we we saw it provides
11 9 durability S3 is low cost out of
all the storage options in Amazon S3 is
the cheapest ns3 is very scalable like
we were saying there there's no required
to pre-provision a storage capacity you
know if you need more go ahead and use
more if you need even more go ahead and
use even more and once you're done some
data needs to be removed just remove the
data so that particular month you will
be paying less so it's very scalable in
nature and it's very available as well
S3 isn't regional service you know it's
not based on one availability zone so
one availability is on going down within
Amazon the whole availability going down
it's not going to affect your ability to
access S3 storage and S3 is secure lot
of security features like bucket policy
a lot of security features like
encryption and then MFA authentication
are possible with S3 that actually adds
a very good security layer on top of the
data that we have stored in 3 and not
only that this S3 is very flexible in
terms of cost flexible in terms of where
I want to store the data in terms of
cost there are a lot of pricing tiers
within S3 S3 itself is a cheap service
now within that we have a lot of pricing
tiers depending on the durability so I
can always choose to put data on a
different storage tier or storage option
in S3 we're going to talk about it as
you stick along and in terms of
flexibility in region I can always
choose any of the region available in
the console or in the S3 console to put
my data to there is no restrictions on
where I can or cannot put the data in
the cloud as long as there are regions
available for me to move the data to and
data transferring with S3 is very simple
all I have to do is browse to the bucket
upload the data and the data gets
uploaded and we can also upload data
using CLI commands are very similar to
Linux commands or very similar to what
we would use in the Run command prompt
in Windows and what we would use in the
Run command prompt in the Powershell all
right let's talk about a basic building
block of S3 which is bucket and objects
now what's a bucket what's an object
object is the actual data and bucket is
the folder where the objects get stored
let me give you a new definition for
objects so object is the actual data
plus some information that reference the
data like is it an JPEG file the name of
the file and at what time it was added
to so they're called metadata right so
object is actually data plus metadata
and bucket is actually a container that
receives the data and safely stores in
it and when we add a data in a bucket
Amazon S3 creates an unique version ID
and allocates it to the object so we can
easily identify it at a later Point let
me show show you a quick lab on S3
foreign
console if you're wondering how I
reached here go to Services Under
storage and S3 is right here
and let's create a bucket called simply
learn
now the bucket names will have to be
unique
so I really doubt if simply learn will
be available let's let's check it anyway
all right it doesn't seem to be
available so
um
let me pick another name or let's call
it simply learn Dot
samuel.com
and I'm gonna put this in
Mumbai or I can choose Oregon
let me choose Oregon
yes let me choose Oregon
and let me create a bucket
sure enough a bucket got created let me
upload an object into the bucket
and you know these objects can be as big
as
five terabytes we talked about it right
all right let me upload an object
all right so that's my object so you get
the relation right here's a bucket
right here's my bucket and within that
is my object now object is the actual
file plus what type of file it is and
then
and then the size of the file the date
in which it got added and the storage
class it is in at the moment
so if I have to access it
I can simply access it through the
internet
so let's talk about how does this S3
bucket work anyway all right so how does
it work a user creates a bucket they
will specify the region in which the
bucket should be deployed we had an
option we could have chosen to deploy in
all the regions Amazon provides S3
service you know beat not Virginia beat
Mumbai B Tokyo beat Sydney uh beat uh
Oregon and we chose Oregon to be the
destination region where we want to
create bucket and save our data there
and when we upload data into the bucket
we can specify three types of storage
classes in our case we pick the default
which was a S3 standard as we saw on the
object data it was on S3 standard so
that's the basic thing later once the
object gets added we can always add a
bucket policies you know policies Define
who access it and who should not access
it what can they access are we going to
allow users only to read or to read
write as well stuff like that so that's
bucket policy we're defining the life
cycle or the lifespan of the data in the
S3 bucket now over the time do you want
the data to automatically move to a
different storage tier and at any point
do you want to sort of expire the data
you know get the data flushed out of
your account automatically you know
those things can be configured in
lifecycle policies and Version Control
is creating multiple versions if you're
going to use S3 for a code repository
creating multiple versions let's say if
you want to roll back to whatever you
had before a month how are you going to
do it if you kept updating the file and
never took a version of it so Version
Control helps us to keep different
versions and it helps us to roll back to
the older version anytime that is a need
so let's talk about the different
storage classes in S3 the different
storage classes in S3 begins with S3
standard now this is the default and
it's very suitable for use case is where
you need less or low latency for example
if you want to access the data of a
student's attendance you would retrieve
them very quickly as much as possible so
that's a good use case to store data in
S3 let's understand the different
storage classes in Amazon SC now let's
take a school for example and the the
different data is present in a school
and the features of those data the
validity of the data all right so let's
take this example and there are
different storage options in S3 let's
take S3 standard for example and what
would be the actual candidate data that
can be stored in S3 standard let's talk
about that so S3 standard in general is
suitable for use cases where the latency
should be very low and in here the good
example are the good candidate file that
can be stored in S3 is a data that needs
to be frequently accessed and that needs
to be retrieved quickly something like
students attendance report or students
attendance sheet which we access daily
and then it needs to be retrieved
immediately as and when we need it the
other type of storage tier in S3 is
infrequent access or in frequent data
access just like the name says the use
case for that is less frequently
accessed data I mean the candidate data
that should go in infrequent access data
is a student's academic record you know
which we don't need to access on a daily
basis but if there is a requirement we
might want to go and look at it then
that's going to be quick so it's not a
data that we would access on a daily
basis but it's data that needs to show
up on your screen real quick and the
third option is Amazon Glacier now
Glacier is really an archival solution
in the cloud so for archives high
performance is not a requirement so
let's talk about some candidate data
that can go into archives something like
students admission fee and it's not
critical also now anytime if you want to
look at the data you can always wait to
retrieve the data so in other words you
know put it in the archive and
retrieving from the archival takes time
so a student's old record are a good
candidate to be put in the archives the
other options are one zone IA storage
class where the data is infrequently
accessed and the data is stored in a
single availability zone for example you
know by default Amazon stores data in
multiple availability zones and there is
a charge for that now it's not an option
but the charge includes storing data in
multiple availability zones but if your
data requirement is you want to keep the
charges in a low even further you can
choose a one's own IE storage class
where it stores data in one availability
Zone and the candidate data that can go
in a one zone in frequent access is
students report card and the other
option with Amazon S3 is standard
reduced redundancy storage it is
suitable for cases where the data is not
critical and the data can be reproduced
quickly for example you know take a copy
of the library book take a copy of the
PDF library book for example now we
would have a source PDF and we would
make copies of it and then we make it
available for the readers to read the
other option in S3 is reduced redundancy
storage here the use case is data that's
not critical and a data can be
reproduced quickly for example a books
in the library are not that critical and
we always have a copy of the book we're
talking about PDF so if the customer
facing or the student facing book gets
deleted I can always copy the same PDF
put it in the destination folder and
make it available for the users to read
that would be a very good use case for
reduced redundancy storage all right
let's summarize everything that we
learned about different storage options
in S3 so S3 standard it's for frequently
accessed data it's the default storage
if you don't mention anything the data
gets stored in S3 standard it can be
used for cloud applications you know
content distribution gaming applications
big data analytic Dynamic websites they
are a very good use case for S3 standard
frequently accessed data the other one
on the contrary is S3 standard
infrequently accessed data just like the
name says the use case is this is good
for data that will be less frequently
accessed and and then the use case are
it's good for backups it's good for
disaster recovery and it's good for
lifelong storage of data Glacier on the
other hand is very suitable for
archiving data which is in frequent only
accessed and the Vault lock feature is
the security feature of the glacier that
also provides a long-term data storage
in the cloud this is the lowest storage
tier within S3 the other options are one
zone in frequent access storage class
just like the name says it's
infrequently accessed and it is stored
in just one availability Zone and use
cases are any data that doesn't require
any high level of security can be stored
here in one zone the fifth storage tier
is reduced redundancy storage this is
good for data that's frequently accessed
it's good for data that is non-critical
and that can be reproduced if it gets
lost and reduce redundancy storage or
RRS is and highly available solution
designed for sharing or storing data
that can be reproduced quickly all right
let's compare and contrast couple of
other features that are available in S3
for example durability availability SSL
support burst bike latency and life
cycle management so in standard the
durability is 11 9 durability it's the
same for standard standard IA one zone
IA Glacier except for reduced redundancy
the durability is 11 9 and availability
of all the storage classes is all the
same except for one zone IA where the
availability zone is 99.5 percentage all
of these products support SSL connection
and the first byte latency of these
products are most of them provide access
with millisecond latency except for
Glacier it provides retrieval time of
couple of minutes to a maximum of hours
and all of them can be used for a life
cycle management you know moving data
from one storage tier to another storage
here that's possible with the all of
these storage options all right now that
we've understood the different types of
storage options available on S3 let's
talk about some of the features that are
available on S3 lifecycle management now
lifecycle management is a service that
helps us to define a set of rules that
can be applied to an object or to the
bucket itself lifecycle is actually
moving the data from one storage tier to
another storage tier and finally
expiring it and and completing the life
cycle of the object with lifecycle
management we can manage and store our
objects in a very cost effective way it
has two features basically transition
actions and expiration actions let's
talk about transition actions with
transition action we can choose to move
the objects or move the data from one
storage class to another storage class
with lifecycle management we can
configure S3 to move our data from one
storage class to another storage class
at a defined time interval or at a
defined schedule let's talk about
transition actions in more detail let's
say we have our data in S3 at the moment
and we haven't used the data for quite
some time and it's that's how it's going
to be for the rest of the time so that
data is a very good candidate to move to
the infrequent axis because S3 standard
is a bit costlier and as three
infrequent access is a bit cheaper than
S3 standards so the kind of usage sort
of fits very well for moving that data
into infrequent access so using
lifecycle transition or lifecycle
management I can move the data to S3
infrequent access after 30 days and
let's say that the data stayed in
infrequent access for 30 more days and
then now I realize that nobody is
looking into the data so I can find an
appropriate storage here for that
particular data again and I can move it
to that particular storage which is
Glacier so in this case after 30 days or
in a total of 60 days from the time the
data got created the data can be moved
to Glacier and what is this really help
us with the life cycle management help
us to automatically migrate our data
from one storage cost to another storage
cost and by that it really helps us to
save the storage cost lifecycle
management can also help us with object
expiration meaning deleting the object
or flushing it out after a certain
amount of time let's say that our
compliance requirement requires that we
keep the data for seven years and we
have like thousands and thousands of
data like that it would be humanly
impossible to check all those or keep
track of all the dates and you know when
they need to be deleted stuff like that
but with lifecycle management it is very
much possible I can simply create a data
and set up lifecycle management for the
data to expire after seven years and
exactly after seven years the data is
going to expire meaning it's going to be
deleted automatically from the account
all right let me show you a lab on
lifecycle management and let me explain
to you how it's actually done so I'm
into the bucket that we have created and
here's a data into the bucket assume we
have thousands and thousands of data in
the bucket and that requires to be put
in a different storage tier over time
and that requires to be expired after
seven years let's say so the way I would
create lifecycle management is go to
management from inside the bucket and
click on life cycle and then add and
lifecycle rule just give it a name name
like expire so all the objects that's
present in this bucket meaning the
current version of it set a transition
for it so the transition is at the
moment they are in S3 so here I would
like to put them in infrequent access
after 30 days and then after it's been
an infrequent access for 30 days I would
like to move it to Glacier all right
plus 30 days so how do you read it so
for the first 30 days it's going to be
in Glacier so how do we actually read it
after I put the data in S3 the data is
going to get moved to standard IA and
then it's going to stay in standard IA
and after 60 days from the data creation
it's going to get moved to Glacier so on
the 31st day it's going to move to
standard IE on the 61st day it's going
to move to Glacier let's say if I want
to sort of delete the data if I want the
data to get deleted automatically after
seven years you know being in a glacier
how do I go about doing it let me open
up a quick calculator
365 into 7 that's
2555 days right after that the data is
gonna get deleted pretty much I have
created a life cycle so after on the
31st day it's going to get moved to
infrequent access and on the 61st day
Glacier and after seven years is over
any data that I put in the bucket it's
going to get deleted all right let's
talk about bucket policies bucket
policies are some permission files that
we can attach to an a bucket that allows
or denies access to the bucket based on
what's mentioned in the policy so bucket
policy is really an IM policy where you
can allow and deny permission to an S3
resource with bucket policy we can also
Define security rules that apply to more
than one file in a bucket now in this
case you know we can create an user or
let's say there's already a user called
simply learn we can allow or deny that
user connection to the s 3 bucket or
connecting to the S3 bucket using bucket
policy and bucket policies are written
in Json script and let's see how that's
done all right there are tools available
for us to help us create bucket policies
what you're looking at is a tool
available online that helps us to create
a bucket policy so let's use this tool
to create a bucket policy I'm going to
create a deny statement I'm going to
deny all actions to the S3 bucket and
what is the Arn to which we want to
attach the Arn of the resources actually
the name of the bucket but it really
expects us to give that key in a
different format so the Arn is available
right here copy bucket Arn so this is
actually going to deny everybody now we
wanted to deny a user just one user now
look at that now we have a policy that
has been created that's denying access
to the bucket and it's denying a user
called simply learn pretty much done so
I can use the policy and go over to
bucket policies so once I save it only
the user calls simply learn won't have
access to the bucket and the rest of
them will have access to it so once I
save it it gets added and only the user
simply learn will not have access to the
bucket because we're denying them
purposefully the other features of S3
include data protection we can protect
our data in S3 with the now one of which
is bucket policy I can also use
encryptions to protect my data I can
also use IM policy to protect my data so
Amazon S3 provides a durable storage not
only that it also gives us unprotected
and scalable infrastructure needed for
any of our object storage requirements
so here the data is protected by two
means one is data encryption and the
other one is data versioning data
encryption is encrypting the data so
others won't get access or even if they
get access to the file they won't be
able to access the data without the
encryption key and versioning is making
multiple copies of the data so let's
talk about them in detail what's data
encryption now data encryption refers to
protecting the data while it is being
transmitted and protecting the data
while it is at rest now data encryption
can happen in two ways one is client
encryption encryption at rest and server
side encryption encryption that's in
motion client-side encryption refers to
when client sends the data they encrypt
the data and send it across to Amazon
and server side encryption is when the
data is being transferred they get
encrypted and stay encrypted throughout
the transfer versioning is another
security feature let's I mean it helps
us so our unintentional edits are not
actually corrupting the data for example
let's say you edited the data and now
you realize that the data is incorrect
and you want to roll back now how do you
roll back without versioning it's not
possible in other words only with
versioning it's possible so versioning
it can be utilized to preserve recovery
and restore any early versions of every
object that we stored in our Amazon S3
bucket unintentional erases or overrides
of the object can be easily regained if
we have versioning enabled and it's
possible only if we have one file with
the same name and anytime we update the
file it keeps the file name but creates
a different version ID take this bucket
and data for example in a photo.pmg is a
file that was initially stored it
attached a version ID to it and then we
edited it let's say we added some
Watermark we you know added some graphic
to it and that's now the new version of
it when we store it we store it with the
same file name it accepts the same file
name but creates a new version ID and
attaches it anytime we want to roll back
we can always go to the console look at
the old versions pick the version ID and
roll back to the old version ID all
right let's take this example now I'm in
a bucket that we've been using for a
while and let me upload an object now
before we actually upload an object this
bucket needs to be versioning enabled so
let me watch enable this Bucket from
this point onwards this bucket is going
to accept versioning so let me upload an
object
photo.jpg let me upload it all right it
successfully got uploaded good enough
it's uploaded now let me upload another
object with the same file name now look
at that it was uploaded at 7 40 35 am
let me upload another object with the
same file name that I have it stored
right here that got up to uploaded but
with the same name
all right so that's the other photo now
what if if I want to switch back the way
I would switch back is simply switch
back to the older version Look at that
this is the latest version and this is
the old version that I have I can simply
switch back to the old version that was
created at such and such time and I can
open it
that's going to open the old file so in
short it creates different version of
the data that I create as long as it's
with the same name and at any point I
can go and roll back to the original
data this is a very good use case if you
want to use S3 for storing our codes
let's talk about other feature like
cross region replication now cross
region replication is an very cool
feature if you want to automatically
keep a copy of the data in a totally
different region for you know data
durability for any additional data
durability or if you want to serve data
to your customers who live in another
country or who are accessing your data
from another country if you want to
serve the data with low latency cross
region replication is a very cool
feature that you can use and get
benefited from and let's see how that's
done so before we actually do a lab on
Cross region replication let's put
together a proper definition for it
cross region application is a feature
that provides automatic copying of every
object uploaded to our bucket or your
bucket Source bucket and it
automatically copies the data to the
destination bucket which is in a
different AWS region as you see here in
the picture I put data only in region
one it's going to copy the data to
region 2. and for us to use cross region
replication versioning must be turned on
so it creates versions and copies the
versions as well if tomorrow the
original region goes down let's say the
other region will be active and it has
the complete data that was present in
the original region or at any point if
we want to Simply you know cut the
region replication and use the other
bucket as in Standalone bucket it can be
used as well because it already has all
the data that was present in the master
or all the data that was present in the
original replication bucket let's see
how that's done so there are two things
needed one is versioning and another one
is role when we transfer data from one
bucket to another bucket we need proper
permissions to do so and these roles
they give us proper permissions to
transfer data from one bucket to another
bucket let's see how that's done right
so here's my bucket a bucket in US organ
let's create another bucket
and
Mumbai
call it dot Mumbai
.com
and put it in
put it in Mumbai
or create that bucket in Mumbai there
you go we have one in Oregon we have one
in Mumbai we're planning to replicate or
create replication between these two
right create application between these
two buckets so go to the first bucket go
to management and start a replication
add a rule so all content in this bucket
is going to get replicated this is my
source bucket it's quite simple select
the destination bucket now my
destination bucket is going to be simply
learn.samual.mumbai.com all right it
says well you have versioning enabled in
your Source bucket but not on your
destination bucket do you want to enable
it now without which we won't be able to
proceed further so let's go ahead and
enable versioning through the console
that shows up and then like I said it's
going to require permission to put data
onto the other bucket now I can create
different roles these are different
roles that are used for different other
services I can also choose to create a
role that specifically gives permission
only to move the data from one bucket to
another bucket three months it's done so
if I go over to my source bucket and if
I add some data to it let's say
index.html assuming I'm adding some
files to it in theory they should move
to the other region automatically there
you go I'm in the Mumbai region and the
data is they got moved to the other
region automatically let's talk about
the other feature called transfer
acceleration now it's a very handy and a
helpful tool or a service to use if we
are transferring data which is very long
distance from us meaning from the client
to the S3 bucket let's say from my local
machine which is in India if I transfer
the data over to Oregon let's say it's a
long distance if it is going to go
through the internet it's going to go
through high latency connections and my
transfer my get delayed if it is an 1
gigabit file it's okay but if we're
talking about anything that's uh you
know five terabyte size now if you're
talking about anything that's a five
terabyte in size then uh it's not going
to be a pleasant experience so in those
cases I can use transfer accelerator uh
with which in a secure way but in a fast
way or a fastest way transfers my data
from the laptop or from client to the S3
bucket and it makes use of a service
called cloudfront to transfer or to
enable the data acceleration so the way
it would do it is instead of copying the
data directly to the location instead of
copying the data directly to the
destination bucket it copies the data
locally into a cloudfront location which
is available very local to whatever
place we are in and from there it copies
the data directly to an S3 bucket not
going through the internet it helps
eliminate a lot of latency that could
get added when transferring the data so
let's see how that's done
right here I'm in the S3 bucket and
under properties I can find transfer
accelerator and if I enable transfer
accelerator so I'm in another bucket let
me go to properties and let me go to
transfer accelerator and enable transfer
accelerator so now if I put data into
this bucket they're gonna get copied to
the local cloudfront location and from
there they're gonna get copied to the S3
Bucket from the cloud front now if we
need to compare you know how the speed
is going to be compared to directly
putting the data to the internet and
using cloudfront there is a tool
available that actually runs for a while
and then comes with a report that tells
me how much will be the improved speed
if I use transfer accelerator and it
shows for all the regions available in
Amazon so from the source to the
destination if you want to put uh you
know what's the normal and what's the
accelerated speed when you transfer the
data those results we will get in the
screen so at the moment this tool is
going through it testing like uploading
some file through the internet and
uploading some file using cloudfront and
it has come up with the calculation that
if I'm uploading file to San Francisco
compared to uploading through the
internet and through cloudfront it's 13
times faster so similar way it's going
to calculate for all the regions
available and it's going to give me a
result also if you want to master the
field of cloud computing the
postgraduate program in cloud computing
designed in collaboration with Caltech
ctme helps you become an Azure AWS and
gcp expert this in-depth cloud computing
certification course lets you master key
architectural principles and develop the
skills needed to become a cloud expert
benefits of this postgraduate program
includes Caltech ctme postgraduate
certificate enrollment and simple learns
job assist you will receive up to 30
CEUs from Caltech ctme you will attend
master classes from Caltech ctme
instructors live virtual classes led by
industry experts Hands-On projects and
integrated Labs online convocation by
Caltech ctme program director 40 plus
Hands-On projects and integrated Labs
Capstone project in four different
domains and Caltech ctm is circle
membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms and services such as AWS
Lambda Amazon S3 Azure app services and
many more so this program also covers
the essential skills to become an expert
in cloud computing and you can consider
this program because it provides you
with a certification from Caltech ctme
well reputed university in the United
States so what are you waiting for check
out the link mentioned in the
description box below and start your
cloud computing career today so without
any further delay let's get started
hi this is the fourth lesson of the AWS
Solutions architect course
migrating to the cloud doesn't mean that
resources become completely separated
from the local infrastructure
in fact running applications in the
cloud will be completely transparent to
your end users
AWS offers a number of services to fully
and seamlessly integrate your local
Resources with the cloud
one such service is the Amazon virtual
private cloud
this lesson talks about creating virtual
networks that closely resemble the ones
that operate in your own data centers
but with the added benefit of being able
to take full advantage of AWS
so let's get started
[Music]
in this lesson you'll learn all about
virtual private clouds and understand
their concept
you'll know the difference between
public private and elastic IP addresses
you'll learn about what a public and
private Southerner is
and you'll understand what an internet
gateway is and how it's used
you'll learn what root tables are and
when they are used
you'll understand what a Nat Gateway is
we'll take a look at security groups and
their importance
and we'll take a look at Network ACLS
and how they're used in Amazon VPC
we'll also review the Amazon VPC best
practices
and also the costs associated with
running a bpc in the Amazon Cloud
welcome to the Amazon virtual private
cloud and subnet section
in this section we're going to have an
overview of what Amazon VPC is and how
you use it and we're also going to have
a demonstration of how to create your
own custom virtual private cloud
we're going to look at IP addresses and
the use of elastic IP addresses in AWS
and finally we'll take a look at subnets
and there'll be a demonstration about
how to create your own subnets in an
Amazon VPC
and here are similar terms that are used
in vpcs
the subnets root tables elastic IP
addresses internet gateways Nat gateways
Network ACLS and security groups and in
the next sections we're going to take a
look at each of these and build our own
custom VPC that we'll use throughout
this course
Amazon defines a VPC as a virtual
private Cloud that enables you to launch
AWS resources into a virtual Network
that you've defined this virtual Network
closely resembles a traditional Network
that you'd operate in your own data
center but with the benefits of using
the scalable infrastructure of AWS
a VPC is your own virtual Network in the
Amazon Cloud which is used as a network
layer for your ec2 resources and this is
a diagram of the default VPC now there's
a lot going on here so don't worry about
that what we're going to do is break
down each of the individual items in
this default VPC over the coming lesson
but what you need to know is that a VPC
is a critical part of the exam and you
need to know all the concepts and how it
differs from your own Networks
throughout this lesson we're going to
create our own VPC from scratch which
you'll need to replicate at the end of
this so you can do well in the exam
each VPC that you create is logically
isolated from other virtual networks in
the AWS Cloud it's fully customizable
you can select the IP address range
create subnet configure root tables
setup Network gateways Define security
settings using security groups and
network access control lists
so each Amazon account comes with a
default VPC that's pre-configured for
you to start using straight away so you
can launch your ec2 instances without
having to think about anything we
mentioned in the opening section A VPC
can span multiple availability zones in
a region
and here's a very basic diagram of a VPC
it isn't this simple in reality
and as we saw in the first section
here's the default Amazon VPC which
looks kind of complicated
but what we need to know at this stage
is that cidr block for the default VPC
is always a 16 subnet mask so in this
example it's
172.31.0.0 16. what that means is this
VPC will provide up to 65
536 private IP addresses so in the
coming sections we'll take a look at all
of these different items that you can
see on this default VPC but why wouldn't
you just use the default VPC
well the default VPC is great for
launching new instances when you're
testing AWS
but creating a custom VPC allows you to
make things more secure and you can
customize your virtual Network as you
can Define your owner IP address range
you can create your own subnets that are
both private and public and you can
tighten down your security settings
by default instances that you launch
into a VPC can't communicate with your
own network
so you can connect your vpcs to your
existing data center using something
called Hardware VPN access so that you
can effectively extend your data center
into the cloud and create a hybrid
environment
now to do this you need a virtual
private Gateway and this is the VPN
concentrator on the Amazon side of the
VPN connection then on your side in your
data center you need a customer Gateway
which is either a physical device or a
software application that sits on your
side of the VPN connection
so when you create a VPN connection a
VPN internal comes up when traffic is
generated from your side of the
connection
VPC peering is an important concept to
understand
appearing connection could be made
between your own bpcs or with a VPC in
another AWS account as long as it's in
the same region
so what that means is if you have
instances in vpca they wouldn't be able
to communicate with instances in vpcb or
C and thus you set up a peering
connection
pairing is a one-to-one relationship a
VPC can have multiple peering
connections to other vpcs but and this
is important transitive peering is not
supported
in other words
bpca can connect to B and C in this
diagram but C wouldn't be able to
communicate with B unless they were
directly paired
also vpcs with overlapping cidrs cannot
be paired so in this diagram you can see
they all have different IP ranges which
is fine but if they had the same IP of
ranges they wouldn't be able to be
paired
and finally for this section if you
delete the default VPC you have to
contact AWS support to get it back again
so be careful with it and only delete it
if you have good reason to do so and
know what you're doing
this is a demonstration of how to create
a custom VPC
so here we are back at the Amazon web
services Management console and this
time we're going to go down to the
bottom left where the networking section
is I'm going to click on VPC
and the VPC dashboard will load up now
there's a couple of ways you can create
a custom VPC there's something called
the VPC wizard
which will build vpcs on your behalf
from a selection of different
configurations for example a VPC with a
single public subnet or a VPC with
public and private subnets now this is
great because you click a button type in
a few details and it does the work for
you however you're not going to learn
much or pass the exam if this is how you
do it so we'll cancel that and we'll go
to your vpcs and we'll click on create a
VPC
and we're presented with the create a
VPC window so let's give our VPC a name
I'm going to call that simply learn
underscore VPC
and this is the kind of naming
convention I'll be using throughout this
course
next we need to give it the cidr block
or the classes into domain routing block
so we're going to give it a very simple
one
10.0.0.0 and then we need to give it the
subnet mask so you're not allowed to go
larger than 15 so if I try to put 15 in
it says no not going to happen
for a reference subnet mask of 15 would
give you around 131 000 IP addresses and
subnet 16 will give you 65
536 which is probably more than enough
for what we're going to do
next you get to choose the tenancy and
there's two options default and
dedicated if you select dedicated then
your ec2 instances will reside on
Hardware that's dedicated to you so your
performance is going to be great but
your cost is going to be significantly
higher so I'm going to stick with
default
and we just click on yes create
it'll take a couple of seconds
and then in our VPC dashboard we can see
our simply learn VPC has been created
now if we go down to the bottom here to
see the information about our new VPC we
can see it has a root table associated
with it
which is our default root table
so there it is and we can see that it's
only allowing local traffic at the
moment
we go back to the VPC again
we can see it's been given a default
Network ACL
and we'll click on that and have a look
and you can see this is very similar to
what we looked at in the lesson
so it's allowing all traffic from all
sources inbound and outbound
now if we go to the subnet section
and just widen the VPC area here
you can see there's no subnets
associated with the VPC we just created
so that means we won't be able to launch
any instances into our VPC
and to prove it I'll just show you we'll
go to the ec2 section
so this is a glimpse into your future
this is what we'll be looking at in the
next lesson and we'll just quickly try
and launch an instance we'll select any
instance it doesn't matter
any size not important so here the
network section if I try and select
simply learn VPC it's saying
no subnets found this is not going to
work
so we basically need to create some
subnets in our VPC
and that is what we're going to look at
in the next lesson
now private IP addresses are IP
addresses that are not reachable over
the internet and they're used for
communication between instances in the
same network
when you launch a new instance is given
a private IP address and an internal DNS
hostname that resolves to the private IP
address of the instance but if you want
to connect to this from the Internet
it's not going to work
so then you'd need a public IP address
which is reachable from the internet you
can use public IP addresses for
communication between your instances and
the internet
each instance that receives a public IP
address is also given an external DNS
hostname
public IP addresses are associated with
your instances from the Amazon Pool of
public IP addresses
when you stop or terminate your instance
the public IP address is released and a
new one is associated when the instance
starts
so if you want your instance to retain
this public IP address
you need to use something called an
elastic IP address
an elastic IP address is a static or
persistent public IP address that's
allocated to your account and can be
Associated to and from your instances as
required an elastic IP address remains
in your account until you choose to
release it there is a charge associated
with an elastic IP address if it's in
your account but not actually allocated
to an instance
this is a demonstration of how to create
an elastic IP address
so we're back at the Amazon web services
Management console we're going to head
back down to the networking VPC section
and we'll get to the VPC dashboard on
the left hand side we'll click on
elastic IPS
now you'll see a list of any elastic IPS
that you have associated in your account
and remember any of the elastic IP
address that you're using that isn't
allocated to something you'll be charged
for so I have one available and that is
allocated to an instance currently so we
want to allocate a new address
and it reminds you that there's a charge
if you're not using it I'm saying yes
allocate
and it takes a couple of seconds
and there's our new elastic IP address
now we'll be using this IP address to
associate with the NAT Gateway when we
build that
AWS defines a subnet as a range of IP
addresses in your VPC
you can launch AWS resources into a
subnet that you select you can use a
public subnet for resources that must be
connected to the internet and a private
subnet for resources that won't be
connected to the internet
the netmask for the default Subnet in
your VPC is always 20 which provides up
to 4096 addresses per subnet and a few
of them are reserved for AWS use
a VPC can span multiple availability
zones but the subnet is always mapped to
a single availability Zone this is
important to know so here's our basic
diagram which we're now going to start
adding to so we can see the virtual
private cloud and you can see the
availability zones and now inside each
availability Zone we've rated a subnet
now you won't be able to launch any
instances unless there are subnets in
your VPC
so it's good to spread them across
availability zones for redundancy and
failover purposes
there's two different types of subnet
public and private
you use a public subnet for resources
that must be connected to the internet
for example web servers
a public subnet is made public because
the main root table sends the subnets
traffic that is destined for the
internet to the internet gateway and
we'll touch on internet gateways next
private subnets are for resources that
don't need an internet connection or
that you want to protect from the
internet for example database instances
so in this demonstration we're going to
create some subnets a public and a
private subnet and we're going to put
them in our custom VPC in different
availability zones
so we'll head to networking and VPC
wait for the VPC dashboard to load up
we'll click on subnets
we'll go to create subnet
and I'm going to give the subnet a name
so it's good to give them meaningful
names
so I'm going to call this first one for
the public subnet
10.0.1.0 and I'm going to put this one
in
the US East
one
B availability Zone
and I'm going to call that simply learn
public
so it's quite a long name I understand
but at least it makes it clear for
what's going on in this example so we
need to choose a VPC so we obviously
want to put it in our simply learned VPC
and I said I wanted to put it in U.S
east 1B I'm using the North Virginia
region by the way
so we click on that then we need to give
it the cidr block
now as I mentioned earlier when I typed
in the name
that's the range I want to use and then
we need to give it the subnet mask and
we're going to go with
24. which should give us 251 addresses
in this range which obviously is going
to be more than enough if I try and put
a different value in that's unacceptable
to Amazon it's going to say
again it's going to give me an error and
tell me not to do that
let's go back to 24 and click and cut
and paste this by the way just because I
need to type something very similar for
the next one
click create
it takes a few seconds
okay so there's our new subnet and I
just widen this
you can see so that's
the IP range that's the availability
Zone it's for simple learn and it's
public
so now we want to create the private
just gonna put the name in I'm going to
give the private the IP address block
that
I'm going to put this one in Us East 1C
and it's going to be the private
subnet
obviously I want it to be in the same
VPC
credibility zone of Us East 1C
and we're going to give it
10.0.2.0
24.
and we'll click yes create
and again it takes a few seconds
okay
it's let me sort by name
so there we go we can see now we've got
our private subnet and our public Subnet
in fact let me just type in simply then
there we are so now you can see them
both there
and you can see they're both in the same
VPC
simply learn VPC
now if we go down to the bottom you can
see the root table associated with these
vpcs
and you can see that they can
communicate with each other internally
but there's no internet access
so that's what we need to do next in the
next lesson you're going to learn about
internet gateways and how we can make
these subnets have internet access
welcome to the networking section
in this section we're going to take a
look at internet gateways root tables
and NAC devices and we'll have a
demonstration on how to create each of
these AWS VPC items
so to allow your VPC the ability to
connect to the internet you need to
attach an internet gateway
and you can only attach one internet
gateway per VPC
so attaching the internet gateway is the
first stage in permitting internet
access to instances in your VPC now
here's our diagram again and now we've
added the internet gateway which is
providing the connection to the internet
to your VPC
but before you can configure internet
correctly there's a couple more steps
for an ec2 instance to be internet
connected you have to adhere to the
following rules
firstly you have to attach an internet
gateway to your VPC which we just
discussed
then you need to ensure that your
instances have public IP addresses or
elastic IP addresses so they're able to
connect to the internet
then you need to ensure that your
subnets root table points to the
internet gateway
and you need to ensure that your network
access control and Security Group rules
allow relevant traffic to flow to and
from your instance so you need to allow
the rules to let in the traffic you want
for example HTTP traffic after the
demonstration for this section we're
going to look at how root tables Access
Control lists and security groups are
used
in this demonstration we're going to
create an internet gateway and attach it
to our custom VPC
so let's go to networking VPC
bring up the VPC dashboard
and on the left hand side we click on
internet gateways
so here's a couple of Internet gateways
I have already
but I need to create a new one so create
internet gateway
I'll give it a name which is going to be
simply learn
internet gateway igw and I'm going to
click create so this is an internet
gateway which will connect a VPC to the
internet because at the moment our
custom VPC has no internet access
so there it is created
simply then igw
but this state is detached because it's
not attached to anything so let me try
and attach it to a VPC
and it gives me an option of all the
vpcs that have no
internet gateway attached to them
currently
so I only have one which is simpler than
bpc
yes attach
now you can see our VPC has internet
attached and you can see that down here
so let's click on that and it will take
us to our VPC
but before any instances in our VPC can
access the internet we need to ensure
that our subnet root table points to the
internet gateway
and we don't want to change the main
root table we want to create a custom
root table and that's what you're going
to learn about next
a root table determines where Network
traffic is directed
it does this by defining a set of rules
every subnet has to be associated with a
root table and a subnet can only be
associated with one root table
however multiple subnets can be
associated with the same root table
every VPC has a default root table and
it's good practice to leave this in its
original state and create a new root
table to customize the network traffic
routes associated with your VPC
so here's our example and we've added
two root tables the main root table and
the custom root table
the new root table or the custom root
table will tell the internet gateway to
direct internet traffic to the public
subnet
but the private subnet is still
Associated to the default route table
the main route table which does not
allow internet traffic to it
all traffic inside the private subnet is
just remaining local
in this demonstration we're going to
create a custom root table associated
with our internet gateway and Associate
our public subnet with it
so let's go to networking and VPC
the dashboard will load and we're going
to go to Route tables
now our VPC only has its main route
table at the moment the default one it
was given at the end time it was created
so we want to create a new root table
and we want to give it a name so we're
going to call it simply learn
I'm going to call it root table rtb for
sure
and then we get to pick which VPC we
want to put it in so obviously we want
to use simply learn VPC
so we click create
which will take a couple of seconds and
here you are here's our new root table
so what we need to do now is change its
root so that it points to the internet
gateway so if we go down here to root
at a minute you can see it's just like
our main root table it just has local
access
so we want to click on edit
and we want to add another root
so the destination is
the internet
which is all the zeros and our Target
and we click on this it gives us the
option of our internet gateway which we
want to do so now we have internet
access to this subnet sorry to this root
table
and we click on Save
for successful so now we can see that as
well as local access we have internet
access
now at the moment if we click on subnet
associations
you do not have any subnet associations
so basically both both our subnets the
public and private subnets are
associated with the main root table
which doesn't have internet access so we
want to change this so we'll click on
edit
and we want our public subnet to be
associated with this root table
let's click on Save
so it's just saving that
so now we can see that our public subnet
is associated with this route table
and this route table is associated with
the internet gateway so now anything we
launch into the public subnet will have
internet access
but what if we wanted our instances in
the private subnet to have internet
access
well there's a way of doing that with a
Nat device and that's what we're going
to look at in the next lecture
you can use a Nat device to enable
instances in a private subnet to connect
to the Internet or other AWS services
but prevent the internet from initiating
connections with the instances in the
private subnet
so we talked earlier about public and
private subnets to protect your assets
from be directly connected to the
internet
for example your web server would sit in
the public Subnet in your database in
the private subnet which has no internet
connectivity
however your private sudden that
database instance might still need
internet access or the ability to
connect to other AWS resources if so you
can use a network address translation
device or a Nat device to do this
and that device forwards traffic from
your private subnet to the internet or
other AWS services and then sends the
response back to the instances
when traffic goes to the internet The
Source IP address of your instance is
replaced with the NAT device address and
when the internet traffic comes back
again then that device translates the
address to your instance's private IP
address
so here's our diagram which is getting
ever more complicated and if you look in
the public subnet you can see we've now
added a Nat device and you have to put
Nat devices in the public subnet so that
they get internet connectivity
AWS provides two kinds of nat devices
and that Gateway and in that instance
AWS recommends in that Gateway that it's
a managed service that provides better
availability and bandwidth than that
instances each Nat Gateway is created in
a specific availability Zone and is
implemented with redundancy in that zone
and that instance is launched from a Nat
Ami an Amazon machine image and runs as
an instance in your VPC so it's
something else you have to look after
whereas in that Gateway being a fully
managed service means once it's
installed you can pretty much forget
about it
and that Gateway must be launched into a
public subnet because it needs internet
connectivity it also needs an elastic IP
address which you can select at the time
of launch
once created you need to update the root
table associated with your private
subnet the point internet bound traffic
to the NAT Gateway this way the
instances in your private subnets can
communicate with the internet
so if you remember back to the diagram
when we had the custom root table which
was pointed to the internet gateway
now we're pointing our main root table
to the NAT Gateway so that the private
subnet also gets internet access but in
a more secure manner
welcome to the create and that Gateway
demonstration where we're going to
create a Nat Gateway so that the
instances in our private subnet can get
internet access
so we'll start by going to networking
and VPC
and the first thing we're going to do is
take a look at our subnets and you'll
see why shortly so here are our simply
learned subnets so this is the private
subnet that we want to give internet
access but if you remember from the
section
that gateways need to be placed in
public subnets so I'm just going to copy
the name of this subnet ID
for the public subnet and you'll see why
in a moment
so then we go to Nat gateways on the
left hand side
and we want to create a new Nat Gateway
so we have to put a subnet in there so
we want to choose our public subnet and
as you can see
it truncates a lot of the subnet names
on this option so it's a bit confusing
so we know that we want to put it in our
simply learn VPC
in the public subnet but you can see
it's truncated so it's actually this one
at the bottom but what I'm going to do
is just paste in
the subnet ID which I copied earlier
so there's no confusion
then we need to give it an elastic IP
address now if you remember from the
earlier demonstration we created one so
let's select that but if you hadn't
allocated one you could click on the
create new EIP button
so we'll do that
okay so it's telling me my Nat Gateway
has been created and in order to use you
on that Gateway ensure that you edit
your root table to include a route with
a target of and then our napped Gateway
ID
so it's given us the option to click on
our edit root tables so we'll go
straight there now here's our here's our
root tables
now
here's the custom root table that we
created earlier and this is the default
the main router which was created when
we launched our when we created our VPC
so we should probably give this a name
so that we know what it is so let me
just call this simply learn rtb
nine
so now we know that's our main root
table
so if you take a look at the main root
table
and the subnet associations
you can see that our private subnet is
associated with this table
so what we need to do is put a root in
here that points to the NAT Gateway so
if we click on roots and edit
and we want to add another root and we
want to say that all traffic
can
either go to the simply in the internet
gateway which we don't want to do we
want to point it to our nap instance
which is this Nat ID here
and we click save
so now any instances launched in our
private subnet we'll be able to get
internet access via around that Gateway
welcome to the using security groups and
network ACL section
in this section we're going to take a
look at security groups and network ACLS
and we're going to have a demonstration
on how you create both of these items in
the Amazon web services console
a security group acts as a virtual
firewall that controls the traffic for
one or more instances
you add rules to each security group
that allow traffic to or from its
Associated instances
basically a security group controls the
inbound and outbound traffic for one or
more ec2 instances
security groups can be found on both the
ec2 and VPC dashboards in the AWS web
Management console we're going to cover
them here in this section and you'll see
them crop up again in the ec2 lesson
and here is our diagram and you can see
we've now added security groups to it
and you can see that ec2 instances are
sitting inside the security groups and
the security groups will control what
traffic Flows In and Out
so let's take a look at some examples
and we'll start with a security group
for a web server now obviously a web
server needs HTTP and https traffic has
a minimum to be able to access it
so here is an example of the security
group table and you can see we're
allowing HTTP and https
the ports that are associated with those
two and the sources and we're allowing
it from the internet we're basically
allowing all traffic to those ports
and that means any other traffic that
comes in on different ports would be
unable to reach the security group and
the instances inside it
let's take a look at an example for a
database server Security Group
now imagine you have a SQL Server
database
then you would need to open up the SQL
Server Port so that people can access it
and which is Port 1433 by default so
we've added that to the table and we've
allowed the source to come from the
internet now because it's a Windows
machine you might want RDP access so you
can log on and do some Administration
so we've also added RDP access to the
security group now you could leave it
open to the internet but that would mean
anyone could try and hack their way into
your box so in this example we've added
a source IP address of
10.00.0.0 so only IP arranges from that
address can RDP to the instance
now there's a few rules associated with
security groups by default security
groups allow all outbound traffic so if
you want to tighten that down you can do
so in a similar way to you can Define
the inbound traffic
Security Group rules are always
permissive you can't create rules that
deny access so you're allowing access
rather than denying it
security groups are stateful so if you
send a request from your instance the
response traffic for that request is
allowed to flow in regardless of the
inbound Security Group rules
and you can modify the rules of a
security group at any time and the rules
are applied immediately
welcome to the create Security Group
demonstration where we're going to
create two security groups one the host
DB servers and one the host web servers
now if you remember from the best
practices section it said it was always
a good idea to tier your applications
into security groups and that's exactly
what we're going to do
so if we go to networking and VPC to
bring up the VPC dashboard
on the left hand side under security we
click on security groups
now you can also get to security groups
from the ec2 dashboard as well
so here's a list of my existing security
groups but we want to create a new
security group and we're going to call
it
simply learn
web server SG Security Group and we'll
give the group name as the same and our
description is going to be simply learn
web servers
security groups
okay and then we need to select our VPC
now it defaults to the default VPC but
obviously we want to put it in our
simply learn VPC so we click yes create
takes a couple of seconds
and there it is there's our new Security
Group
now if we go down to the rules the
inbound rules
you can see there are none so by default
a new security group has no inbound
rules but what about outbound rules
if you remember from the lesson
a new Security Group by default allows
all traffic to be outbound
and there you are all traffic
has destination of everywhere so all
traffic is allowed but we want to add
some rules so let's click on inbound
rules click on edit
now this is going to be a web server so
if we click on the drop down
we need to give it http
so you can either choose custom TCP Rule
and type in your own port ranges or you
can just use the ones they have for you
so http
this pre-populates the port range and
then here you can add the source
now if I click on it it's giving me the
option of saying allow
access from different security groups
so you could create a security group and
say I only accept traffic from a
different Security Group which is a nice
way of securing things down you could
also put in here just your IP address so
that only you could do HTTP requests to
the instance but because it's a web
server
we want people to be able to see our
website otherwise it's not going to be
much use so we're going to say all
traffic so all source
traffic can access our instance on Port
http 80.
I want to add another rule because we
also want to do https
which is
hiding from me
there we are
and again we want to do
the same
and also because this is going to be a
Linux instance we want to be able to
connect to the Linux the instance to do
some work and configuration so we need
to give it SSH access
and again it would be good practice to
tie it down to your specific IP or an IP
range but we're just going to do all for
now and then we click on Save
and there we are there we have our
ranges
so now we want to create our security
group for our DB servers so let's click
create Security Group
and then we'll go through it and give it
a similar name
simply learn
DB servers s key
and the description is going to be
simply than TV servers Security Group
and our VPC is obviously going to be
simpler than VPC
so let's click yes create wait a few
seconds
and here's our new security group as you
can see it has no inbound Rules by
default and outbound rules allow all
traffic
so this is going to be a SQL Server
database server and so we need to allow
SQL Server traffic into the instance
so we need to give
it Microsoft SQL Port access Now the
default port for Microsoft SQL Server is
1433
now in reality I'd probably change the
port the SQL server was running on to
make it more secure
but we'll go with this for now and then
the source so we could choose the IPA
ranges again but what we want to do is
place the DB server in the private
subnet and allow the traffic to come
from the web server so the web server
will accept traffic
and the web server will then go to the
database to get the information it needs
to display on its web on the website or
if people are entering information into
the website we want the information to
be stored in our DB server so basically
we want to say that this the DB servers
can only accept SQL Server traffic from
the web server Security Group
so we can select the simply then web
server Security Group as the source
traffic
but Microsoft SQL Server data
so we'll select that now our SQL Server
is obviously going to be a Windows
instance so from time to time we might
not we might need to log in and
configure it so we want to give RDP
access to now again you would probably
put a specific IP range in there we're
just going to do all traffic for now
then we click save
and there we are so now we have two
security groups
DB servers and web servers
a network ACL is a network access
control list
and it's an optional layer of security
for your VPC that acts as a firewall for
controlling traffic in and out of one or
more of your subnets
you might set up Network ACLS with rules
similar to your security groups in order
to add an additional layer of security
to your VPC
here is our Network diagram and we've
added Network ACLS to the mix now you
can see they sit somewhere between the
root tables and the subnets
this diagram makes it a little bit
clearer and you can see that a network
ACL sits in between a root table and a
subnet
and also you can see an example of the
default Network ACL
which is configured to allow all traffic
to flow in and out of the subnets to
which is associated
each Network ACL includes a rule whose
rule number is an Asterix
this rule ensures that if a packet
doesn't match any of the other numbered
rules it's denied you can't modify or
remove this rule
so if you take a look at this table you
can see on the inbound some traffic
would come in and it would look for the
first rule which is 100 and that's
saying I'm allowing all traffic from all
sources so that's fine the traffic comes
in if that rule 100 wasn't there it
would go to the Asterix Rule and the
Asterix rule is saying traffic from all
sources is denied
let's take a look at the network ACL
rules
each Subnet in your VPC must be
associated with an ACL if you don't
assign it to a custom ACL it will
automatically be Associated to your
default ACL
a subnet can only be associated with one
ACL however an ACL can be associated
with multiple subnets
an ACL contains a list of numbered rules
which are evaluated in order starting
with the lowest as soon as a rule
matches traffic it's applied regardless
of any higher numbered rules that may
contradict it
AWS recommends incrementing your rules
by a factor of 100 so there's plenty of
room to implement new rules at a later
date
unlike security groups ACLS are
stateless responses to allowed inbound
traffic are subject to the rules for
outbound traffic
welcome to the network ACL demonstration
where we're just going to have an
overview of ocls where they are in the
dashboard
now you don't need to know a huge amount
about them for the exam you just need to
know how they work and where they are so
let's go to networking and VPC
and on when the dashboard loads on the
left hand side under security there's
Network ACLS so let's click on that now
you can see some ACLS that are in my my
AWS account so we want the one that's
associated with our simply learned VPC
so if we extend this VPC
column
that's our Network ACL there simply then
VPC now let's give it a name because
it's not very clear to see otherwise
also I'm kind of an obsessive tagger so
let's call it simply learn ACL and click
on the tick so there we go so now it's
much easier to see
so we click on inbound rules so this is
exactly what we showed you in the lesson
the rule is 100 so that's the first rule
that's going to get evaluated and it's
saying allow all traffic from all
sources
and the outbound rules are the same
so if you wanted to tighten down the new
rule you could click edit we would give
it a new rule number say which would be
200 so you should always increment them
in 100 so that means if you had 99 more
rules you needed to put in place you'd
have space to put them in in between
these two
and then you could do whatever you
wanted you could say you know we are
allowing HTTP access from
all traffic
and we're allowing or you could say
actually you know what we're going to
deny it so this is the way of
blacklisting traffic into your VPC
now I'm not going to save that because
we don't need it
but this is where Network ACL sit and
this is where you would make any changes
it's also worth having a look at the
subnet associations with your ACL
so we have two subnets in our simply
learn VPC so we would expect to see both
of them associated with this network ACL
because it's the default
and there they are there's both our
public and our private subnets are
associated and you can also see up here
on the on the dashboard it says default
so this is telling us this is our
default ACL
if you did want to create a new network
ACL you would click create network ACL
you'd give it a name it's just a new ACL
and then you would associate it with
your VPC so we would say simply learn
VPC
takes a few seconds
and there we are there we have our new
one you can see this one says default no
because it obviously isn't the default
ACL for our simply learn VPC
and it has no subnets associated with it
so let's just delete that because we
don't need it but there you are there's
a very brief overview of network pcls
welcome to the Amazon VPC best practices
and costs where we're going to take a
look at the best practices and the costs
associated with the Amazon virtual
private cloud
always use public and private subnets
you should use private subnets to secure
resources that don't need to be
available to the internet such as
database services
to provide secure internet access to the
instances that reside in your private
subnets you should provide a Nat device
when using Nat devices you should use a
Nat Gateway over Nat instances because
there are managed service and require
less Administration effort
you should choose your cidr blocks
carefully Amazon VPC can contain from 16
to 65
536 IP addresses so you should choose
your cidr block according to how many
instances you think you'll need
you should also create separate Amazon
vpcs for development staging test and
production or create one Amazon VPC with
separate subnets with a subnet each for
production development staging and test
you should understand the Amazon VPC
limits there are various limitations on
the VPC components for example you're
allowed five vpcs per region
200 subnets per VPC
200 route tables per VPC
500 security groups per VPC
50 in and outbound rules per VPC
however some of these rules can be
increased by raising a ticket with AWS
support
you should use security groups and
network ACLS to secure the traffic
coming in and out of your VPC Amazon
advises to use security groups for
whitelisting traffic and network ACLS
for blacklisting traffic
and recommends tiering your security
groups you should create different
security groups for different tiers of
your infrastructure architecture inside
VPC if you have web tiers and DB tiers
you should create different security
groups for each of them
creating toy security groups will
increase the infrastructure security
inside the Amazon VPC
so if you launch all your web servers in
the web server security group that means
it'll automatically all have HTTP and
https open conversely the database
Security Group will have SQL Server
ports already open
you should also standardize your
Security Group naming conventions
following a security group naming
convention allows Amazon VPC operation
and management for large-scale
deployments to become much easier
always span your Amazon VPC across
multiple subnets in multiple
availability zones inside a region this
helps in architecting high availability
inside your VPC
if you choose to create a hardware VPN
connection to your VPC using virtual
private Gateway you are charged for each
VPN connection hour that your VPN
connection is provisioned and available
each partial VPN connection hour
consumed is billed as a full hour
you'll also incur standard AWS data
transfer charges for all data
transferred via the VPN connection
if you choose to create a Nat Gateway in
your VPC you are charged for each Nat
Gateway hour that your net Gateway is
provisioned and available data
processing charges apply for each
gigabyte processed through in that
Gateway each partial Nat Gateway hour
consumed is billed as a full hour
this is the practice assignment for
designing a custom VPC where you'll
create a custom VPC using the concepts
learned in this lesson
using the concepts learned in this
lesson recreate the custom VPC as shown
in the demonstrations
the VPC name should be simply learned
VPC the cidr block should be
10.0.0.016 there should be two subnets
one public with a range of
10.0.1.0 and one private with a range of
10.0.2.0 and they should be placed in
separate availability zones
there should be one internet gateway and
one that Gateway and also one custom
root table for the public subnet
also create two security groups simply
learn web server Security Group and
simply learn DB server Security Group
AWS Lambda tutorial now one day at
offers in a growing company there was
discussion going on between two ID
Personnel it was about the new role that
his colleague Jessica has taken Benjamin
the guy standing here wants to know
about it and Jessica's job is new
different and dynamic and interesting
too she scale and manage servers and
operating systems and apply security
patches onto them and monitor all of
these at the same time to ensure the
best quality for application is given to
the users and Benjamin was awestruck
with the amount of work Jessica is doing
and with the time it would take to
complete all of them but Jessica being a
very Dynamic person very suitable for
the job said it was easy for her to
handle all of it and even more but that
easiness on the job did not last longer
as the Environ grew more and and more it
being a startup company Jessica was
getting drained and was not really happy
about all her job as she used to be
Jessica's manual way of scaling and
environment did not last long and she
was also finding out that she missed to
scale down some of the resources and
it's costing her a lot she needs to pay
for the service that she was not at all
using she sort of felt that she was at
the end of the role and there was no way
out from this manual task she was very
desperate and that's when Benjamin
suggested something and that brought
back the peace and joy Jessica initially
had Benjamin suggested about a service
called Lambda that can ease the work
that Jessica is doing at the moment and
Lambda as happy as it looks like it's a
solution that solves the manual
repetitive work and lot more and Lambda
introduced itself as the problem solver
and started to explain the following
things the very same thing that we're
gonna learn about in this series so in
this section we we're going to learn
about the features of AWS Lambda and
we're going to talk about what Lambda is
and then we're going to talk about where
Lambda is being used in the it or in the
cloud environment as we speak and then
we're going to talk about how Lambda
works and some use cases of Lambda and
we're going to be particularly
discussing about the use case about
automatically backing up the data that's
put in the cloud storage let's talk
about Lambda in detail Lambda
automatically runs our code without
requiring us to provision or manage
servers just write the code and upload
it to Lambda and Lambda will take care
of it that means that we don't require
any server to run or to manage and all
you need to do is write the code and
upload it to Lambda and Lambda will take
care of it which also means that we can
stop worrying about provisioning and
managing service the only thing Lambda
expects from you is a code that's
working AWS Lambda also automatically
scales our application by running code
in response to each trigger our code
runs in parallel and processes each
triggers individually scaling precisely
with the size of the workload scaling
here is done automatically based on the
size of the workload Lambda can scale
the application running the code in
response to each and every trigger that
it receives billing in Lambda is meter
on the seconds we only pay for the
amount of time that our code is running
which means that we're not charged for
any of the servers the only payment
required is for the amount of time the
code is computed with AWS Lambda we are
charged for every 100 milliseconds we
are actually charged for 100
milliseconds our code executes and the
number of times our code is triggered
and we don't pay anything when the code
is not running let's talk about what is
AWS Lambda now Lambda is one of the
service that falls under the compute
section or the compute domain of
services that AWS provides along with
the ec2 EBS elastic load balancer Lambda
is also a service that comes under the
bigger umbrella of compute services in
AWS and Lambda allows us to execute code
or any type of application with AWS
Lambda we can run code for virtually any
type of application or backend Services
all we need to do is supply our code in
one of the languages that AWS Lambda
supports as we speak the languages that
are supported by AWS Lambda are node.js
Java c-sharp go and Python and Lambda
can be used to run code in response to
certain events from other services and
based on the event it can run functions
and those functions can be of node.js
java c-sharp Etc now let's talk about
where is Lambda used there are huge
number of use cases for Lambda and there
are many ways AWS Lambda is used
specifically in the business world let's
talk about some of them one use case is
AWS Lambda is used to process images
when it is uploaded in an S3 bucket
let's say the object gets uploaded in an
S3 bucket in a format that we don't
expect it to be which means the file
needs to be formatted so it gets
uploaded in a raw format and AWS Lambda
is triggered anytime a new object is
added to the bucket and the images are
processed and converted into thumbnails
based on the devices the other end
device that would be reading the data it
could be and PC it could be an apple
machine it could be an Android phone it
could be an Apple phone it could be a
tablet What Not So based on the devices
different formats Lambda can get
triggered and convert the video or
convert the picture into the different
format that it requires another use case
for Lambda is to analyze the social
media data let's say let's say we're
collecting the hashtag trendingdata and
the data is received and it's added into
the Kinesis stream to feed into the
Amazon environment and Lambda action get
triggered and it receives the data and
then the data is stored into the
database which can be used by businesses
for later analysis and some of the
companies that have gotten tremendous
benefit using Lambda or Thompson routers
benchlings Nordstrom Coca-Cola robot are
some of the companies to name at the
moment that have received tremendous
amount of benefit by using Lambda let's
look at how Lambda Works in other words
let's look at how the complicated
function behind the scenes work in a
simple and in a seamless way so here
clients send data now clients send data
to Lambda and clients could be anyone
who's sending requests to AWS Lambda it
could be an application or other Amazon
web services that's sending data to the
Lambda and Lambda receives the request
and depending on the size of of the data
or depending on the amount or volume of
the data it runs on the defined number
of containers if it is a single request
or if it is less request it runs on a
single container so the requests are
given to the container to handle and the
container which contains the code the
user has provided to satisfy the query
would run and if you're sort of new to
Containers then let me pause here for a
while and explain to you what container
is now container image is a lightweight
Standalone executable package of a piece
of software that includes everything
needs to run it like the codes the
runtimes the system tools the system
libraries and any of the settings needed
and it is at the moment available both
on Linux and windows based application
and containerized software will always
run the same regardless of the
environment it's running on and
containers isolate software from its
surrounding for example there could be
difference between a development and
staging environment so that's sort of
isolated and this helps in reducing the
conflict between the teams running
different software on the same
infrastructure all right now that we
know containers needed to understand
Lambda so if there are few requests it
sends to a single container but as and
when the request grows it actually
creates multiple containers shares the
multiple requests to the different
containers there and depending on the
volume depending on the size depending
on the number of sessions the more
number of containers are provisioned so
to handle those requests and when the
requests reduce the number of containers
reduce as well and that helps in Saving
costs we're not running any resources
and paying for it when we're not using
them and in fact we're not at all paying
for the resources we'll only charge for
the amount of time a function is running
inside these containers now consider a
situation where you have to set up up a
temporary storage and as a system to
backup the data as soon as the data is
uploaded which is a near real-time
backup now a near real-time manual
backups are nearly impossible and
they're not that efficient too and near
real-time manual backups that's what the
business demands but that's not near
real-time backup that too a manual one
that's not at all efficient even if we
come up with a solution of manually
backing up close to near real Time
That's not going to be efficient looking
at the amount of data that will be put
in and looking at the random times the
data will be put into the source bucket
and there is no way we can do a manual
backup and they keep it as real as
possible but if that's still your
requirement we can use AWS Lambda and
set things up so AWS Lambda manually
handles the backup in other words what
I'm possible or a difficult situation
like that aw slander comes for the
rescue and this is what we do create a
2s3 bucket one would be the source
bucket where the data will be uploaded
and the other one is and destination
bucket where the data will be backed up
from the source bucket and for these
buckets to talk to themselves it's going
to require an IM role and then for the
automatic copy it's going to require and
Lambda function to copy the files from
The Source bucket to the destination
bucket and what triggers the Lambda
function the Lambda function is
triggered every time there's a change in
the metadata for the bucket and this
data is then uploaded into the
destination bucket and after setting all
this up we can literally test it by
putting a data in the source bucket
that's going to automatically replicate
or that's going to automatically copy
the data from The Source bucket to the
destination bucket all right let's see
how we can replicate data between to
bucket well we have a feature to cross
cross region replicate in S3 that's a
feature that comes along with S3 what if
you want to replicate between two
different buckets in two different
accounts in those cases we can use
Lambda to replicate the data between the
buckets so you put one data or you put
data in the source bucket and that data
gets replicated to the destination
bucket and let's see how that's done the
procedure here would be to have two
buckets to begin with and then create an
IM role that lets you access to pull
data from The Source bucket and put data
on the destination bucket and then
create Lambda files or Lambda event or
triggers to actually look for event in
the source bucket and anytime a new data
gets added Lambda gets triggered copies
the data from The Source bucket and
moves the data to the destination bucket
and it uses the IAM role and policy for
the needed permissions and in just a
couple of clicks we have set up a
temporary backup system that can run
seamlessly without any any manual
intervention and that can be as near
real time as possible alright that's the
concept and let's see how it is done
real time through this lab so to begin
with we need two buckets so I have here
a source bucket and a destination bucket
and the source bucket as of now do not
have any data in it so as the
destination bucket all right so that's
one down the second would be to create
an IM role right so let me create an IM
role and the role is going to have this
policy in it a policy that's allowing
get object on the source bucket and a
policy that's allowing put object on the
destination bucket and here I'm going to
use or I'm going to paste my source and
destination buckets Arn all right go to
Services go to S3
Source bucket
all right click on the source bucket and
copy the bucket Arn
so that would be the source bucket Arn
right on on the destination bucket copy
the destination bucket Arn
and this is going to be my destination
bucket Arn so with this I'm going to
create a policy
all right
go to IM
and create a policy
now I I have already created a policy
with the same information all right
destination bucket Arn and a policy is
available with the name S3 bucket copy
Lambda attach the policy to the role
right go to rules create a role Lambda
is the service that's going to use it in
here attach the policy that we have
created
right give it the name
and then create a role now I have a role
created as well
all right copy Lambda and that's having
the policy that we have created sometime
back now create a Lambda function right
so go to Lambda
create a function give it a name like S3
bucket copy all right choose a role that
we want to use
all right that's the rule that we want
to use copy one two
create a function
all right
and then here we're going to use node.js
code right I have a sample code
this can be used as template in here
replace the source bucket with the name
of the source bucket and the destination
bucket with the name of the destination
bucket this is a node.js code that gets
run when an event get triggers now
what's an event anytime there is a new
object placed in the S3 bucket it
creates an event and the even triggers
Lambda and Lambda checks this Source S3
bucket picks the data puts it in the
destination bucket all right paste it
here
paste it here and click on save all
right now before you click on Save just
ensure that you have the appropriate
roles defined that's all you got to do
click on save all right now I already
have created
a Lambda function
all right
which is the same thing same code and
the rule is attached to it
now it's running it's active now let's
put this to test go to S3
pick the source bucket
put some data in it
all right and in theory those data
should be present in the destination
bucket as well
there you go it's all done by Lambda
also if you want to master the field of
cloud computing the postgraduate program
in cloud computing designed in
collaboration with caltex etme helps you
become an Azure AWS and gcp expert this
in-depth cloud computing certification
course lets you master key architectural
principles and develop the skills needed
to become a cloud expert
benefits of this postgraduate program
includes Caltech ctme postgraduate
certificate enrollment and simple learns
job assist you will receive up to 30
CEUs from Caltech ctme you'll attend
masterclasses from Caltech ctm
instructors live virtual classes led by
industry experts Hands-On projects and
integrated Labs online convocation by
Caltech ctme program director 40 plus
Hands-On projects and integrated Labs
Capstone project in four different
domains and Caltech ctma Circle
membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms and services such as AWS
Lambda Amazon S3 Azure app services and
many more so this program also covers
the essential skills to become an expert
in cloud computing and you can consider
this program because it provides you
with a certification from Caltech ctme
well reputed university in the United
States so what are you waiting for check
out the link mentioned in the
description box below and start your
cloud computing career today so without
any further delay let's get started
here we're going to talk about Amazon
ECS a service that's used to manage
Docker containers so without any further
Ado let's get started in this session we
would like to talk about some Basics
about AWS and then we're going to
immediately dive into why Amazon ECS and
what is Amazon ECS in general and then
it uses a service called Docker so we're
going to understand what Docker is and
there are competitive services available
for ECS I mean you could ECS is not the
on and only service to manage Docker
containers but why ECS advantage of ECS
we will talk about that and the
architecture of ECS so how it functions
what are the components present in it
and what are the functions that it does
I mean each and every component what are
all the functions that it does all those
things will be discussed in the
architecture of Amazon ECS and how it
works how it all connects together
that's something we we will discuss and
what are the companies that are using
ECS what were the challenge and how ECS
helped to fix the challenge that's
something we will discuss and finally we
have a wonderful lab that talks about
how to deploy Docker containers on an
Amazon ECS so let's talk about what is
AWS Amazon web service in short call as
AWS is an web service in the cloud that
provides a variety of services such as
compute power database storage content
delivery and a lot of other resources so
you can scale your business and grow not
focus more on your it needs and the rest
of the ID demands rather you can focus
on your business and let Amazon scale
your it or let Amazon take care of your
it so what is that you can do with AWS
with AWS we can create deploy apply any
application in the cloud so it's not
just deploying you can also create your
application in the cloud it has all the
tools and services required the tools
and services that you would have
installed in your laptop or you would
have installed in your on-premises
desktop machine for your development
environment you know the same thing can
be installed and used from the cloud so
you can use cloud for creating and not
only that you can use the same Cloud for
deploying and making your application
available for your end user the end user
could be internal internal users the end
user could be could be in the internet
the end user could be kind of spread all
around the world it doesn't matter so it
can be used to create and deploy your
applications in the cloud and like you
might have guessed now it provides
service over the Internet that's how
your users worldwide would be able to
use the service that you create and
deploy right so it provides service over
the internet so that's for the End
customer and how will you access those
Services that's again through the
internet it's like the extension of your
data center in the internet so it
provides all the services in the
internet it provides compute service
through the internet so in other words
you access them through the internet it
provides database service through the
internet over the internet in other
words you can securely access your
database through the internet and lot
more and the best part is this is a pay
as you go or pay only for what you use
there is no long term or you know
beforehand commitment here most of the
services does not have any commitment so
there is no long term and beforehand
commitment you only pay exactly for what
you use there's no overage there's no
overpaying right there's no buying in
advance right you only pay for what you
use let's talk about what ECS is so
before or ECS before containers right
ECS is a service that manages Docker
containers right it's not a product or
it's not a feature all by itself it's a
service that's dependent on Docker
container so before Docker containers
all the applications were running on VM
or on an host or on a physical machine
right and that's memory bound that's
latency bound the server might have
issues on and on right so let's say this
is Alice and she's trying to access her
application which is running somewhere
in her on-premises and the application
isn't working what could be the reason
some of the reasons could be Memory full
the server is currently down at the
moment we don't have another physical
server to launch the application a lot
of other reasons so a lot of reasons why
the application wouldn't be working in
on-premises some of them are Memory full
issue and server down issue very less a
high availability or in fact single
point of failure and no high
availability if I if I need to tell it
correctly with ECS the services can kind
of breathe free right the services can
run seamlessly now how how is that
possible now those things we will
discuss in the upcoming sessions so
because of containers and ECS managing
containers the applications can run in a
high available mode they can run in a
high available mode meaning if something
goes wrong right there's another
container that gets spunned up and your
application runs in that particular
container very less chances of your
application going down that's what I
mean this is not possible with a
physical Host this is very less possible
with an VM or at least it's going to
take some time for another VM to get
spun up so why ECS or what is ECS Amazon
ECS maintains the availability of the
application and allows every user to
scale containers when necessary so it
not only meets the availability of the
application meaning one container
running your application or one
container hosting your application
should be running all the time so to
make that high availability availability
is making sure your service is running
24.7 so Container makes sure that your
services run 24 bar 7 not only that not
only that suddenly if there is an
increase in demand how do you meet that
Demand right let's say you have like
thousand users suddenly the next week
there are like 2000 users all right so
how do you meet that demand Container
makes it very easy for you to meet that
demand in case of VM or in case of
physical host you literally will have to
go buy another physical host or you know
add more RAM add more memory add more
CPU power to it all right or kind of
Club two three hosts together clustering
you would be doing a lot of other things
to meet that high availability and also
to meet that demand but in case of ECS
it automatically scales the number of
containers it automatically scales the
number of containers needed and it meets
your demand for that particular R so
what is Amazon ECS the full form of ECS
is elastic container service right so
it's basically a container Management
Service which can quickly launch and
exit and manage Docker containers on a
cluster so what's the function of ECS it
helps us to quickly launch and quickly
exit and manage Docker container so it's
kind of a Management Service for the
docker containers you will be running in
Amazon or running in the AWS environment
so in addition to that it helps to
schedule the placement of container
across your cluster so it's like this
you have two physical host you know
joined together as a cluster and ECS
helps us to place your containers now
where should your container be placed
should it be placed in host one should
it be placed in host too so that logic
is defined in ECS we can Define it you
can also let ECS take control and Define
that logic most cases you will be
defining it so schedule the placement of
containers across your cluster let's say
two containers want to interact heavily
you really don't want to place them in
two different hosts all right you would
want to place them in one single host so
they can interact with each other so
that logic is defined by us and these
container services you can launch
containers using AWS Management console
and also you can launch containers using
SDK kids available from Amazon you can
launch through a Java program you can
launch container using an.net program
you can launch container using an
node.js program as an when the situation
demands so there are multiple ways you
can launch containers through Management
console and also programmatically and
ECS also helps to migrate application to
the cloud without changing the code so
anytime you think of migration the first
thing that comes to your mind is that
how will that environment be based on
that I'll have to alter my code what's
what's the IP what is the storage that's
being used what what are the different
parameters I'll have to include the
environment parameters of the new
environment with containers now that
worry is already taken away because we
can create an pretty exact environment
the one that you had in on premises the
same environment gets created in the
cloud so no worries about changing the
application parameter no worries about
changing the code in the application
right you can be like if it ran in my
laptop a container that I was running in
my laptop it's definitely going to run
in the cloud as as well because I'm
going to use the same container in the
laptop and also in the cloud in fact
you're going to ship it you're going to
move the container from your laptop to
Amazon ECS and make it run there so it's
like the same the very same image the
very same container that was running in
your laptop will be running in the cloud
or production environment so what is
Docker we know that it ECS helps to
quickly launch exit and manage Docker
containers what is Docker let's let's
answer that question what is a Docker
now Docker is a tool that helps to
automate the development of an
application as a lightweight container
so that the application can work
efficiently in different environments
this is pretty much what we discussed
right before the slide I can build an
application in my laptop or in on
premises in a container environment
Docker can container environment and
anytime I want to migrate right I don't
have to kind of rewrite the code and
then rerun the code in that new
environment I can simply create an image
a Docker image and move that image to
that production or the new Cloud
environment and simply launch it there
right so no compiling again no
relaunching the application simply pack
all your code in a Docker container
image and ship it to the new environment
and launch the container there that's
all so Docker container is a light
weight package of software that contains
all the dependencies so because you know
when packing you'll be packing all the
dependencies you'll be packing the code
you'll be packing the framework you'll
be packing the libraries that are
required to run the application so in
the new environment you can be pretty
sure you can be guaranteed that it's
going to run because it's the very same
code it's the very same framework it's
the very same libraries that you have
shipped right there's nothing new in
that new environment it's the very same
thing that's going to run in that
container so you can be rest assured
that they are going to run in that new
environment and these Docker containers
are highly scalable and they are very
efficient suddenly you wanted like 20
more Docker containers to run the
application think of adding 20 more
hosts 20 more VMS right how much time
would it take and compared to that time
the amount of time that Docker
containers would require to kind of
scale to that amount like 20 more
containers it's very less or it's
minimal or negligible so it's a highly
scalable and it's a very efficient
service you can subtly scale number of
Docker containers to meet any additional
demand very short boot up time because
it takes a it's not going to load the
whole operating system and these Docker
containers you know they use the Linux
kernel and features of the kernel like C
group and namespaces to kind of
segregate the processes so they can run
independently any environment and it
takes very less time to boot up and the
data that are stored in the containers
are kind of reusable so you can have an
external data volume and I can map it to
the container and whatever the space
that's occupied by the container and the
data that the container puts in that
volume they are kind of reusable you can
simply remap it to another application
you can kind of remap it to the next
successive container you can kind of
remap it to the next version of the
container next version of the
application you'll be launching and you
don't have to go through building the
data again from the scratch whatever
data the container was using previously
or the previous container was using that
data is available for the next container
as well so the volumes that the
containers users are very reusable
volumes and like I said it's isolated
application so it kind of isolates by
its nature it kind of by the way it's
designed by the way it is created it
isolates one container from another
container meaning anytime you run
applications on different containers you
can be rest assured that they are very
much isolated though they are running on
the same host though they are running on
the same laptop let's say though they
are running on the same physical machine
let's say running 10 containers 10
different applications you can be sure
that they are well disconnected or well
isolated applications now let's talk
about the advantages of ECS the
advantage of ECS is improved security
its security is inbuilt in ECS with ECS
we have something called as container
registry you know that's where all your
images are stored and those images are
accessed only through https not only
that those images are actually and
encrypted and access to those images are
allowed and denied through identity and
access management policies IAM and in
other words let's say two container
running on the same instance the one
container can have access to S3 and the
others or the rest of the others are
denied access to S3 so that kind of
granular security can be achieved
through containers when we mix and match
the other security products available in
Amazon like IAM encryption accessing it
using https these containers are very
cost efficient like I've already said
these are lightweight processors right
we can schedule multiple containers on
the same node and this actually allows
us to achieve high density on an ec2
instance imagine an ec2 instance that
that's very less utilized that's not
possible with a container because you
can actually dance or crowd an ec2
instance with more container in it so to
best use those resources in ec2
straightforward you can just launch one
application but with when we use
containers you can launch like 10
different applications on the same ec2
server that means 10 different
applications can actually feed on those
resources available and can benefit the
application and ECS not only deploys the
container it also maintains the state of
the containers and it makes sure that
the minimum a set of containers are
always running based on the requirement
that's another cost efficient way of
using it right and anytime an
application fails and that has a direct
impact on the revenue of the company and
is just make sure that you're not losing
any Revenue because your application has
failed and ECS isn't pretty extensible
Services it's like this in many
organization there are majority of
unplanned work because of environment
variation a lot of firefighting happens
when we kind of deploy the chord from
one or kind of move the chord or
redeploy the code in a new environment a
lot of firefighting happens there right
this Docker containers are pretty
extensible like we discussed already
environment is not a concern for
containers because it's going to kind of
shut itself inside a Docker container
and anywhere the docker container can
run the application will run exactly the
way it performed in the past so
environment is not a concern for the
docker containers in addition to that
ECS is easily scalable we have discussed
this already and it improves it has
improved compatibility we have discussed
this already let's talk about the
architecture of ECS like you know now
the architecture of ECS is the ECS
cluster itself that's group of servers
running the ECS service and it
integrates with Docker right so we have
a docker registry Docker registry is a
repository where we store all the docker
images or the container images so it's
like three components ECS is of three
components one is the ECS cluster itself
right when I say easiest itself I'm
referring to easiest cluster cluster of
servers that will run the containers and
then the repository where the images
will be stored right the repository
where the images will be stored and the
image itself so container image is the
template of instructions which is used
to create a container right so it's like
what's the OS what is the version of
node that should be running and any
additional software do we need so those
question gets answered here so it's the
template template of instructions which
is used to create the containers and
then the registry is the service where
the docker images are stored and shared
so many people can store there and many
people can access or if there's another
group that wants to access they can
access the image from there or one
person can store the image and rest of
the team can access and the rest of the
team can store image and this one person
can pick the image from there and kind
of ship it to the customer or ship it to
the production environment all that's
possible in this container registry and
Amazon's version of the container
registry is ECR and there's a third
party Docker itself has a container
registry that's Docker Hub ECS itself
which is the the group of servers that
runs those containers so these two the
container image and the container
registry they kind of handle Docker in
an image format just an image format and
in ECS is where the container gets live
and then it becomes an compute resource
and starts to handle request starts to
serve the page and starts to do the
batch job you know whatever your plan is
with that container so the class master
of servers ECS integrates well with the
familiar services like VPC VPC is known
for securing VPC is known for isolating
the whole environment from rest of the
customers or isolating the whole
environment or the whole infrastructure
from the rest of the clients in your
account or from the rest of the
applications in your account on and on
so VPC is a service that provides or
gives you the network isolation ECS
integrates well with VPC and this VPC
enables us to launch AWS resources such
as Amazon ec2 instance in a virtual
private Network that we specified this
is basically what we just discussed now
let's take a closer look at the ECS how
does ECS work let's find answer for this
question how does ECS work ECS has got a
couple of components within itself so
these ECS servers can run cross
availability Zone as you can see there
are two availability zones here they can
actually run across availability zones
and ECS has got two modes of fargate
more and the ec2 mode right here we're
seeing fargate more and then here we're
seeing nothing that means it's an ec2
mode and then it has got different
network interfaces attached to it
because they need to be running in an
isolated fashion right so anytime you
want Network isolation you need separate
IP and if you want separate IP you need
separate network interface card and
that's what you have elastic network
interface card separate elastic network
interface card for all those tasks and
services and this runs within an VPC
let's talk about the far gate service
tasks are launched using the far gate
service so we will discuss about tasks
what is forget now fargate is a compute
engine in ECS that allows users to
launch containers without having to
monitor the cluster ECS is a service
that manages the containers for you
right otherwise managing containers will
be an full-time job so easiest manages
it for you and if you and you get to
manage ECS that's the basic service but
if you want Amazon to manage ECS and the
containers for you we can go for forget
so fargate is a compute engine in ECS
that allows users to launch containers
without having to monitor the ECS
cluster and the tasks the tasks that we
discussed the tasks has two components
you see task right here so they have two
components we have ECS container
instance and then the container agent so
like you might have guessed right now
easiest container instance is actually
an ec2 instance right capable of running
containers not all ec2 instances can run
containers so these are like specific
ec2 instance chances that can run
containers they are ECS container
instances and then we have container
agent which is the agent that actually
binds those clusters together and it
does a lot of other housekeeping work
right kind of connects clusters makes
sure that the version needed is present
so it's all part of that agent or it's
all job of that agent container
instances container instances is part of
Amazon ec2 instance which run Amazon ECS
container agent pretty straightforward
definition and then a container agent is
responsible for communication between
ECS and the instance and it also
provides the status of the running
containers kind of monitors the
container monitors the state of the
container make sure that the content is
up and running and if there's anything
wrong it kind of reports it to the
appropriate service to fix the container
on and on it's a container agent when we
don't manage container agent it runs by
itself and you really don't have to do
anything to make the container agent
better it's already better you really
won't be configuring anything in the
agent and then elastic network interface
car is in Virtual interface Network that
can be connected to an instance in VPC
so in other words elastic network
interface is how the container interacts
with another container and that's how
the container interacts with the ec2
host and that's how the container
interacts with the internet external
world and a cluster a cluster is a set
of ECS container instances it's not
something that's very difficult to
understand it's simply a group of ec2
instances that runs that ECS agent and
this cluster cluster handles the process
of scheduling monitoring and scaling the
request we know that ECS can scale the
containers can scale how does it scale
that's all Monitor and managed by this
ECS cluster let's talk about the
companies that are using Amazon ECS
there are a variety of companies that
use ACS clusters to name a few okta
users easiest cluster and OCTA is a
product that use identity information to
Grant people access to applications on
multiple devices at any given point of
time they make sure that they have a
very strong security protection so OCTA
uses Amazon ECS to run their OCTA
application and serve their customers
and abima abhima is an TV channel and
they chose to use microservices and a
Docker containers they already had
microservices and Docker containers and
when they thought about a service that
they can use in AWS ECS was the only
service that they can immediately adapt
to and because in abima TV the engineers
have already been using Docker and
Docker containers it was kind of easy
for them to adapt themselves to ECS and
start using it along with the benefits
that ECS provides previously they had to
do a lot of work but now ECS does it for
them all right similarly remind and
Ubisoft GoPro or some of the famous
companies that use Amazon ECS and get
benefited from its scalability get
benefited from its cost gets benefited
from its Amazon managed Services get
benefited from the portability that ECS
and the migration option that ECS
provides let's talk about how to deploy
a Docker container on Amazon ACS the way
to deploy Docker container on ECS is
first we need to have an AWS account and
then set up and run our first ECS
cluster so in our lab we're going to use
the launch wizard to run an ECS cluster
and run containers in them and then task
definition task definition tells the
size of the container the number of the
container and when we talk about size it
tells how much of CPU do you need how
much of memory do you need and talking
about numbers you know it requires how
many numbers of container you're going
to launch you know is it 5 is it 10 or
is it just one running all the time now
those kind of information goes in the
task definition file and then we can do
some Advanced configuration on ECS like
a load balancers and you know what port
number you want to allow when you don't
want to allow you know who gets access
who shouldn't get access and what's the
IP that you want to allow and deny
requests from on and on and this is
where we would also mention the name of
the container so the differentiate one
container from the other and the name of
the service you know is it an backup job
is it a web application education is a
10. a data container is it going to take
care of your data data backend and the
desired number of tasks that you want to
be running all the time those details go
in when we try to configure the ECS
service right and then you configure
cluster you put in all the security in
the configure your cluster step or
configure cluster stage and finally we
will have an instance and bunch of
containers running in that instance all
right let's do a demo so here I have
logged in to my Amazon portal and let me
switch to the appropriate region I'm
going to pick North Virginia North
Virginia look for ECS and it tells ECS
is a service that helps to run and
manage Docker containers well and good
click on it I'm a not Virginia I just
want to make sure that I'm in the right
region and go to clusters and here we
can create cluster this is our forget
and this is our ec2 type launching for
Linux and windows environment but I'm
going to launch through this walkthrough
portal right this gives a lot of
information here so the different steps
involved here is creating a container
definition which is what we're going to
do right now and then a task definition
and then service and finally the cluster
it's a four-step process so in container
definition we Define the image the base
image we are going to use now here I'm
going to launch an httpd or a simple
HTTP web page right so a simple httpd
2.4 image is fair enough for me and it's
not an heavy application so 0.5 gigabit
of memory is enough and again it's not a
heavy application so 0.25 virtual CPU is
enough in our case right you can edit it
based on the requirement you can always
edit it and because I'm using hdpd the
port mapping is already Port 80 that's
how the container is going to receive
the request and there's no health check
as of now when we want to design
critical and complicated environments we
can include health check right and this
is the CPU that we have chose we can
edit it and I'm going to use the bash
commands to create an HTML page right
this page says that you know Amazon ECS
sample app right and then it says Amazon
ECS sample app your application is
running on a container in Amazon ECS so
that's the page the HTML page that I'm
going to create right index.html so I'm
going to create and put it in an
appropriate location so those pages can
be served from the container right if
you replace this with any of your own
content then it's going to be your own
content ECS comes with some basic logs
and these are the places where they get
stored that's not the focus as of now
all right so I was just saying that you
can edit it and customize it to your
needs we're not going to do any
customization now we're just getting
familiar with ECS now and the task
definition name of the task definition
is first run task definition and then we
are running it in a VPC and then this is
an fargate mode meaning the servers are
completely handled by Amazon and the
task memory is 0.5 gigabit and the task
CPU is 0.25 virtual CPU name of the
service is it a batch job is it an you
know a front end is it an back end or is
it a simple copy job what's the service
name of the service goes here again this
you can edit it and here's a security
group as of now I'm allowing for 80 to
the whole world if I want to rest it to
a certain IP I can do that the default
option for load balancing is no load
balancer but I can also choose to have a
load balancer and use port 80 to map
that Port 80 to The Container Port 80
right I can do that the default is no
load balancer all right let's do one
thing let's use load balancer let's use
load balancer and Port 80 that receives
information on Port 80 HTTP what's going
to be the cluster name when the last
step what is the cluster name cluster
name can be simply learn ECS demo next
we're done and we can create so it's
launching a cluster as you can see and
it's picking the task definition file
that we've created and it's using that
to launch and service and then these are
the log groups that we discussed and
it's creating your VPC remember ECS
clubs well with the VPC it's creating a
VPC and it's creating two subnets here
for high availability it's creating that
Security Group Port 80 allowed to the
whole world and then it's putting it
behind and load balancer right generally
would take like five to ten minutes so
which is need to be patient and let it
complete its creation and once this is
complete we can simply access these
servers using the load balancer URL and
when this is running let me actually
take you to the other products or the
other services that are integrated with
the ECS it's getting created our service
is getting created as of now ECR
repository this is where all our images
are stored now as of now I'm not pulling
my image from ECR I'm pulling it
directly from the internet Docker Docker
Hub but all custom images all custom
images they are stored in this
repository so you can create a
repository call it app one create
repository so here's my repository so
any image that I create locally or any
Docker image that I create locally I can
actually push them push those images
using these commands right here and they
get stored here and I can make my ECS
connect with ECR and pull images from
here so they would be my custom images
and as of now because I'm using a
default image it's directly pulling it
from the internet let's go to ec2 and
look for a load balancer because we
wanted to access the application from
behind a load balancer right so here is
a load balancer created for us and
anytime I put the URL so cluster is now
created you see there's one service
running all right let's click on that
cluster here is the name of our
application and here is the tasks the
different containers that we are running
and if you click on it we have an IP
right IP of that container and it says
it's running it was created at such and
such time and started at such such time
and this is the task definition file
that it this container uses meaning the
template the details the all the version
details they all come from here and it
belongs to the cluster called simply
learn ECS demo right and you can also
get some logs container logs from here
so let's go back and there are no ECS
instances here because remember this is
forget you're not managing any ECS
instance right so that's why you're not
seeing any ECS instance here so let's go
back to tasks and go back to the same
page where we found the IP pick that IP
put it in the browser and you have this
sample HTML page running from an
container so let me go back to load
balancer ec2 and then under ec2 I'll be
able to find a load balancer find that
load balancer pick that DNS name put it
in the browser and now it's accessible
through the load balancer URL right now
this URL can be mapped to other services
like DNS this URL can be emboded hi guys
today we got something very special in
store for you we're going to talk about
the best cloud computing platform
available Amazon web services uh Rahul I
think you said something wrong here the
best cloud computing platform is
obviously Google Cloud platform no it
isn't AWS is more than 100 services that
span a variety of domains all right but
Google Cloud platform has cheaper
instances what do you have to say about
that well I guess there's only one place
we can actually discuss this a boxing
ring so guys I'm apeksha and I will be
Google Cloud platform and I'm Rahul I'll
be AWS so welcome to fight night this is
AWS versus gcp the winner will be chosen
on the basis of their origin and the
features they provide the performance in
the present day and comparing them on
the basis of pricing market share and
options the things they give you for
free and instance configuration now
first let's talk about AWS AWS was
launched in 2004 and is a cloud service
platform that helps businesses grow and
scale by providing them services in a
number of different domains these domain
attention to compute database storage
migration networking and so on a very
important aspect about AWS is its years
of experience now AWS has been in the
market a lot longer than any other cloud
service platform which means they know
how businesses work and how they can
contribute to the business growing also
AWS has over 5.1 billion dollars of
Revenue in the last quarter this is a
clear indication of how much faith and
trust people have in AWS they occupy
more than 40 of the market which is a
significant chunk of the cloud computing
Market they have at least 100 services
that are available at the moment which
means just about every issue that you
have can be solved within AWS service
now that was great but now can we talk
about gcp I hope you know that GCB was
launched very recently in 2011 and it is
already helping businesses significantly
with a suite of intelligent secure and
flexible Cloud Computing Services it
lets you build deploy and scale
applications websites services on the
same infrastructure as Google the
intuitive of user experience that gcp
provides with dashboards Wizards is way
better in all the aspects gcp has just
stepped in the market and it is already
offering a modest number of services and
the number is rapidly increasing and the
cost for a CPU instance or Regional
storage that JCP provides is a whole lot
cheaper and you also get a
multi-regional cloud storage now what do
you have to say on that I'm so glad you
asked let's look at present day in fact
let's look at the cloud market share of
the fourth quarter of 2017. this will
tell you once and for all that AWS is
the leader when it comes to cloud
computing Amazon web services
contributes 47 of the market share
others like Rackspace or Verizon cloud
contribute 36 Microsoft Azure
contributes 10 the Google Cloud platform
contributes four percent and IBM
software contributes three percent 47 of
the market share is contributed by AWS
you need me to convince you any more
wait wait wait all that is fine but we
only started a few years back and have
already grown so much in such a less
amount of time haven't you heard the
latest news our revenue is already a
billion dollars per quarter wait for a
few more years and the world shall see
and AWS makes 5.3 billion dollars per
quarter it's going to take a good long
time before you can even get close to us
yes yes we'll see now let's compare a
few things for starters let's compare
prices for AWS a compute instance of two
CPUs in 8GB ram costs approximately 68
US Dollars now a computer instance is a
virtual machine in which you can specify
what operating system Ram or storage you
want to have for cloud storage it costs
2.3 cents per GB per month with AWS you
really want to do that because gcp wins
this hands down let's take the same
compute instance of two CPUs with 8GB
Ram it will cost approximately 50
dollars per month with gcp and as per my
calculations that's a 25 annual cost
reduction when compared to AWS talking
about cloud storage costs it is only 2
cents per GB per month with gcp what
else do you want me to say let's talk
about market share and options now AWS
is the current market leader when it
comes to cloud computing now as you
remember we contribute at least 47 of
the entire market share AWS also has at
least 100 services available at the
moment which is a clear indication of
how well AWS understands businesses and
helps them grow yeah that's true but you
should also know that gcp is steadily
growing we have over 60 services that
are up and running as you can see here
and a lot more to come it's only a
matter of time when we will have as many
services as you do many companies have
already started adopting gcps a cloud
service provider now let's talk about
things you get for free with AWS you get
access to almost all the services for an
entire year with usage limits now these
limits include an hourly or by the
minute basis for example with Amazon ec2
you get 750 hours per month you also
have limits on the number of requests to
services for example with AWS Lambda you
have 1 million requests per month now
after these limits across you you get
charged standard rates with gcp you get
access to all Cloud platform products
like Firebase the Google Maps API and so
on you also get 300 in credit to spend
over a 12 month period on all the cloud
platform products and interestingly
after the free trial ends you won't be
charged unless you manually upgrade to a
paid account now there is also the
always free version for which you will
need an upgraded billing account here
you get to use a small instance for free
and 5 GB of cloud storage any usage
above this always free usage limits will
be automatically billed at standard
rates now let's talk about how you can
configure instances with AWS the largest
instances offered is of 128 CPUs and 4
tbs of ram now other than the on-demand
method like I mentioned before you can
also use Spartan sensors now these are
for situations where your application is
more fault tolerant and can handle an
interruption now you pay for the spot
price which is effective at a particular
hour now these spot prices do fluctuate
but are adjusted over a period of time
the largest instance offered with Google
cloud is 160 CPUs and 3.75 TBS Ram like
spot instances of AWS Google Cloud
offers short-lived compute instances
suitable for back jobs and fault
tolerant workloads they are called as
preemptable instances so these instances
are available at 80 percent off on
on-demand price hence they've reduced
your compute engine costs significantly
and unlike AWS these come at a fixed
price Google Cloud platform is a lot
more flexible when it comes to instance
configuration you simply choose your CPU
and RAM combination of course you can
even create your own instance types this
way before we wrap it up let's compare
on some other things as well Telemetry
it's a process of automatically
collecting periodic measurements from
remote devices for example GPS gcp is
obviously better because they have
Superior Telemetry tools which help in
analyzing services and providing more
opportunities for improvement when it
comes to application support AWS is
obviously better since they have years
of experience under their belt AWS
provides the best support that can be
given to the customers containers are
better with gcp a container is a virtual
process running in user space as
kubernetes was originally developed by
Google gcp has full native support for
the tool other cloud services are just
fine-tuning a way to provide kubernetes
as a service also the containers help
with abstracting applications from their
environment they originally run it the
applications can be deployed easily
regardless of their environment when it
comes to geographies AWS is better since
they have a head start of a few years
AWS in this span of time has been able
to cover a larger market share and
geographical locations now it's time for
the big decision so who's going to be
yeah who is it going to be gcp or AWS I
think I'm going to go for choosing the
right cloud computing platform is a
decision that's made on the basis of the
user or the organization's requirement
on that that note I believe it's time
for us to wrap this video up we hope you
guys enjoyed this video and learned
something new that AWS is better right
know that choosing a platform entirely
depends on you and your organization's
requirement if you're interested you
could also go through the AWS versus
visual video AWS says you're in gcp are
three of the world's largest cloud
service providers but how are they
different from each other let's find out
hey guys I'm Rahul and I'll be
representing Amazon web services I'm
chinmayin I'll be representing Microsoft
Azure and I'm Shruti and I'll be
representing Google Cloud platform so
welcome to this video on AWS vs Azure
versus gcp talking about market share
Amazon web services leads with around 32
percent of the worldwide public cloud
share Azure owns up to 16 of the
worldwide market share and gcp owns
around nine percent of the world's
market share let's talk about each of
these service providers in detail AWS
provides services that enable users to
create and deploy applications over the
cloud these services is that accessible
via the Internet AWS being the oldest of
the lot was launched in the year 2006.
Azure launched in 2010 is a Computing
platform that offers a wide range of
services to build manage and deploy
applications on the network using tools
and Frameworks launched in the year 2008
gcp offers application development and
integration services for its end users
in addition to Cloud management it also
offers services for Big Data machine
learning and iot now let's talk about
availability zones these are isolated
locations within data center regions
from which public cloud services
originate and operate talking about AWS
they have 69 availability zones within
22 geographical regions this includes
regions in the United States South
America Europe and Asia Pacific they are
also predicted to have 12 more editions
in the future Azure available in 140
countries has over 54 regions worldwide
grouped into six geographies these
geographical locations have more than
100 data centers gcp is available in 200
plus countries across the world as of
today gcp is present in 61 zones and 20
regions with Osaka and Zurich being the
newly added regions now let's talk about
pricing these Services follow the pay as
you go approach you pay only For The
Individual Services you need for as long
as you use them without requiring
long-term contracts or complex licensing
now on screen you can see the pricing
for each of these cloud service
providers with respect to various
instances like general purpose compute
optimized memory optimized and GPU now
let's talk about the compute services
offered first off we have virtual
servers for AWS we have ec2 it is a web
service which eliminates the need to
invest in Hardware so that you can
develop and deploy applications in a
faster manner it provides virtual
machines in which you can run your
applications azure's virtual machines is
one of the several types of computing
resources that Azure offers Azure gives
the user the flexibility to deploy and
manage a virtual environment inside a
virtual Network G tcp's VM service
enables users to build deploy and manage
virtual machines to run workloads on the
cloud now let's talk about the pricing
of each of these Services awcc2 is free
to try it is packaged as part of aws's
free tier that lasts for 12 months and
provides 750 hours per month of both
Linux and Windows Virtual machines Azure
virtual machine service is a part of the
free tier that offers this service for
about 750 hours per month for a year the
user gets access to Windows and Linux
virtual machines gcp's VM service is a
part of a free tier that includes micro
instance per month for up to 12 months
now let's talk about platform as a
service or past services for AWS elastic
Beanstalk is an easy to use service for
deploying and scaling web applications
and services developed with java.net
node.js python and much more it is used
for maintaining capacity provisioning
load balancing Auto scaling and
application Health monitoring the past
backbone utilizes virtualization
techniques with the virtual machine is
independent of the actual Hardware that
hosts it hence the user can write
application code without worrying about
the underlying Hardware Google app
engine is a cloud computing platform as
a service which is used by Developers
for hosting and building apps in Google
data centers the app engine requires the
apps to be written in Java or Python and
store data in Google bigtable and use
the Google query language for this next
let's talk about virtual private server
Services AWS provides light sale it
provides everything you need to build an
application or website along with the
cost effective monthly plan and minimum
number of configurations in simple words
VM image is a more comprehensive image
for Microsoft Azure virtual machines it
helps the user create many identical
virtual machines in a matter of minutes
unfortunately gcp does not offer any
similar service next up we have
serverless Computing Services AWS has
Lambda it is a serverless compute
service that lets you run your code
without facilitating and managing
servers you only pay for the compute
time you use it is used to execute
backend code and scales automatically
when required Azure functions is a
serverless compute service
to run even triggered code without
having to
structure this allows the users to build
applications using serverless simple
functions with the programming language
of their choice gcp Cloud functions make
it easy for developers to run and scale
code in the cloud and build image driven
serverless applications it is highly
available and fault tolerant now let's
talk about storage services offered by
each of these service providers First
off we have object storage AWS provides
S3 it is an object storage that provides
industry standard scalability data
availability and performance it is
extremely durable and can be used for
storing as well as recovering
information or data from anywhere over
the Internet blob storage is an Azure
feature that lets developers store
unstructured data in Microsoft's Cloud
platform along with storage it also
offers scalability it stores the data in
the form of tiers depending on how often
data is being accessed Google Cloud
Storage is an online storage web service
for storing and accessing data on Google
Cloud platform infrastructure unlike the
Google Drive Google cloud storage is
more suitable for Enterprises it also
stores objects that are organized into
buckets Amazon provides EBS or elastic
Block store it provides high performance
block storage and is used along with ec2
instances for workloads that are
transaction or throughput intensive
Azure managed disk is a virtual hard
disk you can think of it like a physical
disk in an on-premises server but
virtualized these managed disks allow
the users to create up to 10 000 VM
disks in a single subscription
persistent storage is a data storage
device that retains data after power to
the device is shut off Google persistent
disk is durable and high performance
block storage for gcp persistent disk
provides storage which can be attached
to instances running in either Google
compute engine or kubernetes engine next
up we have Disaster Recovery Services
AWS provides a cloud-based recovery
service that ensures that your it
infrastructure and data are recovered
while minimizing the amount of downtime
that could be experienced used site
recovery helps ensure business
continuity by keeping business apps and
workloads running during outages it
allows recovery by orchestrating and
automating the replication process of
azure virtual machines between regions
unfortunately gcp has no disaster
recovery service next let's talk about
database Services first off for AWS we
have RDS or relational database service
it is a web service that's cost
effective and automates administration
tasks basically it simplifies the setup
operation and scaling of a relational
database Microsoft Azure SQL database is
a software as a service platform that
includes built-in intelligence that
learns app patterns and adapts to
maximize performance reliability and
data protection it also eases the
migration of SQL Server databases
without changing the user's applications
Cloud SQL is a fully managed database
service which is easy to set up maintain
and administer relational postgresql
MySQL and SQL Server databases in the
cloud hosted on gcp cloud SQL provides a
database infrastructure for applications
running anywhere next we have nosql
database Services AWS provides dynamodb
which is a managed durable database that
provides security backup and restore and
in-memory caching for applications it is
well known for its low latency and
scalable performance Azure Cosmos DB is
Microsoft's globally distributed
multi-model database service it natively
supports nosql it natively supports
nosql created for low latency and
scalable applications gcp cloud data
store is a nosql database service
offered by Google on the gcp it handles
replication and scales automatically to
your applications load with cloud data
stores interface data can easily be
accessed by any deployment Target now
let's talk about the key Cloud tools for
each of these service providers for AWS
in networking and content delivery we
have AWS Route 53 and AWS cloudfront for
management we have AWS Cloud watch and
AWS cloud formation for development we
have AWS code star and AWS code build
for security we have IAM and Key
Management Service for Microsoft Azure
networking and content delivery we have
content delivery Network and express
route for management tools we have Azure
advisor and network Watcher for
development tools for management we have
Azure advisor and network Watcher for
development we have Visual Studio IDE
and Azure blob studio for security we
have Azure security Center and Azure
active directory for gcp we have the
following tools for networking and
content delivery we have Cloud CDN and
Cloud DNS for management we have
stackdriver and gcp monitoring for
development we have Cloud build and
Cloud SDK and finally for security we
have Google cloud IM and Google and
Cloud security scanner now let's talk
about the companies using these Cloud
providers for AWS we have Netflix
Unilever Kellogg's NASA Nokia and Adobe
Pixar Samsung eBay Fujitsu EMC and BMW
among others use Microsoft so as seen on
your screens the companies that use gcp
are Spotify HSBC Snapchat Twitter PayPal
and 20th Century Fox let's talk about
the advantages of each of these Services
Amazon provides Enterprise friendly
Services you can leverage Amazon's 15
years of experience delivering
large-scale Global infrastructure and it
still continues to hone and innovate its
infrastructure management skills and
capabilities secondly it provides
instant access to resources AWS is
designed to allow application providers
isvs and vendors to quickly and securely
host your applications whether an
existing application or a new SAS based
application Speed and Agility AWS
provides you access to its services
within minutes all you need to select is
what you require and you can proceed you
can access each of these applications
anytime you need them and finally it's
secure and reliable Amazon enables you
to innovate and scale your application
in a secure environment it secures and
hardens your infrastructure more
importantly it provides security at a
cheaper cost than on-premise
environments now talking about some of
the advantages of azure Microsoft Azure
offers better development operations it
also provides strong security profile
Azure has a strong focus on security
following the Standard Security model of
detect assess diagnose stabilize and
close Azure also provides a
cost-effective solution the cloud
environment allows businesses to launch
both customer applications and internal
apps in the cloud which saves an I.T
infrastructure costs hence it's Opex
friendly let's now look at the
advantages of gcp Google bills in minute
level increments so you only pay for the
compute time you use they also provide
discounted prices for long-running
workloads for example you use the VM for
a month and get a discount gcp also
provides live migration of virtual
machines live migration is the process
of moving a running VM from one physical
server to another without disrupting its
availability to the users this is a very
important differentiator for Google
Cloud compared to other Cloud providers
gcp provides automatic scalability this
allows a site container scale to as many
CPUs as needed Google Cloud Storage is
designed for 99.9 durability it creates
server backup and stores them in an user
configured location let's talk about the
disadvantages of each of these services
for AWS there's a limitation of the ec2
service AWS provides limitations on
resources that vary from region to
region there may be a limit to the
number of instances that can be created
however you can request for these limits
to be increased secondly they have a
technical support fee AWS charges you
for immediate support and you can opt
for any of these packages developer
which costs 29 per month business which
costs more than 100 an Enterprise that
costs more than fifteen thousand dollars
it has certain network connectivity
issues it also has General issues when
you move to the cloud like downtime
limited control backup protection and so
on however most of these are temporary
issues and can be handled over time
talking about some of the disadvantages
of Microsoft Azure code base is
different when working offline and it
requires modification when working on
the cloud pass across system is not as
efficient as iaas Azure Management
console is frustrating to work with it
is slow to respond and update and
requires far too many clicks to achieve
simple tasks Azure backup is intended
for backing up and restoring data
located on your on-premises servers to
the cloud that's a great feature but
it's not really useful for doing bare
metal industry stores of servers in a
remote data center let's now look into
the disadvantages of gcp so when it
comes to Cloud providers the support fee
is very minimal but in the case of gcp
it is quite costly it is around 150
dollars per month for the most basic
service similar to AWS S3 gcp has a
complex pricing schema also it is not
very budget friendly when it comes to
downloading data from Google Cloud
Storage also if you want to master the
field of cloud computing the
postgraduate program in cloud computing
designed in collaboration with Caltech
ctme helps you become an Azure AWS and
gcp expert this in-depth cloud computing
certification course lets you master key
architecture natural principles and
develop the skills needed to become a
cloud expert benefits of this
postgraduate program includes Caltech
ctme postgraduate certificate enrollment
and simply learns job assist you will
receive up to 30 CEUs from Caltech ctme
you'll attend master classes from
Caltech ctme instructors live virtual
classes led by industry experts Hands-On
projects and integrated Labs online
convocation by Caltech ctme program
director 40 plus Hands-On projects and
integrated Labs Capstone project in four
different domains and Caltech ctma
Circle membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms and services such as AWS
Lambda Amazon S3 Azure app services and
many more so this program also covers
the essential skills to become an expert
in cloud computing and you can consider
this program because it provides you
with a certification from Caltech ctme
well reputed university in the United
States so what are you waiting for check
out the link mentioned in the
description box below and start your
cloud computing career today so without
any further delay let's get started AWS
certifications so what exactly is an AWS
certification it represents a degree of
expertise in AWS it is obtained after
passing one or more exams provided by
Amazon and each different role like
thesis Ops administrator developer or
Solutions architect has a different exam
associated with it this in turn helps
employees demonstrate and validate
technical Cloud knowledge which means
that the people who are certified
actually know what they're talking about
I have seen a common misconception on
people where they believe that just
because they're certified they're
entitled a job that's not true now
without proper practice or hands-on
experience the certification is wasted
on you so remember just because you're
certified does not mean you'll get a job
it only makes the job application
process slightly easier related to
someone who's not been certified now why
is AWS given so much importance some of
the important reasons are an AWS
certified individual gains credibility
for their skills in AWS they also gain
access to free practice exams which help
them prepare for the next awf
certification there's an increase in
monetary compensations which only means
that they get paid more and you gain
recognition for your Knowledge and
Skills in AWS right now we're in the AWS
certification website whose link will be
in the description and now we're going
to talk about the types of AWS
certification as you can see here there
are three levels of aw certification
there's the foundational level associate
level and professional level
certification now the foundational level
certification only requires you to have
a basic understanding of how the AWS
Cloud works for the aw
practitioner is optional for the
architect path developer path and
operations path it is mandatory for the
specialty certifications like the
advanced networking big data and
security certifications now the
associate level certifications are
mid-level certifications for a technical
role now a professional certification is
the highest level of certification that
you can have for a technical role now
you have the solutions architect for the
architect path and the devops engineer
certification for both the developer and
operations path so how do you decide
which of these certifications is
suitable for you so you've seen here
that AWS provides various certifications
for a number of job roles exercise
administrator solution architect
developer so you need to make the right
choice taking into consideration the
areas of your interest and the
experience level that you have now we're
going to talk about each of these
certifications in detail so first let's
talk about the AWS certified Cloud
practitioner now we all understand that
AWS is a widely recognized product in
the market so this certification helps
you validate how well you know the AWS
Cloud so this is just the basic
understanding now it is optional for the
developer path and the operations path I
would suggest it's a good idea to start
here because it forms a solid Bedrock on
all the other things that you're going
to learn soon now more importantly it
does not require any technical knowledge
of other roles such as developmental
architecture Administration and so on so
it's a great place to start for
newcomers now you have the architect
role certifications now this is for you
if you are interested in becoming a
Solutions architect or a solution design
engineer or someone who just works with
designing applications or systems on the
AWS platform now first we have the AWS
certified Solutions architect associate
level certification now this
certification is for you if you want to
show off how well you can architect and
deploy applications on the AWS platform
now it is recommended that you have at
least a year of experience working with
distributed systems on the AWS platform
at the same time it's also required that
you understand the AWS services and be
able to recommend a service based on
requirements you need to be able to use
architectural best practices and you
need to estimate the AWS cost and how
you can reduce them next up you have the
AWS certified Solutions architect take
professional level certification now you
will not get the certification unless
you're done with the aw certified
Solutions architect associate level
certification this is the show of your
technical skills and experience in
designing distributed applications on
the AWS platform now this does require
you to have two years of experience
working with Cloud architecture on AWS
at the same time it also requires you to
be able to evaluate requirements and
then make architectural recommendations
you also need to provide guidance on the
best practices on architectural design
across a number of different platforms
the developer level certifications are
for you if you are interested in
becoming a software developer now the
AWS certified developer associate
certification is to test how well you
know how to develop and maintain
applications on the AWS platform it does
require you to have a year or more of
hands-on experience to design and
maintain AWS based applications like any
software developer role it is necessary
that you know in depth at least one high
level programming language it's also
necessary that you understand the core
of AWS Services uses and basic
architectural best practices you need be
able to design develop and deploy
cloud-based Solutions on AWS platform
and you need to understand how
applications can be created you need to
have experience in developing and
maintaining applications for a number of
AWS services like Amazon SNS dynamodb
sqs and so on now for the AWS certified
devops engineer professional level
certification note here that this
certification is exactly the same as the
one you have under the operation sold so
both of them are the same thing so here
it tests your ability to create operate
and manage distributed applications on
the AWS platform now it is necessary or
it is mandatory to have the AWS
certified developer associate
certification or the AWS certified
sysops administrator certification with
two or more years of hands-on experience
in doing the same in AWS environment it
requires you to be able to develop code
in at least one high level language you
need to be able to automate and test
applications via scripting and
programming and to understand agile or
other development processes the
operation certifications are for you if
you want to become a cisops
administrator a systems administrator or
someone in devops role who wants to
deploy applications networks and systems
in an automatable and repeatable way the
AWS certified Sops administrator
associate certification tests your
knowledge in deployment management and
operations on the AWS platform now you
need to have one or more years of
hands-on experience in AWS based
applications you need to be able to
identify and gather requirements then
Define a solution to be operated on AWS
you need to be able to provide guidance
for the best practices through the life
cycle of a project as well now the
specialty certifications are for you if
you're well versed in AWS and want to
showcase your expertise in other
technical areas the aw certified Big
Data certification showcases your
ability to design and Implement AWS
Services which can help derive value
from a large amount of complex data you
are however required to have completed
the foundational or associate level
certification before you can attempt
this you need a minimum of five years of
hands-on experience in the data
analytics field as well next we have the
aw certified Advanced networking
certification this validates your
ability to design and Implement a
Solutions as well as other hybrid ID
Network architectures at scale this also
requires you to have completed the
foundational or associate level
certification you need to have a minimum
of five years of hands-on experience
architecting and implementing Network
Solutions and lastly we have the AWS
certified security certification it
helps showcase your ability to secure
the AWS platform you're required to have
an associate or Cloud practitioner level
of certification a minimum of five years
of I.T security experience and two years
of hands-on experience securing AWS
workloads now say I wanted to schedule
an examination so for example I want to
do the solutions architect certification
so first I would go there now here I can
click on register now and the process
continues or I can click on learn more
by doing this again I can show you the
examination here I can also get access
to other data like the number of
questions available the cost of an
examination the portions I need to study
and so on now let's talk about Solutions
architect certification with a little
more detail now this certification exam
cost 150 US Dollars and the practice
exam cost 20 US Dollars now here I can
schedule the exam examination or
download the exam guide I've already
downloaded the exam guide and here it is
now this exam guide tells you about what
you need to learn and what is expected
from you here they want you to define a
solution based on requirements and
provide guidance in its implementation
it is also recommended that you know
about how the AWS Services work one
years of hands-on experience with
distributed systems on AWS to identify
and Define technical requirements and so
on the rest is available in the exam
guide and most importantly they tell you
the main content domains and their
weightages now we have five domains
first domain is to design resilient
architectures which holds 34 percent of
weightage at domain two you have to
Define performant architectures three is
to specify secure applications and
architectures cost optimized
architectures and five to define
operationally excellent architectures
now like you've seen here you've
selected one certification and learned
it in detail you can do the same for any
of these other certifications you can
press learn more and download their exam
guide and learn everything that you need
to know there were over 18 million jobs
in the field of cloud computing globally
simply learns AWS certified solution
architect associate level course will
help you understand the key Concepts
required to prepare for the
certification exam
the practice assignments and the three
live projects this course comprises of
24 hours of instructor-led training 15
hours of e-learning over 50
demonstrations to understand
architecting through the AWS console and
you will also be provided with three
question sets totaling 180 questions the
worldwide cloud computing Market grew 28
to 110 billion dollars in revenues in
2015.
in the last quarter of 2015 AWS Revenue
was up 69 year on year AWS Solutions
architect associate level is the perfect
certification to start your journey with
AWS you'll enhance your existing
knowledge of AWS and will deepen your
understanding of every product and
service that AWS offers you will learn
how to design plan and scale in the AWS
Cloud using Amazon recommended best
practices with the certification you'll
be an asset to any organization helping
it to leverage the benefits of best
practices around Advanced cloud-based
Solutions and migrate existing workloads
to the cloud
whether you're looking to Future proof
your career or just prove you'll know
how about the leading cloud provider in
the global market becoming an AWS
Solutions architect associate will put
you well on your way to achieving your
career goals you can reach out to our
teaching assistant support staff anytime
during the course and For assistance
with projects
candidates can submit their queries that
help and support on our website or they
can get connected with our staff using
simply talk and the live chat options
this concludes the course introduction
and we hope you're all geared up to
begin the AWS Solutions architect
associate level course
also if you want to master the field of
cloud computing the postgraduate program
in cloud computing design in
collaboration with Caltech ctme helps
you become an Azure AWS and gcp expert
this in-depth cloud computing
certification course lets you master key
architectural principles and develop the
skills needed to become a cloud expert
benefits of this postgraduate program
includes Caltech ctme postgraduate
certificate enrollment and simple learns
job assist you will receive up to 30
CEUs from Caltech ctme you'll attend
master classes from Caltech ctme
instructors live virtual classes led by
industry experts Hands-On projects and
integrated Labs online convocation by
Caltech ctme program director 40 plus
Hands-On projects and integrated Labs
Capstone project in four different
domains and Caltech ctma Circle
membership
you can also fast track your career with
this cloud computing certification and
its in-depth course curriculum which
covers the key concepts of Microsoft
Azure Amazon web services and Google
Cloud platforms and services such as AWS
Lambda Amazon S3 Azure app services and
many more so this program also covers
the essential skills to become an expert
in cloud computing and you can consider
this program because it provides you
with a certification from Caltech ctme
well reputed university in the United
States so what are you waiting for check
out the link mentioned in the
description box below and start your
cloud computing career today so without
any further delay let's get started so
in an environment where there's a lot of
automation infrastructure automation
you'll be posted with this question how
can you add an existing instance to a
new auto scaling group now this is when
you are taking an instance away from the
auto scaling group to troubleshoot to
fix a problem you know to look at logs
or if you have suspended the auto
scaling you know you might need to
re-add that instance to the auto scaling
group only then it's going to take part
in it right only then the auto scaling
is going to count it has part of it it's
not a straight procedure you know when
you remove them you know it doesn't get
automatically re-added I've had worked
with some clients when their developers
were managing their own environment they
had problems adding the instance back to
the auto scaling group you know
irrespective of what they tried the
instance was not getting added to the
order scaling group and whatever they
fixed that they were provided or
whatever fix that they have provided
where now not you know getting
encountered in the auto scaling group so
like I said it's not a straight you know
a click button procedure there are ways
we'll have to do it so how can you add
an existing instance uh to the auto
scaling group there are a few steps that
we need to follow so the first one would
be to under the ec2 instance console
right under the instance under actions
in specific you know there's an option
called attach to Auto scaling group
right if you have multiple Auto scaling
groups in your account or in the region
that you're working in then you're going
to be posted with the different Auto
scaling groups that you have in your
account let's say you have five other
scaling groups for five different
application you know you're going to be
posted with five different or scaling
groups and then you would select the
auto scaling the appropriate Auto
scaling group and attach the instance to
that particular or a scaling group while
adding to the auto scaling group if you
want to change the instance type you
know that's possible as well sometimes
times when you want to add the instance
back to the auto scaling group there
would be requirement that you change the
instance type to a better one to a
better family to the better instance
type you could do that at that time and
after that you are or you have
completely added the instance back to
the orus killing group so it's actually
an seven step process adding an instance
back to the auto scaling group in an
environment where they're dealing with
migrating the instance or migrating an
application or migrating an instance
migrating and VM into the cloud you know
if the project that you're going to work
with deals with a lot of migrations you
could be posted this question what are
the factors you will consider while
migrating to Amazon web services the
first one is cost is it worth moving the
instance to the cloud given the
additional bills and whistles features
available in the cloud is this
application going to use all of them is
moving into the cloud beneficial to the
application in the first place you know
beneficial to the users who will be
using the application in the first place
so that's a factor to think of so this
actually includes you know cost of the
infrastructure and the ability to match
the demand and Supply transparency is
this application in high demand you know
is it going to be a big loss if the
application becomes unavailable for some
time so there are few things that needs
to be considered before we move the
application to the cloud and then if the
application does the application needs
to be provisioned immediately is there
an urge is there an urge to provision
the application immediately that's
something that needs to be considered if
the application requires to go online if
the application needs to hit the market
immediately then we would need to move
it to the cloud because in on-premises
procuring or buying an infrastructure
buying the bandwidth buying the
switchboard you know buying an instance
you know buying their softwares buying
the license relay to it it's going to
take time at least like two weeks or so
before you can bring up an server and
launch an application in it right so if
the application cannot wait you know
waiting means uh you know Workforce
productivity loss is it so we would want
to immediately launch instances and put
application on top of it in those case
if your application is of that type if
there is the urge in making the
application go online as soon as
possible then that's a candidate for
moving to the cloud and if the
application or if the the software or if
the product that you're launching it
requires Hardware it requires an updated
Hardware all the time that's not going
to be possible in on-premises we try to
deal with Legacy infrastructure all the
time in on-premises but in the cloud
they're constantly upgrading their hard
ways only then they can keep themselves
up going in the market so they
constantly the cloud providers are
constantly updating their hard ways and
if you want to be benefited of your your
application wants to be benefited by the
constant upgrading of the Hardwares
making sure the hardware is as latest as
possible the software version the
licensing is as latest as possible then
that's a candidate to be moved to the
cloud and if the application does not
want to go through any risk if the
application is very sensitive to
failures if the application is very much
stacked to the revenue of the company
and you don't want to take a chance in
you know seeing the application fail and
you know seeing the revenue drop then
that's a candidate for moving to the
cloud and business agility you know
moving to the cloud at least half of the
responsibility is now taken care by the
provider in this case it's Amazon at
least half of the responsibility is
taken care by them like if the hardware
fails Amazon makes sure that they're
fixing the hardware immediately and
notifications you know if something
happens you know there are immediate
notifications available that we can set
it up and make yourself aware that
something has broken and we can
immediately jump in and fix it so you
see there are the responsibility is now
being shared between Amazon and us so if
you want to get that benefit for your
application for your organization for
the product that you're launching then
it needs to be moved to the cloud so you
can get that benefit from the cloud the
other question you could get asked is
what is RTO and RPO in AWS they are
essentially Disaster Recovery terms when
you're planning for Disaster Recovery
you cannot avoid planning disaster
recovery without talking about RTO and
RPO now what's the RTO what's the RPO in
your environment or how do you define
RTO how do you define RPO or some
general questions that get asked RTO is
recovery time objective RTO stands for
the maximum time the company is willing
to wait for the recovery to happen or
for the recovery to finish when an
disaster strikes so RTO is in the future
right how much time is it going to take
to fix and bring everything to normal so
that's rtu on the other hand RPO is
recovery Point objective which is the
maximum amount of data loss your company
is willing to accept as measured in time
RPO always refers to the backups the
number of backups the the frequency of
the backups right because when an outage
happens you can always go back to the
latest backup right and if the latest
backup was before 12 hours you have lost
the in between 12 hours of data data
storage right so RPO is the acceptable
amount if the company wants less RPO RPO
is one R then you should be planning on
taking backups everyone up if RPO is 12
hours then you should be planning on
taking backups every 12 hours so that's
how RPO and RTO you know helps Disaster
Recovery the fourth question you could
get asked is if you would like to
transfer huge amount of data which is
the best option among snowball snowball
Edge and snowmobile again this is a
question that get if the company is
dealing with a lot of data transfer into
the cloud or if the company is dealing
with the migrating data into the cloud
I'm talking about a huge amount of data
data in petabytes snowball and all of
the snowball series deals with the
petabyte sized data migrations so there
are three options available as of now
AWS snowball is an data transport
solution for moving high volume of data
into and out of a specified AWS region
on the other hand AWS snowball Edge adds
additional Computing functions snowball
is simple storage and movement of data
and snowball Edge has a compute function
attached to it a snowmobile on the other
hand is an exabyte scale migration
service that allows us to transfer data
up to 100 petabytes that's like 100 000
terabytes so depending on the size of
data that we want to transfer from our
data center to the cloud we can hire we
can rent any of these three services
let's talk about some cloud formation
questions this is a classic question how
is AWS cloud formation different from
AWS elastic bean stock you know from the
surface they both look like the same you
know you don't go through the console
provisioning resources you don't you
know you don't go through CLI and
provision resources both of them
provision resources through click button
right but underneath they are actually
different Services they support they aid
different services so knowing that is
going to help you understand this
question a lot better let's talk about
the difference between them and this is
what you will be explaining to the
interviewer or the panelist so the cloud
formation in the cloud formation service
helps you describe and provision all the
infrastructure resources in the cloud
environment on the other hand elastic
bean stock provides an simple
environment to which we can deploy and
run application cloud formation gives us
an infrastructure and elastic bean stock
gives us a small contained environment
in which we can run our application and
cloud formation supports the
infrastructure needs of many different
types of application like the Enterprise
application the Legacy applications and
any new modern application that you want
to have in the cloud on the other hand
the elastic bean stock It's a
combination of developer tools they are
tools that helps manage the life cycle
of a single application so cloud
formation in short is managing the
infrastructure as a whole and elastic
Beanstalk in short is managing and
running an application in the cloud and
if the company that you're getting hired
is using cloud formation to manage their
infrastructure using or if they're using
infrastructure or any of the
infrastructure as a code Services then
you would definitely face this question
what are the elements of an AWS cloud
formation template so it has a four or
five basic elements right and the
template is in the form of Json or in
yaml format right so it has parameters
it has outputs it has data it has
resources and then the format or the
format version or the file format
version for the cloud formation template
so parameter is nothing but it actually
lets you to specify the type of easy to
instance that you want the type of RDS
that you want all right so ec2 is an
umbrella RDS is an umbrella and
parameters within that ec2 and
parameters Within than rdas are the
specific details of the ec2 or the
specific details of the RDS service so
that's what parameters in a cloud
formation template and then the element
of the cloud formation template is
outputs for example if you want to
Output the name of an S3 bucket that was
created if you want to Output the name
of the ec2 instance if you want to
Output the name of some resources that
have been created instead of looking
into the template instead of you know
navigating through in the console and
finding the name of the resource we can
actually have them outputted in the
result section so we can simply go and
look at all the resources created
through the template in the output
section and that's what output values or
output does in the cloud formation
template and then we have resources
resources are nothing but what defines
what are the cloud components or Cloud
resources that will be created through
this cloud formation template now ec2 is
a resource RDS is a resource and S3
bucket is a resource elastic load
balancer is a resource and the NAT
Gateway is a resource VPC is a resource
so you see all these components are the
resources and the resource section in
the cloud formation defines what are the
AWS Cloud resources that will be created
through this cloud formation template
and then we have a version a version
actually identifies the capabilities of
the template you know we just need to
make sure that it is of the latest
version type and the latest version is
0909 2010 that's the latest version
number you'll be able to find that on
the top of the cloud formation template
and that version number defines the
capabilities of the cloud formation
template so just need to make sure that
it's the latest all the time still
talking about cloud formation this is
another classic question what happens
when one of the resource in a stack
cannot be created successfully well if
the the resource in a stack cannot be
created the cloud formation
automatically rolls back and terminates
all the resources that was created using
the cloud formation template so whatever
resources that were created through the
cloud formation template from the
beginning let's say we have created like
10 resources and the 11th resource is
now failing cloud formation will roll
back and delete all the 10 resources
that were created previously and this is
very useful when the cloud formation
cannot you know go forward cloud
formation cannot create additional
resources because we have reached the
elastic IP limits elastic IP limit per
region is 5 right and if you have
already used five IPS and a cloud
formation is trying to buy three more
IPS you know we've hit the soft limit
till we fix that with Amazon cloud
formation will not be able to you know
launch additional you know resources and
additional IPS so it's going to cancel
and roll back everything that's through
with a missing ec2 Ami as well if an Ami
is included in the template and but the
Ami is not actually present then cloud
formation is going to search for the Mi
and because it's not present it's going
to roll back and delete all the
resources that it created so that's what
cloud formation does it simply rolls
back all the resources that it created I
mean if it sees a failure it would
simply roll back all the resources that
it created and this feature actually
simplifies the system administration and
layered Solutions built on top of AWS
cloud formation so at any point we know
that there are no orphan resources in
the in in our environment you know
because something did not work or
because there was an you know cloud
formation executed some there are no
orphan resources in our account at any
point we can be sure that if cloud
formation is launching a resource and if
it's going to fail and it's going to
come back and delete all the resources
it's created so there are no off
resources in that account now let's talk
about some questions in elastic Block
store again if the environment deals
with a lot of automation you could be
thrown this question how can you
automate easy to backup using EBS it's
actually a six step process to automate
the ec2 backups we'll need to write a
script to automate the below steps using
AWS API and these are the steps that
should be found in the scripts first get
the list of instances and then and then
the script that we are writing should be
able to connect to AWS using the API and
list the Amazon ABS volumes that are
attached locally to the instance and
then it needs to list the snapshots of
each volume make sure the snapshots are
present and it needs to assign a
retention period for the snapshot
because over time the snapshots are
going to be invalid right once you have
some 10 latest snapshots any snapshot
that you have taken before that 10
becomes invalid because you have
captured the latest and 10 snapshot
coverage is enough for you and then the
fifth point is to create a snapshot of
each volume create a new snapshot of
each volume and then delete the old
snapshot anytime a new snapshot gets
created the oldest snapshot the list
needs to go away so we need to include
options we need to include scripts in
our script lines in our script that make
sure that it's deleting the older
snapshots which are older than the
retention period that we are mentioning
another question that you could see in
the interview be it written interview
beat online interview or beat and
telephonic or face to face interview is
what's the difference between EBS and
instance store let's talk about EBS
first EBS is kind of permanent storage
the data in it can be restored at a
later point when we save data in ABS the
data lives even after the lifetime of
the ec2 instance for example we can stop
the instance and the data is still going
to be present in EBS we can move the EBS
from one instance to another instance
and the data is simply going to be
present there so ABS is kind of
permanent storage when compared to
instance on the other hand instance
store is a temporary storage and that
storage is actually physically attached
to the host of the machine abs is an
external storage an instant store is
locally attached to the instance or
locally attached to the host of The
Machine we cannot detach an instant
store from one instance and attach it to
another but we can do that with EBS so
that's a big difference one is permanent
data and another one is EBS permanent
instant store is a volatile data and
instant store with instant store we
won't be able to detach the storage and
attach it to another instance and
another feature of instant store is data
in an instant store is lost if the disk
fails or the instance is stopped or
terminated so instant store is only good
for storing cache data if you want to
store permanent data then we should
think of using EBS and not instant store
while talking about storage on the same
lines this is another classic question
how can you take backups of EFS like EBS
and if you can take backup how do you
take that backup the answer is yes we
can take EFS to EFS backup solution EFS
does not support snapshot like EBS EFS
does not support snapshot snapshot is
not an option for EFS elastic file
system right we can only take backup
from one EFS to another EFS and this
backup solution is to recover from
unintended changes or deletions of the
EFS and this can be automated right any
data that we store in EFS can be
automatically replicated to another EFS
and once this EFS goes down or gets
deleted or data gets deleted or you know
the whole EFS is for some reason
interrupted or deleted we can recover
the data from we can use the other EFS
and bring the application to consistency
and to achieve this it's not an One Step
configuration it's a cycle there are
series of steps that's involved before
we can achieve EFS to EFS backup the
first thing is to sign in to the AWS
Management console and under EFS or
click on EFS to EFS restore button from
the services list and from there we can
use the region selector in the console
navigation bar to select the actual
region in which we want to work on and
from there ensure that we have selected
the right template you know some of the
templates would be you know EFS to EFS
backup granular backups incremental
backups right so there are some
templates the kind of back backups that
you want to take do you want to take
granular do you want to take increment
backups stuff like that and then create
a name to that solution the kind of
backup that we have created and finally
review all the configurations that you
have done and click on Save and from
that point onwards the data is going to
be copied and from that point onwards
any additional data that you put is
going to copy it and replicate it now
you have an EFS to EFS backup this is
another classic question in companies
which deals with a data management there
are easy options to create snapshots but
deleting snapshots is not always and you
know click button or a single step
configuration so you might be facing a
question like how do you Auto delete old
snapshots and the procedure is like this
as best practice we will take snapshots
of EBS volume to S3 all snapshots get
stored in S3 we know that now and we can
use AWS Ops automator to automatically
handle all snapshots the Ops automator
service it allows us to create copy
delete EBS snapshots so there are cloud
formation templates available for AWS
Ops automator and this automator
template will scan the environment and
it would take snapshots it would you
know copy the snapshot from one region
to another region if you want I know if
you're setting up a Dr environment and
not only that based on the retention
period that we create it's going to
delete the snapshots which are older
than the retention period so life or
managing snapshot is made a lot easier
because of this Ops automator cloud
formation template moving into questions
in elastic load balancer this again
could be an a question in the interview
what are the different types of load
balances in AWS and what's their use
case what's the difference between them
and as of now as we speak there are
three types of load balances which are
available in AWS the first one being
application load balancer just like the
name says the application load balancer
works on the application layer and deals
with the HTTP and https requests and it
it also supports path based routing for
example simplylearn.com
some web page simplylearn.com another
website so it's going to direct the path
based on the slash value that you give
in the URL so that's path based routing
so it supports that and not only that it
can support a port based a colon 8080
colon 8081 or colon 8090 you know based
on that Port also it can take a routing
decision and that's what application
load balancer does on the other hand we
have Network load balancer and the
network load balancer makes routing
decisions and the transport level it's
faster because it has very less thing to
work on it works on Lower OSI layer it
works on a lower layer so it has very
less information to work with than
compared with application layers so
comparatively it's a lot faster and it
handles millions of requests per second
and after the load balancer receives the
connection it selects a Target group for
the default rule using the flow hash
routing algorithm it does simple routing
right it does not do path based or Port
based routing it does simple routing and
because of it it's faster and then we
have classic load balancer which is kind
of expiring as we speak Amazon is
discouraging people using classic load
balancer but there are companies which
are still using classic load balancer
they are the ones over the first one to
step into Amazon when classic load
balancer was the first load balancer or
the only load balancer available at that
point so it supports HTTP https TCP SSL
protocol and it has a fixed relationship
between a load balance report and the
container so initially we only have
classic load balancer and then after
some point Amazon said instead of having
one load balancer address all type of
traffic we're going to have a two load
balances called as the child from the
classic to load balancer and one is
going to specifically address the
application requirement and one is going
to specifically address the network
requirement and let's call it as
application load balancer and network
load balancer so that's how now we have
two different load balances talking
about load balance another classic
question could be what are the different
uses of the various load balancer in AWS
elastic or load balancing there are
three types of load balancer we just
spoke about it application load balancer
is used if we need a flexible
application management and TLS
termination and network load balancer if
we require Extreme Performance and the
load balancing should happen on based on
static IPS for the application and
classic load balancer is an old load
balancer which is for people who are
still running their environment from e
C2 classic Network now this is an older
version of VPC or this is what was
present before VPC was created easy to
Classic network is what was present
before ec2 was created so there are the
three types and they are the use cases
of it let's talk about some of this
security related questions you would
face in the interview when talking about
security and firewall and AWS we cannot
avoid discussion talking about Waf web
application firewall and you would
definitely see yourself in this
situation where you've been asked how
can you use AWS Waf in monitoring your
AWS applications Waf or web application
firewall protects our web application
from common web exploits and Waf helps
us control which traffic Source your
application should be allowed or a Blog
which traffic from a certain Source now
which source or which traffic from a
certain Source should be allowed or
blocked your application with Waf we can
also also create custom rules that
blocks common attack patterns you know
if it is a banking application it has a
certain type of attacks and if it is
simple data management data storage
application it has I mean content
management application it has a separate
type of attack So based on the
application type we can identify a
pattern and create rules that would
actually block that attack based on the
rule that we create and Waf can be used
for three cases you know the first one
is allow all requests and then block all
requests and count all requests for a
new policy so it's also an monitoring
and Management Service which actually
counts all the policies or counts all
the requests that matches a particular
policy that we create and some of the
characteristics we can mention in AWS
web or the origin IPS and the strings
that appear in the request we can allow
block based on Origin IP allow block
based on strings that appear in the
request we can allow block or count
based on the origin country length of
the request yeah we can block and count
the presence of malicious scripts in an
connection you know we can count the
request headers or we can allow block a
certain request header and we can count
the presence of malicious SQL code in a
connection that we get and that want to
reach our application still talking
about security what are the different
AWS IM categories we can control using
AWS IAM we can do the following one is
create and manage IM users and once the
user database gets bigger and bigger we
can create and manage them in groups and
in IM we can use it to manage the
security credentials kind of setting the
complexity of the password you know
setting additional authentications you
know like MFA and you know rotating the
passwords now resetting the password
there are few things we could do with
IAM and finally we can create get
policies that actually grants access to
AWS services and resources another
question you will see is what are the
policies that you can set for your users
password so some of the policies that we
can set for the user password is at the
minimum length or you know the
complexity of the password by at least
having one number or one special
characters in the password so that's one
and then the requirement of a specific
character types including you know
uppercase lowercase number and
non-alphabetic characters so it becomes
very hard for somebody else to guess
what the password would be and and try
to hack them so we can set the length of
the password we can set the complexity
in the password and then we can set an
automatic expiration of the password so
after a certain time the user is forced
to create a new password so the password
is not still old and easy to guess in
the environment and we can also set
settings like the user should contact
the admin I mean when the password is
about to expire so you know you can get
a hold of how the user is setting their
password is it having good complexity in
it is it meeting company standards or
there are few things that we can control
and set for the users when the users are
setting or recreating the password
another question that could be posted in
an interview so to understand your
understanding of IEM is what's the
difference between an IM role and an IM
user let's talk about IM user let's
start small and then go big or let's
start simple and then talk about the
complex one the IM user has a permanent
long term credential and it's used to
interact directly with AWS services and
on the other hand IEM role is an IM
entity that defines a set of permissions
for making AWS service request so IM
user is an permanent credential and role
are temporary credentials and IEM user
has full access to all AWS IEM
functionalities and with role trusted
entities such as IEM users application
or AWS Services assume are the role so
when an IM user is given an a permission
you know it sticks within the IM user
but with roles we can give permissions
to Applications we can give permissions
to users in the same account in a
different account the corporate ID we
can give permissions to ec2 S3 RDS VPC
and lot more role is wide and IM user is
is not so wide you know it's very
constrained only for that IM user let's
talk about manage policies in AWS manage
policies there are two types you know
customer managed and Amazon managed so
managed policies are IM resources that
Express permissions using the IAM policy
language and we can create policies edit
them manage them manage them separately
from the IM user group and roles which
they are attached to so they are
something that we can do to managed
policies if it is customer managed and
we can now update policy in one place
and the permissions automatically extend
to all the attached entries so I can
have like three services four Services
point to a particular policy and if I
edit that particular policy it's going
to reflect on those three or four
services so anything that I allow is
going to be allowed for those four
Services anything that I denied is going
to be denied for the four Services
imagine what would be without the IM
managed policy we'll have to go and
specifically allow deny on those
different instances four or five times
depending on the number of instances
that we have so like I said there are
two types of managed policies one is
managed by us which is customer managed
policies and then the other is managed
by AWS which is AWS managed policy see
this question can you give an example of
an IM policy and a policy summary this
is actually to test how well-versed are
you with the AWS console the answer to
that question is look at the following
policy this policy is used to Grant
access to add update and delete objects
from a specific folder now in this case
name of the folder is example folder and
it's present in a bucket called example
bucket so this is an IAM policy on the
other hand the policy summary is a list
of access level resource and conditions
for each service defined in a policy so
IM policy is all about one particular
resource and the policy summary is all
about multiple resources with IM policy
it was only talking about S3 bucket and
one particular S3 bucket here it talks
about cloud formation template Cloud
watch logs ec2 elastic bean stock
Services summary summary of resources
and the permissions and policies
attached to them that's what policy
summary is all about another question
could be like this what's the use case
of IAM and how does IM help your
business two important or primary work
of IM is to help us manage IEM users and
their access it provides a secure access
to multiple users to their appropriate
AWS resources so that's one it does and
the second thing it does is manage
access for Federated users Federated
users or non-iam users and through IAM
we can actually allow and provide a
secured access to resources in our AWS
account to our employees without the IM
user no they could be authenticated
using the active directory they could be
authenticated using the Facebook
credential Google credential Amazon
credential and a couple of other
credentials third party identity
management right so we could actually
trust them and we could give them access
to our account based on the trust
relationship that we have built with the
other identity of systems right so two
things one is manage users and their
access for manage IAM user and their
access in our AWS environment and second
is manage access for Federated users who
are non-im users and more importantly IM
is a free service and with that will
only be charged for the use of the
resources not for the IM username and
password that we create all right let's
now talk about some of the questions in
Route 53 one classic question that could
be asked in an interview is what is the
difference between latency based routing
and Geo DNS or Geo based DNS routing now
the Geo based DNS routing takes routing
decisions on the basis of the geographic
location of the request and on the other
hand the latency based routing utilizes
latency measurements between networks
and data centers now latency based
routing is used where you want to give
your customers the lowest latency as
possible so that's when we would use
latency based routing and on the other
hand a Geo based routing is when we want
to direct customers to different
websites based on the country they are
browsing from you know you could have
you know two different or three
different websites for the same URL you
know take Amazon the shopping website
for example when we go to amazon.com
from in the US it directs us to the US
web page where the products are
different the currency is different
right and the flag and and a couple of
other advertisements that shows up are
different and when we go to amazon.com
from India it gets directed to the
amazon.com Indian site where again the
currency the product and the
advertisements they're all different
right so depending on the country
they're trying to browse if you want to
direct customers to two or three
different websites we would use a
geo-based routing another use case of
geo-based routing is if you have a
compliance that you should handle all
the DNS requests sorry if you should
handle all the requests you know from a
country within the country then you
would do geo-based routing now you
wouldn't direct the customer to a server
which is in another country all right
you would direct the customer to a
server which is very local to them
that's another use case of jio based
routing and like I said for latency
based routing the whole goal or aim is
to achieve minimum end user latency if
you are hired for the architect role and
if that requires working a lot on the
DNS then you could be posted with this
question what is the difference between
domain and a hosted Zone at domain is
actually a collection of data describing
a self-contained administrative and
Technical unit on the internet right so
for example you know simplylearn.com is
actually a domain on the other hand
hosted zone is actually an container
that holds information about how you
want to Route traffic on the internet to
a specific domain for example
lms.simplylearn.com is an hosted Zone
whereas simplylearn.com is an domain so
in other words in hosted Zone you would
see the domain name plus and a prefix to
it LMS is a prefix here FTP is a prefix
mail.simplieron.com as a prefix so
that's how you would see a prefix in
hosted zones another question from
another classic question from Route 53
would be how does Amazon Route 53
provide High availability and low
latency the way Amazon Route 53 provides
High availability and low latency is by
globally attributed DNS service Amazon
is a global Service and they have DNS
Services globally any customer doing a
query from different parts of the world
they get to reach an DNS server which is
very local to them and that's how it
provides low latency now this is not
true with all the DNS providers there
are DNS providers who are very local to
a country who are very local to a
continent so they don't they generally
don't provide low latency service right
it's always high latency it's low
latency for local users but anybody
browsing from a different country or a
different continent it's going to be
high latency for them but that's not
again true with Amazon Amazon is a
globally distributed DNS provider it has
DNS servers Global wide and like I said
it has optimal locations it has got a
global Service or in other words it has
got servers around the globe different
parts in the globe and that's how they
are able to provide High availability
and because it's not running on just one
server but on many servers they provide
High availability and low latency if the
environment that you're going to work on
is going to take a lot of configuration
backups environmental backups then you
can expect questions in AWS config a
classic question would be how does AWS
config work along with AWS cloudtrail
AWS cloudtrail actually records user API
activity on the account and you know any
HTTP https access or any any sort of
access you know that's made to the cloud
environment that's recorded in the cloud
trail in other words any APA calls the
time is recorded the type of call is
recorded and what was the response given
was it a failure was it successful they
also get recorded in cloudtrail it's
actually a log it actually records the
activity in your Cloud environment on
the other hand config is an a point in
time configuration details of your your
resources for example at a given point
what are all the resources that were
present in my environment what are all
the resources or what are the
configuration in those resources at a
given point they get captured in AWS
config all right so with that
information you can always answer the
question what did my AWS resource look
like at a given point in time that
question gets answered when we use AWS
config on the other hand with cloudtrail
you can answer the question I mean by
looking at the cloud trail or with the
help of cloudtrail you can easily answer
the question who made an APA call to
modify this resource that's answered by
cloudtrail and with cloudtrail we can
detect if a security group was
incorrectly configured and who did that
configuration let's say that happened to
be in downtime and you want to identify
let's say there happened to be a
downtime and you want to identify who
made that change in the environment you
can simply look at cloudtrail and find
out who made the change and if you want
to look at how the environment looks
like before the change you can always
look at AWS config can AWS configure or
AWS config aggregate data across
different AWS accounts yes it can now
this question is actually to test
whether you have used AWS config or not
I know some of the services are very
local is it some of these services are
availability Zone specific some of them
are Regional specific and some of them
are Global Services in Amazon and though
some of the services are region Services
you still can do some changes you know
add some configuration to it and collect
Regional data in it for example S3 is a
regional service but still you can
collect logs from all other regions into
an S3 bucket in one particular region
that's possible and cloud trail is and
Cloud watch is an regional service but
still you can with some changes to it
with some adding permissions to it you
can always monitor the cloud watch that
belongs to cloudwatch logs that belongs
to other regions you know they're not
Global by default but you can do some
changes and make it Global similarly AWS
config is a service that's a
region-based service but still you can
make it act globally we can aggregate
data across a different region and
different accounts in an AWS config and
deliver the updates from different
accounts to one S3 bucket and can access
it from there AWS config also works or
integrates seamlessly with SNS topic so
you know anytime there is a change
anytime a new data gets collected you
can always notify yourself or notify a
group of people about the new log or the
new config or new edit that happened in
the environment let's look at some of
the database questions you know database
should be running on reserved instances
so whether you know that fact or not the
interviewer wants to understand how well
you know that fact by asking this
question how are reserved instant is
different from on-demand DB instances
reserved instances and on-demand
instances are exactly the same when it
comes to their function but they only
differ based on how they are built
reserved instances are purchased for one
year or three year reservation and in
return we get a very low per hour
pricing because we're paying up front
it's generally said that reserved
incidence is 75 percentage cheaper than
on-demand instance and Amazon gives you
that benefit because you know you're
committing for one year and sometimes
you're paying in advance for the whole
year on the other hand on-demand
instances are built on an hourly hourly
price talking about Auto scaling how
will you understand the different types
of Auto scaling the interviewer might
ask this question which type of scaling
would you recommend for RDS and why the
two types of scaling as you would know
now vertical and horizontal and in
vertical scaling we can vertically scale
up the master database with a couple of
clicks all right so that's vertical
scaling vertical scaling is keeping the
same node and making it bigger and
bigger if previously it was running on
T2 micro now we would like to run it on
M3 two times large instance previously
it had one virtual CPU one gigabit now
it's gonna have eight virtual CPU and 30
gigabit of ram so that's vertical
scaling on the other hand the horizontal
scaling is adding more nodes to it
previously it was running on one VM now
it's going to run on 2 3 10 VMS right
that's horizontal scaling so database
can only be scaled vertically and there
are 18 different types of instances we
can resize our rds2 right so this is
true for RDS MySQL postgres SQL mariadb
Oracle Microsoft SQL servers there are
18 type of instances we can vertically
scale up to on the other hand horizontal
scaling are good for replicas so they
are read-only replicas we're not going
to touch the master database we're not
going to touch the primary database but
I can do horizontal scaling only with
Amazon Aurora and I can add additional
read replicas I can add up to 15 Read
replicas for Amazon Aurora and up to
five read replicas for RDS MySQL
postgres SQL and Murray DB RDS instances
and when we add replica we are
horizontally scaling adding more nodes
right I read only nodes so that's
horizontal scaling so how do you really
decide between vertical and horizontal
scaling if you're looking into increase
the storage and the processing capacity
we'll have to do a vertical scaling if
you are looking at increasing the
performance or of the read heavy
database we need to be looking for
horizontal scaling or we need to be
implementing horizontal total scaling in
our environment still talking about
database this is another good question
you can expect in the interview what is
a maintenance window and Amazon RDS will
your DB instance be available during the
maintenance event all right so this is
really to test how well you have
understood the SLA how well you have
understood the Amazon rdas the failover
mechanism of Amazon rdas stuff like that
so RDS maintenance window it lets you
decide when a DB instance modification a
database engine upgrades or software
patching has to occur and you you
actually get to decide should it happen
at 12 in the night or should it happen
at afternoon should it happen early in
the morning should it happen in the
evening you actually get to decide an
automatic scheduling by Amazon is done
only for patches that are security and
durability related sometimes Amazon
takes down and does automatic scheduling
if you know if there is a need for a
patch update that deals with security
and durability and by default the
maintenance window is is for 30 minutes
and the important point is the DB
instance will be available during that
event because you're going to have
primary and secondary right so when that
upgrade happens Amazon would shift the
connection to the secondary do the
upgrade and then switch back to the
primary another classic question would
be what are the consistency models in
dynamodb in dynamodb there is eventual
consistency read this eventual
consistency model it actually maximizes
your read throughput and the best part
with eventual consistency is all copies
of data reach consistency within a
second and sometimes when you write and
when you're you know trying to read
immediately chances that you you would
still be reading the old data that's
eventual consistency on the other hand
there is another consistency model
called the strong consistency you are
strongly consistent read where there is
going to be a delay in writing the data
you know making sure the data is written
in all places but it guarantees one
thing that is once you have done a write
and then you're trying to do a read it's
going to make sure that it's going to
show you the updated data not the old
data now you can be guaranteed of it
that it is going to show the updated
data and not the old data that's
strongly consistent still talking about
database talking about nosql dynamodb or
nosql database which is dynamodb in
Amazon you could be asked this question
what kind of query functionality does
dynamodb support dynamodb supports get
and put operation dynamodb supports or
dynamodb provides flexible querying by
letting you query on non-primary key
attributes using Global secondary index
and local secondary indexes a primary
key can be either a single attribute
partition key or a composite partition
sort key in other words a dynamodb
indexes a composite partition sort key
as a partition key element and a sort
key element and by holding the partition
key you know when doing a search or when
doing a query by holding the partition
key element constant we can search
across the sort key element to retrieve
the other items in that table and at the
composite partition sort key should be a
combination of user ID partition and a
timestamp so that's what the composite
partition sort key is made of let's look
at some of the multiple choice questions
you know sometimes some companies would
have an written test or an McQ type
online test before they call you for the
first level or before they call you for
the second level so these are some
classical questions that companies asked
or companies ask in their multiple
choice online questions let's look at
this question as a developer using this
paper peruse service you can send store
and receive messages between software
components which of the following is
being referred here let's look at it
right we have AWS step functions Amazon
mq Amazon simple Q service Amazon simple
notification service let's read the
question again as a developer using this
paper you service so the service that we
are looking for is a pay-per-view
service you can send store and retrieve
messages between two software components
kind of like a queue there so what would
be the right answer it would be Amazon
simple Q service now Amazon simple Q
service is the one that's used to
decouple the environment it breaks the
tight coupling and then it introduces
decoupling in that environment by
providing a queue or by inserting a
queue between two software components
let's look at this other question if you
would like to host a real-time audio and
video conferencing application on AWS
right it's an audio and video
conferencing application on AWS this
service provides you with the secure and
easy to use application what is this
service let's look at the options they
are Amazon chime Amazon workspace Amazon
mq Amazon appstream all right you might
tend to look at Amazon app stream
because it's real time and video
conference but it's actually for a
different purpose it's actually Amazon
chime that lets you create chat and
create a chat board and then collaborate
with the security of the AWS services so
it lets you do the audio it's lets you
do the video conference all supported by
AWS security features it's actually
Amazon China let's look at this question
as your company's AWS solution architect
you are in charge of Designing thousands
of individual jobs which are similar
which of the following service best
serves your requirement AWS ec2 Auto
scaling AWS snowball AWS forget AWS
batch let's read the question again as
your company's aw the solution architect
you are in charge of Designing thousands
of individual jobs which are similar it
looks like it's batch service let's look
at the other options as well AWS
snowball is actually an storage
Transport service ec2 auto scaling is uh
you know in introducing scalability and
elasticity in the environment and AWS
far gate is container services AWS batch
is the one is being referred here that
actually runs thousands of individual
jobs which are similar AWS batch it's
the right answer let's look at the other
one you are a machine learning engineer
and you're looking for a service that
helps you build and train machine
learning models in AWS which among the
following are we referring to so we have
Amazon sagemaker and AWS deep lens
Amazon comprehend AWS device form let's
read the question again you are a
machine learning engineer and you're
looking for a service that helps you
build and train machine learning models
in AWS which among the following are
referred here the answer is Sage maker
it provides every developer and data
scientist with the ability to build
train and deploy machine learning models
quickly that's what sagemaker does now
for you to be familiar with you know the
products I would recommend you to you
know simply go through the product
description you know there's one page
available on Amazon that explains all
the products a quick neat and simple and
that really helps you to be very
familiar with you know what the product
is all about and what it is capable of
you know is it the DB service is it a
machine learning service or is it a
monitoring service is it a developer
service stuff like that so get that
information get that details before you
attend an interview and that should
really help to answer or face such
questions with a great confidence so the
answer is Amazon Sage maker because
that's the one that provides developers
and a data scientist the ability to
build the train and deploy machine
learning models quickly as possible all
right let's look at this one let's say
that you are working for your company's
ID team and you are designated to adjust
the capacity of the AWS resource based
on the incoming application and network
traffic how do you do it so what's the
service that's actually helps us to
adjust the capacity of the AWS resource
based on the incoming application let's
look at it Amazon VPC Amazon IAM Amazon
inspector Amazon elastic load balancing
Amazon VPC is a networking service
Amazon IAM is an username password
authentication Amazon inspector is a
service that actually does a security
audit in our environment and Amazon
elastic load balancer is a service that
helps in scalability that's in one way
you know indirectly that helps in
increasing the availability of the
application right and monitoring it
monitoring you know how much requests
are coming in through the elastic load
balancer we can actually add just the
environment that's running behind it so
the answer is going to be Amazon elastic
load balancer all right let's look at
this question this cross-platform video
game development engine that supports PC
Xbox PlayStation IOS and Android
platforms allows developers to build and
host their games on Amazon's servers so
we have Amazon game lift Amazon green
grass Amazon Lumberyard Amazon Sumerian
let's read the question again this
cross-platform video game development
engine that supports PC Xbox PlayStation
IOS and Android platforms allows
developers to build and host their games
on Amazon servers the answer is Amazon
lumber yard this lumber yard is an free
AAA gaming engine deeply integrated with
AWS and twitch with full source this
lumber yard provides a growing set of
tools that helps you create an highest
game quality applications and they
connect to a lot of games and vast
compute and storage in the cloud so it's
that service they are referring to let's
look at this question you are the
project manager of your company's Cloud
architect team you are required to
visualize understand and manage your AWS
cost and usage over time which of the
following service will be the best fit
for this we have AWS budgets we have AWS
cost Explorer we have Amazon workmail we
have Amazon connect and the answer is
going to be cost Explorer now cost
Explorer is an option any of the Amazon
console that helps you to visualize and
understand and even manage the AWS cost
over time who's spending more who's
spending less and what is the trend what
is the projected cost for the coming
month all these can be visualized in AWS
cost Explorer let's look at this
question you are a chief Cloud architect
at your company and how can you
automatically Monitor and adjust
computer sources to ensure maximum
performance and efficiency of all
scalable resources so we have a cloud
formation as a service we have AWS
Aurora as a solution we have AWS Auto
scaling and Amazon API Gateway let's
read the question again if the chief
Cloud architect at your company how can
you automatically Monitor and adjust
Computer Resources how can you
automatically Monitor and adjust
Computer Resources to ensure maximum
performance and efficiency of all
scalable resources this is an easy
question to answer the answer is auto
scaling right that's a basic service and
solution architect course is it auto
scaling is the service that helps us to
easily adjust Monitor and ensure the
maximum performance and efficiency of
all scalable resources it does that by
automatically scaling the environment to
handle the inputs let's look at this
question as a database administrator you
will use a service that is used to set
up and manage databases such as MySQL
mayodb and postgres SQL which service
are we referring to Amazon Aurora Amazon
elastic cache AWS RDS AWS database
migration service Amazon Aurora is
Amazon's flavor of the RDS service and
elastic cache is is the caching service
provided by Amazon they are not
full-fledged database and database
migration service just like the name
says it helps to migrate the database
from on-premises to the cloud and from
one a database flavor to another
database flavor Amazon RDS is the
service is the console is the service is
the umbrella service that helps us to
set up manage our databases like MySQL
maridb and postgres SQL it's Amazon RTS
let's look at this last question a part
of your marketing work requires you to
push messages to onto Google Facebook
Windows and Apple through apis or AWS
Management console you will use the
following service so the option of AWS
cloudtrail AWS config Amazon chime AWS
simple notification service all right it
says a part of your marketing work
requires you to push messages it's
dealing with pushing messages to Google
Facebook Windows and Apple through apis
or AWS Management console you will use
the following service it's simple
notification service simple notification
service is a message pushing service and
sqs is pulling similarly SNS is pushing
right here it talks about a pushing
system that pushes messages to Google
Facebook Windows and Apple through API
and it's going to be a simple
notification system or a simple
notification service so that's all we
had for you in this video we can call it
a wrap now if you still have any
questions or doubts let us know in the
comment section below and a team of
experts will help you at the earliest
thank you so much for being here today
we'll see you next time until then stay
safe and keep learning with simply learn
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign