foreign
to the artificial intelligence boot camp
today we embark on an exciting Journey
to the world of AI get ready to Unleash
Your Potential and dive deep into the
realm of Cutting Edge technology
artificial intelligence has
revolutionized our world transforming
Industries and redefining what's
possible this boot camp is designed to
equip you with the knowledge and skills
you need to thrive in this fast paced
and ever evolving field before we dive
into the exciting world of artificial
intelligence let me share with you some
fascinating statistics that highlight
the impact and potential of this
transformative technology
did you know the market of AI is
projected to reach a struggling 733
billion by 2027. this exponential growth
reflects the increasing adoption of AI
across various Industries AI has already
made its Mark in sectors such as
Healthcare Finance transportation and
entertainment the potential of AI goes
beyond just market value it's estimated
that AI technologies have the potential
to increase global economic output by up
to 15.7 trillion dollars by 2030. that's
a stackling 26 boost in global GDP air
powered automations is also reshaping
the workforce landscape according to a
study that is by the world economic
Forum by 2025 around 85 million jobs may
be displaced by AI but at the same time
around 97 million new jobs will be
created AI is not only about Job
displacement it's also transforming
Industries making them more efficient
and enabling new opportunities for
example by 2025 it's expected that the
adoption of AI in manufacturing alone
will lead to a 4.1 percent increase in
productivity and now we are thrilled to
present our AI bootcamp that is designed
to equip you with the knowledge and
skills you need to thrive in this
fast-paced and our evolving field in
this six months intensive and immersive
learning experience you will have the
opportunity to dive head first into the
subject matter gaining hands-on
experience and building a strong
foundation in AI we believe in a
practical approach where you would work
on 25 plus Hands-On projects across
various industry verticals providing you
with real world experience what sets up
boot camp apart is a partnership with
Caltech ctme ensuring academic
excellence and Industry recognition by
successfully completing the boot camp
you will earn a prestigious certificate
and up to 22 CU credits from caltex ctme
validating your expertise in the field
so are you ready to embark on this
transformative Journey are you ready to
unlock the power of AI and take your
career to new heights if so enroll in a
air boot camp today and become part of
the AI Revolution the course link is
mentioned in the description box below
and now in this comprehensive artificial
intelligence bootcamp will cover a wide
range of topics that will Empower you
with the knowledge and skills needed to
excel in the field of AI let's take a
glimpse into the exciting subjects that
await you we'll Begin by exploring the
foundations of artificial intelligence
you will learn what AI is all about and
its various types next we'll delve into
the top 10 a applications that are
making a significant impact across
various Fields moving forward we'll
discuss the future of artificial
intelligence then we will shift our
Focus to machine learning a critical
component of AI you will gain a solid
understanding of what machine learning
is and its various types including
supervised unsupervised and
reinforcement learning we'll dive deeper
into essential machine learning
algorithms starting with linear
regression analysis which enables us to
make predictions Based on data patterns
in addition we'll explore linear
regression in our a powerful programming
language for statistical analysis and
data visualization and logistic
regression in Python which is widely
used for classification tasks
and now we'll compare linear regression
and logistic regression understanding
their differences and when to use each
approach for different types of problems
The Bootcamp will also call
classification in machine learning where
you will learn about disease and tree
algorithms and their application in
solving classification problems we will
delve into assemble methods with the
random Forest algorithm which combines
multiple decision trees for enhanced
accuracy and robustness
additionally we will explore k-means
clustering a popular unsupervised
learning algorithm used for grouping
data points based on similarities
and the boot camp will also introduce
you to nail base classifier K nearest
neighbors algorithm and the Acer
algorithm in artificial intelligence
providing you with a diverse set of
tools to tackle real world challenges as
we progress we'll explore the
fascinating realm of deep learning
discussing neural networks their
architecture and their ability to learn
complex patterns to help you
differentiate between artificial
intelligence machine learning deep
learning and data science
we will dive into the nuances and
understand how they will fit together
stay tuned as we unveil the top 10 AI
tools for 2023 showcasing The Cutting
Edge technologies that are driving AI
advancements and simplifying the
development process we'll also explore
the top 10 AI robots in 2023
highlighting the incredible strides made
in robotics and their impact on various
Industries
furthermore we'll discuss the top 10
artificial intelligence project ideas
2023 then we have the top 10 AI
companies for 2023 and lastly the career
opportunities in artificial intelligence
2023 but before we begin let's check out
what's one of our Learners has to say
about our courses and about this boot
camp
artificial intelligence has already
become a part of our lives
and I wanted to learn more about it
the course is helping me tremendously in
my upskilling journey
hi I'm Daniel dick I'm six to six years
old and I'm semi-retired and live in
California with my wife and son
since childhood I've had a strong
interest in science and technology I was
fascinated by the way systems worked
internally and how algorithms performed
that after 40 years of working in
technology sector I could no longer work
it hit me hard
I looked at how well my daughter was
doing as a software engineer my daughter
became my inspiration upskill and over
the years I pursued various courses in
different fields of computer science
I realized that technology has the power
to change the world
I worked rigorously in the healthcare
sector as a technocrat
and I know technology can revolutionize
it
I wanted to know how it could be used in
the health care sector or other sectors
so I could contribute to those domains
whenever I start something of my own the
course is practical all of my new
details and concepts are explained in
the most simplistic manner
the curriculum was well designed
considering Learners from different
backgrounds it was an amalgamation of
practical labs in theory
program focuses on building a
problem-solving attitude in Learners the
course is helping me tremendously in my
upskilling journey and solving real life
problems whatever the problem is you can
either solve it or learn to solve it by
upskilling yourself
it's a weekend and John decided to watch
the latest movie recommended by Netflix
at his friend's place
before heading out he asked Siri about
the weather and realized it would rain
so he decided to take his Tesla for the
long journey and switch to autopilot on
the highway
after coming home from the eventful day
he started wondering how technology has
made his life easy
he did some research on the internet and
found out that Netflix Siri and Tesla
are all using AI
so what is AI
AI or artificial intelligence is nothing
but making computers-based machines
think and act like humans artificial
intelligence is not a new term John
McCarthy a computer scientist coined the
term artificial intelligence back in
1956 but it took time to evolve as it
demanded heavy computing power
artificial intelligence is not confined
to just movie recommendations and
virtual assistants
broadly classifying there are three
types of AI
artificial narrow intelligence also
called weak AI is the stage where
machines can perform a specific task
Netflix Siri chat Bots spatial
recommendation systems are all examples
of artificial narrow intelligence
next up we have artificial general
intelligence referred to as an
intelligent agent's capacity to
comprehend or pick up any intellectual
skill that a human can
we are halfway in a successfully
implementing this space IBM's Watson
supercomputer and gpt3 fall under this
category and lastly artificial super
intelligence
it is the stage where machines surpass
human intelligence you might have seen
this in movies and imagined how the
world would be if machines occupy it
fascinated by this John did more
research and found out that machine
learning deep learning and natural
language processing are all connected
with artificial intelligence
learning a subset of AI is the process
of automating and enhancing how
computers learn from their experiences
without human health
machine learning can be used in email
spam detection medical diagnosis Etc
learning can be considered a subset of
machine learning it is a field that is
based on learning and improving on its
own by examining computer algorithms
while machine learning uses simpler
Concepts deep learning works with
artificial neural networks which are
designed to imitate the human brain this
technology can be applied in face
recognition speech recognition and many
more applications natural language
processing popularly known as NLP can be
defined as the ability of machines to
learn human language and translate it
chat Bots fall under this category
artificial intelligence is advancing in
every crucial field like healthcare
education robotics banking e-commerce
and the list goes on
like in healthcare AI is used to
identify diseases helping healthcare
service providers and their patients
make better treatment and lifestyle
decisions
coming to the education sector AI is
helping teachers automate grading
organizing and facilitating parent
Guardian conversations
in robotics ai-powered robots employee
real-time updates to detect obstructions
in their path and instantaneously design
their routes artificial intelligence
provides Advanced data analytics that is
transforming banking by reducing fraud
and enhancing compliance
with this growing demand for AI more and
more Industries are looking for AI
Engineers who can help them develop
intelligent systems and offer them
lucrative salaries going north of one
hundred and twenty thousand dollars
future of AI looks promising with the AI
Market expected to reach 190 billion
dollars by 2025 first let's understand
what really is artificial intelligence
artificial intelligence is the science
of building intelligent machines from
vast volumes of data
this data can be structured
semi-structured or unstructured in
nature
AI systems learn from past experiences
and perform human-like tasks artificial
intelligence enhances the speed the
season and effectiveness of human
efforts
AI uses sophisticated algorithms and
methods to build machines that can make
decisions on their own deep learning and
machine learning are the two subsets of
artificial intelligence so you need both
machine learning algorithms and deep
learning networks to build intelligent
systems
AI is now being widely used in almost
every sector of business such as
Transportation Healthcare banking retail
entertainment and e-commerce
now let's look at the different types of
artificial intelligence
so AI can be classified based on
capabilities and functionalities
under capabilities there are three types
of artificial intelligence
they are narrow AI General Ai and super
AI
under functionalities we have four types
of artificial intelligence reactive
machine limited memory theory of mind
and self-awareness let's look at them
one by one
first we will look at the different
types of artificial intelligence based
on capabilities
so what is narrow AI
narrow AI also known as vki focuses on
one narrow task and cannot perform
Beyond its limitations
it aims at a single subset of cognitive
abilities and advances in that Spectrum
applications of narrow AI are becoming
increasingly common in our day-to-day
lives as machine learning and deep
learning methods continue to evolve
Apple Siri is a simple example of an
arrow AI that operates with a limited
predefined range of functions
Siri often has challenges with tasks
outside its range of abilities IBM
Watson supercomputer is another example
of narrow AI which applies cognitive
Computing machine learning and natural
language processing to process
information and answer your questions
IBM Watson once outperformed human
contestant Ken Jenkins to become the
champion on the popular game show
Jeopardy other examples of narrow AI
include Google Translate image
recognition software recommendation
systems spam filtering and Google's page
ranking algorithm
next we have General artificial
intelligence or general AI General AI
also known as strong AI has the ability
to understand and learn any intellectual
task that a human can
General artificial intelligence has
received a one billion dollar investment
from Microsoft through open AI it allows
a machine to apply Knowledge and Skills
in different contexts AI researchers and
scientists have not achieved strong AI
so far to succeed they would need to
find a way to make machines conscious
programming a full set of cognitive
abilities
Fujitsu built the K computer which is
one of the fastest computers in the
world
it is one of the most notable attempts
at achieving strong AI
it took 40 minutes to simulate a single
second of neural activity so it is
difficult to determine whether or not
strong AI will be achieved in the near
future
tan HE2 is a supercomputer created by
China's national university of Defense
technology
it currently holds the record for CPS at
33.86 petaflops although it sounds
exciting the human brain is estimated to
be capable of one example of
now CPS means characters per second that
A system can process
third in the list of AI that is based on
capabilities we have super AI super AI
exceeds human intelligence and can't
perform any tasks better than a human
the concept of artificial super
intelligence is AI evolved to be so akin
to human emotions and experiences that
it doesn't just understand them it
evokes emotions needs beliefs and
desires of its own its existence is
still hypothetical some of the key
characteristics of super AI include the
ability to think solve puzzles meet
judgments and decisions on its own
now if you have enjoyed watching this
video so far please make sure to
subscribe to our YouTube channel and hit
the Bell icon to stay updated with all
the latest Technologies also if you have
any questions related to this video
please put it in the chat section A team
of experts will help you address your
questions
moving ahead now we will see the
different types of artificial
intelligence based on functionalities
in this category first we have reactive
machine
a reactive machine is the basic form of
AI that does not store memories or use
past experiences to determine future
actions
it works only with present data they
simply perceive the world and react to
it reactive machines are given certain
tasks and don't have capabilities Beyond
those duties
IBM's deep blue which defeated just
Grand Master Gary Castro is a reactive
machine that sees the pieces on HS
Bowden reacts to them
it cannot refer to any of its prior
experiences and cannot improve with
practice
deep blue can identify the pieces on a
chessboard and know how each moves
it can make predictions about what moves
might be next for it and its opponent
it can choose the most optimal moves
from among the possibilities deep blue
ignores everything before the present
moment
all it does is look at the pieces on the
chessboard as it stands right now and
choose from possible next moves
up next we have limited memory
limited memory AI learns from past data
to meet decisions
the memory of such systems is
short-lived
while they can use this data for a
specific period of time they cannot add
it to a library of their experiences
this kind of technology is used for
self-driving vehicles
they observe how other vehicles are
moving around them in the present and as
time passes
that ongoing collected data gets added
to the static data within the AI machine
such as lean markers and traffic lights
they are included when the vehicle
decides to change lanes to avoid cutting
of another driver or being hit by a
nearby vehicle
Mitsubishi Electric is a company that
has been figuring out how to improve
such technology for applications like
self-driving cars
then we have theory of mind
theory of Mind represents a very
advanced class of technology and exists
as a concept this kind of AI requires a
thorough understanding that the people
and the things within an environment can
alter feelings and behaviors
it should be able to understand people's
emotions sentiment and thoughts
even though a lot of improvements are
there in this field this kind of AI is
not complete yet
one real world example of theory of Mind
AI is Kismet a robot had made in the
late 90s by a Massachusetts Institute of
Technology researcher Kismet can make
human emotions and recognize them both
abilities are key advancements in theory
of Mind AI but Kismet can't follow gazes
or convey attention to humans
Sophia from Hanson robotics is another
example where the theory of Mind AI was
implemented cameras within Sofia's eyes
combined with computer algorithms allow
her to see
second follow faces sustain eye contact
and recognize individuals
this is able to process speech and have
conversations using natural language
subsystem
finally we have self-awareness
self-awareness AI only exists
hypothetically
such systems understand that internal
traits States and conditions and
perceive human emotions
these machines will be smarter than the
human mind
this type of AI will not only be able to
understand and evoke emotions in those
it interacts with but also have emotions
needs and beliefs of its own
while we are probably far away from
creating machines that are self-aware we
should focus our efforts towards
understanding memory learning and the
ability to base decisions on past
experiences
let me ask you a question which are the
powerful AI assistants that you have
come across please leave your answer in
the comment section below and stay tuned
to get the answer so what are the top 10
applications of artificial intelligence
social media today AI can create social
media posts for you it can draft and
Target social ads
it can also automate monitoring
and it has powers that most of what you
see in any given social network
Facebook's picture recognition tools
have been popular among users for a
while but the company is constantly
working to make the algorithms better so
that users can browse photos without
relying on tags or other nearby
information it will help to tag people
in photos
Twitter
understanding what needs to recommend to
users on their timelines is one of the
numerous ways Twitter employs AI on its
platform it seeks to provide users with
the most relevant tweets possible for an
individualized experience Twitter also
uses AI to combat offensive statements
Instagram artificial intelligence is
used by the platform to improve user
experience filter spam and increase the
effectiveness of targeted advertising
users in the platform can search images
of a specific activity when new event
restaurant food and Discovery
experiences
with the aid of tags and trending
information
automobiles the automotive value chain
which includes manufacturing design
Supply production post-production
driving assistance and Driver risk
assessment systems is successfully
implementing artificial intelligence
Tesla keeps an eye on the driver's eyes
to detect signs of drowsiness and keep
them from dozing off while operating a
vehicle it is primarily used by the
autopilot system to stop drivers from
taking quick power naps while driving
agriculture Forbes reports that Global
spending on Smart agriculture including
artificial intelligence and machine
learning is projected to Triple to 15.3
million dollars by 2025. Precision
agriculture often known as artificial
intelligence systems is assisting in
enhancing the overall quality and
accuracy of harvest artificial
intelligence technology AIDS in the
detection of pest plant diseases and
under nutrition in firms artificial
intelligence sensors can identify and
Target plants before deciding which
herbicide to use in the area
additional intelligence in gaming the
gaming industry is another area where
artificial intelligence technologies
have gained popularity artificial
intelligence can be utilized to develop
intelligent human-like non-person
characters that can communicate with the
players in order to improve game design
and testing it can also be used to
forecast human behavior the 2014 alien
isolation video game employee artificial
intelligence to follow the player around
all the time two artificial intelligence
systems are used in the game one is the
director artificial intelligence who
frequently knows where you are and the
alien artificial intelligence which is
controlled by sensors and behaviors and
constantly changes the player next comes
a artificial intelligence in healthcare
delays can be the difference between
life and death in the healthcare
industry artificial intelligence in
healthcare has the potential to help
providers in many areas of patient care
and operational procedures enabling them
to build on current Solutions and solve
problems quickly
covid-19 has illuminated the process of
developing vaccine in order to create
and Test new medicines drug
manufacturers are increasingly turning
to artificial intelligence Solutions
like deep learning because there are so
many potential chemical combinations
creating new medications requires
processing a lot of data artificial
intelligence overcomes this difficulty
by utilizing its exceptional performance
with massive amounts of data this
capability results in a higher
production of a pro Pharmaceuticals in a
more rapid less expensive and more
effective research and development
procedure
robotics another area where artificial
intelligence is frequently applied is
robotic AI powered robots employ
real-time updates to detect obstructions
in their path and instantaneously design
their Roots robotics uses artificial
intelligence to make machines more
intelligent and capable of acting in
variety of situations
robots can also sense their environment
or take in its views with the views of
sensor similar to how humans have
primary sensors robotics uses a variety
of sensing technology combined there is
a definite Trend toward mobile
autonomous robots that can intelligently
gather process and manage data in order
to make the best decisions for
manufacturing or production
together
AI in banking the use of advanced data
analytics by artificial intelligence
will transform banking in the future by
reducing fraud enhancing compilence
anti-money laundering tasks that would
typically take hours or days can now be
completed in a matter of seconds thanks
to artificial intelligence algorithms
Erica a virtual assistant created by
Bank of America is among the best
instances of artificial intelligence
chatbots in banking applications this
artificial intelligence chatbot can take
care of tasks like updating card
security and reducing credit card
daily life use of artificial
intelligence
autonomous vehicle to teach computers to
think and evolve like humans when it
comes to driving in any environment and
object recognition to prevent accidents
companies like Toyota Audi Tesla use
machine learning algorithms
spam filters now email that we regularly
use features artificial intelligence
that separates out junk emails and sends
it to spam or trash folders allowing us
to see only the filtered material Gmail
a well-known email service has achieved
a filtration capacity of roughly
99.9 percent
next comes facial recognition
facial recognition algorithms are used
by our favorite devices including phones
laptops and personal computers to detect
and identify users in order to Grant
safe access
education sector artificial intelligence
can assist Educators with
non-educational tasks such as
facilitating and automating personalized
messages to students back office duties
such as grading paperwork organizing and
facilitating
routine feedback managing enrollment
courses Etc
a student can receive additional
learning resources or assistance with
voice assistance even without the
lecturer or teacher's direct engagement
this lowers the expense of producing
temporary handbooks and makes it simple
to provide answer to frequently Asked
topics
artificial intelligence is most commonly
used in the field of e-commerce during
personalized shopping recommendation
engines are made possible by artificial
intelligence Technologies allowing you
to interact with your customers more
effectively their browsing history
preferences and interests are taken into
consideration while making these
recommendations
it helps you build stronger bonds with
your clients and increases brand loyalty
for instance almost every page of
Amazon's website offers personalized
product recommendations
credit card frauds fake reviews are two
most of the significant issues that
e-commerce companies deal with by
considering the usage patterns
artificial intelligence can help reduce
the possibility of credit card frauds
taking place
many customers prefer to buy a product
or service based on customer reviews
artificial intelligence can identify and
handle fake reviews
what is the answer to the question that
I have asked about artificial
intelligence assistant they can be
Google Assistant Siri Alexa
Etc
moving on
in the area of artificial intelligence
significant advancement have been made
and the results are clear AI is a
flexible tool that is applied across
sectors to improve decision making boost
efficiency and get rid of repeated tasks
researchers are preparing us for a
revolution that will change the world
Forever by looking into the future of
artificial intelligence this revolution
isn't going to happen in a few decades
but in a few years which means that the
clock is ticking for now for businesses
to start embracing AI artificial
intelligence has become a part of our
lives due to its revolutionary nature
artificial intelligence also faces
numerous debates regarding potential
impacts on individuals although it could
be risky it's also a fantastic
opportunity
it is estimated that the global
artificial intelligence Market will
reach
267 billion dollars by 2027. people are
worrying about losing their jobs as AI
continues to advance because it has
revolutionized Industries across all
sectors but AI has increased the number
of jobs and possibilities available to
individuals across all Industries every
machine needs a person to run it
although AI has replaced certain
occupations it still creates more jobs
for humans
AI can be categorized into three types
firstly weak AI performs specific tasks
like Apple's Siri
General AI performs tasks like a human
super AI is more capable than a human
but both General Ai and super AI are
hypothetical because these Technologies
are not yet developed and research is
going on however the development of such
AI will take a lot of time the
development of AGI will accelerate
within the following 10 years in fact
according to researchers there is a 25
probability that by 2023 AI will
resemble humans when it comes to
thinking capacity additionally the
current data boom computer developments
and improvements in automated methods
and machine algorithms will provide a
solid foundation for platforms with
human level ai ai has occupied crucial
sectors like healthcare
cyber security e-commerce agriculture
robotics education the military system
Finance gaming and many more AI is used
in healthcare to identify diseases
helping healthcare service providers and
their patients make better treatment and
lifestyle decisions
we can say that without AI we couldn't
have made the incredible recent
advancements in fields like robotics
virtual reality
chat Bots face recognition
and autonomous vehicles
smart robot advisors in finance
Insurance legal media and journalism
will deliver instantaneous research or
discoveries in the upcoming years it is
anticipated that artificial intelligence
and machine learning will become more
prominent in the consumer Market which
will immediately affect how well web
development functions virtual assistants
or chat Bots will provide professional
advice other advantages include
strengthening governance through better
decision-making processes shortening
time to market for r d initiatives and
streamlining transportation and supply
chain Networks given all these scenarios
AI is all set to revolutionize every
sector in the near future given that it
is expected to impact 60 percent of
firms artificial intelligence will
continue to spread deeper into a wide
range of Industries in the upcoming
years it is currently driving
development in our smart devices cars
and favorite apps in the coming years AI
will invade every aspect of our life and
eventually surpass human intellectual
capacity it won't be long before
artificial intelligence becomes a common
part of our everyday lives
as it becomes more prevalent it will
allow us to live more efficiently and
improve our lives
by increasing their effectiveness
efficiency and concentration on the
primary tasks that require people's
attention AI Technologies are
transforming many industry sectors
artificial intelligence will change the
world for the better Sci-Fi movies and
books depict a future with sentiment
machines capable of interacting with
humans and Performing tasks just like
people would what if we told you that
this future is already a reality all of
this is already made possible by Machine
learning machine learning is a science
of programming machines to think and act
like humans without being specifically
programmed to we already use machine
learning in our daily life without
knowing it email spam recognition spell
check even the YouTube video
recommendation which brought you here
are implemented using machine learning
machine learning uses algorithms to
learn tasks these algorithms are fed
with data from which they learn to
perform these tasks this means that over
time as changes in data occur we don't
need to reprogram our application just
let it find patterns and learn from the
new data machine learning is a subset of
artificial intelligence which is a
science concerned with imparting
human-like intelligence onto machines
and creating machines which can sense
reason act and adapt deep learning is a
sub-branch of machine learning which is
inspired by the working of the human
brain machine learning is leading us to
a future where machines can learn and
think and has opened us a whole new
plethora of job opportunities
this was a brief intro to machine
learning
talk about a life without machine
learning
someone who doesn't know anything about
machine learning or artificial
intelligence he or she might think that
it is only being used in robots or
machines and stuff
it is actually true as supported by most
of the Sci-Fi movies nowadays but you
wouldn't believe how much more machine
learning is giving us how much of it we
are using in our daily lives
let's say you need information on any
random topic say a computer
first place you would check would
definitely be Google right it would
collect all the information on the word
you search for and present it to you
according to relevance if there was no
Google you would have to do it in a very
hard way that is by going through tens
or hundreds of books and articles even
after which you won't find the answer
if we go back four to five years facial
recognition would be a thing only shown
in movies like Mission Impossible or
Terminator or any of the Sci-Fi movies
you pick but machine learning has made
it possible for Facebook and Instagram
to use this feature for your benefits
you don't have to go through the trouble
of tagging every single person present
in a picture that you have posted
Facebook will automatically recognize
the people in the photo and tag them for
you
that saves a lot of time for you doesn't
it
also Siri Cortana Iris all of them would
not be there to help you if there was no
machine learning
now that we know life would be a lot
more difficult without machine learning
let's look into the things that are run
by Machine learning and we are using
them on our daily basis
let's enter the world of gaming PS4 is
the next boxes have introduced virtual
reality glasses which brings a whole new
level of detail into gaming every time
your head Moves In Real World it
replicates the movement in the virtual
world providing an excellent gaming
experience
then there is gesture control gaming
wherein machine learning tracks your
body movement and makes a corresponding
movement in the game
finally in the game FIFA your opponent
tends to adapt based on the kind of
strategy or gameplay you follow
that is again done by Machine learning
I'm sure you must have shopped a lot
from Amazon so let's look into some of
the places Amazon makes use of machine
learning
let's say you buy a formal t-shirt on
Amazon now as you buy it it suggests you
formal shoes ties Blazers and apparels
that go with what you have bought
that's the recommendation system powered
by Machine learning again the price on
every product that you see on Amazon is
wearing every moment based on demand
that's being done by Machine learning
algorithms
then comes customer segmentation
customer segmentation is one of the most
crucial thing for all the e-commerce
platforms machine learning helps them
differentiate between customers based on
what they buy how frequent are they and
their reviews
this helps the companies to make sure
that their customers are taken care of
and the needs are being fulfilled
now let's talk about an app that you use
on a daily basis to reach office on time
yes it's Uber cabs once you have
traveled with Uber you must have noticed
that it suggests you the places you
might want to go based on your previous
Journeys when you're taking a share how
does the app make sure that the cab you
get is travel by the same route you want
to travel in there are so many factors
like the distance the traffic the
ratings all of it is taken care of by
Machine learning
for the past five minutes we have been
talking about how machine learning is
improving Our Lives overall it is being
used I have probably said machine
learning 100 times already
I think it's about time that I tell you
what machine learning actually is
let's take an ordinary system that you
currently are using it can't do much
except from the basic operations that
you already know about now let's add
artificial intelligence or for a
Layman's point of view let's keep the
same machine the power to think on its
own that is what machine learning
basically is it's an application of
artificial intelligence that provides
systems the ability to learn on their
own and improve from experience without
being programmed externally
so if your computer had machine learning
you may be able to play the difficult
parts of a game for you or probably
solve a complicated mathematical
equation for you that could be really
helpful
let's dive in a little deeper and see
how machine Learning Works
let's say you provide a system with the
input data that carries the photos of
various kinds of fruits now you want the
system to figure out what are the
different fruits and group them
accordingly so what the system does it
analyzes the input data then it tries to
find patterns patterns like shapes size
and color
based on these patterns the system will
try to predict the different types of
fruit and segregate them finally it
keeps track of all such decisions it
took in the process to make sure it's
learning the next time you ask the same
system to predict and segregate the
different types of fruits it won't have
to go through the entire process again
that's how machine Learning Works
now let's look into the types of machine
learning
machine learning is primarily of three
types first one is supervised machine
learning as the name suggests you have
to supervise your machine learning while
you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled
finally there is reinforcement learning
wherein the system learns on its own
let's talk about all these types in
detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train
this is how the training is done we
provide a data set that contains
pictures of a kind of a fruit say an
apple
then we provide another data set which
lets the model know that these pictures
where that of a fruit called Apple
this ends the training phase now what we
will do is we provide a new set of data
which only contains pictures of Apple
now here comes the fun part the system
can actually tell you what fruit it is
and it will remember this and apply this
knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own
this kind of a model is generally used
into filtering spam mails from your
email accounts as well yes surprise
aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their similarities
so you provide the data to the system
and let the system do the rest of the
work simple isn't it this kind of a
model is used by Flipkart to figure out
the products that are well suited for
you honestly speaking this is my
favorite type of machine learning out of
all the three and this type has been
widely shown in most of the Sci-Fi
movies lately let's find out how it
works imagine a newborn baby
you put a burning candle in front of the
baby the baby does not know that if it
touches the flame its fingers might get
burnt
so it does that anyway and gets hurt
the next time you put that candle in
front of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement learning works
we provide the machine with a data set
wherein we ask it to identify a
particular kind of a fruit in this case
an apple
so what it does as a response it tells
us that it's a mango but as we all know
it's a completely wrong answer so as a
feedback we tell the system that it's
wrong it's not a mango it's an apple
what it does it learns from the feedback
and keeps that in mind
when the next time when we ask a same
question it gives us the right answer it
is able to tell us that it's actually an
apple that is a reinforced response so
that's how reinforcement learning works
it learns from his mistakes and
experiences this model is used in games
like Prince of Persia or Assassin's
Creed or FIFA wherein the level of
difficulty increases as you get better
with the games
just to make it more clear for you let's
look at a comparison between supervised
and unsupervised learning firstly the
data involved in case of supervised
learning is labeled as we mentioned in
the examples previously
we provide the system with a photo of an
apple and let the system know that this
is actually an apple
that is called label data so the system
learns from the label data and makes
future predictions
now unsupervised learning does not
require any kind of label data because
its work is to look for patterns in the
input data and organize it
the next point is that you get a
feedback in case of supervised learn
that is once you get the output the
system tends to remember that and uses
it for the next operation
that does not happen for unsupervised
learning and the last point is that
supervised learning is mostly used to
predict data whereas unsupervised
learning is used to find out hidden
patterns or structures in data
I think this would have made a lot of
things clear for you regarding
supervised and unsupervised learning
now let's talk about a question that
everyone needs to answer before building
a machine learning model
what kind of a machine learning solution
should we use
yes you should be very careful with
selecting the right kind of solution for
your model because if you don't you
might end up losing a lot of time energy
and processing cost
I won't be naming the actual Solutions
because you guys aren't familiar with
the mute so we will be looking at it
based on supervised unsupervised and
reinforcement learning
so let's look into the factors that
might help us select the right kind of
machine learning solution
first factor is the problem statement
describes the kind of model you will be
building or as the name suggests it
tells you what the problem is for
example let's say the problem is to
predict the future stock market prices
so for anyone who is new to machine
learning would have trouble figuring out
the right solution
but with time and practice you will
understand that for a problem statement
like this solution based on supervised
learning would work the best for obvious
reasons
then comes the size quality and nature
of the data if the data is cluttered you
go first unsupervised if the data is
very large and categorical we normally
go for supervised learning Solutions
finally we choose the solution based on
their complexity
as for the problem statement wherein we
predict the stock market prices it can
also be solved by using reinforcement
learning but that would be very very
difficult and time consuming unlike
supervised learning
algorithms are not types of machine
learning in the most simplest language
they are methods of solving a particular
problem
so the first kind of method is
classification which falls under
supervised learning classification is
used when the output you are looking for
is a yes or a no or in the form a or b
or true or false like if a shopkeeper
wants to predict if a particular
customer will come back to his shop or
not he will use a classification
algorithm
the algorithms that fall under
classification are decision tree knife
base random Forest logistic regression
and KNN
the next kind is regression this kind of
a method is used when the predicted data
is numerical in nature like if the
shopkeeper wants to predict the price of
a product based on its demand it would
go for regression
the last method is clustering
clustering is a kind of unsupervised
learning again it is used when the data
needs to be organized
most of the recommendation system used
by Flipkart Amazon Etc make use of
clustering
another major application of it is in
search engines the search engines study
your old search history to figure out
your preferences and provide you the
best search results
one of the algorithms that fall under
clustering is k-means
now that we know the various algorithms
let's look into four key algorithms that
are used widely
we will understand them with very simple
examples
the four algorithms that we will try to
understand are K nearest neighbor linear
regression decision tree and knife base
let's start with our first machine
learning solution K nearest neighbor
nearest neighbor is again a kind of a
classification algorithm as you can see
on the screen the similar data points
form clusters
the blue one
the red one
and the green one there are three
different clusters
now if we get a new and unknown data
point it is classified based on the
cluster closest to it or the most
similar to it
K in KN is the number of nearest
neighboring data points we wish to
compare the unknown data with
let's make it clear with an example
let's say we have three clusters in a
cost to durability graph
first cluster is of footballs
the second one is of tennis balls
and the third one is of basketballs
from the graph we can say that the cost
of footballs is high and the durability
is less the cost of tennis balls is very
less but the durability is high and the
cost of basketballs is as high as the
durability
now let's say we have an unknown data
point
we have a black spot which can be one
kind of the pause but we don't know what
kind it is
so what we'll do we'll try to classify
this using KNN so if we take K is equal
to 5 we draw a circle keeping the
unknown date upon the center and we make
sure that we have five balls inside that
Circle in this case we have a football a
basketball and three tennis balls now
since we have the highest number of
tennis balls inside the circle
the classified ball would be a tennis
ball
so that's how K nearest neighbor
classification is done
linear regression is again a type of
supervised learning algorithm this
algorithm is used to establish linear
relationship between variables one of
which would be dependent and the other
one would be independent
like if we want to predict the weight of
a person based on his height weight
would be the dependent variable and
height would be independent
let's have a look at it through an
example
let's say we have a graph here showing a
relationship between height and weight
of a person
let's put the y axis as h
and the x-axis as weight
so the green dots are the various data
points
these green dots are the data points
and D is the mean squared error that is
the perpendicular distances from the
line to the data points are the error
values
this error tells us how much the
predicted values vary from the original
value
Let's ignore this blue line for a while
so let's say if this is our regression
line
you can see the distance from all the
data points from this line is very high
if we take this line as a regression
line the error in the prediction will be
too high
so
in this case the model will not be able
to give us a good prediction
let's say we draw another regression
line here like this
even in this case you can see that the
perpendicular distance of the data
points from the line is very high
so the error value will still come as
high as the last one
so this model will also not be able to
give us a good prediction
so what to do
so finally we draw a line which is this
blue line so here we can see that the
distance of the data points from the
line is very less
relative to the other two lines we drew
so the value of D for this line will be
very less
so in this case if we take any value on
the x axis the corresponding value on
the y-axis will be our prediction
and given the fact that the D is very
low our prediction should be good also
this is how regression works we draw a
line a regression line that is in such a
way that the value of D is the least
eventually giving us good predictions
this algorithm that is decision tree is
a kind of an algorithm you can very
strongly relate to it uses a kind of a
branching method to realize the problem
and make decisions based on the
conditions
let's take this graph as an example
imagine yourself sitting at home at home
then bored you feel like going for a
swim
what you do is you check if it's sunny
outside so that's your first condition
if the answer to that condition is yes
you go for a swim if it's not sunny in
the next question you would ask yourself
is if it's raining outside so that's
condition number two
if it's actually raining you cancel the
plan and stay indoors if it's not
raining then you would probably go
outside and have a walk
so that's the final note
that's how decision tree algorithm works
you probably use this every day it
realizes a problem and then takes the
decisions based on the answers to every
conditions
nightbase algorithm is mostly used in
cases where a prediction needs to be
done on a very large data set it makes
use of conditional probability
conditional probability is the
probability of an event say a happening
given that another event B has already
happened
this algorithm is most commonly used in
filtering spam mails in your email
account
let's say you receive a mail
the model goes through your old spam
Mill records
then it uses base theorem to predict if
the present male is a spam mail or not
so p c of a is the probability of even C
occurring when a has already occurred p
a of C is the probability of event a
occurring when C has already occurred
and P C is the probability of even C
occurring and Pa is a probability of
event a operating
let's try to understand Knight base with
a better example Knight base can be used
to determine on which days to play
cricket
based on the probabilities of a day
being rainy windy or sunny the model
tells us if a match is possible
if we consider all the weather
conditions to be event a for us
and the probability of a match being
possible even C
so the model applies the probabilities
of event A and C into the base theorem
and predicts if a game of cricket is
possible on a particular day or not
in this case if the probability of C of
a is more than 0.5 we can be able to
play a game of cricket if it's less than
0.5 we won't be able to do that that's
how 9 vessel algorithm Works let's look
at the definition of each of these
learning techniques
supervised learning uses labeled data to
train machine learning models labeled
data means that the output is already
known to you
the model just needs to map the inputs
to the outputs an example of supervised
learning can be to train a machine that
identifies the image of an animal
below you can see we have a trained
model that identifies the picture of a
cat
unsupervised learning uses unlabeled
data to train machines unlabeled data
means there is no fixed output variable
the model learns from the data discovers
patterns and features in the data and
Returns the output here is an example of
an unsupervised learning technique that
uses the images of vehicles to classify
if it's a bus or a truck
so the model learns by identifying the
parts of a vehicle such as the length
and width of the vehicle the front and
rear end covers roof hoods the types of
Wheels used Etc based on these features
the model classifies if the vehicle is a
bus or a truck
reinforcement learning trains a machine
to take suitable accents and maximize
reward in a particular situation
it uses an agent and an environment to
produce actions and Rewards the agent
has a start and an end state but there
might be different parts for reaching
the end State like a maze in this
learning technique there is no
predefined target variable an example of
reinforcement learning is to train a
machine that can identify the shape of
an object given a list of different
objects such as square triangle
rectangle or a circle
in the example shown the model tries to
predict the shape of the object which is
a square here
now
let's look at the different machine
learning algorithms that come under
these learning techniques
some of the commonly used supervised
learning algorithms are linear
regression logistic regression support
Vector machines K nearest neighbors
decision tree random forest and knife
base
examples of unsupervised learning
algorithms are key means clustering
hierarchical clustering DB scan
principle component analysis and others
choosing the right algorithm depends on
the type of problem you are trying to
solve
some of the important reinforcement
learning algorithms are Q learning Monte
Carlo sarsa and deep Q Network now let's
look at the approach in which these
machine learning techniques work
so supervised learning takes labeled
inputs and Maps it to known outputs
which means you already know the target
variable
unsupervised learning finds patterns and
understands the trends in the data to
discover the output so the model tries
to label the data based on the features
of the input data
while reinforcement learning follows
trial and error method to get the
desired solution after accomplishing a
task the agent receives an award
an example could be to train a dog to
catch the ball if the dog learns to
catch a ball you give it a reward such
as a biscuit
now let's discuss the training process
for each of these learning methods
so supervised learning methods need
external supervision to train machine
learning models and hence the name
supervised
they need guidance and additional
information to return the result
unsupervised learning techniques do not
need any supervision to train models
they learn on their own and predict the
output
similarly reinforcement learning methods
do not need any supervision to train
machine learning models
and with that
let's focus on the types of problems
that can be solved using these three
types of machine learning techniques so
supervised learning is generally used
for classification and regression
problems we'll see the examples in the
next slide
an unsupervised learning is used for
clustering and Association problems
while reinforcement learning is reward
based so for every task or for every
step completed there will be a reward
received by the agent
and if the task is not achieved
correctly
there will be some penalty used
now let's look at a few applications of
supervised unsupervised and
reinforcement learning
as we saw earlier supervised learning
are used to solve classification and
regression problems for example You can
predict the weather for a particular day
based on humidity precipitation wind
speed and pressure values
you can use supervised learning
algorithms to forecast sales for the
next month or the next quarter for
different products similarly you can use
it for stock price analysis or
identifying if a cancer cell is
malignant or benign
now talking about the applications of
unsupervised learning we have customer
segmentation So based on customer
Behavior likes dislikes and interests
you can segment and cluster similar
customers into a group another example
where unsupervised learning algorithms
are used is customer churn analysis
now let's see what applications we have
in reinforcement learning so
reinforcement learning algorithms are
widely used in the gaming Industries to
build games it is also used to train
robots to perform human tasks profit
estimation of a company if I was going
to invest in a company I would like to
know how much money I could expect to
make so we'll take a look at a venture
capitalist firm and try to understand
which companies they should invest in so
we'll take the idea that we need to
decide the companies to invest in we
need to predict the profit the company
makes and we're going to do it based on
the company's expenses and even just a
specific expense in this case we have
our company we have the different
expenses so we have our RND which is
your research and development we have
our marketing we might have the location
we might have what kind of
administrations going through based on
all this different information we would
like to calculate the profit now in
actuality there's usually about 23 to 27
different markers that they look at if
they're a heavy duty investor we're only
going to take a look at one basic one
we're going to come in and for
Simplicity let's consider a single
variable R and D and find out which
companies to invest in based on that so
when we take our r d and we're plotting
The Profit based on the r d expenditure
how much money they put into the
research and development and then we
look at the profit that goes with that
we can predict a line to estimate the
profit so we draw a line right through
the data when you look at that you can
see how much they invest in the RND is a
good marker as to how much profit
they're going to have we can also note
that companies spending more on R D make
good profit so let's invest in the ones
that spend a higher rate in their r d
what's in it for you first we'll have an
introduction to machine learning
followed by Machine learning algorithms
these will be specific to linear
regression and where it fits into the
larger model then we'll take a look at
applications of linear regression
understanding linear regression and
multiple linear regression finally we'll
roll up our sleeves and do a little
programming in use case profit
estimation of companies let's go ahead
and jump in let's start with our
introduction to machine learning along
with some machine learning algorithms
and where that fits in with linear
regression let's look at another example
of machine learning based on the amount
of rainfall how much would be the crop
yield so here we have our crops we have
our rainfall and we want to know how
much we're going to get from our crops
this year so we're going to introduce
two variables independent and dependent
the independent variable is a variable
whose value does not change by the
effect of other variables and is used to
manipulate the dependent variable it is
often denoted as X in our example
rainfall is the independent variable
this is a wonderful example because you
can easily see that we can't control the
rain but the rain does control the crop
so we talk about the independent
variable controlling the dependent
variable let's Define dependent variable
as a variable whose value change when
there is any manipulation the values of
the independent variables it is often
denoted as Y and you can see here our
crop yield this dependent variable and
it is dependent on the amount of
rainfall received now that we've taken a
look at a real life example let's go a
little bit into the theory and some
definitions on machine learning and see
how that fits together with linear
regression numerical and categorical
values let's take our data coming in and
this is kind of random data from any
kind of project we want to divide it up
into numerical and categorical so
numerical is numbers age salary height
where categorical would be a description
the color a dog's breed gender
categorical is limited to very specific
items where numerical is a range of
information now that you've seen the
difference between numerical and
categorical data let's take a look at
some different machine learning
definitions when we look at a different
machine learning algorithms we can
divide them into three areas supervised
unsupervised reinforcement we're only
going to look at supervise today
unsupervised means we don't have the
answers we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information until after the fact but to
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polynomial linear regression now on
these three simple linear regression is
the examples we've looked at so far
where we have a lot of data and we draw
a straight line through it multiple
linear regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it'd be multiple
linear regression and finally we have
polynomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricket
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some euclidean geometry the simplest
form of a simple linear regression
equation with one dependent and one
independent variable is represented by y
equals m times X plus C and if you look
at our model here we plotted two points
on here X1 and y1 X2 and Y2 y being the
dependent variable remember that from
before and X being the independent
variable so why depends on whatever X is
m in this case is the slope of the line
where m equals the difference in the Y2
minus y1 and X2 minus X1 and finally we
have C which is the coefficient of the
line or where it happens to cross the
zero axes let's go back and look at an
example we used earlier of linear
regression we're going to go back to
plotting the amount of crop yield based
on the amount of rainfall and here we
have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y-axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we created a nice line
right through the middle to give us a
nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes
I want you to note that when we get into
the case study and we actually apply
some python script that this math you're
going to see here is already done
automatically for you you don't have to
have it memorized it is however good to
have an idea what's going on so if
people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was one we got y equals 2
when X was 2 y was 4 and so on and so on
if we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is what the means is of each of the
data coming in the X and the Y the means
doesn't mean anything other than the
average so we add up all the numbers and
divide by the total so one plus two plus
three plus four plus five over five
equals three and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get three
comma four which draws a nice line down
the middle a good estimate here we're
going to dig deeper into the math behind
the regression line now remember before
I said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whiz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the X Y points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x times Y and
then it takes all of X and adds them up
all of Y adds them up all of x squared
adds them up and so on and so on and you
can see we have the sum of equal to 15
the sum is equal to 20 all the way up to
x times Y where the sum equals 66 this
all comes from our formula for
calculating a straight line where y
equals the slope times X plus a
coefficient C so we go down below and
we're going to compute more like the
averages of these and we'll explain
exactly what that is in just a minute
and where that information comes from is
called the square means error but we'll
go into that in detail in a few minutes
all you need to do is look at the
formula and see how we've gone about
Computing it line by line instead of
trying to you have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and then the
top part if you read through the
brackets you have the number of data
points times the sum of x times Y which
we computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 times 20. and on
the bottom we have the number of lines
times the sum of x squared easily
computed as 86 for the sum minus I'll
take all that and subtract the sum of x
squared and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 0.6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equals 0.6 and C equals 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y equals
0.6 times where x equals 1 plus 2.2
equals 2.8 so on and so on and here the
Blue Points represent the actual y
values and the brown points represent
the predicted y values based on the
model we created the distance between
the actual and predicted values is known
as residuals or errors the best fit line
should have the least sum of squares of
these errors also known as e-square if
we put these into a nice chart we can
see X and you can see Y what the actual
values were and you can see why it
predicted you can easily see where we
take y minus y predicted and we get an
answer what is the difference between
those two and if we square that y minus
y prediction squared we can then sum
those squared values that's where we get
the 0.64 plus the 0.36 plus 1 all the
way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for each line and conclude
the best fit line having the least e
Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least squared distance between
the data points and the regression line
now we only looked at the most commonly
used formula for minimizing the distance
there are lots of ways to minimize the
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc which you want to take away from
this is whatever formula is being used
you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations and F
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two values when
you're figuring out a solution so let's
move on to the next topic multiple
linear regression let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have well we'll start with
the simple linear regression where we
had y equals M plus X plus C and we're
trying to find the value of y now with
multiple linear regression we have
multiple variables coming in so instead
of having just X we have X1 X2 X3 and
instead of having just one slope each
variable has its own slope attached to
it as you can see here we have M1 M2 M3
and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y equals
M1 times X1 plus M2 times X2 so on all
the way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R and D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict as profit but instead of
predicting it just on the R and D we're
going to look at other factors like
Administration costs marketing cost and
so on and from there we're going to see
if we can figure out what the profit of
that company is going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data to see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
and just see how valid our solution is
so let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy because it's such a
visual to look at it's so easy to use
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries were
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as PD
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is usually denoted as NP and you have to
almost have that for your SK learn
toolbox so you always import that right
off the beginning pandas although you
don't have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so so we
usually like to use the pandas when we
can and I'll show you what that looks
like the other three lines are for us to
get a visual of this data and take a
look at it so we're going to import
matplotlibrary.pi plot as PLT and then
Seaborn as SNS Seabourn works with the
matplot library so you have to always
import matplot library and then Seaborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the Seaborn is so easy to use
it just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber signed matplot library
in line that is only because I'm doing
an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting so we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in the slide you'll
see companies equals pd.read CSV and it
has a long line there with the file at
the end one thousand companies.csv
you're going to have to change this to
fit whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals
pd.reader underscore CSV and I've
changed this path to match my computer C
colon slash simply learn slash 1000
underscore companies.csv and then below
there we're going to set the x equals to
companies under the I location and
because this is companies as a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except up
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a pandas data set but we can easily
extract the data from our pandas data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies dot head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the pandas sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
zero one two three four and then it
shows your different columns R and D
spend Administration marketing spend
State profit it even notes that the top
top are column names it was never told
that but pandas is able to recognize a
lot of things that they're not the same
as the data rows why don't we go ahead
and open this file up in a CSV so you
can actually see the raw data so here
I've opened it up as a text editor and
you can see at the top we have r d spend
comma Administration comma marketing
spin comma State comma profit carries
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
165
349.20 compared to the administration
cost of 136
897.80 so on so on helps to create the
the profit of 192 261 and 83 cents that
makes no sense to me whatsoever no pun
intended so let's flip back here and
take a look at our next set of code
where we're going to graph it so we can
get a better understanding of our data
and what it means so at this point we're
going to use a single line of code to
get a lot of information so we can see
where we're going with this let's go
ahead and paste that into our notebook
and see what we got going and so we have
the visualization and again we're using
SNS which is pandas as you can see we
imported the matplot Library dot Pi plot
is PLT which then the Seaborn uses and
we imported the Seaborn as SNS and then
that final line of code helps us show
this in our inline coding without this
it wouldn't display and you could
display it to a file in other means and
that's the matplot library in line with
the Amber sign at the beginning so here
we come down to the single line of code
Seabourn is great because it actually
recognizes the panda data frame so I can
just take the comp companies dot core
for coordinates and I can put that right
into the Seaborn and when we run this we
get this beautiful plot and let's just
take a look at what this plot means if
you look at this plot on mine the colors
are probably a little bit more purplish
and blue than the original one we have
the columns and the rows we have R and D
spending we have Administration we have
marketing spending and profit and if you
cross index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so r d spending is going
to be the same as r d spending and the
same thing with Administration costs so
right down the middle you get this dark
row or dark diagonal row that shows that
this is the highest corresponding data
that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with r d spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
jupyter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies dot
head and we printed the top five rows of
data we have our columns going across we
have column 0 which is R and D spending
column one which is Administration
column two which is marketing spending
and column three is State and you'll see
under State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a New York it has a 0 or 1 or a
two and then finally we need to do a one
hot encoder which equals one hot encode
or categorical features equals three and
then we take the X and we go ahead and
do that equal to one hot encoder fit
transform X to array this final
transformation preps our data Force so
it's completely set the way we need it
is just a row of numbers even though
it's not in here let's go ahead and
print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers
next on setting up our data we have
avoiding dummy variable trap this is
very important why because the computers
automatically transformed our header
into the setup and it's automatically
transformed all these different
variables so when we did the encoder the
encoder created two columns and what we
need to do is just have the one because
it has both the variable and the name
that's what this piece of code does here
let's go ahead and paste this in here
and we have x equals x colon comma one
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label
encoding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first step is
going to be in splitting the data now
whenever we create a predictive model of
data we always want to split it up so we
have a training set and we have a
testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and I'll go ahead and shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our RND spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test split we take X and
we take y we've already created those X
has the columns with the data in it and
Y has a column with profit in it and
then we're going to set the test size
equals 0.2 that basically means 20
percent so twenty percent of the rows
are going to be tested we're going to
put them off to the side so since we're
using a thousand lines of data that
means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting so setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahead and
put that code in there and walk through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the sklearn dot linear
underscore model we're going to import
linear regression now I don't know if
you recall from earlier when we were
doing all the math let's go ahead and
flip back there and take a look at that
do you remember this or we had this long
formula on the bottom and we were doing
all this summarization and then we also
looked at setting it up with the
different lines and then we also looked
all the way down to multiple linear
regression where we're adding all those
formulas together all of that is wrapped
up in this one section so what's going
on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor.fet in this case we do X
train and Y train because we're using
the training data X being the data n and
y being profit what we're looking at and
this does all that math for us so within
one click and one line we've created the
whole linear regression model and we fit
the data to the linear regression model
and you can see that when I run the
regressor it gives an output linear
regression it says copy x equals True
Fit intercept equals true in jobs equal
one normalize equals false it's just
giving you some general information on
what's going on with that regressor
model now that we've created our linear
regression model let's go ahead and use
it and if you remember we kept a bunch
of data aside so we're going to do a why
predict variable and we're going to put
in the X test and let's see what that
looks like scroll up a little bit paste
that in here predicting the test set
results so here we have y predict equals
regressor dot predict X test going in
and this gives us y predict now because
I'm in Jupiter in line I can just put
the variable up there and when I hit the
Run button it'll print that array out I
could have just as easily done print y
predict so if you're in a different IDE
that's not an inline setup like the
Jupiter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side is going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor dot coefficient
underscore when I run this you'll see
our coefficients here and if we can do
the regressor coefficient we can also do
the regressor intercept and let's run
that and take a look at that this all
came from the multiple regression model
and we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y equals M1 times X1 plus M2
times X2 and so on and so on plus C the
coefficient so these variables fit right
into this formula y equals slope one
times column one variable plus slope 2
times column two variable all the way to
the m into the n and x to the N plus C
the coefficient or in this case you have
minus 8.89 to the power of 2 etc etc
times the First Column and the second
column and the third column and then our
intercept is the minus one zero three
zero zero nine point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r squared value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in Co and so we're going to use
this from
sklearn.metrix we're going to import R2
score that's the r squared value we're
looking at the error so in the R2 score
we take our y test versus our y predict
y test is the actual values we're
testing that was the one that was given
to us so we know are true the why
predict of those 200 values is what we
think it was true and when we go ahead
and run this we see we get a
0.9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93 percent correct but
you do want that in the upper 90s oh and
higher shows that this is a very valid
prediction based on the R2 score and if
r squared value of 0.91 or 92 as we got
on our model could remember it does have
a random generation involved this proves
the model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression
brings you comprehensive artificial
intelligence boot camp that will cover a
wide range of topics that will Empower
you to the knowledge and skills needed
to excel in the field of AI to learn
more about this course you can find the
course Link in the description box below
near regression now now that we have
come this far I'm planning to take a
closer look at our cells so that we can
estimate cells in the future how about
we hire a data scientist you can see our
two corporate individuals looks more
like they should be agents or secret
agents someplace discussing how to
better forward their company and so
they're going to come in and ask the
data scientist to come in good idea this
will help us to keep a constant track of
ourselves so why do we need linear
regression let's assume that we need to
predict the number of skiers based on
snowfall so this happens to be we've
kind of jumped one business to the next
so we're looking at the skiing business
very popular in a lot of areas and it's
based on snowfall a lot of times you
figure you don't have snow you don't
have skiers but can we actually use
something more specific instead of look
it's snowing and instead of saying Hey
look it's snowing we can actually start
drawing graphs and the graph shows that
with an increase in snowfall the
frequency of skiers also increases so
and there's a pretty direct correlation
if you've ever been up to ski areas when
there's a lot of snowfall skiers all
shove because they know it's going to be
better skiing right afterward so it's
kind of easy to see why skiers and
snowfall would go together and it
usually draws a nice straight line and
you can easily predict how many skiers
and that way you can also predict how
many people you need to service them how
many lifts they have up and running and
all the stuff that goes with running a
ski area thus we see that the number of
skiers are directly proportional to the
amount of snowfall regression models a
relation between a dependent Y and an
independent X variable this is real
important to understand regression
because when we talk about linear
regression it's the basis of almost all
our machine learning algorithms out
there and it's usually an underlying
part of the math in our deep learning so
it all starts here with linear
regression and we talk about a dependent
Y and an independent X variable these
are numbers we're usually talking about
floats so we want to have an actual
number value coming out and that's
different than something that's
categorical or we want to know yes or no
true false so regression means we're
looking for a number the independent
variable variable is known as predictor
variable and dependent variable is known
as the response variable so we have one
predictor variable the amount of
snowfall and one response variable how
many skiers are going to show up and
then we can take this relationship and
it can be expressed as y equals beta
we're going to use the Greek letter beta
and we have beta naught plus beta1 X1
and if you continue out to be plus beta
2 x 2 and so on till the nth degree in
this case snowfall is an independent
variable and skiers is a dependent
variable so we kind of had a little
quick overview let's go ahead and talk a
little bit more about what is linear
regression so we're going to go from y
predicting a number of skiers for
snowfall and we looked at the formula a
little bit but let's look a little bit
closer at what exactly is going on with
linear regression and the question is
going to come up in an interview what is
linear regression linear regression is a
type of statistical analysis that
attempts to show the relationship
between two variables linear regression
creates a predictive model on any data
showing Trends and data the model is
found by using the least Square method
there are other methods the least Square
method happens to be the most commonly
used out there and we usually start with
the least Square methods since it's the
most common and usually works the best
on most models and we're already looking
at how linear regression works but let's
dig deeper into it and let's take a
closer look how linear regression works
I will provide you with a data set which
has rent area other information in it
looks like our secret agents have put on
their casual wear for this one you need
to predict rent accordingly we are given
area and rent here so you can do linear
regression with more variables but we're
just going to look at the area and then
from that try to figure out what the
rent should be we plot the graph and if
you look at it here you can see that the
graph kind of a linear pattern with a
little dip in it so the area seems the
rent seems to be based on the area in
most of our work as data scientists we
usually try to start with a physical
graph if we can so we can actually look
at it and say hey does what I'm fitting
to this graph look right you can solve a
lot of problems that arise by looking at
the graph and saying no that doesn't
look right at all or oh I should be
looking over here for this then we find
the mean of area and rent so the mean
just means average and if you take one
two three four five and add them all
together and divide by in this case
there's five areas you get three and the
rent is two four six five and eight if
you add them all together and divide by
five you get five and we plot the mean
on the graph so you can see right here
here's the mean of the rent and the area
it's kind of a very Central Point which
is what we're looking for the average of
everything in there the best fit line
passes through the mean this is real
important when you're eyeballing it as
humans we can do this fairly easy if
it's a straight line through a bunch of
data it gets more complicated
mathematically when you start adding
multiple variables and instead of a line
you have a curve but for basic linear
regression we're going to draw a line
through the data and and the line should
go through the means that's going to be
part of the best fit for that line but
we see there are multiple lines that can
pass through the means so depending on
how you look at it you can kind of
Wiggle the line around back and forth so
we keep moving the line to ensure that
the best fit line has the least squared
distance from the data points and this
is you can see right here residual we
have our data point and then we look at
this distance between them and we square
that distance and that's what they mean
by the least squared distance we want
all those distances to add up to the
smallest amount we can we talk about the
distance we call it the residual it
equals the Y actual minus the Y
predicted very straightforward we're
just looking at the distance you can
accidentally switch these and it'll come
out the same because we're going to
square it but when you're working with
other sets of linear regression you want
to make sure you do the Y axial minus
the Y predicted the value of M and C for
the best fit line Y equals MX plus C can
be calculated using the mentioned
formula and you can see here we have m
equals the number of points that's what
n stands for then we have the sigma the
Greek symbol is there which is a
summation of x times y minus the
summation of x times the summation of Y
over the number of the summation of x
squared minus the summation of x squared
of X all of it squared that can be a
little confusing trying to say that and
then of course your C value is going to
be the summation of Y times the
summation of x squared minus the
summation of x times the summation of x
times y over the total count of the
summation x squared minus the summation
of x squared that's a mouthful normally
we don't worry too much about these
formulas other than to know they're
there now if you're a mathematician you
might go back in there and work on those
and those people who originally started
to put together the different models in
R of course had to know all this math to
build them and had to test it out that's
one of the nice things about R what is
important is that you know about these
formulas so that you know where it's
coming from so if you're using one
linear regression model this is how this
one works there's other linear
regression models based on different
math and so you'd have to be aware when
that math changes and how that changes
the model we find the corresponding
values so in this case if you're going
to break this up and you're building
your own model instead of using the one
in R you would have your rent you would
have your area you would find your rent
squared your area squared and your rent
times area and if we go back one we can
see that that's all the different little
pieces in this model we have x times y
we have x squared we have the sum of X
which is going to be squared we have the
sum of X the sum of Y this is the same
these are all the little pieces in here
that we see for m equals and C equals
and so once we compute all these we have
15 25 55 145 88 very easy to do on a
computer thankfully you could have
millions of these points and it would do
it in a very short period we can then
plug in those values very easily into
this formula and get a final answer and
we'll see that m equals 1.3 and C equals
1.1 and then when we take this for the Y
predicted equals m of i x of I plus C we
can take these and actually run the data
we have so we're going to see how it
predicts so now we find the Y the value
of y predicted so we have our X in and
we want to know what Y what we think y
will be based on our formula we
generated the linear regression formula
and so when we come in here we have our
X we have our actual y then we have our
y predict what we think it's going to be
and then we have we take y minus y
predict and we get those values minus
0.4 it's off by 0.3 this one's off by
one this one's on off by minus 1.3 by
0.4 and we go ahead and square that and
then we can sum that square and average
it out and we'll get a value this is
just a summation down here of Y minus y
predicted square is 3.1 and we call that
the least Square value for this line is
3.1 and we go ahead and divide that by
five so when we plot the Y predict and
this is the best field it line and you
can see that it does a pretty good job
going right through the middle of lines
and in something like rent versus area
if you're trying to figure out how much
rent to charge or how many people are
allowed to be in the area that's going
to work but if you're looking for the
rent value compared to the area this
gives you a good idea based on the area
of what you should rent the place for
it's close enough that in the business
world this would work you're not
Computing something down to the
millimeters or micrometers or Nuclear
Physics we're just charging people and
so if you're off by a couple dollars
it's not a big deal you'll be pretty
close to what you think it's worth and
get the right value for your property
use case predicting the revenue using
linear regression now that you have a
good idea of how linear regression works
and we're kicking back on our lounging
sofa today let's work on a real life
scenario where you have to predict the
revenue so we're going to take a data
set and we'll pull this up in just a
minute we'll have a paid organic social
and revenue there are three attributes
we'll be working on the first one is our
paid traffic so all the traffic through
the advertisement that comes the
non-paid traffic from search engines so
these are people coming to the website
and are doing so because they did a
search for something specific so it's
pulling it from your website all the
traffic coming from social networking
sites so we want to know what's coming
from Twitter and Facebook and somehow
they've linked into your website and
coming into your marketing area to buy
something and we'll use this to predict
the revenue so we want to know how all
this traffic comes together and how it's
going to affect our cells if the
traffic's coming in we're going to make
use of the multiple linear regression
model so you'll see that this is very
similar to we had before where we had y
equals m times X plus C but instead we
have y equals M1 X1 plus M2 X2 plus m3x3
plus c m is the slope so you have M1 M2
M3 the three different slopes of three
different lines Why is the revenue so
we're only looking for one variable and
remember this is regression so we're
looking for a number X1 is going to be
the paid traffic and of course M1 the
corresponding slope X2 the organic
traffic and X3 the social traffic we
will follow these steps to create the
regression model we'll start by
generating inputs using the CSV files
we'll then import the libraries
splitting the data into train and test
very important whenever you're doing any
kind of data science applying regression
on paid traffic organic traffic social
traffic so we're going to apply a
regression model on them and then we're
going to validate the model that's the
whole reason we split it to a training
and testing is so we can now validate to
see just how good our model is now
before we open up our I always like to
just take a quick look at the data
itself and we're using the rev.csv file
and this is a DOT CS file or comma
separated variable file we'll just open
that with wordpad any kind of text
editor would be nice to show you what's
going on and you can see we have paid
organic social and revenue across the
top so those are titles for your columns
and the values going down and this is
the same as what we saw on the slide
just a minute ago paid organic social
and then the revenue from them they
probably have it in more of a
spreadsheet than just a word pad and I'm
using the newer version of our studio
which already has a nice layout
automatically opens up with your console
and terminal window down and left you
have your script and we'll run it in
script and then when I run it line by
line in script you'll see it show up on
the console bottom left I could type it
straight into the console and it would
do the same thing and then on the right
we have our environment and under the
environment we'll see plots if you're in
an older version the plots will pop up
and you usually are running just in the
console and then you have to open up a
different window to run the script and
you can run the script from there this
opens up everything at the same time
when you use the new R Studio setup and
the first thing we want to do is we want
to go ahead and generate a variable
we'll call it cells and we want to load
this with our data set and so uh in R we
use the less than minus sign that's the
same as assigned to it's like an arrow
pointing at cells it says whatever it
goes past here is going to be assigned
to cells we want to read in our CSV file
and then I went ahead and fixed the CSV
file up a little bit when I say the CSV
file I mean the path to it
um and we'll go ahead and just paste
that in there and this is my edited
version and you'll see I took wherever
we had the backward slash I switched it
for a forward slash because a backwards
slash is an escape character and
whenever you're programming in just
about any scripting language you want to
do the forward slash instead so like a
good Chef I prep this and copied it into
my clipboard and we can see the full
path here and the full path ends in our
rev.csv files that's the path on my
machine and I'm going to run that and
now we have down here you see it appear
right here oh it must have an error in
my path it says object rev not found let
me fix that real quick and run it again
and this time it does assign the cells
the data set and then we'll just type in
cells and this is the same as printing
it so if you work in other languages
you'll do like print and then sales or
maybe print sales.head if you're in
pandas in python or something like that
but in R you just type in the variable
we'll run this and you can see it
printed out all those columns so that's
the same thing we just looked at and it
says Max print so it emitted 750 rows so
so it has a total of a thousand rows and
we can scroll up here on our console
where the answer came in and if I scroll
up to the top whoops way up to the top
there we go you'll see paid organic
social and revenue so everything looks
good on this and there's a lot of cool
things we can do with it one of the
things we can do is I can summary it so
let's do a summary of cells and we'll
run that and the summary of cells comes
in it tells you your Min first quarter
median mean third quarter Max for paid
organic social breaks it up and you can
actually get kind of a quick view as to
what the data is doing you can take more
time to study this a lot of people look
at this and you can see from the median
the mean that tells you a lot and is
divided into quarters so you have your
first quarter value and your third
quarter setup on there and Max it just
gives you a quick idea what's going on
in the data summary is a really
wonderful tool one of the reasons I like
to jump into R before I might do a major
project in something else or you can run
everything enough and let's take a look
at the head head of cells and let me run
this and you can see it just shows the
head of our data the first six rows in
this case and it starts with one
important to know the other different
systems start with zero and go up R is
one that starts with the first is one
and then if we want to do a plot I'll go
ahead and do a plot cells another really
powerful tool that you can just jump
right into with r once you have your
data in here and you'll see that the
plot comes over on the right hand side
and what it does is it Compares them so
if I look at my paid and my organic and
you cross a two you'll see there's a
nice line down the middle big black line
where plus the two together let me just
widen this a little bit so we can see
more of it there we go same thing with
paid and social paid and revenue so each
one of these has a clear connection to
each other so that's a good sign we have
a nice correlation between the data and
you can just eyeball that that's a good
way to jump in and explore your data
rather quickly and see what's going on
so we'll go ahead and I'll put a pound
sign there a hashtag and splitting the
data into training and test data and we
want to split it so that we have
something to work with to train our data
set and when we do our data set you do
this with any data sets you want to make
sure you have a good model and then once
we've trained it we're going to take the
other set of data we pulled off to the
side and we'll test it to see how good
it is if you test it with the training
data that's already got that memorized
and should give you a really good answer
in fact when you get into more advanced
machine learning tools you watch those
very closely you watch your training set
and how it does versus your testing set
and if your training set starts doing
better than your testing set then you're
starting to over train it now that's
more with deep learning and some other
packages that are beyond what we're
working with today but you know this is
important it's very important to
understand that that test and that
training correlate with each other gives
you an idea what's going on and make
sure that you're set up correctly and
we'll go ahead and set a seed of two and
that's for our random gen generators so
that when we run them you can see them
with just a red there's a ways to
randomize a number you can do like date
and stuff like that but that way we
always use the same random numbers so
we'll seat it with two and then we're
going to need a library in this case
we're going to import the library and we
want to import CA tools and CA tools has
our split function so we're going to be
able to split our data along with many
other tools we're going to need for this
example so they're all built into the ca
tools that's why we need to import that
a lot of times they call them library or
sometimes you hear them referred to as
packages so let's go ahead and run that
line and that brings the library in so
now we can use all those different tools
that are in there and then I'm going to
do a split and we're going to assign to
the split that's going to be our
variable sample split okay so that's one
of the tools we just brought in is to
sample is the actual keyword or the
function word sample.split and we're
going to take our cells and we're going
to go ahead and with split ratio equals
to 0.7 and let's do 0.7 so we make sure
it knows it's a decimal point or float
and what this is saying is that we're
going to take our cells data the
variable we created called cells that
has the data in there and I want to
split it and I'm going to split it so
that 70 percent of it goes in one side
we'll use that for training and 30
percent will go into the other side
which we'll use then for testing out our
model let me go ahead and run that and
let's hold on one second got an error
there spit is definitely very different
than split I don't think we want to spit
our data out we which is actually kind
of what we're doing is spitting it out
but we want to split we want to split
the data something get that spelled
correctly and then when I type in Split
we can just run that because that's now
a variable you'll see that it has true
false false true so it generates a
number of interesting different
statistics going across there as far as
the way it splits the data and right
here if you have not used R in a while
or if you're new to R completely that
line one that's what that little one
means down there and then true false
Falls true that means that's how we're
looking at the data we're splitting all
those different pieces of data in line
one different directions and so we now
want to create our trained set and we're
going to assign that and when we take it
and we assign to the train set a subset
of cells and then the subset of cells is
going to be based on split equal let me
put this in Brackets true so you can see
the values down here is capital T
capital r capital u capital E so I just
want to reflect that on the train and
this is going to take everything where
our split variable is equal to true and
it's going to set cells equal to that
and we'll go ahead and run that and then
we're going to set our test variable as
a subset of cells and if we assign the
true to the train set then we want to
assign the false to our test set so now
we'll have this will have 30 percent of
the variables in it and train will have
70 percent we'll go ahead and run that
so we've now created both our training
set and our test set and we can just
real quickly type in train hit the run
on there and you can see our train set
if we scroll way up to the top I'll have
the column names on there which should
match what we had before paid organic
social and revenue and then we'll type
in test when I hit run on there that's
going to do the same thing it'll sped
out all the test variables going out so
now that we have the test and train
variables we want to go ahead and create
the model and you'll see this in any of
the machine learning tutorials they
always refer to them as models we're
modeling a function of some kind on the
data to fit the data and then we're
going to put the test data through there
to see how well it does and so we're
going to create the variable called
models and I'm going to assign that LM
that is our linear regression model I
love how simplified R makes it just LM
linear regression and then model M
I guess it dates back because the linear
regression model is like one of the
first major models used so they kept it
easy on there and revenue happens to be
the main variable that we want to track
so if you remember correctly from our
formula why is the revenue and then we
have X is our paid traffic X2 organic
traffic and X3 are social traffic so why
is what we want to predict in here and
then you'll see this notation where we
have our squiggle and the period which
means we're going to match up the
revenue lines with the lines of the data
we're putting in here and then I'll put
comma and then our data equals train and
of course that's the training data that
we created so when I hit the Run
remember we did all the discussion about
all those different functions and
formulas to compute our model and how
that's set up when it comes down to it
we spend all our time setting up our
data and then I hit the Run button on
the single line and our model's been
trained so we now have a trained model
here and we can do a summary of the
model summary is such a wonderful
command because you can do that on r
that works on all kinds of different
things on there oops and of course it
does help if I remember that my model
has a capital N when I run it and you'll
see right here tells you a little bit
about the model use a summary on there
comes down here it has our residuals
this is all the information on there as
far as like the minimum the median the
Max has all our coefficients in there if
you remember correctly from our formula
let's just go ahead and switch back over
to that we have M1 X1 M2 X2 M3 X3 plus C
well this right here here's our
intercept and then we have our different
values for each of these so you have our
intercept our paid organic and social
and then it also shows us error and
information on the error and one of the
really nice things about when you're
working with r you can come down here
and you see where we have our Stars down
here and it says three stars really good
two stars maybe one star probably no
correlation and we can see with all of
these it has three stars on them so out
of three stars we get three stars on all
of these there's no four stars four
stars would mean that you have the exact
same data going in as coming out and
then if we're going to do this we want
to actually run a prediction on in here
and that's that we saved our test data
for so let's come down here we'll do our
prediction we'll take this variable and
we'll assign it predict model remember
the predict comes from when we imported
that package the ca tools that's all
part of the ca Tools in there so we're
going to predict we're going to use the
model and then we're going to use our
test data pretty straightforward quick
and easy we'll run this and then if we
go ahead and type in the predict it'll
print out what's in that variable and
you'll see down here the predicted
values expects to come out it's going to
say our revenue is and it goes through
and it gives it both the line number and
the actual Revenue value so that's quick
and easy boy we got a prediction really
fast and this is also how you would do
it if you had new data coming in after
this you could predict the revenue based
on these other factors so now that we
have a training model which or we train
the data with our training data or we
trained the model with our training data
and we've done a prediction on our model
with our test data we want to look this
up and we took a quick glance from our
training thing our training said it
trained really well but that's not the
final word on it the final word is going
to be comparing it so we want to go
ahead and do comparing predicted versus
actual values what I'm going to do is
I'm going to do a plot and I'm going to
do our test and we're going to do test
Revenue DOT type equals and we can put
this in as a type type 1 or L and then
we have lty equals 1.8 and we're going
to set up for column blue that's a lot
of stuff in there let's go ahead and
just run that so we can see what that
looks like and what that's doing it's
going to generate this nice graph we see
on the right there's our graph you can
see the data comes up and down we're
plotting the prediction on there so this
is the values we predicted and then
let's go ahead and do lines and I
actually did this backwards so let me
try that again and we'll start with the
red one we're going to do the first one
in red and I want to start with the
actual test Revenue so here's our test
revenue and we'll go ahead and run this
and so we have a red plot over here in
red go ahead and take a look at that and
pull that over you can see how that
looks goes up and down hard to track
there and then we're going to do a
couple things here we'll plot our test
Revenue
and then we'll make it a little prettier
though it's kind of hard to say see the
way I was crunched up here on the right
as far as size and sizing and everything
but we'll go ahead and have our columns
in blue and run our prediction on there
too you can see they overlap the two
graphs so we have our test revenue and
our prediction and then finally what I
started with is we'll go ahead and plot
the prediction fully on the bottom and
run that and if you look over here on
the graph we put the blue lines over the
red lines and so you'll see a couple
spots where there's red underneath there
but for the most part our prediction is
pretty right on so it looks really tight
looks like a good set of predictions for
what we're working on and this is where
we're looking at the slide right before
we started diving into the r Studio we
can see in the slide here here's a red
this little bit better spread out than
what I have on my screen from AR studio
and the graph shows a predicted Revenue
we see that the two lines are very close
so again they're tight it's right on
this is what we're looking for in our
prediction model but it's not good
enough just having a graph you always do
both and the reason we do both is you
want to have a visual of it because
sometimes you look at these you get to
the graph you're like oh my gosh what is
that and then sometimes you look at the
graph you go that's right on how come my
accuracy doesn't look right and you
realize that your accuracy might be off
or you're plotting it incorrectly so
let's go ahead and look up the accuracy
and we'll use rmsc that's going to be
the variable name we're going to give it
and sqrt the square root of the mean and
mean just means average and then we want
prediction minus
sales revenue and then we're going to
take this whole thing and we're going to
square it so what we've got here is
we're going to go through let's just
explain this formula we're using is I'm
going to look at the what I predicted
for to be and what the actual cells
what's our prediction versus our sales
comparison to the revenue and when we
compare those two we're going to find
the average and then I'm going or we're
going to square each one of those values
and then we're going to average the
square and then find the square root of
that and that's quite a mouthful the
reason we do it this way the reason we
Square the value and then we find the
means is to generate an answer based on
doesn't matter whether it's plus or
minus there's a lot of other formulas
you can use there to check your accuracy
and all kinds of other things you can do
but a quick straightforward way of doing
is just like this and then let's go
ahead and run this
and then we type in the rmsc and they'll
give us an actual printed value out
let's just see what that looks like and
so we have
866.6338 for our accuracy and so we look
at the
866.63 and we compare that to say 50
000. that's pretty close that's that's
actually a pretty good accuracy given
this model takes a little bit more when
you're playing with accuracies and start
dividing out other different aspects of
it to get something you can communicate
better with but you always start with
the square root means that's always a
good place to start what is logistic
regression let's say we have to build a
predictive model or a machine learning
model to predict whether the passengers
of the Titanic ship have survived or not
the Shipwrecked so how do we do that so
we use logistic regression to build a
model for this how do we use logistic
regression so we have the information
about the passengers their ID whether
they have survived or not their class
and name and so on and so forth and we
use this information where we already
know whether the person has survived or
not that is the labeled information and
we help the system to train based on
this information with based on this
labeled data this is known as label data
and during the process of building the
model we probably will remove some of
the non-essential parameters or
attributes here we only take those
attributes which are really required to
make these predictions and once we train
the model we run new data through it
whereby the model will predict whether
the passenger has survived or not let's
start with what is supervised learning
supervised learning is one of the two
main types of machine learning methods
here we use what is known as labeled
data to help the system learn this is
very similar to how we human beings
learn so let's say you want to teach a
child to recognize an apple how do we do
that we never tell the child okay this
is an apple has a certain diameter on
the top a certain diameter at the bottom
and this has a certain RGB color no we
just show an apple to the child and tell
the child this is Apple and then next
time when we show an apple child
immediately recognizes yes this is an
app supervised learning works very
similar on the similar lines so where
does logistic regression fit into the
overall machine learning process machine
learning is divided into two types
mainly two types there is a third one
called reinforcement learning but we
will not talk about that right now so
one is supervised learning and the other
is unsupervised learning unsupervised
learning uses techniques like clustering
and Association and supervised learning
users techniques like classification and
regression now supervised learning is
used when you have labeled data you have
historical data then you use supervised
learning when you don't have labeled
data then you used unsupervised learning
it's it's supervised learning there are
two types of techniques that are used
classification and regression based on
what is the kind of problem we have
solved let's say we want to take the
data and classify it it could be binary
classification like a zero or a one an
example of classification we have just
seen whether the passenger has survived
or not survived like a zero or one that
is known as binary classification
regression on the other hand is you need
to predict a value what is known as a
continuous value classification is for
discrete values regression is for
continuous values let's say you want to
predict a share price or you want to
predict the temperature that will be
there what will be the temperature
tomorrow that is where you use
regression whereas classification are
discrete values is will the customer buy
the product or will not buy the product
will you get a promotion or you will not
get a promotion I hope you're getting
the idea or
would be multi-class classification as
well let's say you want to build an
image classification model so the image
classification model would take an image
as an input and classify into multiple
classes whether this image is of a cat
or a dog or an elephant or a tiger so
there are multiple classes so not
necessarily binary classification so
that is known as multi-class
classification so we are going to focus
on classification because logistic
regression is one of the algorithms used
for classification now the name may be a
little confusing in fact whenever people
come across logistic regression it
always causes confusion because the name
has regression in it but we are actually
using this for performing classification
okay so yes it is logistic regression
but it is used for classification and in
case you are wondering is there
something similar for regression yes for
regression we have linear regression
keep that in mind so linear regression
is used for regression logistic
regression is used for classification so
in this video we are going to focus on
supervised learning and within
supervised learning we are going to
focus on classification and then within
classification we are going to focus on
logistic regression algorithm so first
of all classification so what are the
various algorithms available for
performing classification the first one
is decision tree there are of course
multiple algorithms but here we will
talk about a few addition trees are
quite popular and very easy to
understand and therefore they use for
classification then we have K nearest
neighbors this is another algorithm for
performing classification and then there
is logistic regression and this is what
we are going to focus on in this video
and we are going to go into little bit
of details about logistic regression all
right what is logistic regression as I
mentioned earlier positive regression is
an algorithm for performing binary
classification so let's take an example
and see how this one let's say your car
has not been serviced for quite a few
years and now you want to find out if it
is going to break down in the near
future so this is like a classification
problem find out whether your car will
break down or not so how are we going to
perform this classification so here's
how it looks if we plot the information
along the X and Y axis X is the number
of years since the last service was
performed and why is the probability of
your car breaking down and let's say
this information was this data that was
collected from several car users it's
not just your car but several car users
so that is our labeled data so the data
has been collected and for for the
number of years and when the car broke
down and what was the probability and
that has been plotted along X and Y axis
so this provides an idea or from this
graph we can find about whether your car
will break down or not we'll see how so
first of all the probability can go from
zero to one as you all aware probability
can be between zero and one and as we
can imagine it is intuitive as well as
the number of years are on the Lower
Side maybe one year two years or three
years still after the service the
chances of your car breaking down are
very limited right so for example
chances of your car breaking down on the
probability of your car breaking down
within two years of your last service
are 0.1 probability similarly three
years is maybe 0.3 and so on but as the
number of years increases let's say if
it was six or seven years there is
almost a certainty that your car is
going to break down that is what this
graph shows so this is an example of a
application of the classification
algorithm and we will see in little
details how exactly logistic regression
is a slide here one more thing needs to
be added here is that the dependent
variables outcome is discrete so if we
are talking about whether the car is
going to break down or not so that is a
discrete value the Y that we are talking
about the dependent variable that we are
talking about what we are looking at is
whether the car is going to break down
or not yes or no that is what we are
talking about so here the outcome is
discrete and not a continuous way so
this is how the logistic regression
curve looks let me explain a little bit
what exactly and how exactly we are
going to uh determine the class or the
outcome rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is considered to
be 0 and any value above 0.5 is
considered to be 1. so an output of
let's say 0.8 will mean that the car
will break down so that is considered as
an output of 1 and let's say an output
of 0.29 is considered as 0 which means
that the car will not break down so
that's the way logistic regression works
now let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove that
confusion so what is linear regression
linear regression is a process is once
again an algorithm for supervised
learning however here you are going to
find a continuous value you are going to
determine a continuous value it could be
the price of a real estate property it
could be your height how much height
you're going to get or it could be a
stock price these are all continuous
values these are not discrete compared
to a yes or no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say at the HR
team of a company tries to find out what
should be the salary hike of an employee
so they collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
this it is strain and it learns from
this labeled information so that when a
new employee's information is fed based
on the rating it will determine what
should be done so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get five
thousand five thousand five hundred five
thousand six hundred it is not discrete
like a cat or a dog or an apple or a
banana these are discrete or a yes or a
no these are discrete values right so
this way you are trying to find
continuous values is where we use linear
regression so let's say just to extend
on the scenario we now want to find out
whether this employee is going to get a
promotion or not so we want to find out
that is the discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the label data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employer's
got promotion or not yes no right so
there is nothing in between and what is
the employees rating and ratings can be
continuous that is not an issue but the
output is discrete in this case whether
employee got promotion yes no okay so if
we try to plot that and we try to find a
straight line this is how it would look
and as you can see it doesn't look very
right because looks like there will be
lot of errors this root mean square
error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at 0 and but still the straight line
doesn't look right therefore instead of
using a linear equation we need to come
up with something different and
therefore the logistic regression model
looks somewhat like this so we calculate
the probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what the depression is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and this is how the math behind logistic
regression works so we are trying to
find the odds for a particular event
happening and this is the formula for
finding the odd so the probability of an
event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus p
this is how you measure the odds now the
values of the odds range from 0 to
Infinity so when this probability is 0
then the odds will the value of the odds
is equal to zero and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probability itself remains
between 0 and 1. now this is how an
equation of a straight line love so Y is
equal to beta0 plus beta 1 x where beta0
is the y-intercept and beta 1 is the
slope of the line if we take the odds
equation and take a log of both sides
then this would look somewhat like this
and the term logistic is actually
derived from the fact that we are doing
this we take a log of p x by 1 minus p x
this is an extension of the calculation
of odds that we have seen right and that
is equal to beta0 plus beta 1X which is
the equation of the straight line and
now from here if you want to find out
the value of PX we will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of PX like this p x is
equal to 1 by 1 plus e to the power of
minus beta0 plus beta1 X and recall this
is nothing but the equation of the line
which is equal to y y is equal to beta0
plus beta 1 X so that this is the
equation also known as the sigmoid
function and this is the equation of the
logistic regression and all right and if
this is plotted this is how the sigmoid
curve is obtained so let's compare
linear and logistic regression how they
are different from each other let's go
back so linear regression is solved or
used to solve regression problems and
logistic regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and linear regression
helps to estimate the dependent variable
when there is a change in the
independent variable table whereas here
in case of logistic regression it helps
to calculate the probability or the
possibility of a particular event
happening and linear regression as the
name suggests is a straight line that's
why it's called linear regression
whereas logistic regression is a sigmoid
function and the curve is the shape of
the curve is s it's an s-shaped curve
this is another example of application
of logistic regression in weather
prediction that it's going to rain or
not rain now keep in mind both are used
in weather prediction if we want to find
the discrete values like whether it's
going to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not is
going to be sunny or not it is going to
snow or not these are all logistic
regression examples a few more examples
classification of objects this is a
again another example of logistic
regression now here of course one
distinction is that these are
multi-class classification so logistic
regression is not used in its original
form but it is used in a slightly
different form so we say whether it is a
dog or not a dog I hope you understand
so instead of saying is it a dog or a
cat or a elephant we convert this into
saying so because we need to keep it to
Binary classification so we say is it a
dog or not a dog is it a cat or not a
cat so that's the way logistic
regression can be used for classifying
objects otherwise there are other
techniques which can be used for
performing multi-class classification
Healthcare plastic regression is used to
find the survival rate of a patient so
they take multiple parameters like drama
score and age each and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into jupyter notebook and show the
code but before that let me take you
through a couple of slides to explain
what we are trying to do so let's say
you have an eight by eight image and
there the image has a number one two
three four and you need to train your
model to predict what this number is so
how do we do this so the first thing is
obviously in any machine learning
process you train your model so in this
case we are using logistic regression so
and then we provide a training set to
train the model and then we test how
accurate our model is with the test data
which means that like any machine
learning process we split our initial
data into two parts training set is set
with the training set we train our model
and then with the test set we test the
model till we get good accuracy and then
we use it for for inference right so
that is typical methodology of
training testing
brings you comprehensive artificial
intelligence bootcamp that will cover a
wide range of topics that will Empower
you to the knowledge and skills needed
to excel in the field of AI to learn
more about this course you can find the
course Link in the description box below
machine learning models so let's take a
look at the code and see what we are
doing so I'll not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using matplotlib some of the images or a
sample of these images and then we split
the data into training and test as I
mentioned earlier and we can do some
exploratory analysis and then we build
our model we train our model with the
training set and then we test it with
our test set and find out how accurate
our model is using the infusion Matrix
the heat map and use heat map for
visualizing this and I will show you in
the code what exactly is the confusion
Matrix and how it can be used for
finding the accuracy in our example we
get an accuracy of about 0.94 which is
is pretty good or 94 percent which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and this is used for
identifying the accuracy of a
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs we are
expecting
so the the most important part here is
that the model will be most accurate
when we have the maximum numbers in its
diagram like in this case that's why it
has almost 93 94 percent because the
diagonals should have the maximum
numbers and the others other than
diagnose the cells other than the
diagonal should have very few numbers so
here that's what is happening so there
is a two here there are there's a one
here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and actual value are the same so
along the diagonals that is true which
means that let's let's take this
diagonal right if the maximum number is
here that means that like here in this
case it is 34 which means that 34 of the
images that have been fed are rather
actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two missed
classifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100 accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
hundred percent accurate model okay so
that's uh just of how to use this Matrix
how to use this confusion Matrix I know
the name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information on what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and the actual name that's all right
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and
the predicted value and the actual value
is exactly the same whereas in this case
right it has uh there are I think 37
plus 5 yeah 42 have been fed the images
42 images are of Digit 3 and the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into jupyter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and
um and then we will see how well it is
trained and whether it is able to
predict these numbers correct or not so
let's get started so the first part is
as usual we are importing some libraries
that are required and then the last line
in this block is to load the digits so
let's go ahead and run this code then
here we will visualize the shape of
these digits so we can see here if we
take a look this is how the shape is
1797 by 64. these are like eight by
eight images so that's that's what is
reflected in this shape now from here
onwards we are basically once again
importing some of the libraries that are
required like numpy and matplot and we
will take a look at some of the sample
images that we have loaded so this one
for example creates a figure and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at zero one two three four so this is
how the images this is how the data is
okay and based on this we will actually
train our logistic regression model and
then we will test it and see how well it
is able to recognize so the way it works
is the pixel information so as you can
see here this is an eight by eight pixel
kind of a image and the each pixel
whether it is activated or not activated
that is the information available for
each pixel now based on the pattern of
this activation and non-activation of
the various pixels this will be
identified as a zero for example right
similarly as you can see so overall each
of these numbers actually has a
different pattern of the pixel
activation and that's pretty much that
our model needs to learn from for which
a number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and tests so that the training
data set is used to train the system so
we pass this probably multiple times and
then we test it with the test data set
and the split is usually in the form of
there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
20.23 that means 23 percent of that
entire data is used for testing and the
remaining 77 percent is used for
training so there is a readily available
function which is called train test
split so we don't have to write any
special code for the splitting it will
automatically split the data based on
the proportion that we give here which
is test size so we just give the test
size automatically training size will be
determined and we pass the data that we
want to split and the the results will
be stored in X underscore train and Y
underscore train for the training data
set and what is X underscore train these
are these are the features right which
is like the independent variable and why
underscore train is the label right so
in this case what happens is we have the
input value which is or the features
value which is in X underscore train and
since this is a labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that that's this is what will
be used for company comparison to find
out whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way X underscore train
y underscore train is for the training
data set X underscore test y underscore
test is for the test data set okay so
let me go ahead and execute this code as
well and then we can go and check
quickly what is the how many entries are
there and in each of this so X
underscore train the shape is 1383 by 64
and Y underscore train has 1383 because
there is uh nothing like the second part
is not required here and then X
underscore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the logistic regression library
and which is a part of scikit-learn so
we we don't have to implement the
logistic regression process itself we
just call these uh the function and let
me go ahead and execute that so that we
have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic
regr is a is an instance of logistic
regression and then we use that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line way is where we
are passing our data the training data
set and this is our the the predictors
and this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
and the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and we want to see how
accurate this is so what we'll do is we
will test it out in probably faces so
let me first try out how well this is
working for one image okay I will just
try it out with one image my the first
entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
a particular value new input you use the
predict method okay so let's run the
predict method and we pass this
particular image and we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 seems to be fine so let me just
go ahead and test the entire data set
okay that's basically what we will do so
now we want to find out how accurately
this has is performed so we use the
score method to find what is the
percentages of accuracy and we see here
that it has performed up to 94 percent
accurate okay so that's on this part now
what we can also do is we can
um also see this accuracy using what is
known as a confusion Matrix so let us go
ahead and try that as well so that we
can also visualize how well this model
has done so let me execute this piece of
code which will basically import some of
the libraries that are required and we
we basically create a confusion Matrix
an instance of confusion matrix by
running confusion Matrix and passing
these values so we have so this
confusion underscore Matrix method takes
two parameters one is the Y underscore
test and the other is the prediction so
what is the Y underscore test these are
the labeled values which we already know
for the test data set and predictions
are what the system has predicted for
the test data set okay so this is known
to us and this is what the system has
the model has generated so we kind of
create the confusion Matrix and we will
print it and this is how the confusion
Matrix looks as the name suggests it is
a matrix and the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
of them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers which means the accuracy is very
low the diagonal indicates a correct
prediction that so this means that the
actual value is same as the predicted
value here again actual value is same as
the predictive value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 plus 44 Plus 43 and so
on and divide that by the total number
of test observations that will give you
the percentage accuracy using a
confusion Matrix now let us visualize
this confusion Matrix tricks in a
slightly more sophisticated way using a
heat map so we will create a heat map
with some We'll add some colors as well
it's uh it's like a more visually
visually more appealing so that's the
whole idea so if we let me run this
piece of code and this is how the heat
map looks and as you can see here the
diagonals again are all the values are
here most of the values so which means
reasonably this seems to be reasonably
accurate and yeah basically the accuracy
score is 94 percent this is calculated
as I mentioned by adding all these
numbers divided by the total test value
so the total number of observations in
test data set okay so this is the
confusion Matrix for logistic regression
all right so now that we have seen the
confusion Matrix let's take a quick
sample and see how well the system has
classified and we will take a few
examples of the data so if we see here
we picked up randomly a few of them so
this is uh number four which is the
actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of 9. so this has also been
predicted correctly nine and actual
value is 9 and this is the image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images Okay so
let's summarize what we have learned in
this video we discussed about what is
logistic regression and how it fits into
the overall machine learning we
discussed about the different types of
machine learning and how logistic
regression is one of the algorithms for
classification and then we compared
logistic regression with linear
regression because the names may be
sounding similar but these are
completely different algorithms linear
regression is used for predicting
continuous values whereas logistic
regression is used for finding discrete
events and then we talked about the math
behind logistic regression and we derive
type that function as well the sigmoid
function we did a quick tabular
comparison between linear and logistic
regression we saw some examples of
day-to-day life how logistic regression
is used for some of the class for
solving some of the classification
problems and then in the end we looked
at the code to see how we can Implement
logistic regression to train the model
to identify images there's all kinds of
regression models that come out of this
so we put them aside about side we have
our linear regression which is a
predictive number
used to predict a dependent output
variable based on Independent input
variable
accuracy is a measured using least
squares estimation
so that's where you take it you could
also use absolute value the least
squares is more popular there's reasons
for that mathematically and also for
computer runtime
but it does give you an accuracy based
on the the least Square estimation the
best fit line is a straight line and
clearly that's not always used in all
the regression models there's a lot of
variations on that the output is a
predicted integer value
again this is we're talking about we
talk about linear regression and we're
talking about regression it means the
numbers coming out linear usually means
we're looking for that line
versus a different model and it's used
in business domain forecasting stocks
it's used as a basis of almost of most
um predictions with numbers so if you're
looking at a lot of numbers you're
probably looking at a linear regression
model
for instance if you do just the high
lows of the stock exchange and you're
going to take a lot more of that if you
want to make money off the stock
you'll find that the linear regression
model fits uh probably better than
almost any of the other models what even
you know high-end neural networks and
all these other different machine
learning and AI models because they're
numbers they're just a straight set of
numbers you have a high value a low
value volume that kind of thing so when
you're looking at something that's
straight numbers and are connected in
that way use you're talking about a
linear regression model and that's where
you want to start the logistic
regression model used to classify
dependent output variable based on
Independent input variable so just like
the linear regression model and like all
of our machine learning tools you have
your features coming in and so in this
case you might have a label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever
accuracy is measured using maximum
likelihood estimation the best fit is
given by a curve and we saw that
um
we're talking about linear regression
you definitely are talking about a
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
euclidean line but it's now got that
sigmoid activation which turns it into a
heavily weighted curve
and the output is a binary value between
0 and 1. and it's used for
classification image processing as I
mentioned is is what people usually
think of
although they use it for classification
of
like a window of things so you could
take a window of stock history and you
could clap generate classifications
based on that and separate the data that
way if it's going to be that this
particular pattern occurs is going to be
upward trending or downward trending
in fact a number of stock
Traders use that not to tell them how
much to bid or what to bid but they use
it as to whether it's worth looking at
the stock or not whether the Stock's
going to go down or go up and it's just
a zero one do I care do I even want to
look at it so let's do a demo so you can
get a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history
we will also classify on a mushroom data
set to find the poisonous and
non-poisonous mushrooms
and when you look at these two datas the
first one we're looking at the price so
the price is a number so let's predict
the price which the insurance should be
sold to and the second one is we're
looking at either as poisonous or it's
not poisonous so first off before we
begin the demo I'm in the Anaconda
Navigator
in this one I've loaded the python 3.6
and using the Jupiter notebook and you
can use jupyter notebook by itself you
can use the Jupiter lab which allows
multiple tabs it's basically the
notebook with tabs on it
but the Jupiter notebook is just fine
and it'll go into Google Chrome which is
what I'm using for my Internet Explorer
and from here we open up new and you'll
see python3 and again this is loaded
with python 3.6 and we're doing the
linear versus logic regression or logit
you'll see l-o-g-i-t
um is one of the one of the names that
kind of pops up when you do a search on
here but it is a logic we're looking at
the logistic regression models
and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
and so in programming we got a lot of
stuff to unfold here in our in our
startup as we pre-load all of our
different parts
let's go ahead and break this up we have
at the beginning import pandas
so this is our data frame uh it's just a
way of storing the data think of a you
talk about data frame think of a
spreadsheet a rows and columns it's a
nice way of viewing the data
and then we have we're going to be
bringing in our pre-processing labeling
code coder I'll show you what that is
when we get down to it it's easier to
see in the data but there's some data in
here like sex it's male or female so
it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here
we have our test split model
if you're going to build a model you do
not want to use all the data you want to
use some of the data and then test it to
see how good it is and if it can't have
seen the data you're testing on until
you're ready to test it on there and see
how good it is
and then we have our logistic regression
model our categorical one and then we
have our linear regression model these
are the two these right here let me just
um
clear all that there we go these two
right here are what this is all about
logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for a specific
number
and then finally usually at the very end
we have to take and just ask how
accurate is our model did it work if
you're trying to predict something in
this case we're going to be doing
um
Insurance costs how close to the
insurance cost does it measure that we
expect it to be you know if you're an
insurance company you don't want to
promise to pay everybody's medical bill
and not be able to
and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as far as whether you're going to get a
poisonous mushroom and die or not
um so we'll look at both of those and
we'll get talk a little bit more about
the shortcomings and the value of these
different processes so let's go ahead
and run this this is loaded the data set
on here and then because we're in
jupyter Notebook I don't have to put the
print on there we just do data set and
by and it prints out all the different
data on here and you can see here for
our insurance because we're starting
with we're loading that with our pandas
and it prints it in a nice format where
you can see the age sex body mass index
number of children smoker so this might
be something that the insurance company
gets from the doctor it says hey we're
gonna this is what we need to know
to give you a quote for what we're going
to charge you for your insurance
and you can see that it has 1 338 rows
and seven columns you can count the
columns one two three four five six
seven so there's seven columns on here
and the column we're really interested
in is charges I want to know what the
charges are going to be what can I
expect
not a very good Arrow drawn
um
what to expect them to charge on there
so is this going to be you know is this
person going to cost me uh 16 884
dollars or is this person only going to
cost me uh
3866. how do we guess that so that we
can guess what the minimal charge is for
their insurance
thing you really need to notice on this
data and I mentioned it before but I'm
going to mention it again because
pre-processing data is so much of the
work in data science
sex well how do you how do you deal with
female versus male
are you a smoker yes or no what does
that mean
region how do you look at Region it's
not a number how do you draw a line
between Southwest and Northwest
um you know they're objects it's either
your Southwest or your Northwest it's
not like I'm southwest I guess you could
do longitude and latitude but the data
doesn't come in like that it comes in as
true false or whatever you know it's
either your Southwest or your Northwest
so we need to do a little bit of
pre-processing of the data on here to
make this work
oops there we go okay so let's take a
look and see what we're doing with
pre-processing
and again this is really where you spend
a lot of time with data science is
trying to understand how and why you
need to do that and so we're going to do
you'll see right up here
label uh and then we're going to do the
do a label encoder one of the modules we
brought in so this is sklearns label
encoder
I like the fact that it's all pretty
much automated but if you're doing a lot
of work with label encoder you should
start to understand how that fits
and then we have
label dot fit
right here where we're going to go ahead
and do the data set dot sex dot drop
duplicates and then for data set sex
we're going to do the label transform
the data set sex and so we're looking
right here at male or female
and so it usually just converts it to a
zero one because there's only two
choices on here
same thing with the smoker it's zero or
one so we're going to transfer the
transa change the smoker 0 1 on this
and then finally we did region down here
region does it a little bit different
we'll take a look at that and it it's I
think in this case it's probably going
to do it because we did it on this label
transform
um with this particular setup it gives
each region a number like 0 1 2 3. so
let's go ahead and take a look and see
what that looks like go and run this
and you can see that our new data set
has age that's still a number six is
zero or one so zero is female one is
male
number of children we left that alone a
smoker one or zero says no or yes on
there we actually just do one for no
zero or no yeah one for no I'm not sure
how it organized them but it turns the
smoker into zero or one yes or no
and then region it did this as 0 1 2 3
so it's three regions
now a lot of times in in when you're
working with data science and you're
dealing with regions or even word
analysis
um instead of doing one column and
labeling it zero one two three a lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region Southwest
would be one column yes or no true zero
one but for this this particular setup
this will work just fine on here now
that we spend all that time getting it
set up uh here's the fun part uh here's
the part where we're actually using our
setup on this and you'll see right here
we have our
why linear regression data set drop the
charges because that's what we want to
predict
and so our X I'm sorry our X linear data
set dropped the charges because that's
what we're going to predict we're
predicting charges right here so we
don't want that as our input for our
features
and our y output is charges that's what
we want to guess we want to guess what
the charges are
and then what we talked about earlier is
we don't want to do all the data at once
so we're going to take
0.3 means 30 percent we're going to take
30 percent of our data and it's going to
be as the trade as the testing site so
here's our y test and our X test down
there
um and so that part our model will never
see it until we're ready to test to see
how good it is and then of course right
here you'll see our training set and
this is what we're going to train it
we're going to trade it on seventy
percent of the data
and then finally the big ones this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model
and then we're going to fit the data on
here
and then at this point I always like to
pull up
um if you if you if you're working with
a new models good to see where it comes
from this comes from the site kit learn
and this is the SK learn linear model
linear regression that we imported
earlier
and you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers I mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for linear fit a
regression model fit
um and what you one of the things
when I'm looking at this as I look for
methods
and you'll see here's our fit that we're
using right now and here's our predict
and we'll actually do a little bit in
the middle here as far as looking at
some of the parameters hidden behind it
the math that we talked about earlier
and so we go in this we go ahead and run
this
you'll see it loads a linear regression
model and just has a nice output that
says hey I loaded the linear regression
model
and then the second part is we did the
fit and so this model is now trained our
linear model is now trained on the
training data
and so one of the things we can look at
is the for idx and call a name and
enumerate X linear train columns
kind of an interesting thing this prints
out the coefficients so when you're
looking at the back end of the data you
remember we had that formula BX X1 plus
bxx2 plus the inner plus the intercept
and so forth these are the actual
coefficients that are in here this is
what it's actually multiplying these
numbers by
and you can see like region gets a minus
value so when it heads it up I guess a
region you can read a lot into these
numbers uh it gets very complicated
there's ways to mess with them if you're
doing a basic linear regression model I
usually don't look at them too closely
but you might start looking in these and
saying hey you know what uh smoker look
how smoker impacts the cost
it's just massive so this is a flag that
hey the value of the smoker really
affects this model
and then you can see here where the body
mass index so somebody who is overweight
is probably less healthy and more likely
to have cost money and then of course
age is a factor and then you can see
down here we have the sexist benefactor
also and it just it changes as you go in
there negative number it probably has
its own meaning on there
again it gets really complicated when
you dig into the workings and how the
linear model works on that and so we can
also look at the intercept this is just
kind of fun so it starts at this
negative number and then adds all these
numbers to it that's all that means
that's our intercept on there and that
fits the data we have on that
and so you can see right here we can go
back and
oops give me just a second there we go
we can go ahead and predict the unknown
data and we can print that out
and if you're going to create a model to
predict something we'll go ahead and
predict it here's our y prediction value
linear model predict
and then we'll go ahead and create a new
data frame
in this case from our X linear test
group
we'll go ahead and put the cost back
into this data frame and then the
predicted cost we're going to make that
equal to our y prediction
and so when we pull this up you can see
here that we have the actual cost and
what we predicted the cost is going to
be
there's a lot of ways to measure the
accuracy on there but we're going to go
ahead and jump into our mushroom data
and so in this you can see here we've
run our basic model we built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here
now with mushrooms we want a yes or no
we want to know whether we can eat them
or not
and so here's our mushroom file we're
going to run this take a look at the
data and again you can ask for a copy of
this file send a note over to
simplylearn.com
and you can see here that we have a
class the cap shape cap surface and so
forth so there's a lot of feature in
fact there's 23 different columns in
here going across
and when you look at this I'm not even
sure what these particular like p e e p
e I don't even know what the class is on
this
I'm going to guess by the notes that the
class is poisonous or edible
so if you remember before we had to do a
little pre-coding on our data same thing
with here
we have our cap shape which is b or X or
k
we have cap color these really aren't
numbers so it's really hard to do
anything with just a a single number so
we need to go ahead and turn those into
a label encoder which again there's a
lot of different encoders with this
particular label encoder it's just
switching it to 0 1 2 3 and giving it an
integer value
in fact if you look at all the columns
all of our columns are labels and so
we're just going to go ahead and loop
through all the columns in the data and
we're going to transform it into a label
encoder and so when we run this you can
see how this gets shifted from
xbxxk to zero one two three four five or
whatever it is class is 0 1 1 being
poisonous zero looks like it's editable
and so forth on here so we're just
encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring zero one two three
four five in this particular example
it's not going to make that big of a
difference how we encode it
and then of course we're looking for the
class whether it's poisonous or edible
so we're going to drop the class in our
X Logistics model and we're going to
create our y Logistics model is based on
that class so here's our x y
and just like we did before we're going
to go ahead and split it
using 30 percent for test
70 percent to program the model on here
and that's right here whoops there we go
there's our train and test
and then you'll see here on this next
setup
this is where we create our model all
the magic happens right here
we go ahead and
create a logistics model I have up the
max iterations if you don't
change this for this particular problem
you'll get a warning that says this has
not converged
because that that's what it does is it
goes through the math and it goes hey
can we minimize the error and it keeps
finding a lower and lower error and it
still is changing that number so that
means it hasn't conversed yet it hasn't
find the lowest amount of error it can
and the default is 100. there's a lot of
settings in here so when we go in here
to let me pull that up from the sklearn
so we pull that up from the SK learn
model
you can see here we have our logistic it
has our different settings on here that
you can mess with most of these work
pretty solid on this particular setup so
you don't usually mess a lot usually I
find myself adjusting the iteration and
I'll get that warning and then increase
the iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our methods
and you can see there's a lot of methods
available on here and certainly there's
a lot of different things you can do
with it
but the most basic thing we do is we fit
our model make sure it's set right and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced but for the most part
these are the two which I use when I'm
going into one of these models and
setting them up
so let's go ahead and close out of our
sklearn setup on there
and we'll go ahead and run this
and you can see here it's now loaded
this up there we now have a
logistic model and we've gone ahead and
done a predict here also just like I was
showing you earlier
so here's where we're actually
predicting the data so we we've done our
first two lines of code as we create the
model we fit the model to our training
data and then we go ahead and predict
for our test data now in the previous
model we didn't dive into the test score
I think I just showed you graph and we
can go in there and there's a lot of
tools to do this we're going to look at
the model score on this one let me let
me just go ahead and run the model score
and it says that it's pretty accurate
we're getting a roughly 95 percent
accuracy wow that's good one 95 accuracy
95 accuracy might be good for a lot of
things
but when you look at something as far as
whether you're going to pick a mushroom
on the side of the trail and eat it we
might want to look at the confusion
Matrix and for that we're going to put
in our y logistic test the actual values
of edible and unedible and we're going
to put in our prediction value
and if you remember on here let's see I
believe it's poisonous was one zero is
edible
so let's go ahead and run that 0 1 0 is
good so here is a confusion Matrix and
this is if you're not familiar with
these we have true true true false
true false false false
so it says out of the edible mushrooms
we correctly labeled 1201 mushrooms
edible that were edible
and we correctly measured
1113 poisonous mushrooms as poisonous
but here's the kicker
I labeled
56 edible mushrooms as being
poisonous well that's not too big of a
deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous
so probably not the best choice to use
this model to predict whether you're
going to eat a mushroom or not
and you'd want to dig a Little Deeper
before you start picking mushrooms off
the side of the trail so a little
warning there when you're looking at any
of these data models looking at the
error and how that error fits in with
what domain you're in domain in this
case being edible mushrooms
be a little careful make sure that
you're looking at them correctly
so we've looked at uh edible or not
edible we've looked at a regression
model as far as the N values what's
going to be the cost and what our
predicted cost is so we can start
figuring out how much to charge these
people for their insurance
and so these really are the fundamentals
of data science when you pull them
together when I say data science to talk
about your machine learning code
and hopefully you got a little
classification is probably one of the
most widely used tools in machine
learning in today's world and is also
one of the simpler versions to start
understanding how a lot of machine
Learning Works we're going to start by
taking a look at what exactly is
classification the important
terminologies around classification
we'll look at some real world
applications my favorite popular
classification algorithms and there are
a lot out there so we're only going to
touch briefly on a variety of them so
you can see how they the different
flavors work and we'll have some
Hands-On Demos in Python embedded
throughout the tutorial classification
classification is a task that requires
the use of machine learning algorithms
to learn how to assign a class label to
a given data you can see in this diagram
we have our unclassified data it goes
through a classification algorithms and
then you have classified data it's hard
to just see it as data and that really
is where you kind of start and where you
end when you start running these machine
learning algorithms and classification
and the classification algorithms is a
little black box in a lot of respects
and we'll look into that you can see
what I'm talking about when we start
swapping in and out different models
let's say we are given the task of
classifying a given bunch of fruits and
vegetables on the basis of their
category I.E fruits are to be grouped
together and vegetables are to be
grouped together and so we have a data
set we'll call it the bunch is divided
into clusters one of which consists of
the fruits while the other has the
vegetables you can actually look at this
as any kind of data when we talk about
breast cancer can we sort out an images
to see what is malignant what is B9 very
popular one can you classify flowers the
iris data set certainly in Wildlife can
you classify different animals and track
where they're going classification is
really the bottom starting point or the
Baseline for a lot of machine learning
and setting it up and trying to figure
out how we're going to break the data up
so we can use it in a way that is
beneficial so here the fruits and the
vegetables are grouped into clusters and
each clusters has a specific
characteristic I.E whether they are a
fruit or a vegetable and you can see we
have a pile of fruits and vegetables we
feed it into the algorithm and the
algorithm separates them out and you
have fruits and vegetables so some
important terminologies before we dig
into how it sorts them out and what that
all means we look at the terminologies
you have a classifier that's the
algorithm that is used to map the input
data to a specific category the
classification model the model that
predicts or draws a class to the input
data given for training feature it is an
individual measurable property of the
phenomena being observed and labels the
characteristics on which the data points
of a data set are categorized the
classifier and the classification model
go together a lot of times the class a
fire is part of the classification model
and then you choose which classifier you
use after you choose which model you're
using where features are what goes in
labels are what comes out so you
classifier model is right in the middle
of that that's that little black box we
were just talking about clusters they
are a group of data points which have
some common characteristics binary
classification it is a classification
condition with two outcomes which are
either true or false multi-label
classification this is a classification
condition where each sample is assigned
to a set of labels or targets
multi-class classification the
classification with more than two
classes here each sample is assigned to
one and only one label when we look at
this group of terminologies a few
important things to notice going from
the top clusters when we cluster data
together we don't necessarily have to
have an end goal we just want to know
what what features cluster together
these features then are mapped to the
outcome we want in many cases the first
step might not even care about the
outcome only about what data connects
with other data and there's a lot of
clustering algorithms out there that do
just the clustering part binary
classification it is a classification
condition with two outcomes which are
either true or false we're talking
usually it's a it's either a cat or it's
not a cat it's either a dog or it's not
a dog that's the kind of thing we talk
about binary classification and then
that goes into
multi-label classification think of
label as you can have an object that is
brown you can have an object that is
labeled as a dog so it has a number of
different labels that's very different
than a multi-class classification where
each one's a binary you can either be a
cat or a dog you can't be both a cat and
a dog real world applications so to make
sense of of this of course the challenge
is always in the details is to
understand how we apply this in the real
world so in real world applications we
use this all the time we have email spam
classifier so you have your email inbox
coming in it goes through the email
filter that we usually don't see in the
background and it goes this is either
valid email or it's a Spam and it puts
it in the spam filter if that's what it
thinks it is Alexa's voice classifier
Google Voice any of the voice
classifiers they're looking for points
so they try to group words together and
then they try to find those groups so
words trigger a Class A classifier so it
might be that the classifier is to open
your tasks program or open your text
program so she can start sending a text
sentimental sentiment analysis is really
big when we're tracking products we're
tracking marketing trying to understand
whether something is liked or disliked
is huge that's like one of the biggest
driving forces in sales nowadays and you
almost have to have these different
filter is going on if you're running a
large business of any kind fraud
detection you can think of banks they
find different things on your bank
statement and they detect that there's
something going on there they have
algorithms for tracking the logs on
computers they start finding weird logs
on computers they might find a hacker I
mentioned cat and dog so here's our
image classification we have a neighbor
who runs an outdoor webcam and we like
to have it come up with a classification
when the wild animals in our area are
out like foxes we actually have a
mountain lion that lives in the area so
it's nice to know when he's here
handwriting prediction classifying A B C
D and then classifying words to go with
that so let's go ahead and roll our
sleeves up and take a look at some
popular classification algorithms before
we look at the algorithms let's go back
and take a look at our definitions we
have a classifier and a classification
model so we're looking at the classifier
an algorithm that is used to map the
input data to a specific category one of
those algorithms is a logistic
regression the logistic regression is
the classification algorithm used to
model the probability of a certain class
or event existing such as pass fail or
win lose Etc it provides its output
using the logistic function or sigmoid
function to return the probability value
that can then be mapped to two or more
discrete classes a sigmoid function is
an activation function that fits the
variable and limit the output to a range
between 0 and 1. a standard sigmoid
function or logistic function is
represented by the formula FX equals 1
over 1 plus e to the minus X where X is
the equation of the line and E is the
exponential and just taking a quick look
at this you can think of this as being a
point of uncertainty and so as we get
closer and closer to the middle of the
line it's either activated or not and we
want to make that just shoot way up so
so you'll see a lot of the activation
formulas kind of have this nice S curve
where it approaches one and approaches
zero and based on that there's only a
small region of error and so you can see
in the sigmoid logistic function the one
over one plus e minus x to the minus X
you can see in frames that that nice S
curve we also can use a tangent
variation there's a lot of other
different models here as far as the
actual algorithm this is the most
commonly used one let's go ahead and
roll up our sleeves and take a look at a
demo that is going to use a logistic
regression so we're going to have the
activation formula and the model because
you have to have you have to have both
for this we will go into our Jupiter
notebook now I personally use the
Anaconda navigator to open up the
Jupiter notebook to set up my IDE as a
web-based it's got some advantages that
it's very easy to display in but it also
has some disadvantages in that if you're
trying to do multi-threads and multiply
processing you start running into single
get issues with python and then I jump
to pycharm really depends on whatever ID
you want just make sure you've installed
numpy and the sklearn modules into your
python in whatever environment you're
working in so that you'll have access to
that for this demo now the team in the
back has prepared my code for me which
I'll start bringing in one section at a
time so we can go through it before we
do that it's always nice to actually see
where this information is coming from
and what we're working with so the first
part is we're going to import our
packages which you need to install into
your Python and that's going to be your
numpy we usually use numpy as in p and
then from SK learn the learn model we're
going to use a logistic regression and
from sklearnmetrics we're going to
import the classification report
confusion Matrix and if we go ahead and
open up S the site kit-learn.org and go
under their API you can see all the
different features and models they have
and we're looking at the linear model
logistic regression one of the more
common classifiers out there and if we
go ahead and go into that and dig a
Little Deeper you'll see here where they
have the different settings it even says
right here note the regularization is
applied by default so by default that is
the activation formula being used now
we're not going to spend we might come
back to this look at some of the other
models because it's always good to see
what you're working with but let's go
ahead and jump back in here and we have
our Imports we're going to go ahead and
run those so these are now available to
us as we go through our Jupiter notebook
script and they put together a little
piece of data for us this is simply
going through zero to one actually let's
go ahead and print this out over here
we'll go ahead and print X just so you
can see we're actually looking for and
when we run this you can see that we
have our X 0 1 2 through 9 we reshaped
it the reason for this is just looking
for a row of data usually we have
multiple features we just have the one
feature which happens to be zero through
nine and then we have our 10 answers
right down here 0 1 0 0 1 1 1 1 1. you
can bring in a lot of different data
depending on what you're working with
you can make your own you can instead of
having this as just a single you could
actually have like multiple features in
here but we just have the one feature
for this particular demo and this is
really where all the magic happens right
here and I told you it's like a black
box that's the part that is is kind of
hard to follow and so if you look right
here we have our model we talked about
the model right there and then we went
ahead and set it for Library linear as I
showed you earlier that's actually
default so it's not that important
random State equals zero this stuff you
don't worry too much about and then with
the site kit learn you'll see the model
fit this is very common into site kit
they use similar stuff in a lot of other
different packages but you'll you'll see
that that's very common you have to fit
your data and that means we're just
taking the data and we're fitting our X
right here which is our features that's
our X and here's why these are the
labels we're looking for so before we
were looking at is it fraud is it not is
it cat is it not that kind of thing and
this is looking at 0 1 so we want to
have a binary set up on this and we'll
go ahead and run this you can see right
here it just tells us what we loaded it
with as our defaults and that this model
has now been created and we've now fit
our data to it and then comes the fun
part you work really hard to clean your
data to bake it and cook it there's all
kinds of I don't know why they go with
the cooking terms as far as how we get
this data formatted then you go through
and you pick your model you pick your
solver and you have to test it to see
hey which one's going to be best and so
we want to go ahead and evaluate the
model and you do this is that once
you've figured out which one is going to
work the best for you you want to
evaluate it so you can compare it to
your last Model and you can either
update it to create a new one or maybe
change the solver to something else I
mentioned tangent and that's one of the
other common ones that's commonly used
with language for some reason the
tangent even though it looks almost to
me identical to the one we're using with
the sigmoid function if for some reason
it activates better with language even
though it's a very small shift in the
actual math behind it we already looked
at the data early but we'll go and look
at it again just you can see we look at
we have our rows of 0 1 row it only has
one entity and we have our output that
matches these rows and these do have to
match you'll get an error if you put in
something with a different shape so if
you have 10 rows of data and nine
answers it's going to give you an error
because you need to have 10 answers for
it a lot of times you separate this too
when you're doing larger models but for
this we're just going to take a quick
look at that the first thing we want to
start looking that is The Intercept one
of the features inside our linear
regression model we'll go ahead and run
that and print it you'll see here we
have an intercept of minus
1.516 and if we're going to look at The
Intercept we should also look at our
coefficients and if you run that you'll
see that we get a list we get the our
coefficient is the 0.7035 you can just
think of this as your euclidean geometry
for very basic model like this where it
intercepts the Y at some point and we
have a coefficient multiplying by it a
little more complicated in the back end
but that is the just this simple model
which is the one feature in there and
we'll go ahead and reprint the Y because
I want to put them on top of each other
with the Y predict and so these were the
Y values we put in and this is the Y
predict we had coming out and you can
see here we go there's the Y actual and
there's what the prediction comes in now
keep in mind that we used the ACT actual
complete data as part of our training
that is if you're doing a real model a
big stopper right there because you
can't really see how good it did unless
you split some data off to test it on
this is the first step is you want to
see how your model actually tests on the
data you trained it with and you can see
here there is this point right here
where it has it wrong and this point
right here where it also has it wrong
and it makes sense because we're going
our input is 0 1 0 through 9 and it has
to break it somewhere and this is where
the break is so it says this half the
data is going to be zero because that's
what it looked like to me if I was
looking at it without an algorithm and
this data is probably going to be one
and I I didn't I forgot to point this
out so let's go back up here I just kind
of glanced over this window here where
we did a lot of stuff let's go back and
just take a look at that what was done
here is we ran a prediction so this is
where our predict comes in is our
model.predict so we had a model fit we
created the model we programmed it to
give us the right answer now we go ahead
and predict what we think it's going to
be there's our model that predict
probability of X and then we have our y
predict which is very similar but this
has to do more with the probability
numbers so if you remember down below we
had the setup where we're looking at
that sigmoid function that's what this
is returning and the Y predict is
returning a 0 or a one and then we have
our confusion Matrix we'll look at that
and we have our report which just
basically Compares our y to our y
predict which we just did it's kind of
nice a simple data so it's really easy
to see what we're doing that's why we do
use the simple data this can get really
complicated when you have a lot of
different features and things going on
in splits so here we go we've had our I
printed out our actual and our
prediction so this is the actual data
this is what the predict ran and then
we'll go ahead and do we're going to
print out the confusion Matrix we were
just talking about that this is great if
you have a lot of data to look at but
you can see right here a confusion
Matrix says if you remember from the
confusion Matrix we have the two this is
two correct one two and uh it's been a
while since I looked at a confusion
Matrix there's the two and then we have
this one which is our six that's where
the six comes from and then we have this
one which is the one false this is the
two one so we have this one here and
this one here which is misclassified
this really depends on what data you're
working with as to what your is
important you might be looking at this
model and if this model this confusion
Matrix comes up and says that you
misclassified even one person as being
non-malignant cancer that's a bad model
I wouldn't want that classification I'd
want this number to be zero I wouldn't
care if this false positive was off by a
little bit more long as I knew that I
was correct on the important factor that
I don't have cancer so you can see that
this confusion Matrix really aims you in
the right direction of what you need to
change in your model how you need to
adjust it and then there's of course a
report reports are always nice if you
notice we generated a report earlier
we'll go and just print the report up
and you can remember this is a report
it's a classification report y comma y
predict so we're just putting in our two
values basically what we did here
visually with our actual and our
predicted value and we'll go ahead and
run the report and you you can see it
has the Precision the recall your F1
score your support translated into a
accuracy macro average and weighted
average so it has all the numbers a lot
of times when working with clients or
with the shareholders in the company
this is really where you start because
it has a lot of data and they can just
kind of stare at it and try to figure it
out and then you start bringing in like
the confusion Matrix I almost do this in
Reverse as to what this show I would
never show your shareholders The
Intercept of the coefficient that's for
your internal team only working on
machine language but the confusion
Matrix and the report are very important
those are the two things you really want
to be able to show on these and you can
see here we did a decent job of
classifying the data managed to get a
significant portion of it correct we had
or was it accuracy here is a 0.80 F1
score that kind of thing so you know
it's a pretty accurate model of course
this is pretty goofy because it's very
simple model and it's just splitting the
model between ones and zeros so that was
our demo of the on logistic regression
on there let's go and take a look at K
nearest neighbors uh this one is another
very highly used and important algorithm
to understand K nearest neighbors is a
simple algorithm which stores all
available cases and classifies new cases
based on the similarity measure the K
nearest neighbor finds out the class of
the new data point by finding its
nearest neighbors if there are three
data points of Class A and two data
points of Class B near to the new data
point then the k n classifies the new
data point as Class A the K and K
nearest neighbors is the number of
nearest neighbors we are looking for I
hear I.E if we say k equals three this
means that we are looking for nearest
three neighbors of unclassified data
point usually we take the K value
between 3 to 10 as it leads to a better
result a smaller value of K means that
noise will have a bigger influence on
the result and a larger value of K makes
it computationally expensive hence the
data scientists before the range of K
between 3 and 10. when we talk about
noise remember the data we just looked
at was 0 1 1 0 0 it had some some values
where it cut it and set everything to
the right is a one everything to the
left is a zero but I had some ones and
zeros mixed in there that's called noise
that's what they're talking about is
there's some things that are right in
the middle in the classification which
makes it very hard to classify so
suppose we're trying to find the class
for a new Point indicated by the red
color and you can see it's kind of right
between the cat right between the dogs
let k equal three so we are finding the
3nn for the red data point but by
looking at the plot on the right we can
see that the red data point belongs to
the class dogs as it has two votes for
class dog and one vote for class cat and
if you ask the question well what are
you measuring the distance what is that
distance it could be the measurements of
the ears whether they're pointed or
floppy that might be one of the features
you're looking at is how floppy the ears
are another one might be the whiskers
versus the nose and then you take those
measurements and using the one of the
most common things in K means
measurement is the euclidean geometry
you can figure out the distance between
those points there's a lot of different
algorithms for that but you can think
about it that you do have to have some
kind of solid data to measure and so we
can conclude that the new data point
belongs to the class dog so let's go
ahead and see what this looks like in
code and do a demo on the K nearest
Neighbors in here and we'll go right
back into our Jupiter notebook and open
up a new Python Programming script page
of course once we're in here we'll want
to look at the site kit learn I did just
a quick search for us K neighbors KN
neighbors classifier this actually is
the older version 0.0 no 1.023 is the
one we want and you'll see here that we
have all their defaults in Neighbors
equals five it defaults we were talking
about that between three and ten there's
different ways to weigh it there's an
algorithm based on it I mentioned
euclidean geometry finding the distance
there's other algorithms for figuring
out what that distance is and how to
weight those and there's a lot of other
parameters you can adjust for the most
part the K means uh basic setup is a
good place to start and just let the
defaults go we might play with some of
those we'll see what the guys in the
back did and from here we're going to
import numpy we're going to use pandas
if you haven't been running pandas
pandas is our data frame which sits on
top of numpy's data frames are you know
numpy's is our number array pandas is
our data frame matplot library because
we're going to plot some graphs
everybody likes some pretty pictures it
makes it a lot easier to see what's
going on when you have a nice display
and that's also what the Seaborn is in
here in the setup that sits on top of
the matplot library the ones we really
want to look at right here are the what
we're bringing in from sklearn these
ones right here so from sklearn we're
going to load I mentioned the breast
cancer that's a very popular one because
it has I believe it's 36 measurements so
there's 36 features and unless you're a
expert you're not going to know what any
of those features really mean you can
sort of guess but there's special
measurements they take of when they take
a image and of course our confusion
Matrix so that we can take a look and
see what the data looks like and how
good we did and then we have our KN at
Neighbors classifier on here and then I
mentioned that whenever you do training
and testing you want to split the data
up you don't want to train the data and
then test it on the same data that just
tells you how good your training model
is it doesn't tell you whether it
actually works on unknown data and so
this just splits it off so that we can
train it and then we can take a look at
data we don't have in there and see how
good it did and we'll go ahead and load
our data up so here's our our setup on
that oops there we go so we're going to
go ahead and load the data up we have
our x value and that's going to come
from our breast cancer.data and column
breast cancer feature names so there's
our actual all our different features
we'll print that out here in a second
and then we have our mean area mean
compactness so I guess we're going to
take the data and we're only going to
use a couple of the columns this just
makes it easier to read of course when
you actually we're going to do this
you'd want to use all your columns and
then we have our Y and this is simply
whether it's either malignant or B9 and
then we want to go ahead and drop the
first line because that's how it came in
on there and we'll go ahead okay let's
just take a look at this a little closer
here let's go and run this real quick
and just because I like to see my data
before I run it we can look at this and
we can look at the original features
remember we're only going to use two
features off of here just to make it a
little easier to follow and here's the
actual data and you can see that this is
just this massive stream of data coming
in here it's going to just skip around
because there's so much in there to set
up I think there's like 500 if I
remember correctly and you can see
here's all the different measurements
they take but we don't we don't really
need to see that on here we're just
going to take a look at just the two
columns and then also our solution we'll
go ahead and just do a Quick Print y on
here so you can see what the Y looks
like and it is simply just zero zero
zero you know
b90001 so a one means it's B9 0 means
it's malignant that's what we're looking
at on that go and cut that out of there
the next stage is to go ahead and split
our data I mentioned that earlier we'll
just go ahead and let them do the
splitting forest for us we have x-train
X test y train y test and so we go ahead
and train test split X Y random State
equals one it makes it nice and easy for
us we'll go and run that and so now we
have our training and our testing train
means we're going to use that to train
the model and then we're going to use
the test to test to see how good our
model does and then we'll go ahead and
create our model here's our knnn model
the K Neighbors classifier in Neighbors
equals five the metrics is euclidean
remember I talked about euclidean this
is simply your c squared equals a
squared plus b squared plus a squared
equals B squared plus c squared plus c
squared plus D Squared and then you take
the square root of all that that's what
they're talking about here it's just the
length of the hypotenuse of a triangle
but you can actually do that in multiple
Dimensions just like you do in two
Dimensions with the regular triangle and
here we have our fit this should start
to look familiar since we already did
that in our last example that's very
standard for s site kit and any other
one although sometimes the fit
algorithms look a little bit more
complicated because they're doing more
things on there especially when you get
into neural networks and then you have
your key neighbors it just tells you
we've created a k neighbor's setup they
kind of wanted us to reformat the Y but
it's not that big of a deal for this and
it comes out and shows you that we're
using the euclidean metric for our
measurement so now we've created a model
here's our live model we fitted the data
to it we say hey here's our training
data let's go ahead and predict it so
we're going to take our y predict equals
K and N predict y test so this is data
we have not this model has not seen this
data and so we're going to create a
whole new set of data off of there now
before we look at our prediction in fact
let's um I'm going to bring this down
and put it back in here later let's take
a look at our x-test data versus the Y
test what does it look like and so we
have our mean area we're going to
compare it to our mean compactness we're
going to go ahead and run that and we
can see here the data if you look at
this just eyeballing it we put it in
here we have a lot of blue here and we
have a lot of orange here and so these
dots in the middle especially like this
one here and these here these are the
ones that are going to give us false
negatives and so we should expect this
is your noise this is where we're not
sure what it is and then B9 is in this
case it's done in blue and malignant is
done in one so if you look at hit
there's two points based on these
features which makes it really hard to
have a hundred percent where the hundred
percent is down here or up here that's
kind of thing I'd be looking for when
we're talking about cancer and stuff
like that we really don't want any false
negatives you want everything you false
positive great you're going to go in
there and have another setup in there
where you might get a get an autopsy or
something like that done on it again
that's very data specific on here so now
let's go ahead and pull in and get our
our prediction in here and we'll create
our y prediction we'll go and run that
so now this is loaded with what we think
the unknown data is going to be and we
can go ahead and take that and go ahead
and plot it because it's always nice to
have some pretty pictures and when we
plot it we're going to do the mean area
versus mean compactness again you look
at this map and you can see that there's
some clear division here we can clearly
say on some of the stuff that our y
prediction if we look at this map up
here and this map down here we probably
got some pretty good deal it looks
pretty good like they match a lot this
is of course just eyeballing it really
you don't want to eyeball these things
you want to show people the pictures so
they can see it you can say hey this is
what it looks like but we really want
the confusion Matrix and we do the Y
test and the Y predict we can see in the
confusion Matrix here it did pretty good
and we'll just go ahead and point that
out real quick here's our 42 which is
positive and our 79 and if I remember
correctly I'd have to look at the data
which one of these is a false negative I
believe it's the nine that's scary I
would not want to be one of those nine
people told that I don't have cancer and
then suddenly find out I do so we would
need to find a way to sort this out and
there is different ways to do that a
little bit past this but you can start
messing with the actual euclidean
geometry and the activation
measurements and start changing those
and how they interact but that's very
Advanced there's also other ways to
classify them or to create a whole other
class right here of we don't knows those
are just a couple of the solutions you
might use for that but for a lot of
things this works out great you can see
here you know maybe you're trying to
sell something well if this was not life
dependent and this was if I display this
ad 42 of these people are going to buy
it and if I display this other ad if I
don't display it 79 people are going to
go a different direction or whatever it
is so maybe you're trying to display
whether they're going to buy something
if you add it on to the website in which
case that's really a good numbers you've
just added a huge number of cells to
your company so that was our K nearest
neighbors let's go and take a look at
support Vector machines so support
Vector machines is the main objective of
a support Vector machine algorithm is to
find a hyperplane in an n-dimensional
space n is the number of features that
distinctly classifies the data points if
you remember we were just looking at
those nice graphs we had earlier in fact
let me go ahead and flip back on over
there if we were looking at this data
here we might want to try to find a nice
line through the data and that's what
we're talking about with this next setup
so the main objective support Vector
machine algorithm is to find a
hyperplane in an n-dimensional space n
is the number of features that
distinctly classifies the data points to
separate the two classes of data there
are many hyperplanes that can be chosen
our objective is to find the plane that
has the maximum margin I.E the maximum
distance between data points of both
classes the dimensions of the hyperplane
depends on the number of features if
there are two input features then the
hyperplane is just a line if there are
three features then the hyperplane
becomes a two-dimensional plane the line
that separates the data into two classes
is called as support vector classifier
or hard margin that's what I just showed
you on the other data you can see here
we look for a line that splits the data
evenly the problem with hard margin is
that it doesn't allow outliers and
doesn't work with none linearly
separable data and we just were looking
at that let me flip back on over here
and when we look at this setup in here
and we look at this data here we go look
how many outliers are in here these are
all with all these blue dots on the
wrong side of the line would be
considered an outlier and the same with
the red line and so it becomes very
difficult to divide this data unless
there's a clear space there we go
therefore we introduce soft margins
which accept the new data point and
optimize a model for non-linear data
points soft margins pass through the
data points at the border of the classes
the support Vector machine can be used
to separate the two classes of shapes
here we can see that although triangles
and diamond shapes have pointy edges but
we are able to classify them in two
categories using a support Vector
machine let's go ahead and see what that
looks like in a demo and flip back on
over to our jupyter notebook we always
want to start with taking a look at the
sklearn API in this case the svm SVC
there is a significant number of
parameters because SK the svm has been a
lot of development in the recent years
and has become very popular if we scroll
all the way down to methods you'll see
right here as our fit and our predict
that's what you should see in most of
the scikit learn packages so this should
look very familiar to what we've already
been working on and we'll go ahead and
bring in our import and run that we
should already have pandas numpy our
matplot library which we're going to be
using to graph some of the things a
little bit different right here you'll
see that we're going to go ahead and
bring in one of the fun things you can
do with test data is make circles
circles are really hard to classify you
have a ring on the inside and a ring on
the outside and you can see where that
can cause some issues and we'll take a
look at that a little closer in a minute
here's our svm setup on there and then
of course our metrics because we're
going to use that to take a closer look
at things so we go and run run this oops
already did once we've gone ahead and
done that we're going to go ahead and
start making ourselves some data and
this part probably a little bit more
complicated than we need I'm not going
to go too much in detail on it we're
going to make a mesh grid and we'll see
what that looks like in a minute we're
defining the minimums you can see in
here create a measurement of points
here's our parameters of x y and H it's
going to return xxyy you can also send a
note to us make sure you get a copy of
this if you want to dig deeper into this
particular code especially and we have a
y Min y Max Y Man y Max plus one here's
our mesh grid we're actually going to
make XX and YY plot the Contours all the
way through and this is just a way of
creating data it's kind of a fun way to
create some data we go plot Contours ax
c l f x x y y and return it out add some
perimeters here so that when we're doing
it we have our setup on there train
property and then we'll go ahead and
make our data just throw that right in
there too in the same setup and run that
so now we have X and Y we're going to
make our circles we have n samples
equals samples in this case we're going
to have 500 samples we went ahead and
gave it a noise of 0.05 random State
123. these are all going into oops let's
go back up here we go make our mesh grid
make circles there it is so this is
going into our make circles up here and
this is part of that setup on this and
then once we've gone ahead and make the
circle let's go ahead and plot it that
way you can see we're talking about
right now what I'm saying is really
confusing because without a visual it
doesn't really mean very much what we're
actually doing so let me go ahead and
run this with the plot and let's go back
up here and just take a look we've made
our Circle we have our n samples equals
samples we're going to have 500 we're
going to have training property 0.8
here's our data frame we go and load it
into the data frame so we can plot it
groups DF Group by label this is kind of
a fun way if you have multiple columns
you can really quickly pull whatever
setup is in there and then we go ahead
and plot it and you can see here we have
two rings that are formed and that's all
this is doing is just making this data
for us this is really hard data to
figure out a lot of programs get
confused in this because there's no
straight line or anything like that but
we can add planes and different setups
on here and so you can see we have some
very interesting data we have our zero
is in blue and our ones in the yellow in
the middle and the data points are an x
y coordinate plot on this one of the
things we might want to do on here is go
ahead and find the Min to Max ratio set
up in there and we can even do let's
just do a print X you can see what the
data looks like that we're producing on
this there we go so x equals x minus X
Min over x max minus X Min all we're
doing is putting this between 0 and 1.
whatever this data is we want a 0 to 1
setup on here if you look at this all
our data is 0.8.5 that's what this
particular line is doing that's a pretty
common thing to do in processing data
especially in neural networks neural
networks don't like big numbers they
create huge biases and cause all kinds
of problems and that's true in a lot of
our models some of the models it doesn't
really matter but when you're doing
enough of these you just start doing
them you just start putting everything
between zero and one there's even some
algorithms to do that in the SK learn
although it's pretty as you can see it's
pretty easy to do it here so let's go
ahead and jump into the next thing which
is a linear kind of setup C equals 1.0
this is the svm regularization parameter
we're going to use models here's our svm
and this one's just a notch there we go
so here we are with our model we're
going to create the the setup with the
SVC kernel is linear and we'll come back
to that because that's an important uh
setup in there as far as what our kernel
is and you'll you'll see why that's so
important here in a minute because we're
going to look at a couple of these and
so this one is we're actually going to
be changing how that processes it uh and
then our C here's our one our Point O
and then the rest of this is plotting
we're just adding titles making the
Contours so it's a pretty graph you can
actually spend this would be a whole
class just to do all the cool things you
can do with Scatter Plots and regular
plots and colors and things like that in
this case we're going to create a graph
with a nice white line down the middle
so that you can see what's going on here
and when we do this you can see that as
it as it split the data the linear did
just that it drew a line through the
data and it says this is supposed to be
blue and this is supposed to be red and
it doesn't really fit very well that's
because we used a linear division of the
data and it's not very linear data on it
it's anything but linear so when we when
we look at that it's like well okay that
didn't work what's the next option well
there's a lot of choices in here one of
them is just simply we can change this
from the kernel being linear to poly and
we'll just go back up here and use the
same chart oops here we go so here we go
linear kernel we'll change this to poly
and then when we come in here and create
our model here's our model up here
linear we can actually just go right in
and change this to the poly model and if
you remember when we go back over here
to the SVC and let's scroll back up here
there's a lot of different options oops
even further okay so we come up here and
we start talking about the kernel here's
the kernel there's linear there's poly
RBF sigmoid pre-computed there's a lot
of different ways to do this setup and
their actual default is RBF very
important to note that so when you're
running these models understanding which
parameter is really has a huge effect on
what's going on in this particular one
with the svm the kernel is so important
you really need to know that and we
switched our kernel to poly and when we
run this you can see it's changed a
little bit we now have quite an
interesting looking diagram and you can
see on here it now has these
classifications correct but it messes up
in this blue up here and it messes up on
this blue is correct and this blue is
supposed to be red you can see that it
still isn't quite fitting on there and
so that is we do a polyfit you can see
if you have a split in data where
there's a group in the middle this one
kind of data and the groups on the
outside are different the polyfit or the
poly kernels what's going to be fit for
that so if that doesn't work then what
are we going to use well they have the
RBF kernel and let's go ahead and take a
look and see what the RBF looks like and
let me go and turn there we go turn my
drawing off and the RBF kernel oops
RBF and then of course for our title
it's always nice to have it match with
the r b f kernel and we go ahead and run
this and you can see that the RBF kernel
does a really good job it actually has
divided the date app on here and this is
the kind of what you expect here was
that ring here's our inner ring and an
outer ring of data and so the RBF fits
this data package quite nicely and that
we talk about svm it really is powerful
in that it has this kind of sorting
feature to it in its algorithms this is
something that is really hard to get the
SK means to do or that K means setup and
so when you start looking at these
different machine learning algorithms
understanding your data and how it's
grouped is really important it makes a
huge difference as far as what you're
Computing and what you're doing with it
so that was a demo on the support Vector
certainly you could have done continued
on that and done like a confusion Matrix
and all that kind of fun stuff to see
how good it was and split the data up to
see how it vectorizes the visual on
that's so important it makes a big
difference just to see what it looks
like and that giant donut and why it it
does Circle so well or your poly version
or your linear version so we've looked
at some very numerical kind of setups
where there's a lot of math involved
euclidean geometry that kind of thing a
totally different machine learning
algorithm for approaching this is the
decision trees and there's also forests
that go with the decision trees they're
based on multiple trees combined the
decision tree is a supervised learning
algorithm used for classification it
creates a model that predicts a value of
a Target variable by learning simple
decision rules inferred from the data
features a decision tree is a
hierarchical tree structure where an
internal node represents features or
attribute the branch represents a
decision Rule and each Leaf node
represents the outcome and you can see
here where they have the first first one
yes or no and then you go either left or
right and so forth one of the coolest
things about decision trees is and I'll
see people actually run a decision tree
even though their final model is
different because the decision tree
allows you to see what's going on you
can actually look at it and say why did
you go right or left what was the choice
where's that break and that is really
nice if you're trying to share that
information with somebody else as to why
when you start getting into the why this
is happening decision trees are very
powerful so the topmost note of a
decision tree is known as the root node
it learns to partition on the basis of
the attribute value it partitions a tree
in a recursive manner so you have your
decision node if you get yes you go down
to the next note that's a decision note
and either yes you go to if it ends on a
leaf node then you know your answer
which is yes or no so there's your
there's your n classification set up on
there here's an example of a decision
tree that tells whether I'll sleep or
not at a particular regular evening mine
would be depending on whether I have the
news on or not do I need to sleep no
okay I'll work uh yes is it raining
outside yes I'll sleep no I'll work so I
guess if it's not raining outside it's
harder to fall asleep where they have
that nice rain coming in and again this
is really cool about a decision tree is
I can actually look at it and go oh I
like to sleep when it rains outside so
when you're looking at all the data you
can say oh this is where the twitch
comes in when it rains outside I'll
sleep really good if it's not raining or
if I don't need sleep then I'm not going
to sleep I'm going to go work so let's
go ahead and take a look at that that
looks like in the code just like we did
before we go ahead and open up the site
kit set up just to say what the decision
tree classifier has you have your
parameters which will look a little bit
more in depth at as we write the code
but it has different ways of splitting
it the strategy used to choose a split
at each node Criterion max depth
remember the tree how far down do you
want it do you want to take up the space
of your whole computer with a and map
every piece of data or you know the
smaller that number is the smaller the
level the tree is and the less
processing it takes but is also more
General so you're less likely to get as
in depth and answer
um and then of course minimal samples
you need for to split samples for the
leaves there's a lot of things in here
as far as what how big the tree is and
how to define it and when do you define
it and how to weight it and they have
their different attributes which you can
dig deeper into that can be very
important if you want to know the why of
things and then we go down here to our
methods and you'll see just like
everything else we have our fit method
very important and our predict the two
main things that we use what is what
we're going to predict our X to be equal
to and we'll go ahead and go up here and
start putting together the code we're
going to import our numpy our pandas
there's our confusion Matrix our train
test Split Decision tree classifier
that's the big one that we're actually
working with that's the line right here
where we're going to be oops decision
tree there it is decision tree
classifier that's the one I was looking
for and of course we want to know the
accuracy and the classification report
on here and we're going to do a little
different than we did in the other
examples
and there's a reason for this let me go
and run this and load this up here we're
going to go ahead and build things on
functions and this is when you start
splitting up into a team this is the
kind of thing you start seeing a lot
more both in teams and for yourself
because you might want to swap one data
to test it on a different data depending
on what's going on so we're going to
have our import data here the data set
link the balance and so forth this just
returns balanced data let me just go
ahead and print because I'm curious as
to what this looks like import data and
it's going to return the balance data so
if I run that if we go ahead and print
this out here and run that you can see
that we have a whole bunch of data that
comes in there and some interesting
setup on here has let's see brrr I'm not
sure exactly what that represents on
here uh one one one one one two and so
forth so we have a different set of data
here the shape is 5 have columns one two
three four five seems to have a number
at the beginning which I'm going to
guess
brl a letter I mean and then a bunch of
numbers in there one one one one let's
see down here we got five five five uh
set up on this and let's see balance
data and since it said balance data I'm
going to guess that b means balanced R
means you need to move it right and L
means it needs to be moved left or
skewed to the left I'm not sure which
one ah let's go and close that out and
we'll go ahead and create a function to
split the data set X balance data equals
data values wide balance equals data
values of zero there's that letter
remember left right and balance and then
we're looking for the values of one
through five and we go ahead and split
it just like you would X train y train
set random State 100 test size is 0.3 so
we're taking thirty percent of the data
simply n brings you comprehensive
artificial intelligence bootcamp that
will cover a wide range of topics that
will Empower you to the knowledge and
skills needed to excel in the field of
AI to learn more about this course you
can find the course Link in the
description box below and it's going to
return your X your y your y train your
your X train your X test your y train
your y test again we do this because if
you're running a lot of these you might
want to switch how you split the data
and how you train it I tend to use a
bifold method I'll take a third of the
data and I'll train it on the other two
thirds and test it on that third and
then I'll switch it I'll switch which
third is a test data and then I can
actually take that information and
correlate it and it gives me a really
robust package for figuring out what the
complete accuracy is but in this case
we're just going to go ahead this is our
function for splitting data and this is
where it kind of gets interesting
because remember we're talking a little
bit about the different settings in our
model and so in here we're going to
create a decision tree but we're going
to use the genie setup and where did
that come from what's the genie on here
so so if we go back to the top of their
page and we have what Criterion are we
going to use we're going to use Genie
they have Genie and entropy those are
the two main ones that they use for the
decision tree so this one's going to be
Genie and if we're going to have a
function that creates the Genie model
and it even goes down here and here's
our fit train of the Genie model we'll
probably also want to create one for
entropy sometimes I even just I might
even make this just one function with
the different setups and I know one of
my one of the things I worked on
recently I had to create a one that
tested across multiple models and so I
would send the parameters to the models
or I would send this part right here
where it says decision tree classifier
that whole thing might be what I send to
create the model and I know it's going
to fit we're going to have our X train
and we're gonna have our predict and all
that stuff is the same so you can just
send that model to your function for
testing different models again this just
gives you one of the ways to do it and
you can see here we're going to chain
train with the genie and we're also
going to chain train with the entropy to
see how that works and if you're going
to have your models going two separate
models you're sending there we'll go
ahead and create a prediction this
simply is our y predict equals our
whatever object we sent whatever model
we sent here the clf object and predict
against our X test and you can see here
print y predict and return y predict set
up on here and we'll load that
definition up and then if you're going
to have a function that runs a predict
and print some things out we should also
have our accuracy function so here's our
calculate the accuracy what are we
sending we're sending our y test data
this could also be y actual and Y
predict and then we'll print out a
confusion Matrix and then we'll print
out the accuracy of the score on here
and print a report classification report
bundle it all together there so if we
bring this all together we have all the
steps we've been working towards which
is importing our data by the ways you'll
spend 80 of your time importing data in
most machine learning setups and cooking
it and burning it and getting it
formatted so that it it works with
whatever models you're working with the
decision tree has some cool features in
that if you're missing data it can
actually pick that up and just skip that
it says I know how to split this there's
no way of knowing whether it rained or
didn't rain last night so I'll look at
something else like whether you watched
TV after eight o'clock you know that
blue screen thing uh so we have our
function importing our data set we bring
in the data we split the data so we have
our X text test and Y train and then we
have our different models our clf Genie
so it's a decision tree classifier using
the genie setup and then we can also
create the model using entropy and then
once we have that we have our function
for making the prediction and we have
our function for calculating the
accuracy and then if you're going to
have that we should probably have our
main code involved here this probably
looks more familiar if you're depending
on what you're working on if you're
working on like a pie charm then you
would see this in throwing something up
real quick in Jupiter notebook uh so
here's our our main data import which
we've already defined we get our split
data we create our Genie we create our
entropy so there's our two models going
on here there's our two models so these
are two separate data models we've
already sent them to be trained then
we're going to go ahead and print the
results using Genie index so we'll start
with the genie and we want to go ahead
with the genie and print our our
predictions Y X test to the genie and
calculate the accuracy on here and then
we want to print the results using
entropy so this is just the same thing
coming down like we did with the genie
we're going to put out our my predict
entropy and our calculations so let's go
ahead and run that and just see what
this piece of code does we do have like
one of our data needs to be is getting a
warning on there that's nothing major
because it's just a simple warning
probably an update of a new version is
coming out uh and so here we are we have
of our data set it's got 625 you can
actually see an example of the data set
B meaning balanced I guess and here's
our five data points one one one means
it's balanced it's skewed to the right
with one one one two and so forth on
here and then we're going to go ahead
and predict from our prediction whether
it's to the right or to the left you can
think of a washing machine that's ski
that's banging on one side of the thing
or maybe it's an automated car or down
the middle of the road that's in balance
and it starts going varying to the right
so we need to correct for it and when we
print out the confusion Matrix we have
three different variables r l and B so
we should see the three different
variables on here and you have as far as
whether it predicts in this case the
balance there's not a lot of balanced
loads on here and didn't do a good job
guessing whether it's balanced or not
that's what I took from this first one
the second one I'm guessing is the right
so it did pretty good job guessing the
right balance you can see that a bunch
of of them came up left unbalanced
probably not good for an automated car
as it tells you 18 out of the uh 18
missed things and tells you to go the
wrong direction and here we are going
the other way 19 to 71 and of course we
can back that up with an accuracy report
on here and you can see the Precision
how well the left and right balance is
79 79 precision and so forth and then we
went and used the entropy and let me
just see if we can get so we can get
them both next to each other here's our
entropy of our the first setup our first
model which is the genie Model 67 18
1971 63 22 2070 pretty close the two
models you know that's not a huge
difference in numbers this second one of
entropy did slightly it looks like
slightly worse because it did one better
as far as the right balance and did what
is this four worse on the left balance
or whatever so slightly worse if I was
guessing between these I'd probably use
the first one they're so close though
that wouldn't be it wouldn't be a Clear
Choice as to which one worked better and
there's a lot of numbers you can play
with here which might give better
results depending on what the data is
going in now one of the takeaways you
should have from the different category
routines we ran is that they run very
similar you certainly change the
perimeters in them as to whether you're
using what model you're using and how
you're using it and what data they get
applied to but when you're talking about
the scikit learn package it does such an
awesome job of making it easy you split
your data up you train your data and you
run the prediction and then you see what
kind of accuracy what can confusion
confusion Matrix It generates so we talk
about algorithm selection logistic
regression K nearest neighbors logistic
regression is used when we have a
binomial outcome for example simple to
predict whether an email is Spam or not
whether the tumor is malignant or not
the logistic regression works really
good on that you can do it in a k
nearest neighbors also the question is
which one will it work better in I find
a logistic regression models work really
good in a lot of raw numbers so if
you're working with say the stock market
is this a good investment or a bad
investment so that's one of the things
that it handles the numbers better Kate
nearest neighbors are used in scenarios
where non-parametric no fixed number of
parameters algorithms are required it is
used in pattern recognition Data Mining
and intrusion detection so K means
really good in finding the patterns I've
seen that as a preprocessor to a lot of
other processors where you use the K
nearest neighbors to figure out what
data groups together very powerful
package support Vector machines support
Vector machines are used whenever the
data has higher Dimensions the human
genome micro array svms are extensively
used in the hard handwriting recognition
models and you can see that we were able
to switch between the parabolic and the
circular setup on there where you can
now have that donut kind of data and be
able to filter that out with the support
Vector machine and then decision trees
are mostly used in operational
researches specifically in decision
analysis to help identify a strategy
most likely to reach any goal they are
preferred where the model is easy to
understand I like that last one it's a
good description is it easy to
understand so you have data coming in
when am I going to go to bed you know
was it raining outside you can go back
and actually look at the pieces and see
those different decision modes takes a
little bit more to dig in there and
figure out what they're doing but you
can do that and you can actually help
you figure out why people love it for
the Y Factor so uh strengths and
limitations big one on all of these the
streets and limitations we talk about
logistic regressions the strings are it
is easy to implement and efficient to
train it is relatively easy to
regularize the data points remember how
we put everything between zero and one
when you look at logistic regression
models you don't have to worry about
that as much limitations as a high
Reliance on proper representation of
data It could only predict a categorical
outcome with the K nearest neighbors it
doesn't need a separate training period
new data can be added seamlessly without
affecting the accuracy of the model kind
of an interesting thing because you can
do partial training that can become huge
if you're running across a really large
data sets or the data is coming in it
can continually do a partial fit on the
data with the K nearest neighbors and
continue to adjust that data it doesn't
doesn't work on high dimensional and
large data sets we were looking at the
breast cancer 36 different features what
happens when you have 127 features or a
million features and you say well what
do you have a million features in well
if I was analyzing uh Logic the legal
documents I might have a tokenizer that
splits a words up to be analyzed and
that tokenizer might create one million
different words available that might be
in the document for doing weights
sensitive to noisy data outliers and
missing values that's a huge one with K
nearest neighbors they really don't know
what to do with the missing value how do
you compute the the distance if you
don't know what the value is the svm
works more efficiently on high
dimensional data it is relatively memory
efficient so it's able to create those
planes with only a few different
variables in there as opposed to having
to store a lot of data for different
features and things like that it's not
suitable for a large data sets the svm
you start running this over gigabytes of
data causes some huge issues
underperforms if the data has noise or
overlapping that's is a big one we were
looking at that where the svm splits it
and it creates a soft buffer but what
happens when you have a lot of stuff in
the middle that's hard to sort out
doesn't know what to do with that causes
sbm to start crashing or not perform as
well decision trees handles non-linear
perimeters and missing values
efficiently the missing values is huge
I've seen this in was it the wine
tasting data sets where they have three
different data sets and they share
certain features but then each one has
some features that aren't in the other
ones and has to figure out how to handle
those well the decision tree does that
automatically instead of having to
figure a way to fill that data in before
processing like you would with the other
models it's easy to understand and has
less training period so it trains pretty
quickly comes up there and just keeps
forking the tree down and moving the
parts around and so it doesn't have to
go through the data multiple times
guessing and adjusting it just creates
the tree as it goes overfitting in high
variants are the most annoying part of
it that's that's an understatement uh
that has to do with how many Leafs and
how many decisions you have it do the
more you have the more overfit it is to
the data it also uh just in making the
choices and how the choices come in it
might overfit to a specific feature
because that's where it started at and
that's what it knows and it really is
challenged with large data sets they've
been working on that with the data
Forest but it's not suitable for large
data sets it's really something you'd
probably run on a single machine and not
across not across a data pool or
anything solution tree one of the many
powerful tools in the machine learning
library begins with a problem I think I
have to buy a car so in making this
question you want to know how do I
decide which one to buy and you're going
to start asking questions is a mileage
greater than 20 is a price less than 15
will it be sufficient for six people
does it have enough airbag anti-lock
brakes all these questions come up then
as we feed all this data in we make a
decision remember decision comes up oh
hey this seems like a good idea here's a
car so as we go in through this decision
process using a decision tree we're
going to explore this maybe not in
buying a car but in how to process data
what's in it for you let's start by
finding out what is machine learning and
why we even want to know about it for
processing our data we'll go into the
three basic types of machine learning
and the problems that are used by
Machine learning to solve finally we'll
get into what is a decision tree what
are the problems that decision tree
solves what are the advantages and
disadvantages of using a decision tree
and then we want to dig in a little deep
into the mechanics how does the decision
tree work and then we'll go in and do a
case loan repayment prediction are we
actually are going to put together some
python code and show you the basic
python code for generating a decision
tree what is machine learning there are
so many different ways to describe what
is machine learning in today's world and
illustrate it we're going to take a
graphic here and making decisions or
trying to understand what's going on and
really underlying machine learning
learning is people want to wish they
were smarter wish we could understand
the world better so you can see a guy
here who's uh saying hey how can I
understand the world better and someone
comes up and says let's use artificial
intelligence machine learning is a part
of artificial intelligence and now he's
gives a big smile on his face because
now he has artificial intelligence to
help him make his decisions and they can
think in new ways so this brings in new
ideas so what is machine learning this
is a wonderful graph here you can see
where we have a learn predict decide
these are the most basic three premises
of machine learning in learning we can
describe the data in new ways and able
to learn new aspects about what we're
looking at and then we can use that to
predict things and we can use that to
make decisions so maybe it's something
that's never happened before but we can
make a good guess whether it's going to
be a good investment or not it also
helps us categorize stuff so we can
remember it better so it's easier to
pull it out of the catalog we can
analyze data in new ways we never
thought possible and then of course
there's the very large growing industry
of recognizing we can do facial
recognition driver recognition automated
car recognition all these are part of
machine learning going back to our guy
here who's in his ordinary system and
would like to be smarter make better
choices what happens with machine
learning is an application of artificial
intelligence wherein the system gets the
ability to automatically learn and
improved based on experience so this is
exciting because you have your ordinary
guy who now has another form of
information coming in and this is with
the artificial intelligence helps him
see things he never saw or track things
he can't track so instead of having to
read all the news feeds he can now have
an artificial intelligence sorted out so
he's only looking at the information he
needs to make a choice with and of
course we use all those machine learning
tools back in there and he's now making
smarter choices with less work types of
machine learning let's break it into
three primary types of learning first is
supervised learning where you already
have the data and the answers so if you
worked at a bank you you'd already have
a list of all the previous loans and who
defaulted on them and who made good
payments on them you then program your
machine learning tool and that lets you
predict on the next person whether
they're going to be able to make their
payments or not on their loan if you
have one category we already know the
answers the next one would be you don't
know the answers you just have a lot of
information coming in unsupervised
learning allows you to group liked
information together so if you're
analyzing photos it might group all the
images of trees together and all the
images of houses together without ever
knowing what a house or a tree is which
leads us to the third type of machine
learning the third type of machine
learning is reinforcement learning
unlike supervised or unsupervised
learning you don't have the data prior
to starting so you get the data one line
at a time and then whether you make a
good choice or a bad choice the machine
learning tool has to then adjust
accordingly so you get a plus or minus
feedback you can liken this to the way a
human learns we experience life one
minute at a time time and we learn from
that and either our memories is good or
we learn to avoid some problems in
machine learning to understand where the
decision tree fits into our machine
learning tools we have to understand the
basics of some of the machine learning
problems and three of the primary ones
fall underneath classification problems
with categorical Solutions like yes or
no true or false one or zero this might
be does it belong to a particular group
yes or no then we have regression
problems where there's a continuous
value needs to be predicted like product
prices profit and you can see here this
is a very simple linear graph you can
guess what the next value is based on
the first four it kind of follows a
straight line going up and clustering
this is problems where the data needs to
be organized to find specific patterns
like in the case of product
recommendation they group all the
different products that people just like
you viewed on a shopping site and say
people who bought this also bought this
the most commonly used for the decision
trees for classification for figuring
out is it red or is it not is it a fruit
or is it a vegetable people yes or no
true false left or right zero one and so
we talk about classification we're going
to look at the basic machine learning
these are the four main tools used in
classification there's a Nave Bays
logistic regression decision tree and
random Forest the first two are for
simpler data so if your data is not very
complex you can usually use these to do
a fairly good representation by drawing
a line through the data or a curve
through the data they work Wonderful in
a lot of problems but as things get more
complicated the decision tree comes in
and then if you have a very large amount
of data you start getting into the
random Forest so the decision tree is
actually a part of the random Forest but
today we're just going to focus on the
decision tree
what is a decision tree let's go through
a very simple example before we dig in
deep decision tree is a tree shape
diagram used to determine a course of
action each branch of the tree
represents a possible decision or
currents or reaction let's start with a
simple question how to identify a random
vegetable from a shopping bag so we have
this group of vegetables in here and we
can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
is going to be a what looks to be a red
chili and if it's true it's going to be
a bell pepper from the capsicum family
so it's a capsicum
problems that decision tree can solve so
let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if-then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error
before we dig deeper into the mechanics
of the decision tree let's take a look
at the advantages of using a decision
tree and we'll also take a glimpse at
the disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and non-linear parameters don't
affect its performance so even if the
data doesn't fit an easy curved graph
you can still use it to create an
effective decision or prediction
if we're going to look at the advantages
of a decision tree we also need to
understand the disadvantages of a
decision tree the first disadvantage is
overfitting overfitting occurs when the
algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variance
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data
decision tree important terms before we
dive in further we need to look at some
basic terms we need to have some
definitions to go with our decision tree
in the different parts we're going to be
using we'll start with entropy entropy
is a measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is a measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side High height is
less than 10 true or false and as you
see as we split it the entropy continues
to be less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because it'll do it for you but
we'll look on the actual math at how
they compute entropy finally we went on
the different parts of our tree and they
call the leaf node Leaf node carries a
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root node
how does a decision tree work wonder
what kind of animals I'll get in the
jungle today maybe you're the hunter
with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a the funky e of K where I
equals 1 to k k would represent the
number of animal the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math don't really have to memorize this
math just be aware that it's there and
this is what's going on in the
background and so we have three drafts
two tigers one monkey two elephants a
total of eight animals gather and if we
plug that into the formula we get an
entropy that equals three over eight so
we have three drafts a total of eight
times the log usually they use base two
on the log so log base 2 of 3 over 8
plus in this case this hits the
elephants two over eight two elephants
over a total of eight times log base two
two over eight plus one monkey over a
total of eight log base 2 1 over 8 and
plus two 2 over 8 of the Tigers log base
2 over 8. and if we plug that into our
computer or calculator I obviously can't
do logs in my head we get an entropy
equal to
0.571 the program will actually
calculate the entropy of the data set
similarly after every split to calculate
the gain now we're not going to go
through each set one at a time to see
what those numbers are we just want you
to be aware that this is a Formula or
the mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
is probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has to decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decide to split both
the nodes using height as the condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes of
the Tigers the monkey and the elephants
all separate into their own groups this
tree can now predict all the classes of
animals present in the data set with the
hundred percent accuracy that was easy
use case loan repayment prediction let's
get into my favorite part and open up
some Python and see what the programming
code and the scripting looks like in
here we're going to want to do a
prediction and we start with this
individual here who's requesting to find
out how good his customers are going to
be whether they're going to be pay their
loan or not for this bank and from that
we want to generate a problem statement
to predict if a customer will repay loan
amount or not and then we're going to be
using the decision tree algorithm in
Python let's see what that looks like
and let's dive into the code in our
first few steps of implementation we're
going to start by importing the
necessary packages that we need from
Python and we're going to load up our
data and take a look at what the data
looks like so the first thing I need is
I need something to edit my Python and
run it in so let's flip on over and here
I'm using the Anaconda Jupiter notebook
now you can use any python IDE you like
to run it in but I find the Jupiter
notebooks really nice for doing things
on the Fly and let's go ahead and just
paste that code in the beginning and
before we we start let's talk a little
bit about what we're bringing in and
then we're going to do a couple things
in here we have to make a couple changes
as we go through this first part of the
import the first thing we bring in is
numpy as NP that's very standard when
we're dealing with mathematics
especially with a very complicated
machine learning tools you almost always
see the numpy come in for your num your
number it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to taking your basic data and storing it
in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation and just a minute we're going
to change that and we'll go over that
too and then there's also the SK dot
tree import decision tree classifier
that's the actual tool we're using
remember I told you don't be afraid of
the mathematics it's going to be done
for you well the decision tree
classifier has all that mathematics in
there for you so you don't have to
figure it back out again and then we
have sklearn.metrix for accuracy score
we need to score our our setup that's
the whole reason we're splitting it
between the training and testing data
and finally we still need the sklearn
import tree and that's just the basic
tree function that's needed for the
decision tree classifier and finally
we're going to load our data down here
and I'm going to run this and we're
going to get two things on here one
we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix that
and I'm going to put in here my file
path I'm just going to call it full file
name and you'll see it's on my C drive
and this is very lengthy setup on here
where I stored the data2.csv file
don't worry too much about the full path
because on your computer it'll be
different the data.2 CSV file was
generated by simply learn
if you want a copy of that you can
comment down below and request it here
in the YouTube
and then if I'm going to give it a name
full file name
I'm going to go ahead and change it here
to full
file name so let's go ahead and run it
now and see what happens
and we get a warning
when you're coding understanding these
different warnings and these different
errors that come up is probably the
hardest lesson to learn
so let's just go ahead and take a look
at this and use this as a opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection
so if we go up here we have
sklearn.cross validation and if you
research this and go to the sklearn site
you'll find out that you can actually
just swap it right in there with model
selection
and so when I come in here and I run it
again
that removes a warning what they've done
is they've had two different developers
develop it in two different branches
and then they decided to keep one of
those and eventually get rid of the
other one that's all that is and very
easy and quick to fix
before we go any further I went ahead
and opened up the data from this file
remember the the data file we just
loaded on here the data underscore 2.csv
let's talk a little bit more about that
and see what that looks like both as a
text file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got one two
three four five columns and each column
has data in it and let me flip this over
because we're also going to look at this
in an actual spreadsheet so you can see
what that looks like and here I've
opened it up in the open Office calc
which is pretty much the same as Excel
and zoomed in and you can see we've got
our columns and our rows of data a
little easier to read in here we have a
result yes yes no we have initial
payment last payment credit score house
number if we scroll way down
we'll see that this occupies a thousand
and one lines of code or lines of data
with the first one being a column and
then 1000 lines of data
now as a programmer
if you're looking at a small amount of
data I usually start by pulling it up in
different sources so you can see what
I'm working with
but in larger data you won't have that
option it'll just be too too large so
you need to either bring in a small
amount that you can look at it like
we're doing right now or we can start
looking at it through the python code so
let's go ahead and move on and take the
next couple steps to explore the data
using python let's go ahead and see what
it looks like in Python to print the
length and the shape of the data so
let's start by printing the length of
the database we can use a simple Lin
function from python
and when I run this you'll see that it's
a thousand long and that's what we
expected there's a thousand lines of
data in there if you subtract the column
head this is one of the nice things when
we did the balance data from the panda
read CSV you'll see that the header is
row zero so it automatically removes a
row
and then shows the data separate it does
a good job sorting that data out for us
and then we can use a different function
and let's take a look at that and again
we're going to utilize the tools in
panda
and since the balance underscore data
was loaded as a panda data frame
we can do a shape on it and let's go
ahead and run the shape and see what
that looks like
what's nice about this shape is not only
does it give me the length of the data
we have a thousand lines it also tells
me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the pandas module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data.head and this is a
pandas print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
a print job here of data set just a
simple print statement
and when we run that
and let's just take a closer look at
that let me zoom in here
there we go
pandas does such a wonderful job of
making this a very clean
readable data set so you can look at the
data you can look at the column headers
you can have it when you put it as the
head it prints the first five lines of
the data
and we always start with zero so we have
five lines we have zero one two three
four instead of one two three four five
that's a standard scripting and
programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word doc where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
site to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces
so first we have our X and Y
where did X and Y come from well X is
going to be our data
and Y is going to be the answer or the
target you can look at it source and
Target
in this case we're using X and Y to
denote the data in and the data that
we're actually trying to guess what the
answer is going to be and so to separate
it we can simply put in x equals the
balance of the data.values the first
brackets means that we're going to
select all the lines in the database so
it's all the data and the second one
says we're only going to look at columns
one through five remember always start
with zero zero is a yes or no and that's
whether the loan went default or not so
we want to start with one if we go back
up here that's the initial payment and
it goes all the way through the house
number
well if we want to look at one through
five we can do the same thing for Y
which is the answers and we're going to
set that just equal to the zero row so
it's just the zero row and then it's all
rows going in there so now we've divided
this into two different data sets
one of them with the
data going in and one with the answers
next we need to split the data
and here you'll see that we have it
split into four different parts
the first one is your X training your X
test your y train your y test
simply put we have X going in where
we're going to train it and we have to
know the answer to train it with
and then we have X test where we're
going to test that data and we have to
know in the end what the Y was supposed
to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to
0.3 so that's roughly 30 percent will be
used in the test and then we use a
random state so it's completely random
which rows it takes out of there and
then finally we get to actually build
our decision tree and they've called it
here clf underscore entropy that's the
actual decision tree or decision tree
classifier and in here they've added a
couple variables which we'll explore in
just a minute and then finally we need
to fit the data to that so we take our
clf entropy that we created and we fit
the X train and since we know the
answers for X trade or the Y train we go
and put those in and let's go ahead and
run this and what most of these sklearn
modules do is when you set up the
variable in this case when we set the
clf entropical decision tree classifier
it automatically prints out what's in
that decision tree there's a lot of
variables you can play Within here and
it's quite beyond the scope of this
tutorial real to go through all of these
and how they work but we're working on
entropy that's one of the options we've
added that it's completely a random
state of 100 so 100 percent and we have
a max depth of three now the max depth
if you remember above when we were doing
the different graphs of animals means
it's only going to go down three layers
before it stops and then we have minimal
samples of leaves as five so it's going
to have at least five leaves at the end
so I'll have at least three splits I'll
have no more than three layers and at
least five end leaves with the final
result at the bottom now that we've
created our decision tree classifier not
only created it but trained it let's go
ahead and apply it and see what that
looks like so let's go ahead and make a
prediction and see what that looks like
we're going to paste our predict code in
here
and before we run it let's just take a
quick look at what it's doing here we
have a variable y predict that we're
going to do
and we're going to use our variable clf
entropy that we created
and then you'll see dot predict and it's
very common in the SK learn modules that
they're different tools have the predict
when you're actually running a
prediction
in this case we're going to put our X
test data in here
now if you delivered this for use an
actual commercial use and distributed it
this would be the new loans you're
putting in here to guess
whether the person is going to be pay
them back or not in this case we need to
test out the data and just see how good
our sample is how good of our tree does
at predicting the loan payments and
finally since Anaconda Jupiter notebook
is works as a command line for python we
can simply put the why predict en to
print it I could just as easily have put
the print
and put brackets around y predict en to
print it out we'll go ahead and do that
it doesn't matter which way you do it
and you'll see right here that it runs a
prediction this is roughly 300 in here
remember it's 30 percent of a thousand
so you should have about 300 answers in
here
and this tells you which each one of
those lines of our test went in there
and this is what our why predict came
out
so let's move on to the next step we're
going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit
there we go
so you have a nice full picture
and we'll see here we're just going to
do a print accuracy is
and then we do the accuracy score
and this was something we imported
earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from
that's coming from here down here from
sklearn.metrix import accuracy score
and you could probably run a script make
your own script to do this very easily
how accurate is it how many out of 300
do we get right and so we put in our y
test that's the one we ran the predict
on and then we put in our y predict en
that's the answers we got and we're just
going to multiply that by a hundred
because this is just going to give us an
answer as a decimal and we want to see
it as a percentage and let's run that
see what it looks like
and if you see here we got an accuracy
of 93.6667
so when we look at the number of loans
and we look at how good our model fit we
can tell people it has about a 93.6
fitting to it so just a quick recap on
that we now have accuracy set up on here
and so we have created a model that uses
a decision tree algorithm to predict
whether a customer will repay the loan
or not the accuracy of the model is
about 94.6 percent the bank can now use
this model to decide whether it should
approve the loan request from a
particular customer or not and so this
information is really powerful we may
not be able to as individuals understand
all these numbers because they have
thousands of numbers that come in but
you can see that this is a smart
decision for the bank to use a tool like
this to help them to predict how good
their profit is going to be off of the
loan balances and how many are going to
default or not to Forest currently today
is used in remote sensing for example
they're used in the etm devices if
you're a space buff that's the enhanced
thematic mapper they use on satellite
lights which see far outside the human
Spectrum for looking at land masses and
they acquire images of the Earth's
surface the accuracy is higher and
training time is less than many other
machine learning tools out there also
object detection multi-class object
detection is done using random Forest
algorithms a good example is a traffic
we try to sort out the different cars
buses and things and it provides a
better detection and complicated
environments it's very complicated up
there and then we have another example
connect and let's take a little closer
look at connect connect they use a
random Forest as part of the game
console and what it does is it tracks
the body movements and it recreates it
in the game and let's see what that
looks like we have a user who performs a
step in this case it looks like Elvis
Presley going there
that is then recorded So the connect
registers the movement and then it marks
the user based on accuracy and it looks
like we have prints going on this one
from Elvis Presley to Prince it's great
so Mark's user base on the accuracy if
we look at that a little closer we have
a training set to identify body parts
where are the hands where are the feet
what's going on with the body
that then goes into a random Forest
classifier that learns from it once
we've trained the classifier
and then identifies the body parts while
the person's dancing is able to
represent that in a computer format and
then based on that it scores the game
and how accurate you are as being Elvis
Presley or Prince and you're dancing
let's take an overview of what we're
going to cover today what's in it for
you we're going to start with is what is
machine learning we're not going to go
into detail on that we're going to
specifically look how the random Force
fits in the machine learning hierarchy
then we're going to look at some
applications of random Forest what is
classification which is his primary use
why use random Force what's the benefits
of it and how does it actually come
together what is random forest and then
we'll get into random forest and the
decision tree how all that's like the
final step in how it works and finally
we'll get some python code in there and
we'll use the case the iris flower
analysis now if you don't know what any
of these terms mean or where we're going
with this don't worry we're going to
cover all the basics and have you up and
running and even having doing some basic
script in Python by the end let's take a
closer look at types of machine learning
specifically we're going to look at
where the decision tree fits in with the
different machine learning packages out
there we'll start with the basic types
of machine learning there's supervised
learning where you have lots of data and
you're able to train your models there's
unsupervised learning where it has to
look at the data and then divide it
based on its own algorithms without
having any training and then there's
reinforcement learning where you get a
plus or negative if you have the answer
correct this particular tool belongs to
the supervised learning let's take a
closer look at that what that means in
supervised learning supervised learning
falls into two groups classification and
regression we'll talk about regression a
little later and how that differs this
particular format goes underneath
classification so we're looking at
supervised learning and classification
in the machine learning tools
classification is a kind of problem
where in the outputs are categorical in
nature like yes or no true or false also
0 or 1. in that particular framework
there's the k n n where the NN stands
for nearest neighbor Nave Bays the
decision tree which is part of the
random Forest that we're studying today
so why random Forest It's always
important to understand why we use this
tool over the other ones what are the
benefits here and so with the random
Forest the first one is there's no
overfitting if you use of multiple trees
reduce the risk of overfitting training
time is less overfitting means that we
have fit the data so close to what we
have as our sample that we pick up on
all the weird parts and instead of
predicting the overall data you're
predicting the weird stuff which you
don't want high accuracy runs
efficiently in large database for large
data it produces highly accurate
predictions in today's world of Big Data
this is really important and this is
probably where it really shines this is
where why random Forest really he comes
in it estimates missing data data in
today's world is very messy so when you
have a random Forest it can maintain the
accuracy when a large proportion of the
data is missing what that means is if
you have data that comes in from five or
six different areas and maybe they took
one set of Statistics in one area and
they took a slightly different set of
Statistics in the other so they have
some of the same shared data but one is
missing like the number of children in
the house if you're doing something over
demographics and the other one is
missing the size of the house it will
look at both of those separately and
build two different trees and then it
can do a very good job of guessing which
one fits better even though it's missing
that data let us dig deep into the
theory of exactly how it works and let's
look at what is random forests random
forests or random decision Forest is a
method that operates by constructing
multiple decision trees the decision of
the majority of the trees is chosen by
the random Forest as the final decision
in this uh we have some nice Graphics
here we have a decision tree and they
actually use a real tree to denote the
decision tree which I love and given a
random some kind of picture of a fruit
this decision tree decides that the
output is it's an apple and we have a
decision tree two where we have that
picture of the fruit goes in and this
one decides that it's a limit and the
decision three tree gets another image
and it decides it's an apple and then
this all goes together and what they
call the random forest in this random
Forest then looks at it and says okay I
got two votes for apple one vote for
lemon the majority is Apples so the
final decision is apples to understand
how the random Forest works we first
need to dig a little deeper and take a
look at the random forest and the actual
decision tree and how it builds that
decision Tree in looking closer at how
the individual decision trees work we'll
go ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diet diagrammed used to determine
a course of action each branch of the
tree represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like they switched from
lemons to oranges we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up is the color orange well goes
hmm orange or red well if it's true then
it comes out as the orange and if it's
false that leaves apples
so in this example it sorts out the
fruit in the bowl or the images of the
fruit a decision tree these are very
important terms to know because these
are very Central to understanding the
decision train when working with them
the first is entropy everything on the
decision tree and how it makes this
decision is based on entropy entropy is
a measure of Randomness or
unpredictability in the data set then
they also have Information Gain
the leaf node the decision node and the
root node we'll cover these other four
terms as we go down the tree but let's
start with entropy so starting with
entropy we have here a high amount of
Randomness what that means is that
whatever's coming out of this decision
if it was going to guess based on this
data it wouldn't be able to tell you
whether it's a lemon or an apple it
would just say it's a fruit
so the first thing we want to do is we
want to split this apart and we take the
initial data set we're going to create a
data set one and a data set two we just
split it in two and if you look at these
new data sets after splitting them the
entropy of each of those sets is much
less so for the first one whatever comes
in there it's going to sort that data
and it's going to say okay if this data
goes this direction it's probably an
apple and if it goes into the other
direction it's probably a lemon so that
brings us up to Information Gain it is a
measure of decrease in the entropy after
the data set is split what that means in
here is that we've gone from one set
which has a very high entropy to two
lower sets of entropy and we've added in
the values of E1 for the first one and
E2 for the second two which are much
lower and so that information gain is
increased greatly in this example and so
you can find that the information grain
simply equals decision E1 minus E2 and
as we're going down our list of
definitions we'll look at the leaf node
and the leaf node carries the
classification or the decision so we
look down here to the leaf node we
finally get to our set one or our set
two when it comes down there and it says
okay this object's gone into set one if
it's gone into set one it's going to be
split by some means and we'll either end
up with apples on the leaf node or a
lemon on the leaf node and on the right
it'll either be an apple or lemons those
Leaf nodes are those final decisions or
classifications that's the definition of
leaf node in here if we're going to have
a final Leaf where we make the decision
we should have a name for the nodes
above it and they call those decision
nodes a decision node decision node has
two or more branches and you can see
here where we have the five apples and
one lemon and in the other case the five
lemons and one apple they have to make a
choice of which tree it goes down based
on some kind of measurement or
information given to the tree and that
brings us to our last definition the
root node the topmost decision node is
known as the root node and this is where
you have all of your data and you have
your first decision it has to make or
the first split in information
so far we've looked at a very general
image with the fruit being split let's
look and see exactly what that means to
split the data and how do we make those
decisions on there let's go in there and
find out how does a decision tree work
so let's try to understand this and
let's use a simple example and we'll
stay with the fruit we have a bowl of
fruit and so let's create a problem
statement and the problem is we want to
classify the different types of fruits
in the bowl based on different features
the data set in the bowl is looking
quite messy and the entropy is high in
this case so if this ball was our
decision maker it wouldn't know what
choice to make it has so many choices
which one do you pick Apple grapes or
lemons and so we look in here we're
going to start with a training set
so this is our data that we're training
our data with and we have a number of
options here we have the color and under
the color we have red yellow purple we
have a diameter three three one three
three one and we have a label Apple
lemon Grapes apple lemon grapes and how
do we split the data we have to frame
the conditions to split the data in such
a way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives us the
highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
three color equals yellow red diameter
equals one and when we look at that
you'll see over here we have one two
three four threes that's a pretty hearty
selection so let's say the condition
gives us a maximum gain of three so we
have the most pieces fall into that
range so our first split from our
decision node is we split the data based
on the diameter is it greater than or
equal to three if it's not that's false
it goes into the great bolt and if it's
true it goes into a bowl fold of lemon
and apples the entropy after splitting
has decreased considerably so now we can
make two decisions if you look at
they're very much less chaos going on
there this node has already attained an
entropy value of zero as you can see
there's only one kind of label left for
this Branch so no further splitting is
required for this node however this node
on the right is still requires a split
to decrease the entropy further so we
split the right node further are based
on color if you look at this if I split
it on color that pretty much cuts it
right down the middle it's the only
thing we have left in our choices of
color and diameter too and if the color
is yellow it's going to go to the rifle
and if it's false it's going to go to
the left Bowl so the entropy in this
case is now zero so now we have three
moles with zero entropy there's only one
type of data in each one of those bowls
so we can predict a lemon with a hundred
percent accuracy and we can predict the
Apple also with 100 accuracy along with
our grapes up there so we've looked at
kind of a basic tree in our forest but
what we really want to know is how does
a random Forest work as a whole so to
begin our random Forest classifier let's
say we already have built three trees
and we're going to start with the first
tree that looks like this just like we
did in the example this tree looks at
the diameter if it's greater than or
equal to three it's true otherwise it's
false so one side goes to the smaller
diameter one side goes to larger
diameter and if the color is orange it's
going to go to the right true we're
using oranges now instead of lemons and
if it's red it's going to go to the left
false we build a second tree very
similar but split differently instead of
the first one being split by a diameter
this one when they created it if you
look at that first Bowl it has a lot of
red objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to the left if
it's true it goes off to the right let's
go ahead and bring these three trees so
you can see them all in one image so
this would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out you can't see the color on it so
it's missing data remember one of the
things we talked about earlier is that a
random Forest works really good if
you're missing data if you're missing
pieces so this fruit has an image but
maybe as a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have they put the color in
there so ignore the color down there but
the diameter equals three we find out it
grows in the summer equals yes and the
shape is a circle and if you go to the
right you can look at what one of the
decision trees did this is the third one
is the diameter greater than equal to
three is a color orange well it doesn't
really know on this one but if you look
at the value it say true and you go to
the right tree two classifies it as
cherries is a color equal red is is a
shape a circle true it is a circle so
this would look at it and say oh that's
a cherry and then we go to the other
classifier and it says is the diameter
equal one well that's false does it grow
in the summer true so it goes down and
looks at as oranges so how does this
random Forest work the first one says
it's an orange the second one said it
was a cherry and the third one says it's
an orange
and you can guess that if you have two
oranges and one says it's a cherry when
you add that all together the majority
of the vote says orange so the answer is
it's classified as an orange even though
we didn't know the color and we're
missing data on it I don't know about
you but I'm getting tired of fruit so
let's switch and I did promise you we'd
start looking at a case example and get
into some python coding today we're
going to use the case the iris flower
analysis
oh this is the exciting part as we roll
up our sleeves and actually look at some
python coating before we start the
python coding we need to go ahead and
create a problem statement wonder what
species of Iris do these flowers belong
to let's try to predict the species of
the flowers using machine learning in
Python let's see how it can be done so
here we begin to go ahead and Implement
our python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into Python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case I'm going to be using the
Anaconda Jupiter notebook which is one
of my favorites certainly there's
notepad plus plus and eclipse and dozens
of others or just even using the python
terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our Jupiter note
book and I've already opened up a new
page for Python 3 code and I'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the
sklearn.datasets import load Iris now
this isn't the actual data this is just
the module that allows us to bring in
the data the load Iris and the iris is
so popular it's been around since 1936
when Ronald Fisher published a paper on
it and they're measuring the different
parts of the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import our random forest
classifier from the sklearn module so
sklearn dot Ensemble import random force
classifier and then we want to bring in
two more modules and these are probably
the most commonly used modules in Python
and data science with any of the other
modules that we bring in and one is
going to be pandas we're going to import
pandas as PD p is a common term used for
pandas and pandas is basically creates a
data format for us where when you create
a pandas data frame it looks like an
Excel spreadsheet and you'll see that in
a minute when we start digging deeper
into the code panda is just wonderful
because it plays nice with all the other
modules in there and then we have numpy
which is our numbers Python and the
numbers python allows us to do different
mathematical sets on here we'll see
right off the bat we're going to take
our NP and we're going to go ahead and
Seed the randomness with it with zero so
np.random.seed is seating that as zero
this code doesn't actually show anything
we're going to go ahead and run it
because I need to make sure I have all
those loaded and then let's take a look
at the next module on here the next six
slides including this one are all about
exploring the data remember I told you
half of this is about looking at the
data and getting it all set so let's go
ahead and take this code right here the
script and let's get that over into our
jupyter notebook and here we go we've
gone ahead and run the Imports and I'm
going to paste the code down here
and let's take a look and see what's
going on the first thing we're doing is
we're actually loading the iris data and
if you remember up here we loaded the
module that tells it how to get the IRS
data now we're actually assigning that
data to the variable Iris and then we're
going to go ahead and use the DF to
Define data frame
and that's going to equal PD and if you
remember that's pandas as PD so that's
our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Irish feature names
and we're going to do the DF head and
let's run this you can understand what's
going on here
the first thing you want to notice is
that our DF has created what looks like
an Excel spreadsheet and in this Excel
spreadsheet we have set the columns so
up on the top you can see the four
different columns and then we have the
data iris.data down below it's a little
confusing without knowing where this
data is coming from so let's look at the
bigger picture and I'm going to go print
I'm just going to change this for a
moment and we're going to print olivirus
and see what that looks like
so when I print all a virus I get this
long list of information and you can
scroll through here and see all the
different titles on there
what's important to notice is that first
off there's a brackets at the beginning
so this is a python dictionary
and in a python dictionary you'll have a
key or a label and this label pulls up
whatever information comes after it so
feature names which we actually used
over here under columns is equal to an
array of simple length simple width
petal length petal width these are the
different names they have for the four
different columns and if you scroll down
far enough you'll also see data down
here oh goodness it came up right
towards the top and data is equal to the
different data we're looking at
now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names the target names which is
further down and we'll show you that
also in a minute let's go ahead and set
that back
to the Head
and this is one of the neat features of
pandas and Panda data frames
is when you do df.head or the panda
dataframe dot head it will print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
header set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be zero one two three four not
one two three four five so now we've got
our IRS data imported into a data frame
let's take a look at the next piece of
code in here and so in this section here
of the code we're going to take a look
at the Target and let's go ahead and get
this into our notebook this piece of
code so we can discuss it a little bit
more in detail so here we are in our
jupyter notebook I'm going to put the
code in here and before I run it I want
to look at a couple things going on so
we have DF species and this is
interesting because right here you'll
see where I have DF species in Brackets
which is the key code for creating
another column and here we have
iris.target now these are both in the
pandas setup on here so in pandas we can
do either one I could have just as
easily done Iris and then in Brackets
Target depending on what I'm working on
both are acceptable let's go ahead and
run this code and see how this changes
and what we've done is we've added the
target from the iris data set as another
column on the end
now what species is this is what we're
trying to predict so we have our data
which tells us the answer for all these
different pieces and then we've added a
column with the answer that way when we
do our final setup we'll have the
ability to program our our neural
network to look for these this different
data and know what a Sentosa is or a
Vera color which we'll see in just a
minute or virginica those are the three
that are in there and now we're going to
add one more column I know we're
organizing all this data over and over
again it's kind of fun there's a lot of
ways to organize it what's nice about
putting everything onto one data frame
is I can then do a printout and it shows
me exactly what I'm looking at and I'll
show you that where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here
and we're going to run that
and let's talk a little bit about what
we're doing now we're exploring data and
one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it DF remember you can put brackets this
is creating another column is train so
we're going to use part of it for
training and this equals NP remember
that stands for numpy DOT random.uniform
so we're generating a random number
between 0 and 1 and we're going to do it
for each of the rows that's where the
length DF comes from so each row gets a
generated number and if it's less than
0.75 it's true and if it's greater than
0.75 it's false this means we're going
to take 75 percent of the data roughly
because there's a Randomness involved
and we're going to use that to train it
and then the other 25 percent we're
going to hold off to the side and use
that to test it later on on so let's
flip back on over and see what the next
step is so now that we've labeled our
database for which is training and which
is testing let's go ahead and sort that
into two different variables train and
test and let's take this code and let's
bring it into our project and here we go
let's paste it on down here and before I
run this let's just take a quick look at
what's going on here is we have up above
we created remember there's our def.head
which prints the first five rows and
we've added a column is train at the end
and so we're going to take that we're
going to create two variables we're
going to create two new data frames
one's called train one's called test 75
percent in train 25 in test
and then to sort that out
we're going to do that by doing DF our
main original data frame with the iris
data in it and if DF is trained equals
true
that's going to go in the train and if
DF is train equals false it goes in the
test and so when I run this
we're going to print out the number in
each one let's see what that looks like
and you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75 percent in
one and 25 percent for us to test our
model on afterward so let's jump back to
our code and see where this goes in the
next two steps
we want to do one more thing with our
data and let's make it readable to
humans I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our code
let's see here we go paste it in and
you'll see here we've done a couple very
basic things we know that the columns in
our data frame again this is a panda
thing the DF columns
and we know the first four of them 0 1 2
3 that'd be the first four are going to
be the features or the titles of those
columns and so when I run this
you'll see down here that it creates an
index sepa length sepa width petal
length and petal width and this should
be familiar because if you look up here
here's our column titles going across
and here's the first four
one thing I want you to notice here is
that when you're in a command line
whether it's Jupiter notebook or you're
running command line in the terminal
window if you just put the name of it
it'll print it out this is the same as
doing print
features
and the shorthand is you just put
features in here if you're actually
writing a code
and saving the script and running it by
remote you really need to put the print
in there but for this when I run it
you'll see it gives me the same thing
but for this we want to go ahead and
we'll just leave it as features because
it doesn't really matter and this is one
of the fun thing about Jupiter notebooks
is I'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that for our
final step in prepping our data before
we actually start running the training
and the testing is we're going to go
ahead and convert the species on here
into something the computer understands
so let's put this code into our script
and see where that takes us
all right here we go we've set y equal
to PD dot factorize train species of
zero so let's break this down just a
little bit we have our pandas right here
PD factorize what's factorize doing I'm
going to come back to that in just a
second let's look at what train species
is and why we're looking at the group
zero on there
and let's go up here and here is our
species
remember this on we created this whole
column here for species
and then it has cytosis cytosis cytosis
cytosa and if you scroll down enough
you'd also see virginica and varicolor
we need to convert that into something
the computer understands zeros and ones
so the trained species of zero because
this is in the format of a of an array
of arrays so you have to have the zero
on the end and then species is just that
column factorize goes in there and looks
at the fact that there's only three of
them so when I run this you'll see that
y generates an array that's equal to in
this case it's the training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go we have two
lines of code oh my goodness that was a
lot of work to get to two lines of code
but there is a lot in these two lines of
code so let's take a look and see what's
going on here and put this into our full
script that we're running and let's
paste this in here and let's take a look
and see what this is we have we're
creating a variable clf and we're going
to set this equal to the random forest
classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals two if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to across the system and
things like that and then the random
state is just how it starts zero is fine
for here
but let's go ahead and run this
we also have clf.fit train features
comma Y and before we run it let's talk
about this a little bit more clf dot fit
so we're fitting we're training it we
are actually creating our random Forest
classifier right here this is a code
that does everything and we're going to
take our training set remember we kept
our test off to the side and we're going
to take our training set with the
features and then we're going to go
ahead and put that in and here's our
Target the Y so the Y is 0 1 and 2 that
we just created and the features is the
actual data going in that we put into
the training set let's go ahead and run
that
and this is kind of an interesting thing
because it printed out the random Force
classifier
and everything around it
and so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and it printed out y instead of print
y
EP this does the same thing it treats
this as a variable and prints it out but
if you're actually running your code
that wouldn't be the case and what is
printed out is it shows us all the
different variables we can change and if
we go down here you can actually see in
jobs equals two you can see the random
State equals zero those are the two that
we sent in there you would really have
to dig deep to find out all these the
different meanings of all these
different settings on here some of them
are self-explanatory if you kind of
think about it a little bit like Max
features is auto so all the features
that we're putting in there is just
going to automatically take all four of
them whatever we send it it'll take some
of them might have so many features
because you're processing words there
might be like 1.4 million features in
there because you're doing legal
documents and that's how many different
words are in there at that point you
probably want to limit the maximum
features that you're going to process in
leaf nodes that's the endnotes remember
we had the fruit and we're talking about
the leaf nodes like I said there's a lot
in this we're looking at a lot of stuff
here so you might have in this case
there's probably only think three leaf
nodes maybe four you might have
thousands of leaf nodes at which point
you do need to put a cap on that and say
okay you can only go so far and then
we're going to use all of our resources
on processing this and that really is
what most of these are about is limiting
the process and and making sure we don't
overwhelm a system and there's some
other settings in here again we're not
going to go over all of them warm start
equals false alarm start is if you're
programming it one piece at a time
externally since we're not we're not
going to have like we're not going to
continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the SK learn and if you're digging in
deep and running a major project on here
for today though all we need to do is
fit or train our features and our Target
y so now we have our training model
what's next if we're going to create a
model
we now need to test it remember we set
aside the test feature test group 25 of
the data so let's go ahead and take this
code and let's put it into our script
and see what that looks like okay here
we go
and we're going to run this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the setosa the
virginica and the Versa color and what
we're putting into our predict is the
test features and I always kind of like
to know what it is I am looking at so
real quick we're going to do test
features and remember features is an
array
of simple length sepal width petal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here seeing so
what features looks like this is just
playing with the with pandas data frames
you'll see that it's an index so when
you put an index in like this
into test features into test it then
takes those columns and creates a panda
data frames from those columns and in
this case
we're going to go ahead and put those
into our predict
so we're going to put each one of these
lines of data
the 5.0 3.4 1.5 0.2 and we're going to
put those in and we're going to predict
what our new Forest classifier is going
to come up with and this is what it
predicts it predicts uh zero zero zero
one two one one two two two and and
again this is the flower type satosha
virginica and Versa color so now that
we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our script
so we're pasting it in here and you'll
see that we're doing uh predict and
we've added underscore proba for
probability so there's our clf DOT
predict probability so we're running it
just like we ran it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10.
so you'll see down here instead of
looking at all of them uh which was what
27 you'll see right down here that this
generates a much larger field on the
probability and let's take a look and
see what that looks like and what that
means
so when we do the predict underscore
praba for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for setosa
it predicts a zero for virginica and it
predicts a zero for Versa color and so
on and so on and so on and let's uh you
know what I'm going to change this just
a little bit let's look at 10
to 20 just because we can
and we start to get a little different
of data and you'll see right down here
it gets to this one this line right here
and this line has 0 0.5 0.5 and so if
we're going to vote and we have two
equal votes it's going to go with the
first one so it says uh satosha gets
zero votes virginica gets 0.5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and this we're going to do in these
next two steps we're going to start by
setting up our predictions and mapping
them to the name so let's see what that
looks like
and let's go ahead and paste that code
in here and run it and this goes along
with the next piece of code so we'll
skip through this quickly and then come
back to a little bit so here's Iris dot
Target names
and uh if you remember correctly this
was the the names that we've been
talking about this whole time the
Sentosa virginica versus color and then
we're going to go ahead and do the
prediction again we've run we could have
just hit a variable equal to this
instead of re-running it each time but
we'll go ahead and run it again clf dot
predict test features remember that
Returns the zeros the ones and the twos
and then we're going to set that equal
to predictions so this time we're
actually putting it in a variable and
when I run this
it distributes it it comes out as an
array and the array is setosis cytosis
cytosa we're only looking at the first
five we could actually do let's do the
first 25 just so we can see a little bit
more on there and you'll see that it
starts mapping it to all the different
flower types the Versa color and the
virginica in there and let's see how
this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our script
and let's put that down here and paste
it there we go and we'll go ahead and
run it and let's talk about both these
sections of code here
and how they go together the first one
is our predictions and I went ahead and
did predictions through 25 let's just do
five
and so we have cytosis cytosis cytosis
cytosis that's what we're predicting
from our test model
and then we come down here we look at
test species I remember I could have
just done
test.species.head and you'll see it says
cytosis cytosis cytosis cytosa and they
match so the first one is what our
forest is doing
and the second one is what the actual
data is now is we need to combine these
so that we can understand what that
means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our Jupiter notebook here
let's see let's go ahead and paste that
in and then I'm going to because I'm on
the Jupiter notebook I can do a control
minus so you can see the whole line
there
there we go resize it and let's take a
look and see what's going on here we're
going to create in pandas remember PD
stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species
so across the top you'll see the Sentosa
versus color virginica and the actual
species satosa versicolor virginica and
so the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have setosa where they meet you have
versicolor where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30. if you had 13 plus 5 plus 12
you get 30. and then we notice here
where it says virginica but it was
supposed to be versacolor this is
inaccurate so now we have two two
inaccurate predictions and 30 accurate
predictions so we'll say that the model
accuracy is 93 that's just 30 divided by
32 and if we multiply by a hundred we
can say that it is 93 percent accurate
so we have a 93 accuracy with our model
I did want to add one more quick thing
in here on our scripting before we wrap
it up so let's flip back on over to my
script in here we're going to take this
line of code from up above I don't know
if you remember it but predicts equals
the iris dot Target underscore names so
we're going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point I
would go ahead and change this and this
is an array of arrays this is really
important when you're running these to
know that
so you need the double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here
and then I actually want to see what the
answer is so let's go ahead and type in
preds and print that out and when I run
this
you'll see that I've now predicted two
flowers so maybe I measured in my front
yard as versacolor and versacolor
not surprising since I put the same data
in for each one
this would be the actual end product
going out to be used on data that you
don't know the answer for
so that's going to conclude our
scripting part of this so what is
k-means clustering k-means clustering is
an unsupervised learning algorithm in
this case you don't have labeled data
unlike in supervised learning so you
have a set of data and you want to group
them and as the name suggests you want
to put them into clusters which means
objects that are similar in nature
similar in characteristics need to be
put together so that's what K means
clustering is all about the term k is
basically is a number so you need to
tell the system how many clusters you
need to perform so if K is equal to 2
there will be two clusters if K is equal
to 3 3 clusters and so on and so forth
that's what the k stands for and of
course there is a way of finding out
what is the best or Optimum value of K
for a given data we will look at that so
that is K means cluster so let's take an
X example k-min's clustering is used in
many many scenarios but let's take an
example of Cricket the game of cricket
let's say you received data of a lot of
players from maybe all over the country
or all over the world and this data has
information about the runs scored by the
people or by the player and the wickets
taken by the player and based on this
information we need to Cluster this data
into two clusters batsman and Bowlers so
this is an interesting example let's see
how we can perform this so we have the
data which consists of primarily two
characteristics which is the runs and
the wickets so the bowlers basically
take wickets and the batsman score runs
there will be of course a few Bowlers
who can score some runs and similarly
there will be some batsmen who will Who
would have taken a few wickets but with
this information we want to Cluster
those players into batsmen and Bowlers
so how does this work let's say this is
how the data is so there are information
there is information on the y-axis about
the Run scored and on the x-axis about
the victims taken by the players so if
we do a quick plot this is how it would
look and when we do the clustering we
need to have the Clusters like shown in
the third diagram out here we need to
have a cluster which consists of people
who have scored High runs which is
basically the batsman and then we need a
cluster with people who have taken a lot
of wickets which is typically the
bowlers there may be a certain amount of
overlap but we will not talk about it
right now so with cayman's clustering we
will have here that means K is equal to
2 and we will have two clusters which is
batsman and molars so how does this work
the way it works is the first step in
k-means clustering is the allocation of
two centroids randomly so two points are
assigned as so-called centroids so in
this case we want two clusters which
means K is equal to two so two points
have been randomly assigned as centroids
keep in mind these points can be
anywhere there are random points they
are not initially they are not really
the centroids centroid means it's a
central point of a given data set but in
this case when it starts off it's not
really the central idea okay so these
points though in our presentation here
we have shown them one point closer to
these data points and another closer to
these data points they can be assigned
randomly anywhere okay so that's the
first step the next step is to determine
the distance of each of the data points
from each of the randomly assigned
centroids so for example we take this
point and find the distance from this
centroid and the distance from this
center right this point is taken and the
distance is formed from this centroid
and the center and so on and so forth so
for every point the distance is measured
from both the centroids and then
whichever distance is less that point is
assigned to that centroid so for example
in this case visually it is very obvious
that all these data points are assigned
to this centroid and all these data
points are assigned to this centroid and
that's what is represented here in blue
color and in this yellow color the next
step is to actually determine the
central point or the actual centroid for
these two clusters so we have this one
initial class faster this one initial
cluster but as you can see these points
are not really the centroid centroid
means it should be the central position
of this data set Central position of
this data set so that is what needs to
be determined as the next step so the
central point of the actual centroid is
determined and the original randomly
allocated centroid is repositioned to
the actual centroid of this new clusters
and this process is actually repeated
now what might happen is some of these
points may get reallocated in our
example that is not happening probably
but it may so happen that the distance
is found between each of these data
points once again with these centroids
and if there is if it is required some
points may be reallocated we will see
that in a later example but for now we
will keep it simple so this process is
continued till the centroid
repositioning stops and that is our our
final cluster so this is our so after
iteration we come to this position this
situation where the centroid doesn't
need any more repositioning and that
means our algorithm has converged
convergence has occurred and we have the
cluster two clusters we have the
Clusters with a centroid so this process
is repeated the process of calculating
the distance and repositioning the
centroid is repeated till the
repositioning stops which means that the
algorithm has converged and we have the
final cluster with the data points and
the centroids so this is what you're
going to learn from this session we will
talk about the types of clustering what
is k-means clustering application of
k-means clustering k-means clustering is
done using distance measure so we will
talk about the common distance measures
and then we will talk about how cayman's
clustering works and go into the details
of k-means clustering algorithm and then
we will end with the demo and a use case
for k-min's clustering so let's begin
first of all what are the types of
clustering there are primarily two
categories of clustering hierarchical
clustering and then partitional
clustering and each of these categories
are further subdivided into
agglomerative and divisive clustering
and k-means and fuzzy c means clustering
let's take a quick look at what each of
these types of clustering are
in hierarchical clustering the Clusters
have a tree like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is the
bottom-up approach we begin with each
element as a separate cluster and merge
them into successively larger clusters
so for example we have a b c d e f which
start by combining BMC form one cluster
DNA form one more then we combine d e
and f one more bigger cluster and then
add BC to that and then finally a to it
compared to that divisive clustering or
divisive clustering is a top-down
approach we begin with the whole set and
proceed to divide it into successfully
smaller cluster so we have a b c d e f
we first take that as a single cluster
and then break it down into a b c d e
and f then we have partitional
clustering split into two subtypes
k-means clustering and fuzzy c means in
k-min's clustering the objects are
divided into the number of clusters
mentioned by the number K that's where
the K comes from so if we say k is equal
to 2 the objects are divided into two
clusters C1 and C2 and the way it is
done is the features or characteristics
are compared and all objects having
similar characteristics are clubbed
together so that's how K means
clustering is done we will see it in
more detail as we move forward and fuzzy
c means is very similar to k-means in
the sense that it clubs objects that
have similar characteristics together
but while in cayman's clustering two
objects cannot belong to or any object a
single object cannot belong to two
different clusters in c means objects
can belong to more than one cluster so
that is the primary difference between k
means and fuzzy c means so what are some
of the applications of k-means
clustering k-means clustering is used in
a variety of examples or variety of
business cases in real life starting
from academic performance diagnostic
system search engines and wireless
sensor networks and many more so let us
take a little deeper look at each of
these examples academic performance So
based on the scores of the students
students are categorized into a b c and
so on clustering forms a backbone of
search engines when a search is
performed the search results need to be
grouped together the search engines very
often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so clustering
especially k-means clustering uses
distance measure so let's take a look at
what is distance pressure so while these
are the different types of clustering in
this video we will focus on k-means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measures
there is euclidean distance there is
Manhattan distance then we have squared
euclidean distance measure and cosine
distance measure these are some of the
distance measures supported by k-means
clustering let's take a look at each of
these what is euclidean distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square root of Y2
minus y1 whole square plus x 2 minus X1
whole Square so this is an extension of
this formula so that is the euclidean
distance between two points what is the
squared euclidean distance measure it's
nothing but the square of the euclidean
distance as the name suggests so instead
of taking the square root we leave the
square as it is and then we have
Manhattan distance measure in case of
Manhattan distance it is the sum of the
distances across the x-axis and the
y-axis and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
k-means now let's go and check how
exactly K means clustering works okay so
this is how k-mains clustering works
this is like a flowchart of the whole
process there is a starting point and
then we specify the number of clusters
that we want now there are a couple of
ways of doing this we can do by trial
and error so we specify a certain number
maybe K is equal to 3 or 4 or 5 to start
with and then as we progress we keep
changing until we get the best clusters
or there is a technique called elbow
technique whereby we can determine the
value of K what should be the best value
of K how many clusters should be formed
so once we have the value of K we
specify that and then the system will
assign that many centroid so it picks
randomly that to start with randomly
that many points that are considered to
be the centroids of these clusters and
then it measures the distance of each of
the data points from these centroids and
assigns those points to the
corresponding centroid from which the
distance is minimum so each data point
will be assigned to the centroid Which
is closest to it and thereby we have K
number of initial clusters however this
is not the final clusters The Next Step
it does is for the new groups for the
Clusters that have been formed it
calculates the main position thereby
calculates the new centroid position the
position of the centroid moves compared
to the randomly allocated one so it's an
iterative process once again the
distance of each point is measured from
this new centroid point and if required
the data points are reallocated to the
new centroids and the mean position or
the new centroid is calculated once
again if the centroid moves then the
iteration continues which means the
convergence has not happened the
clustering has not converged so as long
as there is a movement of the centroid
this iteration keeps happening but once
the centroid stops moving which means
that the cluster has converged or the
clustering process has converged that
will be the end result so now we have
the final position of the centroid and
the data points are allocated
accordingly to the closest centroid I
know it's a little difficult to
understand from this simple flowchart so
let's do a little bit of visualization
and see if we can explain it better
let's take an example if we have a data
set for a grocery shop so let's say we
have a data set for a grocery shop and
now we want to find out how many
clusters this has to be spread across so
how do we find the optimum number of
clusters there is a technique called the
elbow method so when these clusters are
formed there is a parameter called
within sum of squares at the lower this
value is the better the cluster is that
means all these points are very close to
each other so we use this within sum of
squares as a measure to find the optimum
number of clusters that can be formed
for a given data set so we create
clusters or we let the system create
clusters of a variety of numbers maybe
of 10 10 clusters and for each value of
K the within SS is measured and the
value of K which has the least amount of
within SS or WSS that is taken as the
optimum value of K so this is the
diagrammatic representation so we have
on the y-axis the within sum of squares
or WSS and on the x-axis we have the
number of clusters so as you can imagine
if you have K is equal to 1 which means
all the data points are in a single
cluster the within SS value will be very
high because they are probably scattered
all over the moment you split it into
two there will be a drastic fall in the
within SS value and that's what is
represented here but then as the value
of K increases the decrease the rate of
decrease will not be so high it will
continue to decrease but probably the
rate of decrease will not be high so
that gives us an idea so from here we
get an idea for example the optimum
value of K should be either 2 or 3 or at
the most 4 but beyond that increasing
the number of clusters is not
dramatically changing the value in WSS
because that pretty much gets stabilized
okay now that we have got the value of K
and let's assume that these are our
delivery points the next step is
basically to assign two centroids
randomly so let's say C1 and C2 are the
centroids assigned randomly now the
distance of each location from the
centroid is measured and each point is
assigned to the centroid Which is
closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from C2 so these points will be
assigned which are close to C1 will be
assigned to C1 and these points or
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial grouping is done this is
part of C1 and this is part of C2 then
the next step is to calculate the actual
centroid of this data because remember
C1 and C2 are not the centroids they've
been randomly assigned points and only
thing that has been done was the data
points which are closest to them have
been assigned but now in this step the
actual centroid will be calculated which
may be for each of these data sets
somewhere in the middle so that's like
the main point that will be calculated
and the centroid will actually be
positioned or repositioned there same
with C2
so the new centroid for this group is C2
in this new position and C1 is in this
new position
once again the distance of each of the
data points is calculated from the
centroids now remember it's not
necessary that the distance Still
Remains the or each of these data points
still remain in the same group by
recalculating the distance it may be
possible that some points get
reallocated like so you see this so this
point earlier was closer to C2 because
C2 was here but after recalculating
repositioning it is observed that this
is closer to C1 than C2 so this is the
new grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is the repetitive
process iterative process and if the
centroid doesn't change once the
centroid stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these data
points as a part of each cluster so I
hope this helps in understanding the
whole process iterative process of K
means clustering so let's take a look at
the K means clustering algorithm let's
say we have X1 X2 X3 and number of
points as our inputs and we want to
split this into K clusters or we want to
create K clusters so the first step is
to randomly pick K points and call them
centroids they are not real centroids
because centroid is supposed to be a
center point but they are just called
centroids and we calculate the distance
of each and every input point from each
of the centroids so the distance of X1
from C1 from C2 C3 each of the distances
we calculate and then find out which
distance is the lowest and assign X1 to
that particular random centroid repeat
that process for X2 calculate its
distance from each of the centroid c 1 C
to C3 up to c k a and find which is the
lowest distance and assign X2 to that
particular centroid same with X3 and so
on so that is the first round of
assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and so there are K groups all
these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
have to calculate the actual centroids
for each of these groups which is like
the mean position which means that the
position of the randomly selected
centroids will now change and they will
be the main positions of this newly
formed K groups and once that is done we
once again repeat this process of
calculating the distance right so this
is what we are doing as part of Step 4
we repeat step two and three so we again
calculate the distance of X1 from the
centroid C1 C2 C3 and then C which is
the lowest value and assign X1 to that
calculate the distance of X2 from C1 C
to C3 or whatever up to c k and find
whichever is the lowest distance and
assign X2 to that centroid and so on in
this process there may be some
reassignment X1 Pro was probably
assigned to Cluster C2 and after doing
this calculation maybe now X1 is
assigned to C1 so that kind of
reallocation may happen so we repeat the
steps 2 and 3 till the position of the
centroids don't change or stop changing
and that's when we have convergence so
let's take a detailed look at each of
these steps so we randomly pick K
cluster centers we call them centroids
because they are not initially they are
not really the centroids so we let us
name them C1 C2 up to CK and then step
two we assign each data point to the
closest Center so what we do we
calculate the distance of each x value
from each C value so the distance
between X1 C1 distance between X1 C2 X1
C3 and then we find which is the lowest
value right that's the minimum value we
find and assign X1 to that particular
centroid then we go next to X2 find the
distance of X2 from C1 X2 from C2 X2
from C3 and so on up to c k and then
assign it to the point or to the
centroid which has the lowest value and
so on so that is Step number two in Step
number three We Now find the actual
centroid for each group so what has
happened as a part of Step number two we
now have all the points all the data
points grouped into K groups because we
we wanted to create K clusters right so
we have K groups each one may be having
a certain number of input values they
need not be equally distributed by the
way based on the distance we will have K
groups but remember the initial values
of the C1 C2 were not really the
centroids of these groups right we
assign them randomly so now in step 3 we
actually calculate the centroid of each
group which means the original point
which we thought was the centroid will
shift to the new position which is the
actual centroid for each of these groups
okay and we again calculate the distance
so we go back to step 2 which is what we
calculate again the distance of each of
these points from the newly positioned
centroids and if required we reassign
these points to the new centroids so as
I said earlier there may be a
reallocation so we now have a new set or
a new group we still have K groups but
the number of items and the actual
assignment may be different from what
was in Step 2 here okay so that might
change then we perform step 3 once again
to find the new centroid of this new
group so we have again a new set of
clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster all right so let's see
a couple of demos of k-means clustering
we will actually see some live demos and
python notebook using python notebook
but before that let's find out what's
the problem that we are trying to solve
the problem statement is let's say
Walmart wants to open a chain of stores
across the State of Florida
and it wants to find the optimal store
locations now the issue here is if they
open too many stores close to each other
obviously the they will not make profit
but if they if the stores are too far
apart then they will not have enough
sales so how do they optimize this now
for an organization like Walmart which
is an e-commerce giant they already have
the addresses of their customers in
their database so they can actually use
this information or this data and use
k-means cluster to find the optimal
location now before we go into the
python notebook and show you the Live
code I wanted to take you through very
quickly a summary of the code in the
slides and then we will go into the
python notebook so in this block we are
basically importing all the required
libraries like numpy matplotlab and so
on and we are loading the data that is
available in the form of let's say the
addresses for simplicity's sake we will
just take them as some data points then
the next thing we do is quickly do a
scatter plot to see how they are related
to each other with respect to each other
so in the scatter plot we see that there
are a few distinct groups already being
formed so you can actually get an idea
about how the cluster would look and how
many clusters what is the optimal number
of clusters and then starts the actual
k-means clustering process so we will
assign each of these points to the
centroids and then check whether they
are the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids and
then go through this iterative process
till the whole process converges and
finally we get an output like this so we
have four distinct clusters and
which is if we can say that this is how
the population is probably distributed
across Florida State and the centroids
are like the location where the store
should be the optimum location where the
store should be so that's the way we
determine the best locations for the
store and that's how we can help Walmart
find the best locations for the stores
in Florida so now let's take this into
python notebook let's see how this looks
when we are learning running the code
live all right so this is the code for
k-means clustering in jupyter Notebook
we have a few examples here which we
will demonstrate how k-means clustering
is used and even there is a small
implementation of k-means clustering as
well okay so let's get started okay so
this block is basically importing the
various libraries that are required like
matplotlib and Empire and so on and so
forth which would be used as a part of
the code then we are going and creating
blobs which are similar to clusters now
this is a very neat feature which is
available in scikit-learn make blobs is
a nice feature which creates clusters of
data sets so that's a wonderful
functionality that is readily available
for us to create some test data kind of
thing
so that's exactly what we are doing here
we are using make blobs and we can
specify how many clusters we want so
centers we are mentioning here so it
will go ahead and so we just mentioned
four so it will go ahead and create some
test data for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make blobs actually provides now
from here onwards we will basically run
the standard k-means functionality that
is readily available so we really don't
have to implement k-means itself the
k-means functionality or the or the
function is readily available you just
need to feed the data and we'll create
the Clusters so this is the code for
that we import k-means and then we
create an instance of k-means and we
specify the value of K this n underscore
clusters is the value of K remember K
means in K means K is basically the
number of clusters that you want to
create and it is a integer value so this
is where we are specifying that so we
have K is equal to 4. and so that
instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training kind of
thing but that's the call okay so we are
calling fit and what we are doing here
we are just passing the data so X has
these values the data that has been
created right so that is what we are
passing here and this will go ahead and
create the Clusters and then we are
using
after doing uh fit We Run The predict
which basically assigns for each of
these observations which cluster it
belongs to all right so it will name the
Clusters maybe this is cluster one this
is two three and so on or I will
actually start from zero cluster 0 1 2
and 3 maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in y
underscore K means when we call predict
that is what it does and we can take a
quick look at these y underscore K means
or with the cluster numbers that have
been assigned for each observation so
this is the cluster number assigned for
observation one maybe this is for
observation two observation three and so
on so we have how many about I think 300
samples right so all the 300 samples
there are 300 values here each of them
the cluster number is given and the
class faster number goes from 0 to 3 so
there are four clusters so the numbers
go from 0 1 2 3 so that's what is seen
here okay now so this was a quick
example of generating some dummy data
and then clustering that okay and this
can be applied if you have proper data
you can just load it up into X for
example here and then run the game so
this is the central part of the k-means
clustering program example so you
basically create an instance and and you
mentioned how many clusters you want by
specifying this parameter and underscore
clusters and that is also the value of K
and then pass the data to get the values
now the next section of this code is the
implementation of a k means now this is
kind of a rough implementation of the
k-means algorithm so we will just walk
you through I will walk you through the
code at each step what it is doing and
then we will see a couple of more
examples of how came in structuring can
be used in maybe some real life examples
real life use cases all right so in this
case here what we are doing is basically
implementing k-means clustering and
there is a function for a library
calculates for a given two pairs of
points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what k-means
does right so it calculates the distance
of each point or each data set from
predefined centroid and then based on
whichever is the lowest this particular
data point is assigned to that percent
right so that is basically available as
a standard function and we will be using
that here so as explained in the slides
the first step that is done in case of
k-means clustering is to randomly assign
some centroids so as a first step we
randomly allocate a couple of centroids
which we call here we are calling as
centers
and then we put this in a loop and we
take it through an iterative process
for each of the data points we first
find out using this function pairwise
distance argument for each of the points
we find out which one which Center or
which randomly selected centroid is the
closest and accordingly we assign the
data or the data point to that
particular centroid or class staff and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
center position right so we calculate
the new centroid and then we check if
the new centroid is the coordinates or
the position is the same as the previous
centroid the positions we will compare
and if it is the same that means the
process has converged so remember we do
this process till the centroids or the
centroid doesn't move anymore right so
the centroid gets relocated each time
this reallocation is done so the moment
it doesn't change anymore the position
of the centroid doesn't change anymore
we know that convergence has occurred so
till then so you see here this is like
an infinite Loop while true is an
infinite Loop it only breaks when the
centers are the same the new center and
the old Center positions are the same
and once that is done we return the
center Center labels now of course as
explained this is not a very
sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in if
the change will be very minor so in that
case also with that is actually
convergence right right so for example
the change is 0.0001 we can consider
that as convergence otherwise what will
happen is this will either take forever
or it will be never ending so that's a
small flaw here so that is something
additional checks may have to be added
here but again as mentioned this is not
the most sophisticated implementation uh
this is like a kind of a rough
implementation of the k-means cluster
ing so if we execute this code this is
what we get as the output so this is the
definition of this particular function
and then we call that find underscore
clusters and we pass our data X and the
number of clusters which is 4 and if we
run that and plot it this is the output
that we get so this is of course each
cluster is represented by a different
color so we have a cluster in green
color yellow color and so on and so
forth and these big points here these
are the centroids this is the final
position of the centroids and as you can
see visually also this appears like a
kind of a center of all these points
here right similarly this is like the
center of all these points here and so
on so this is the example or this is an
example of a implementation of K means
clustering and next we will move on to
see a couple of examples of how k-means
clustering is used in maybe some real
life scenario or use cases in the next
example or demo we are going to see how
we can use k-means clustering to perform
color compression we will take a couple
of images so there will be two examples
and we will try to use k-min's
clustering to compress the colors this
is a common situation in image
processing when you have an image with
millions of colors but then you cannot
render it on some devices which may not
have enough memory so that is the
scenario where where something like this
can be used
so before again we go into the python
notebook let's take a look at quickly
the the code as usual we import the
libraries and then we import the image
and then we will flatten it so the
reshaping is basically we have the image
information stored in the form of pixels
and if the image is like for example 427
by 640 and it has three colors so that's
the overall dimension of the of the
initial image we just reshape it and
then feed this to our algorithm and this
will then create clusters of only 16
clusters so this this colors there are
millions of colors and now we need to
bring it down to 16 colors so we use K
is equal to 16 and this is uh when we
visualize this is how it looks there are
these are all about 16 million possible
colors the input color space has 16
million possible colors and we just
sub compressor to 16 colors so this is
how it would look when we compress it to
16 colors and this is how the original
image looks and after compression to 16
colors this is uh the new image looks as
you can see there is not a lot of
information that has been lost though
the image quality is definitely reduced
a little bit
so this is an example which we are going
to now see in Python notebook let's go
into the python node and once again as
always we will import some libraries and
load this image called
flower.jpg okay so let me load that and
this is how it looks this is the
original image which has I think 16
million colors and this is the shape of
this image which is basically what is
the shape is nothing but the overall
size right so this is 427 pixel by 640
pixel and then there are three layers
which is this three basically is for RGB
which is red green blue so color image
will have that right so that is the
shape of this now what we need to do is
data let's take a look at how data is
looking so let me just create a new cell
and show you what is in data basically
we have captured this information
so data is what let me just show you
here
all right so let's take a look at China
what are the values in China and if we
see here this is how the data is stored
this is nothing but the pixel values
okay so this is like a matrix and each
one has about for this 427 by 640 pixel
XL all right so this is how it looks now
the issue here is these values are large
the numbers are large so we need to
normalize them to between 0 and 1 right
so that's why we will basically create
one more variable which is data which
will contain the values between 0 and 1
and the way to do that is divide by 255
so we divide China by 255 and we get the
new values in data so let's just run
this piece of code and this is the shape
so we now have also yeah what we have
done is we changed using reshape we
converted into the three dimensional
into a two-dimensional data set and let
us also take a look at how
let me just insert
are probably a cell here and take a look
at how data is looking all right so this
is how data is looking and now you see
this is the values are between 0 and 1
right so if you earlier noticed in case
of china the values were large numbers
now everything is between 0 and 1. this
is one of the things we need
all right so after that the next thing
that we need to do is to visualize this
and we can take random set of maybe 10
000 points and plot it and check and see
how this looks so let us just plot this
there so this is how the original the
color the pixel distribution is these
are two plots one is red against Green
and another is red against Blue and this
is the original distribution of
the car so then what we will do is we
will use k-means clustering to create
just 16 clusters for the various colors
and then apply that to the image now
what will happen is since the data is
large because there are millions of
colors using regular k-means may be a
little time consuming so there is
another version of k-means which is
called mini batch came in so we will use
that which is which processes in the
overall concept Remains the Same but
this basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code and also visualize this so that
we can see that there are this this is
how the 16 colors uh would look so this
is red against Green and this is red
against Blue there is quite a bit of
similarity between this original color
schema and the new one right so it
doesn't look very very completely
different or anything like now we apply
this the newly created colors to the
image and we can take a look how this is
looking now we can compare both the
images so this is our original image and
this is our new image so as you can see
there is not a lot of information that
has been lost it pretty much looks like
the original image yes we can see that
for example here there is a little bit
it appears a little dullish compared to
this one right because we kind of took
off some of the finer details of the
color but overall the high level
information has been maintained at the
same time the main advantage is that now
this can be this is an image which can
be rendered on a device which may not be
that very sophisticated now let's take
one more example with a different image
in the second example we will take an
image of the Summer Palace in China and
we repeat the same process this this is
a high definition color image with
millions of colors and also
three-dimensional
had now we will reduce that to 16 colors
using k-means clustering and we do the
same process like before we reshape it
and then we cluster the colors to 16 and
then we render the image once again and
we will see that the color the quality
of the image is slightly deteriorates as
you can see here this has much finer
details in this which are probably
missing here but then that's the
compromise because there are some
devices which may not be able to handle
this kind of a high density images
so let's run this chord in Python
notebook all right so let's apply the
same technique for another picture which
is even more intricate and has probably
much complicated color schema so this is
the image now once again we can take a
look at the shape which is 427 by 640 by
3 and this is the new data would look
somewhat like this compared to the
flower image so we have some new values
here and we will also bring this as you
can see the numbers are much big so we
will much bigger so we will now have to
scale them down to values between 0 and
1 and that is done by dividing by 255 so
let's go ahead and do that
and reshape it okay so we get a
two-dimensional Matrix and we will then
as a next step we will go ahead and
visualize this how it looks the 16
colors and this is basically how it
would look 16 million colors and now we
can
create the Clusters out of this the 16
k-means clusters we will create so this
is how the distribution of the pixels
would look with 16 colors and then we go
ahead and apply this
visualize how it is looking for with the
with the new just the 16 color so once
again as you can see this looks much
richer in color but at the same time and
this probably doesn't have as we can see
it doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done using
k-means clustering and we have also seen
in the previous examples of how to
implement k-means the code to roughly
how to implement k-means clustering and
we use some sample data using blob to
just execute the k-means clustering all
right so have you ever wondered how your
mail provider implements spam filtering
or how online news channels perform news
text classification or how companies
perform sentimental analysis of Their
audience on social media all of this and
more is done through a machine learning
algorithm called naive Bayes classifier
what is naive Bayes let's start with a
basic introduction to the Bayes theorem
named after Thomas Bayes from the 1700s
who first coined this in the western
literature naive Bayes classifier works
on the principle of conditional
probability as given by the Bayes
theorem before we move ahead at let us
go through some of the simple Concepts
and the probability that we will be
using let us consider the following
example of tossing two coins here we
have two quarters and if we look at all
the different possibilities of what they
can come up as we get that they could
come up as head heads they come up as
head tell tell head and Telltale when
doing the math on probability we usually
denote probability as a p a capital P so
the probability of getting two heads
equals one-fourth you can see in our
data set we have two heads and this
occurs once out of the four
possibilities and then the probability
of at least one tail occurs three
quarters of the time you'll see on three
of the twin tosses we have tails in them
and out of four that's three fourths and
then the probability of the second coin
being head given the first coin is tell
is one half and the probability of
getting two heads given the first coin
is a head is one half we'll demonstrate
that in just a minute and show you how
that math works now when we're doing it
with two coins it's easy to see but when
you have something more complex you can
see where these Pro these formulas has
really come in and work so the Bayes
theorem gives us a conditional
probability of an event a given another
event B has occurred in this case the
first coin toss will be B and the second
coin toss a this could be confusing
because we've actually reversed the
order of them and go from B to a instead
of a to B you'll see this a lot when you
work in probabilities the reason is
we're looking for event a we want to
know what that is so we're going to
label that a since that's our focus and
then given another event B has occurred
in the Bayes theorem as you can see on
the left the probability of a occurring
given B has occurred equals the
probability of B occurring given a has
occurred times the probability of a over
the probability of B this simple formula
can be moved around just like any
algebra formula and we could do the
probability of a after a given B times
probability of b equals the probability
of B given a times probability of a you
can easily move that around and multiply
it and divide it out let us apply base
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example HH out of
four one fourth and we have three with
tails in them giving us three quarters
or three-fourths seventy-five percent
the second condition the second set
three and four we'll explore a little
bit more in detail now we stick to a
simple example with two coins because
you can easily understand the math the
probability of throwing a tail doesn't
matter what comes before it and the same
with the head so still going to be fifty
percent or one half but when that com
when that probability gets more
complicated let's say you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is tells again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being Tails given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have one
half times one half over one-half or
one-half equals 0.5 or 1 4. so the Bayes
theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understand standing naive
Bayes and machine learning like with any
of our other machine learning tools it's
important to understand where the naive
Bayes fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervisor's learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
I this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease fees or
other elements and news classification
when you look at the Google news and it
says well is this political or is this
world news or a lot of that's all done
with the naive Bayes understanding naive
Bayes classifier now we already went
through a basic understanding with the
coins and the two heads and two tells
and head tell tale heads Etc we're going
to do just a quick review on that and
remind you that the naive Bayes
classifier is based on the Bayes theorem
which gives a conditional probability of
event a given event B and that's where
the probability of a given b equals the
probability of B given a times
probability of a over probability of B
remember this is an algebraic function
so we can move these different entities
around we can multiply by the
probability of B so it goes to the left
hand side and then we could divide by
the probability of a given B and just as
easy come up with a new formula for the
probability of B to me staring at these
algebraic functions kind of gives me a
slight headache
it's a lot better to see if we can
actually understand how this data fits
together in a table and let's go ahead
and start applying it to some actual
data so you can see what that looks like
so we're going to start with the
shopping demo problem statement and
remember we're going to solve this first
in table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the person
is going to buy based on these traits so
we can maximize them and find out the
best system for getting somebody to come
in and purchase our goods and products
from our store now having a nice visual
is great but we do need to dig into the
data so let's go ahead and take a look
at the data set we have a small sample
data set of 30 rows we're showing you
the first 15 of those rows for this demo
now the actual data file you can request
just type in below under the comments on
the YouTube video and we'll send you
some more information and send you that
file as you can see here the file is
very simple columns and rows we have the
day the discount the free delivery and
did the person purchase or not
brings you comprehensive artificial
intelligence boot camp that will cover a
wide range of topics that will Empower
you for the knowledge and skills needed
to excel in the field of AI to learn
more about this course you can find the
course Link in the description box below
and then we have under the day whether
it was a weekday a holiday was it the
weekend this is a pretty simple set of
data and long before computers people
used to look at this data and calculate
this all by hand so let's go ahead and
walk through this and see what that
looks like when we put that into tables
also note in today's world we're not
usually looking at three different
variables in 30 rows nowadays because
we're able to collect data so much we're
usually looking at 27 30 variables
across hundreds of rows the first thing
we want to do is we're going to take
this data and based on the data set
containing our three inputs Day discount
and free delivery we're going to go
ahead and populate that to frequency
tables for each attribute so we want to
know if they had a discount how many
people buy and did not buy did they have
a discount yes or no do we have a free
delivery yes or no on those dates how
many people made a purchase how many
people didn't and the same with the
three days of the week was it a weekday
a weekend a holiday and did they buy yes
or no as we dig in deeper to the this
table for our Bayes theorem let the
event by B A now remember we looked at
the coins I said we really want to know
what the outcome is did the person buy
or not and that's usually event a is
what you're looking for and the
independent variables discount free
delivery in day BB so we'll call that
probability of B now let us calculate
the likelihood table for one of the
variables let's start with day which
includes weekday weekend and holiday and
let us start by summing all of our rows
so we have the weekday row and out of
the weekdays there's nine plus two so
it's 11 weekdays there's eight weekend
days and 11 holidays that's a lot of
holidays and then we want to sum up the
total number of days so we're looking at
a total of 30 days let's start pulling
some information from our chart and see
where that takes us and when we fill in
the chart on the right you can see that
9 out of 24 purchases are made on the
weekday 7 out of 24 purchases on the
weekend and 8 out of 24 purchases on a
holiday and out of all the people who
come in 24 out of 30 purchase you can
also see how many people do not purchase
on the weekdays two out of six didn't
purchase and so on and so on we can also
look at the totals and you'll see on the
right we put together some of the
formulas the probability of making a
purchase on the weekend comes out at 11
out of 30. so out of the 30 people who
came into the store throughout the
weekend weekday and holiday 11 of those
purchases were made on the weekday and
then you can also see the probability of
them not making a purchase and this is
done for doesn't matter which day of the
week so we call that probability of no
buy would be 6 over 30 or 0.2 so there's
a 20 percent chance that they're not
going to make a purchase no matter what
day of the week it is and finally we
look at the probability of B of A in
this case we're going to look at the
probability of the weekday and not
buying two of the no buys were done on
the weekend out of the six people who
did not make purchases so when we look
at that probability of the week day
without a purchase is going to be 0.33
or 33 percent let's take a look at this
at different probabilities and based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37 percent 0.367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20 percent and the probability of a
weekday no purchase is roughly two out
of six so two out of six of our no
purchases were made on the weekday and
then finally we take our P of a b if you
elect we've kept the symbols up there we
got P of probability of B probability of
a probability of B if a we should
remember that the probability of a if B
is equal to the first one times the
probability of no buys over the
probability of the weekday so we could
calculate it both off the table we
created we can also calculate this by
the formula and we get the 0.3 six seven
which equals or 0.33 times 0.2 over
0.367 which equals 0.179 or roughly 17
to 18 percent and that'd be the
probability of no purchase done on the
weekday and this is important because we
can look at this and say as the
probability of buying on the weekday is
more than the probability of not buying
on the weekday we can conclude that
customers will most likely buy the
product on a weekday now we've kept our
chart simple and we're only looking at
one aspect so you should be able to look
at the table and come up with the same
information or the same conclusion that
should be kind of intuitive at this
point next we can take the same setup we
have the frequency tables of all three
independent variables now we can
construct the likelihood tables for all
three of the variables we're working
with we can take our day like we did
before we have weekday weekend and
holiday and we filled in this table and
then we can come in and also do that for
the discount yes or no did they buy yes
or no and we fill in that full table so
now we have our probabilities for a
discount and whether the discount leads
to a purchase or not and the probability
for free delivery does that lead to a
purchase or not and this is where it
starts getting really exciting let us
use these three likelihood tables to
calculate whether a customer will
purchase a product on a specific
combination of Day discount and free
delivery or not purchase here let us
take a combination of these factors day
equals holiday discount equals yes free
delivery equals yes let's dig deeper
into the math and actually see what this
looks like and we're going to start with
looking for the probability of them not
purchasing on the following combinations
of days we're actually looking for the
probability of a equal no buy no
purchase and our probability of B we're
going to set equal to is it a holiday do
they get a discount yes and was it a
free delivery yes before we go further
let's look at the original equation the
probability of a if B equals a
probability of B given the condition a
and the probability times the
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that just
a second in the formula times the full
probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is a day equal a holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six or a no
purchase on a free delivery day and
three out of six or a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the 6 out of 30. we divide that out
by the probability of each category over
the total number so we get the 20 out of
30 had a discount 20 23 out of 30 had a
yes for free delivery and 11 out of 30
were on a holiday we plug all those
numbers in we get
0.178 so in our probability math we have
a 0.178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times a day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our
0.986 so so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.986 we have a
probability of no purchase equals
0.178 so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals
0.98686 plus 0.178 and that equals the
1.164 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.71
percent and the likelihood of no
purchase is 15.29 percent given these
three different variables so it's if
it's on a holiday if it's a with a
discount and has free delivery then
there's an 84.71 percent chance that the
customer is going to come in and make a
purchase hooray they purchased our stuff
we're making money if you're we're
owning a shop that's like is the bottom
line is you want to make some money so
you can keep your shop open and have a
living now I promised you that we were
going to be finishing up the math here
with a few pages so we're going to move
on and we're going to do two steps the
first step is I want you to understand
why you want to enter why you want to
use the naive Bayes what are the
advantages of naive bays and then once
we understand those advantages we just
look at that briefly then we're going to
dive in and do some python coding
advantages of naive Bayes classifier so
let's take a look at the six advantages
of the naive Bayes classifier and we're
going to walk around this lovely wheel
looks like an origami folded paper the
first one is very simple and easy to
implement certainly you could walk
through the tables and do this by hand
you got to be a little careful because
the notations can get confusing you have
all these different probabilities and I
certainly mess those up as I put them on
you know is it on the top of the bottom
got to really pay close attention to
that when you put it into python it's
really nice because you don't have to
worry about any of that you let the
python handle that the python module but
understanding it you can put it on a
table and you can easily see how it
works and it's a simple algebraic
function it needs less training data so
if you have smaller amounts of data this
is great powerful tool for that handles
both continuous and discrete data it's
highly scalable with number of
predictors and data points so as you can
see just keep multiplying different
probabilities in there and you can cover
not just three different variables or
sets you can now expand this to even
more categories number five it's fast it
can be used in real time predictions
this is so important this is why it's
used in a lot of our predictions on
online shopping carts referrals spam
filters is because there's no time delay
as it has to go through and figure out a
neural network or one of the other mini
setups where you're doing classification
and certainly there's a lot of other
tools out there in the machine learning
that can handle these but most of them
are not as fast as the naive bays and
then finally it's not sensitive to
irrelevant features so it picks up on
your different probabilities and if
you're short on date on one probability
you can kind of it automatically adjust
for that those formulas are very
automatic and so you can still get a
very solid predictability even if you're
missing data or you have overlapping
data for two completely different areas
we see that a lot in doing census and
studying of people and habits where they
might have one study that covers one
aspect another one that overlaps and
because of two overlap they can then
predict the unknowns for the group that
they haven't done the second study on or
vice versa so it's very powerful in that
it is not sensitive to the irrelevant
features and in fact you can use it to
help predict features that aren't even
in there so now we're down to my
favorite part we're going to roll up our
sleeves and do some actual programming
we're going to do the use case text
classification now I would challenge you
to go back and send us a note on the
notes below underneath the video and
request the data for the shopping cart
so you can plug that into python code
and do that on your own time so you can
walk through it since we walk through
all the information on it but we're
going to do a python code doing text
classification very popular for doing
the naive Bayes so we're going to use
our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've traded it and we've shown you
agree for what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever interface IDE you
want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make sure all of the versions of
the modules match your python version
and in here you'll notice the first line
is your percentage matplot library in
line now three of these lines of code
are all about plotting the graph this
one lets the notebook notes and this is
an inline setup that we want the graphs
to show up on this page without it in a
notebook like this which is an Explorer
interface it it won't show up now a lot
of Ides don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop-up and
the graph pops up on there so you have
that set up also but for this we want
the matplot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP numpy is NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library.pi plot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import Seaborn as S and S and
we're going to do the sns.set now
Seaborn sits on top of Pi plot and it
just makes a really nice heat map it's
really good for heat maps and if you're
not familiar with heat maps that just
means we give it a color scale term
comes from the brighter red it is the
hotter it is in some form of data and
you can set it to whatever you want and
we'll see that later on so those you'll
see that those three lines of code here
are just importing the graph function so
we can graph it and as a data science
test you always want to graph your data
and have some kind of visual it's really
hard just to shove numbers in front of
people and they look at it and it
doesn't mean anything and then from the
sklearn.data sets we're going to import
the fetch 20 news groups very common one
for analyzing tokenizing words and
setting them up and exploring how the
words work and how do you categorize
different things when you're dealing
with documents and then we set our data
equal to fetch 20 news groups so our
data variable will have the data in it
and we're going to go ahead and just
print the target names data.target names
and let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp OS Ms
windows.miscellaneous and it goes all
the way down to talk
politics.miscellaneous talk
religion.missella genius these are the
categories we've already assigned to
this news group and it's called fetch20
because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually going to here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does
first we're going to set our categories
now if you noticed up here I could have
just as easily set this equal to
data.target underscore names because
it's the same thing but we want to kind
of spell it out for you so you can see
the different categories it kind of
makes it more visual so you can see what
your data is looking like in the
background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 News Group subset
equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates
train.data and we're just going to look
at data piece number five and let's go
ahead and run that and see what that
looks like and you can see when I print
train dot data number five under train
it prints out one of the Articles this
is article number five you can go
through and read it on there and we can
also go in here and change this to test
which should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number 5 is a different article but it's
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
length of train dot data and if we run
that you'll see that the training data
has 11
314 articles so we're not going to go
through all those articles that's a lot
of articles but we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewording the Second Amendment IDs
vtt line 58 lines 58 in article
Etc I'm going to scroll all the way down
and see all the different parts to there
now we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bayes and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but a deal
specifically working with words and text
and what they call tokenizing those
words so let's take this code and let's
uh skip on over to our Jupiter notebook
and walk through it and here we are in
our jupyter notebook let's paste that in
there and I can run this code right off
the bat it's not actually going to
display anything yet but it has a lot
going on in here so the top we have the
print module from the earlier one I
didn't know why that was in there so
we're going to start by importing our
necessary packages and from the sklearn
features extraction dot text we're going
to import TF IDF vectorizer I told you
we're going to throw a module at you we
can't go too much into the math behind
this or how it works you can look it up
the notation for the math is usually
tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times are used in a document
how many times or how many documents are
used in and it's a well used formula
it's been around for a while it's a
little confusing to put this in here but
let's let them know that it just goes in
there and waits the different words in
the document for us that way we don't
have to wait and if you put a weight on
it if you remember I was talking about
that up here earlier if these are all
emails they probably all have the word
from in them from probably has a very
low weight it has very little value in
telling you what this document's about
same with words like in an article in
articles in cost of on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the article's doing well how
do you figure out all those weights in
the different articles that's what this
module does that's what the TF IDF
vectorizer is going to do for us and
then we're going to import our
sklearn.naive Bays and that's our
multinomial in B multinomial naive base
pretty easy to understand that where
that comes from and then finally we have
the skylearn pipeline import make
pipeline now the make pipeline is just a
cool piece of code because we're going
to take the information we get from the
TF IDF vectorizer and we're going to
pump that into the multinomial in B so a
pipeline is just a way of organizing how
things flow it's used commonly you
probably already guess what it is if
you've done any businesses they talk
about the sales pipeline if you're on a
work crew or project manager you have
your pipeline of information that's
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial in B now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model dot fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the
train.target is what category they
already categorized that that particular
article as and what's Happening Here is
the trained data is going into the tfid
vectorizer so when you have one of these
articles that goes in there it waits all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bayes we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're going to set that equal
to model dot predict most of the sklearn
use the term dot predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already read this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that take a walk
through it and see what that looks like
so back to our jupyter notebook I'm
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
the whole screen too big so we have here
from sklearnmetrics import confusion
Matrix
and that's just going to generate a set
of data that says I the prediction was
such the actual truth was either agreed
with it or is something different and
it's going to add up those numbers so we
can take a look and just see how well it
worked and we're going to set a variable
matte equal to confusion Matrix and we
have our test Target our test data that
was not part of the training very
important in data science we always keep
our test data separate otherwise it's
not a valid model if we can't properly
test it with new data and this is the
labels we created from that test data
these are the ones that we predict it's
going to be so we go in and we create
our SN heat map the SNS is our Seaborn
which sits on top of the pi plot so we
create a sns.heat map we take our
confusion Matrix and it's going to be
met.t and do we have other variables
that go into the sns.heat map we're not
going to go into detail what all the
variables mean The annotation equals
true that's what tells it to put the
numbers here so you have the 166 the one
the zero zero zero one format d and c
bar equals false have to do with the
format if you take those out you'll see
that some things disappear and then the
X tick labels and the y t labels those
are our Target names and you can see
right here that's the alt atheism comp
graphics composms windows.miscellaneous
and then finally we have our plt.x label
remember the SNS or the Seaborn sits on
top of our matplot library our PLT and
so we want to just tell that X label
equals a true is is true the labels are
true and then the Y label is prediction
label so when we say a true this is what
it actually is and the prediction is
what we predicted and let's look at this
graph because that's probably a little
confusing the way we rattled through it
and what I'm going to do is I'm going to
go ahead and flip back to the slides
because they have a black background
they put in there that helps it shine a
little bit better so you can see the
graph a little bit easier so in reading
this graph what we want to look at is
how the color scheme has come out and
you'll see a line right down the middle
diagonally from upper left to bottom
right what that is is if you look at the
labels we have our predicted label on
the the left and our true label on the
right those are the numbers where the
prediction and the true come together
and this is what we want to see is we
want to see those lit up that's what
that heat map does is you can see that
it did a good job of finding those data
and you'll notice that there's a couple
of red spots on there where I missed you
know it's a little confused we talk
about talk religion miscellaneous versus
talk politics miscellaneous social
religion Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics so you could
understand why it might mislabel them
but overall it did a pretty good job if
we're going to create these models we
want to go ahead and be able to use them
so let's see what that looks like to do
this let's go ahead and create a
definition a function to run and we're
going to call this function let me just
expand that just a notch here there we
go I like mine in big letters predict
categories we want to predict the
category we're going to send it as a
string and then we're sending it train
equals train we have our training model
and then we had our pipeline model
equals model this way we don't have to
resend these variables each time the
definition knows that because I said
train equals train and I put the equal
for model and then we're going to set
the prediction equal to the model dot
predict s so it's going to send whatever
string we send to it it's going to push
that string through the pipeline the
model pipeline it's going to go through
and tokenize it and put it through the
TF IDF convert that into numbers and
weights for all the different documents
and words and then I'll put that through
our naive Bayes and from it we'll go
ahead and get our prediction we're going
to predict what value it is and so we're
going to return train.target
namespredict of zero and remember that
the train.target names that's just
categories I could have just as easily
put categories in there dot predict of
0. so we're taking the prediction which
is a number and we're converting it to
an actual category we're converting it
from I don't know what the actual number
numbers are let's say 0 equals alt
atheism so we're going to convert that 0
to the word or one maybe it equals comp
Graphics so we're going to convert
number one into comp Graphics that's all
that is and then we got to go ahead and
and then we need to go ahead and run
this so I load that up and then once I
run that we can start doing some
predictions let me go ahead and type in
predict category and let's just do the
predict category Jesus Christ and it
comes back and says it's social religion
Christian that's pretty good now note I
didn't put print on this one of the nice
things about the Jupiter notebook editor
and a lot of inline editors is if you
just put the name of the variable out as
returning the variable train.target
underscore names it'll automatically
print that for you in your own ID you
might have to put in print let's see
where else we can take this and maybe
you're a space science buff so how about
sending load to International
Space Station mission
and if we run that we get science space
or maybe you're a automobile buff and
let's do um oh they were going to tell
me Audi is better than BMW but I'm going
to do BMW is better than an Audi so
maybe you're a car buff and we run that
and you'll see it says recreational I'm
assuming that's what Rec stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous
so when we take our definition or our
function and we run all these things
through Kudos we made it we were able to
correctly classify texts into different
groups based on which category they
belong to using the naive Bayes
classifier now we did throw in the
pipeline the TF idea vectorizer we threw
in the graphs those are all things that
you don't necessarily have to know to
understand the naive Bayes setup or
classifier but they're important to know
one of the main uses for the naive Bayes
is with the TF IDF tokenizer vectorizer
where it tokenizes the word and has
labels and we use the pipeline because
you need to push all that data through
and it makes it really easy and fast you
don't have to know those to understand
naive Bayes but they certainly help for
understanding the industry and data
science and we can see their categorizer
our naive Bayes classifier we were able
to predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction in our
trained model by now we 've got a
well-known machine learning models make
predictions by learning from the past
data available so we have our input
values our machine learning model Builds
on those inputs of what we already know
and then we use that to create a
predicted output is that a dog a little
kid looking over there and watching the
black cat cross their path no dear you
can differentiate between a cat and a
dog based on their characteristics
cats cats have sharp claws uses to climb
smaller length of ears meows and purrs
doesn't love to play around dogs they
have dull claws bigger length of ears
barks loves to run around you usually
don't see a cat running around people
although I do have a cat that does that
where dogs do and we can look at these
we can say we can evaluate their
sharpness of the claws how sharp are
their claws and we can evaluate the
length of the ears and we can usually
sort out cats from dogs based on even
those two characteristics now tell me if
it is a cat or a dog an odd question
usually little kids no cats and dogs by
now unless you live a place where
there's not many cats or dogs so if we
look at the sharpness of the claws the
length of the ears and we can see that
the cat has a smaller ears and sharper
claws than the other animals its
features are more like cats it must be a
cat sharp claws length of ears and goes
in the cat group because KNN is based on
feature similarity we can do
classification using kn n classifier so
we have our input value the picture of
the black cat it goes into our trained
model and it predicts that this is a cat
coming out so what is KNN what is the k
n algorithm K nearest neighbors is what
that stands for it's one of the simplest
supervised machine learning algorithms
mostly used for classification so we
want to know is this a dog or is not a
dog is it a cat or not a cat it
classifies a data point based on how his
neighbors are classified KNN stores all
available cases and classifies new cases
based on a similarity measure and here
we've gone from cats and dogs right into
wine another favorite of mine k n stores
all available cases and classifies new
cases based on a similarity measure and
here you see we have a measurement of
sulfur dioxide versus the chloride level
and then the different wines they've
tested and where they fall on that graph
based on how much sulfur dioxide and how
much chloride K and K N is a perimeter
that refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equals
five and we'll talk about K in just a
minute a data point is classified by the
majority of votes from its five nearest
neighbors here the unknown point would
be classified as red since four out of
five neighbors are red so how do we
choose K how do we know k equals five I
mean this was the value we put in there
so we're going to talk about it how do
we choose the factor K and N algorithm
is based on feature similarity choosing
the right value of K is a process called
parameter tuning and is important for
better accuracy so at k equals three we
can classify we have a question mark in
the middle as either a as a square or
not is it a square or is it in this case
a triangle and so if we set k equals to
3 we're going to look at the three
nearest neighbors we're going to say
this as a square and if we put k equals
to 7 we classify as a triangle depending
on what the other data is around it you
can see as the K changes depending on
where that point is that drastically
changes your answer and we jump here we
go how do we choose the factor of K
you'll find this in all machine learning
choosing these facts factors that's the
face you get he's like oh my gosh you
say choose the right K did I set it
right my values in whatever machine
learning tool you're looking at so that
you don't have a huge bias in One
Direction or the other and in terms of
knnn the number of K if you choose it
too low the bias is based on it's just
too noisy it's it's right next to a
couple things and it's going to pick
those things and you might get a skewed
answer and if your K is too big then
it's going to take forever to process so
you're going to run into processing
issues and resource issues so what we do
the most common use and there's other
options for choosing K is to use the
square root of n so N is a total number
of values you have you take the square
root of it in most cases you also if
it's an even number so if you're using
uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of n and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use knnn we can use K N when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where you might
get into a gig of data if it's really
clean it doesn't have a lot of noise
because K N is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the knnn and also just
using for smaller data sets k n works
really good how does a k n algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false are either
normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using k n n so
if we have new data coming in this says
57 kilograms and 177 centimeters is that
going to be normal or underweight to
find the nearest neighbors we'll
calculate the euclidean distance
according to the euclidean distance
formula the distance between two points
in the plane with the coordinates x y
and a b is given by distance D equals
the square root of x minus a squared
plus y minus B squared and you can
remember that from the two edges of a
triangle we're Computing the third Edge
since we know the X side and the Y side
let's calculate it to understand clearly
so we have our unknown point and we
placed it there in red and we have our
other points where the data is scattered
around the distance D1 is the square
root of 170 minus 167 squared plus 57
minus 51 squared which is about 6.7 and
distance 2 is about 13 and distance 3 is
about 13.4 similarly we will calculate
the euclidean distance of unknown data
point from all the points in the data
set and because we're dealing with small
amount of data that's not that hard to
do and it's actually pretty quick for a
computer and it's not a really
complicated Mass you can just see how
close is the data based on the euclidean
distance hence we have calculated the
euclidean distance of unknown data point
from all the points as shown where X1
and y1 equal 57 and 170 whose class we
have to classify so now we're looking at
that we're saying well here's the
euclidean distance who's going to be
their closest neighbors now let's
calculate the nearest neighbor at k
equals three and we can see the three
closest neighbors put some at normal and
that's pretty self-evident when you look
at this graph it's pretty easy to say
okay what you know we're just voting
normal normal three votes for normal
this is going to be a normal weight so
majority of neighbors are pointing
towards normal hence as per k n
algorithm the class of 57 170 should be
normal so a recap of k n n positive
integer K is specified along with a new
sample we select the K entries in our
database which are closest to the new
sample we find the most common
classification of these entries this is
the classification we give to the new
sample so as you can see it's pretty
straightforward we're just looking for
the closest things that match what we
got so let's take a look and see what
that looks like in a use case in Python
so let's dive into the predict diabetes
use case so use case predict diabetes
the objective predict whether a person
will be diagnosed with diabetes or not
we have a data set of 768 people who
were or were not diagnosed with diabetes
and let's go ahead and open that file
and just take a look at that data and
this is in a simple spreadsheet format
the data itself is comma separated very
common set of data and it's also a very
common way to get the data and you can
see here we have columns a through I
that's what one two three four five six
seven eight eight columns with a
particular attribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes that's why
they're taking it well that could cause
issue in some of the machine learning
packages but for a very basic setup this
works fine for doing the KNN and the
next thing you notice is it didn't take
very much to open it up I can scroll
down to the bottom of the data there's
768. it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer computer I
can look at it I can manipulate it and
it's not going to really tax just a
regular desktop computer you don't even
need an Enterprise version to run a lot
of this so let's start with importing
all the tools we need and before that of
course we need to discuss what IDE I'm
using certainly can use any particular
editor for python but I like to use for
doing very basic visual stuff the
Anaconda which is great for doing demos
with the jupyter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python36 I have a couple
different versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where you can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see I have
a number of windows open up at the top
the one we're working in and since we're
working on the KNN predict whether a
person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
Cell actually we want to go ahead and
first insert a cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run this python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice remind us what we're working on and
by now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy is NP
pandas is the pandas data frame and
numpy is a number array very powerful
tools to use in here so we have our
Imports so even brought in our pandas
are numpy our two general python tools
and then you can see over here we have
our train test split by now you should
be familiar with splitting the data we
want to split part of it for training
our thing and then training our
particular model and then we want to go
ahead and test the remaining data to see
how good it is pre-processing a standard
scalar preprocessor so we don't have a
bias of really large numbers remember in
the data we had like number of
pregnancies isn't going to get very
large where the amount of insulin they
take and get up to 256 so 256 versus 6
that will skew results so we want to go
ahead and change that so they're all
uniform between
-1 and 1. and then the actual tool this
is the K neighbors classifier we're
going to use
and finally the last three are three
tools to test all about testing our
model how good is it we just put down
test on there and we have our confusion
Matrix our F1 score and our accuracy so
we have our two general python modules
we're importing and then we have our six
modules specific from the SK learn setup
and then we do need to go ahead and run
this so these are actually imported
there we go and then move on to the next
step and so in this set we're going to
go ahead and load the database we're
going to use pandas remember pandas is
PD and we'll take a look at the data in
Python we looked at it in a simple
spreadsheet but usually I like to also
pull it up so we can see what we're
doing so here's our data set equals
pd.read CSV that's a pandas command and
the diabetes folder I just put in the
same folder where my IPython script is
if you put in a different folder you
need the full length on there we can
also do a quick Links of the data set
that is a simple python on command Len
for length we might even let's go ahead
and print that we'll go print and if you
do it on its own line links.dataset in
the jupyter notebook it'll automatically
print it but when you're in most of your
different setups you want to do the
print in front of there and then we want
to take a look at the actual data set
and since we're in pandas we can simply
do data set head and again let's go
ahead and add the print in there if you
put a bunch of these in a row you know
the data set one head data set two head
it only prints out the last one so I
usually always like to keep the print
statement in there but because most
projects only use one data frame pandas
data frame doing it this way it doesn't
really matter the other way works just
fine and you can see when we hit the Run
button we have the 768 lines which we
knew and we have our pregnancies it's
automatically given a label on the left
remember the head only shows the first
five lines so we have zero through four
and just a quick look at the data you
can see it matches what we looked at
before we have pregnancy glucose blood
pressure sure all the way to age and
then the outcome on the end and we're
going to do a couple things in this next
step we're going to create a list of
columns where we can't have zero there's
no such thing as zero skin thickness or
zero blood pressure zero glucose any of
those you'd be dead so not a really good
Factor if they don't if they have a zero
in there because they didn't have the
data and we'll take a look at that
because we're going to start replacing
that information with a couple of
different things and let's see what that
looks like so first we create a nice
list as you can see we have the values
talked about glucose blood pressure skin
thickness and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on a very common thing to
do and then for this particular setup we
certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the N A but we're
going to go ahead and do it as a data
set column equals datasetc column dot
replace this is this is still pandas you
can do a direct there's also that you
look for your nan a lot of different
options in here but the Nan num pnan is
what that stands for is none it doesn't
exist so the first thing we're doing
here is we're replacing the zero with a
numpy none there's no data there that's
what that says that's what this is
saying right here so put the 0 in and
we're going to play zeros with no data
so if it's a zero that means a person's
well hopefully not dead hope they just
didn't get the data the next thing we
want to do is we're going to create the
mean which is the integer from the data
set from the column dot mean where we
skip in A's we can do that and that is a
pandas command there the skip n a so
we're going to figure out the mean of
that data set and then we're going to
take that data set column and we're
going to replace all the npnan with the
means why did we do that and we could
have actually just taken this step and
gone right down here and just replace 0
and Skip anything where except you can
actually there's a way to skip zeros and
then just replace all the zeros but in
this case we want to go ahead and do it
this way so you can see that we're
switching this to a non-existent value
then we're going to create the mean well
this is the average person so if we
don't know what it is if they did not
get the data and the data is missing one
of the tricks is you replace it with the
average what is the most common data for
that this way you can still use the rest
of those values to do your computation
and it kind of just brings that
particular value of those missing values
out of the equation let's go ahead and
take this and we'll go ahead and run it
doesn't actually do anything so we're
still preparing our data if you want to
see what that looks like we don't have
anything in the first few lines just not
going to show up but we certainly could
look at a row let's do that let's go
into our data set with the printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones shows you can see I skipped a
bunch in the middle because that's what
it does we have too many lines in
Jupiter notebook it'll skip a few and
and go on to the next in a data set let
me go and remove this and we'll just
zero out that and of course before we do
any processing before proceeding any
further we need to split the data set
into our train and testing data that way
we have something to train it with and
something to test it on and you're going
to notice we did a little something here
with the pandas database code there we
go my drawing tool we've added in this
right here off the data set and what
this says is that the first one in
pandas this is from the PV pandas it's
going to say within the data set we want
to look at the eye location and it is
all rows that's what that says so we're
going to keep all the rows but we're
only looking at zero column 0 To 8.
Remember column nine here it is right up
here we printed it in here as outcome
well that's not part of the training
data that's part of the answer yes
column nine but it's listed as eight
number eight so zero to eight is nine
columns so 8 is the value and when you
see it in here zero this is actually
zero to seven it doesn't include the
last one and then we go down here to Y
which is our answer and we want just the
last one just column eight and you can
do it this way with this particular
notation and then if you remember we
imported the train test split that's
part of the SK learn right there and we
simply put in our X and our y we're
going to do random State equals zero you
don't have to necessarily seed it that's
a seed number I think the default is one
when you seated I have to look that up
and then the test size test size is 0.2
that simply means we're going to take 20
percent of the data and put it aside so
that we can test it later that's all
that is and again we're going to run it
not very exciting so far we haven't had
any printout other than to look at the
data but that is a lot of this is
prepping this data once you prep it the
actual lines of code are quick and easy
and we're almost there with the actual
running of our k n n we need to go ahead
and do a scale the data if you remember
correctly we're fitting the data in a
standard Taylor which means instead of
the data being from you know 5 to 303 in
one column and the next column is one to
six we're going to set that all so that
all the data is between minus one and
one that's what that standard scalar
does keeps it standardized and we only
want to fit the scalar with the training
set but we want to make sure the testing
set is the X test going in is also
transformed so it's processing it the
same so here we go with our standard
scalar we're going to call it SC
underscore X for the scalar and we're
going to import the standard scalar into
this variable and then our X train
equals SC underscore x dot fit transform
so we're creating the scalar on the X
train variable and then our X test we're
also going to transform it so we've
trained and transformed the X train and
then the X test isn't part of that
training it isn't part of the of
training the Transformer it just gets
transformed that's all it does and again
we're going to run this and if you look
at this we've now now gone through these
steps all three of them we've taken care
of replacing our zeros for key columns
that shouldn't be zero and we replace
that with the means of those columns
that way that they fit right in with our
data models we've come down here we
split the data so now we have our test
data and our training data and then
we've taken and we've scaled the data so
all of our data going in now no we don't
TR we don't train the Y part the Y train
and Y test that never has to be trained
it's only the data going in that's what
we want to train in there then Define
the model using K neighbors classifier
and fit the train data in the model so
we do all that data prep and you can see
down here we're only going to have a
couple lines of code where we're
actually building our model and training
it that's one of the cool things about
Python and how far we've come it's such
an exciting time to be in machine
learning because there's so many
automated tools let's see before we do
this let's do a quick link of and let's
do y we want let's just do length of Y
and we get 768 and if we import math we
do math dot square root let's do y train
there we go that's actually supposed to
be X train before we do this let's go
ahead and do import math and do math
square root length of Y test and when I
run that we get 12.409 I want to show
you where this number comes from we're
about to use 12 is an even number so if
you know if you're ever voting on things
remember the neighbors all vote don't
want to have an even number of neighbors
voting so we want to do something odd
and let's just take one away we'll make
it 11. let me delete this out of here
that's one of the reasons I love Jupiter
notebook because you can flip around and
do all kinds of things on the fly so
we'll go ahead and put in our classifier
we're creating our classifier now and
it's going to be the K neighbors
classifier and neighbors equal 11.
remember we did 12 minus 1 for 11. we
have an odd number of neighbors P equals
2 because we're looking for is it are
they diabetic or not and we're using the
euclidean metric there are other means
of measuring the distance you could do
like square square means values all
kinds of measure this but the euclidean
is the most common one and it works
quite well it's important to evaluate
the model let's use the confusion Matrix
to do that and we're going to use the
confusion Matrix wonderful tool and then
we'll jump into the F1 score
and finally accuracy score which is
probably the most commonly used quoted
number when you go into a meeting or
something like that so let's go ahead
and paste that in there and we'll set
the cm equal to confusion Matrix why
test why predict so those are the two
values we're going to put in there and
let me go ahead and run that and print
it out and the way you interpret this is
you have the Y predicted which would be
your title up here we could do let's
just do p r e d
predicted across the top and actual
going down actual
it's always hard to write in here actual
that means that this column here down
the middle that's the important column
and it means that our prediction said 94
and prediction in the actual agreed on
94 and 32. this number here the 13 and
the 15 those are what was wrong so you
could have like three different if
you're looking at this across three
different variables instead of just two
you'd end up with the third row down
here and the column going down the
middle so in the first case we have the
the and I believe the zero has a 94
people who don't have diabetes the
prediction said that 13 of those people
did have diabetes and were at high risk
and the 32 that had diabetes it had
correct but our prediction said another
15 out of that 15 it classified as
incorrect so you can see where that
classification comes in and how that
works on the confusion Matrix then we're
going to go ahead and print the F1 score
let me just run that and you see we get
a 0.69 in our F1 score the F1 takes into
account both sides of the balance of
false positives where if we go ahead and
just do the accuracy account and that's
what most people think of is it looks at
just how many we got right out of how
many we got wrong so a lot of people
when you're a data scientist and you're
talking to other data scientists they're
going to ask you what the F1 score the F
score is if you're talking to the
general public or the decision makers in
the business they're going to ask what
the accuracy is and the accuracy is
always better than the F1 score but the
F1 scores more telling it lets us know
that there's more false positives than
we would like on here but 82 percent not
too bad for a quick flash look at
people's different statistics and
running an sklearn and running the k n n
the K nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or the very whether they
should go get a checkup and have their
glucose checked regularly or not the
print accurac score we got the 0.818 was
pretty close to what we got and we can
pretty much round that off and just say
we have an accuracy of 80 percent tells
us it is a pretty fair fit in the model
so what is the star algorithm before we
talk about the a-star algorithm let's
talk a bit about search algorithms
in computer science a search algorithm
is an algorithm which typically solves a
search Problem
they used to retrieve information which
is stored within some data structure
like arrays Stacks cheese hash buckets
Etc or calculate
the search space of a problem domain
either with discrete or continuous
variables
so consider you have a stack s which
contains four elements
now a stack is an ordered collection of
items in which you can add a new item
and remove an existing item at the same
time
in Stacks you can put in an item on top
of another item and you can't access the
elements below the top element without
removing the top element first
so suppose we want to find the index of
say an element in the stack say 64. so
we're gonna run it through a search
algorithm to find 64 within our data
structure
what the search algorithm is going to do
is that it's gonna take a look at the
top element and see that it's not 64.
it's going to remove the top element and
move on to the next element again it's
gonna compare and see if it's 64. if
it's not this element 2 will get removed
and then it will move on to the next
element in the stack which is 64.
in this case it's gonna return the index
of this element
which is at stack index 2. and using
stack index 2 we can access this element
so in a way we found the element in the
stack
this is our search algorithms work this
is just an instance or a criteria on
which we were searching but using search
algorithms you can have multiple
different search conditions so you can
search for every element which is below
a certain number say if 40 within the
stack or you can also search for
elements which are at a certain index so
this is how powerful search algorithms
are
some examples of search algorithms
include binary search linear search jump
search interpolation search Etc
now a vital aspect of search algorithms
is pathfinding which is used to find
paths that can be taken to Traverse from
one point to another by finding the most
optimum route
the a-star algorithm is one algorithm
like this it is a searching algorithm
that is used to find the shortest path
between an initial and a final point
so assume that you have a grid or a maze
in this case
your personal is at the start
and they have to reach the end of the
maze which is here at this point
denoted by this flag
Now using a star algorithm
we can perform map traversal and find
the shortest path which has to be taken
within a map to go from the starting
position to the end position
now in this case there are many
different paths that can be taken
one option is to go along the top edges
of the map
the other one is to go along the bottom
edges
and the shortest part as you can see
which takes only four grids is to go
directly through the center of the map
cut across its diagonal and reach the
end
now one major aspect of a star algorithm
is that it always takes the shorter
paths first which makes it an optimal
and complete algorithm
an optimal algorithm will find the least
cost outcome for a problem while a
complete algorithm finds all the
possible outcomes of a problem
a star algorithm combines all of these
to find all the possible outcomes of a
problem starting with the least cost
solution
now this is what makes a star algorithms
so handy
a star was initially designed as a graph
traversal problem to help build a robot
that can find its own course and because
of this it Still Remains a widely
popular algorithm for graph traversal
now let's take a look at the basic
concept behind the a-star algorithm
so we already know that a star algorithm
is an optimal and complete algorithm but
another aspect that makes it so powerful
is the use of weighted graphs in its
implementation
a weighted graph is nothing but a graph
which uses numbers to represent the cost
of taking each path or course of action
this means that the algorithms can take
the path with the least cost and find
the best route in terms of distance and
time
in path finding algorithms
for algorithms such as a star you're
obviously going to need a graph or a map
or a path to travel right
so
for computers we feed in something
called a graph a graph contains of two
major components
first it contains something called a
node
so this is a node
and it contains something called a path
a path is nothing but a line which
connects two nodes
using this we know the direction or the
path that we have to take when we're
traversing through our graph right so um
first let's just make a very simple
graph okay right now we have two nodes A
and B I'm just gonna add two more
see
and D
now this connection with four nodes a b
c and d and their paths is called a
graph this whole thing basically this
whole thing is your graph these are the
graphs that these algorithms will have
to travel through and find the shortest
distance on so now how do we know where
we need to go in a graph
for this we have something called a
start note and an end node
so let's say r a is nothing but a start
node
and I see
over here
this node
is our end
node
right so now we know that we have to go
from this node a
all the way to this node C
in this case it becomes very easy I can
either go from A to B to C or from a to
d to C
but how do I know which path is the most
Optimum path or the path which has the
least cost this is made very easy by the
use of weighted graphs this is a basic
graph but in weighted graphs all these
paths between nodes
have a certain weight to them or a
certain number assigned to them
like say let's just say this is to
the cost of this path is 3 this path has
a cost of 5 and this path has a cost of
one so now as you can see we've assigned
numbers to the paths between different
nodes these numbers tell us the cost of
traversing through that path so when I
take this path between a to B I have I
get a cost of 2 incurred on me and when
I go from B to C I have a cost of three
similarly when I go from a to d if I
take this path I'll have a cost of 5 and
D to C will have a cost of 1. so this
helps us in finding out which paths we
should be taking when we're traversing
from one node to the other
now in this case let's look at the
different possible paths we can take to
reach C
the first option is that we go from a
to B
and this will give us a cost of two
and then from B
we go to C
which will give us a cost of 3.
now the total cost of taking these parts
can easily be obtained by finding the
summation of the individual cost of
these parts
so this will give us five
all we have to do is add up the weights
that each path has when we're traveling
it and we get to know what is going to
be the cost of a path taken so a path a
B to C has a cost of 5. similarly our
path a to d and D to C has a cost of 5
plus 1 because a to d is 5 and a to c is
1 which is 6.
so between 5 and 6 we know that 5 is
lesser
which means that this path A to B to C
is going to be our least cost path and
that is the path that we should be
taking
makes use of something called weighted
graphs right
so a star algorithm makes use of these
graphs in these ways now let's consider
that I have some other node here let's
say that I have a node
e okay
now B is connected to e and e is also
connected to C
let's say
that the cost of taking this path B to E
is 1
and E to C is 0. so now let's calculate
the cost of taking the path a b e and C
okay so A to B
to e to C is nothing but
2 plus 1 plus 0 which is equal to 3.
when compared to A to B to C
which is nothing but 2 plus 3 which is 5
and a to d to C which is nothing but
5 plus 1 which is 6 this path is
obviously going to be the shorter path
right but let's consider a programmer or
let's say we ourselves don't actually
want to take this path because coding
for it is hard or because it's very much
isolated from the map so we prefer to go
transverse along some other nodes
and this is not a preferred note but
looking at the graph no matter what we
do depending on how the weights are
distributed
we are still going to have to go via
this node
now
to overcome this programmers make use of
something called heuristic values what
are heuristic values so far using a
weighted graph we've given importance to
our parts we found out which paths are
the easiest to take and which paths are
the ones which give us a higher cost of
implementation
Now using heuristic values we're gonna
do the same to our nodes
the heuristic value of a node in a graph
attempts to capture the importance of
that nodes value within that graph so
the heuristic value is nothing but an
arbitrary value which is assigned to
these nodes based on the programmers
whims and needs
just like how we have the weightage of
paths the heuristic value gives us the
weight of a node
and tells us
how expensive it is to Traverse through
that node it is represented by h of n
so now let's assign heuristic values to
our graph and understand how exactly it
changes how we Traverse this graph right
so let's say our initial start node a
has heuristic value of 1. and our node B
has a heuristic value of say
three
I node e which is out of the way and
which we don't really want to Traverse
much on uh we can assign a very high
heuristic value to it to ensure that
it's not a path which is regularly taken
so we can assign a restrict value of 10
to E
let's assign other arbitrary values to
the other nodes let's give d a heuristic
value of 1 and see a heuristic value of
0.
so now as you can see
even though the parts over here take on
a very short value it would become a lot
more expensive to Traverse through e
just because of its single high
heuristic value as compared to the other
nodes how does
having a heuristic value in a graph
change how we are traversing so after
implementing the heuristic value
to a graph we now have two major values
which come into the play the heuristic
value
which is nothing but the value of the
node and the
path weight
we can represent this as G of n
which is nothing but the weights in our
weighted graph right so as you can see
when we're traversing from one node a to
a node B we're gonna have to consider
both of these values
so the cost function of a graph or the
total cost of taking a path
becomes the value of the path weight
which is given by the weighted graph
along with the heuristic value which is
assigned by the programmer or by you
yourself
and this here is the basic function
behind a a star algorithm this is the
formula that a star algorithm works on
so initially the a star algorithm will
calculate the cost to all of its
immediate neighboring nodes and choose
the one which is incurring the least
cost this whole process will repeat
until no new nodes can be chosen and all
the paths have been traversed
and then you should consider the best
path among them
and how do you find the cost function
again you're just gonna add up the
heuristic value and the path value that
you've taken
now G of n which is the path value is
nothing but the cost of traversing from
one node to another
very obviously this will vary from node
to node whereas h of n or heuristic
approximation of a node's value is not a
real value but an approximation cost
that you or some other person has
assigned to various nodes
now that we know the basic concept
behind the Acer algorithm let's take a
look at how exactly the a star algorithm
works with the help of a case study
so now let's take a look at how the a
star algorithm works
to do this we're first gonna consider a
weighted graph since a star algorithm
uses weighted graphs
to make its calculations easier
so consider this to be our weighted
graph we have a node a
a node B
a node C
a node d
and a node
e
okay
now node a is connected to node B node B
is connected to node e
node e is connected to node d
is connected to C and C in turn is
connected to a
C is also connected to b and a is also
connected to E
so now let's assign the path values to
all the different paths in our weighted
graph let's say A to B has a path value
of 1 B to e has a path value of 6 D to e
has a path value of
5 C to D has a path value of 9 a to c
has a path value of 10. and a to e has a
path value of 2 and this has a path
value of 3
3. now let's assign heuristic values
let's give a the heuristic value of 2
uh be the heuristic value of
um
five
let's just give this a seven let's make
this
four
and let's give C A heuristic value zero
okay
so now this is what a weighted graph
looks like
um and uh from the basic uh working you
already know that the cost function of
your weighted graph is nothing
but the path value
along with the heuristic value
so let's now say
that a here is where we have to start
from
and e and e is our
Target node okay and this is where we
are going to be
starting from
now a is our starting node so there are
three different parts that we can take a
to b a to c or a to E
so first let's consider that we take a
to B okay
the first thing that you have to
consider is the heuristic value of a
itself
so F of a is the first value that you're
going to be considering
right
now F of a here is going to be the uh
path Value Plus heuristic value now
because a is the starting node and there
is no other node leading up to it it is
the parent node itself so the path value
here is going to be 0 because there's no
path leading up to it it's the starting
node and the starting node always has a
path value of 0.
next we're gonna add the heuristic value
of a in this case 2 so 0 Plus 2. so F of
a is 2.
right
so this is the first node
that you've considered okay
now after taking node a we have three
different options a to c a to e or a to
B
so
the cost function of taking the path A
to B
is going to be so now let's find the
cost of taking the path A to B right A
to B so you have to add the graph node
Value Plus a heuristic value
the weight of taking the node A to B is
nothing but one
and
the heuristic value of B is 5. so this
is going to give us C next a
to C
so I started
next let's look at the cost of taking
the path a to e right F of a to E
so this is going to be 2 because the
cost of taking this path is 2
along with the heuristic value of e
which is 7 so this is going to be 9.
and let's look at F of a to c
now the cost of taking this is going to
be 10
plus the value of C which is 0. so this
is going to be 10.
now the way how a star algorithm works
is it will calculate the cost function
of all the neighboring nodes and go with
the node which has the least cost
function in this case the node being B
so we're not gonna take this path or
this path because the cost function
value is way more than the cost function
value of f of a to B
so after a we are going to be at node B
so right now this is the direction that
we've traveled in now when we're at node
B to reach node e we have two different
options such we have two options that we
can take we can either go from B to e or
we can go from B to C okay uh the goal
of a star algorithm is to always
minimize
the cost of the path that you're taking
so using this in mind let's calculate
F of
B to C
which is going to be nothing but the
cost of the path from B to C which is 3
plus a heuristic value of C which is 0.
so this is going to be 3. next let's
calculate the cost of B to E
now this is going to be again nothing
but the path value in this case 6 Plus
heuristic value in this case of e which
is 7
which is nothing but 13.
so again as you can see
this is the correct path to be taken
and we are not going to be taking this
path because its value is way too high
so the next path that we're going to be
taking is this path B to C
so now we're at node C
from node C we can either go to a which
is not possible because we do not go
back in the direction that we came from
to B again that's not what we're going
to be doing or to D so the only option
that we have available is to go from C
to d right so what we're going to do is
we're gonna find the cost of going from
C to D
again this is going to be the value of
the path which is 9 plus the heuristic
value of D which is 4. the cost of
traversing this path is 13 but we don't
have any other
path available to us because this path
has already been closed you can't
backtrack in a star algorithm or go back
to the path where you came from
and again we've already considered a to
c before right like over here
we've already considered a to c and
we've already ruled it out so again this
is not a path that you can take
so what we're going to be doing right
now is that we're gonna be traversing
from C to D
and the cost of taking this path is 13.
so now
this is the path that we're taken from C
to D
very D now at T again we can't go back
to this path we can't go back to C the
only way we can go is e
so we're gonna consider F of D to E
and this is gonna be again the path that
you're gonna take from D to e which is
five
plus the heuristic value of e which is
7.
this whole thing is going to be 12.
now once we're at C our a star algorithm
is going to check if this is the target
node
and lo and behold it is so we've
traversed from a to e let's see the path
we've taken first the starting note that
we started off from was a
from there we looked at the cost of
taking a to c a to E and A to B and
finally we settled for going from A to B
from B again we consider the paths B to
e and B to C we cannot consider a to B
because this path has already been
traveled on and it's already been
considered before
now after consider doing B to e and B to
C we found that B to C was the shortest
path
from C we didn't have any other option
we couldn't backtrack or go back to B
and we couldn't go to a because this
path was already been considered and
already it has been ruled out so the
only path we could take from here was to
D
and again from D the only path that we
could take was to e so now let's look at
the path that we've taken while using
our Acer algorithm we started off at the
node a
this was a starting node
from node a we had three options we
could go to node B to node e or to node
C
after considering all of these options
we realized that A to B was the path of
least cost that we could take once at B
we considered two parts B to e and B to
C
and we found out that B to C was the
path of least cost
from C
there was no other path we could take we
cannot go back to the node we came from
which is B
because a star algorithm does not
support backtracking and we couldn't go
back to a either because this path had
already been considered and we'd already
ruled this path out right so the only
path you could take was to
d
now once we arrived at node D the only
node that we could travel to again was e
so using a star algorithm this is the
final path that we've taken to Traverse
from our start node a to our Target node
e
and with the help of this case study
we've also explained to you exactly how
the a star algorithm works we use the
path values and the heuristic values to
find the cost of taking different paths
available to us at each node
and then we take the path which has the
least cost among all available parts
the same process will continue until
we've reached the final Target node and
again remember you cannot go back on the
path that you came from or you cannot go
on a path which has already been
considered
so now you know how the astar algorithm
works but what is the flowchart of the a
star algorithm
now a flowchart is a picture of the
separate steps of a process in a
sequential order
it's basically a generic tool that can
be adapted for a wide variety of
purposes and you can use it to describe
various processes in this case this is
the basic flowchart of the a star
algorithm it tells the direction of
control flow within the a-star program
and using this you can Implement a star
algorithm in any desired language of
your choice
it's the first step of our a star
algorithm is to initialize the start
node and put it in an open list
now in this algorithm we're gonna first
create two lists an open list and a
close list the open list will contain
all the nodes that have been visited but
the neighbors have yet not been explored
on the other hand the close list
contains nodes that along with their
neighbors have already been visited
so the first node or our starting node
is going to be put in an open list
because we're still exploring it
then we're gonna calculate the cost
function of the starting node
from here onwards we're gonna remove
this node from the open list and put it
in the close list since we've already
visited it and calculated its cost
function
if we're also looking
at the Neighbors which are surrounding
this node or if this node has many
different nodes around it then we're
gonna calculate the cost function for
those nodes too and save the index of
the node which has the smallest cost
function f
next we're gonna look at if the node
we're currently on
is a final node or a Target node
if that's the case we're gonna terminate
the algorithm and use pointers of all of
our nodes to get the desired path
if that's not the case then we're gonna
look at the successive nodes of n or all
the nodes which come after this node
which are not in the close list
we're gonna calculate the cost function
of each of these neighboring nodes
remove them from the open list
as they're not already in the close list
so they're obviously going to be in the
open list so we're going to remove them
from the open list and then we're gonna
put them in the close list
and again we're gonna save the index of
the neighboring node with the smallest
cost function
and then this process is going to keep
repeating over and over until we've
reached the target node which is where
our program will get terminated
so basically what we're doing is we're
going to a certain node calculating the
cost function for that node
looking at all the successive
neighboring nodes of it and going to the
neighbor which has the least cost
function
now this process will again get repeated
so from this neighboring node we're
going to find all the other nodes which
are connected to it and then we are
going to go to the node which has the
least cost function so using this we're
gonna hop from one node to another
depending on which node has the lowest
cost function
once this is over we're gonna again take
some other node and we're going to
repeat this process until we found all
the possible paths to a final
destination node
and in this way the a star algorithm is
not only going to find the shortest path
but it's also going to find all possible
paths
now let's see how the a star algorithm
can be implemented with the help of
python
in this demo we will use the ASR
algorithm to find the least cost path
between various nodes in the weighted
graph which is shown below
so in this graph as you can see we have
six different nodes a b c d e and G the
weights of the paths between these nodes
is denoted by these numbers in black and
the heuristic values of the nodes itself
are denoted by the numbers in red Now
using these values we can find the
shortest distance between any two nodes
let's see how we can do this with the
help of python
so now let's start by creating a class
for the a star algorithm called the a
star algo
now in this algorithm we're gonna have
to describe the open and the close list
and to do this we're gonna be using two
sets so our sets are our open list and
our closed list
we'll also be using two dictionaries
one to store distance from the starting
node and another for parent nodes
and we're gonna start by initializing to
them to zero
the open set is gonna have the start
node already in it now we all know that
the distance of the starting node from
itself is 0 itself so we're gonna
initialize the distance of the start
node to zero
and we also know that the start node is
the root node and it has no parent nodes
so the start node is its own parent node
the first step of initializations is to
put the start note in the closed set
to set the distance of the start node to
itself as 0
and to set the start node as its own
parent node
now that we're done with the first
couple of initializations let's move on
to finding all the neighboring nodes
so now we're gonna have to find the
neighboring nodes with the lowest value
of the cost function
we also have to code for the condition
of reaching the destination node if this
is not the case we're gonna have to put
the current nodes in the open list and
if they're not already in it then we're
gonna set the parent nodes
so that's what we're gonna do first
we're gonna find the node with the
lowest cost function we're gonna take
the entire open set
and while the length of it is greater
than 0 or basically well we're not at
the parent node
we're gonna Loop through it
we're gonna go to all the neighboring
nodes of the open node
and then we're gonna find their cost
function now here this is the function
that you're gonna use to find the cost
function
now here what we're gonna do is we're
gonna calculate the heuristic value for
n
and
if the cost function of n is 0 or if its
cost function is less than the cost
function of its neighboring node
then we're gonna go we're gonna move on
to the next node which is V
now if n is equal to the stop node or
we've looped back and come to n itself
then we're not gonna go ahead or we're
gonna exit this Loop
and we're going to move on to some other
node
otherwise if the neighbor has a lower G
value then the current node and is in
the close list we're gonna replace this
new node as the neighbors parent and
that's what we're doing here exactly now
if the neighbor is not in both the lists
then we're gonna add it to the open list
and set its G value
and that's basically what's going on
here
so let's run this for the a star
algorithm
now we're gonna Define a function to get
the different neighbors and their
distances
next we're going to create a function to
store the heuristic values and retrieve
them as and when required
now we're going to describe our graph
here so these are all of our nodes and
the nodes that they're connected to
along with the weight of their parts so
here a is connected to B and the weight
of this path is 2 is also connected to e
and the weight of this path is 3. in the
same way B is connected to C and the
path has a weight of 1 and GB is also
connected to G and the path has a weight
of 9.
so here this is what we're gonna do to
describe our entire weighted graph and
then we're gonna finally call our a star
algorithm and give our starting node and
a final node
and
it very quickly finds the path between
our starting node and our final node
and over here as we can see the best
part to take would be from a to e to D
and finally to G ever wondered how
Google Translates an entire web page to
a different language in a matter of
seconds or your phone gallery group's
images based on their location all of
this is a product of deep learning but
what exactly is deep learning deep
learning is a subset of machine learning
which in turn is a subset of artificial
intelligence artificial intelligence is
a technique that enables a machine to
mimic human behavior machine learning is
a technique to achieve AI through
algorithms trained with data and finally
deep learning is a type of machine
learning inspired by the structure of
the human brain in terms of deep
learning this structure is called an
artificial neural network let's
understand deep learning better and how
it's different from machine learning say
we create a machine that could
differentiate between tomatoes and
cherries if done using a machine
learning we'd have to tell the machine
the features based on which the two can
be differentiated these features could
be the size and the type of stem on them
with deep learning on the other hand the
features are picked out by the neural
network without human intervention of
course that kind of Independence comes
at the cost of having a much higher
volume of data to train our machine now
let's dive into the working of neural
networks here we have three students
each of them write down the digit 9 on a
piece of paper notably they don't all
write it identically the human brain can
easily recognize the digits but what if
a computer had to recognize them that's
where deep learning comes in here's a
neural network trained to identify
handwritten digits each number is
present as an image of 28 times 28
pixels now that amounts to a total of
784 pixels neurons the core entity of a
neural network is where the information
processing takes place each of the 784
pixels is fed to a neuron in the first
layer of our neural network this forms
the input layer on the other end we have
the output layer with each neuron
representing a digit with the hidden
layers existing between them the
information is transferred from one
layer to another over connecting
channels each of these has a value at
attached to it and hence is called a
weighted Channel all neurons have a
unique number associated with it called
bias
this bias is added to the weighted sum
of inputs reaching the neuron which is
then applied to a function known as the
activation function the result of the
activation function determines if the
neuron gets activated every activated
neuron passes on information to the
following layers this continues up till
the second last layer
the one neuron activated in the output
layer corresponds to the input digit the
weights and bias are continuously
adjusted to produce a well-trained
network so where is deep learning
applied in Customer Support when most
people converse with customer support
agents the conversation seems so real
they don't even realize that it's
actually a bot on the other side in
medical care neural networks detect
cancer cells and analyze MRI images to
give detailed results self-driving cars
with seem like science fiction is now a
reality Apple Tesla and Nissan are only
a few of the companies working on
self-driving cars so deep learning has a
vast scope but it too faces some
limitations the first as we discussed
earlier is data while deep learning is
the most efficient way to deal with
unstructured data a neural network
requires a massive volume of data to
train let's assume we always have access
to the necessary amount of data
processing this is not within the
capability of every machine and that
brings us to our second limitation
computational power training a neural
network requires graphical processing
units which have thousands of cores as
compared to CPUs and gpus are of course
more expensive and finally we come down
to training time deep neural networks
take hours or even months to train the
time increases with the amount of data
and number of layers in the network some
of the popular deep learning Frameworks
include tensorflow High torch Keras deep
learning 4J Cafe and Microsoft cognitive
toolkit considering the future
predictions for deep learning and AI we
seem to have only scratched the surface
in fact horse technology is working on a
device for the blind that uses deep
learning with computer vision to
describe the world to the users
replicating the human mind at the
entirety may be not just an episode of
Science Fiction for too long the future
is indeed full of surprises and that is
deep learning for you in short last
summer my family and I visited Russia
even though none of us could read
Russian we did not have any trouble in
figuring our way out all thanks to
Google's real-time translation of
Russian boards into English this is just
one of the several applications of
neural networks neural networks form the
base of deep learning a subfield of
machine learning where the algorithms
are inspired by the structure of the
human brain neural networks take in data
train themselves to recognize the
patterns in this data and then predict
the outputs for a new set of similar
data let's understand how this is done
let's construct a neural network that
differentiates between a square circle
and triangle neural networks are made up
of layers of neurons these neurons are
the core processing units of the network
first we have the input layer which
receives the input the output layer
predicts our final output in between
exists the hidden layers which perform
most of the computations required by our
Network here's an image of a circle this
image is composed of 28 by 28 pixels
which make up for 784 pixels each pixel
is fed as input to each neuron of the
first layer neurons of one layer are
connected to neurons of the next layer
through channels each of these channels
is assigned a numerical value known as
weight the inputs are multiplied to the
corresponding weights and their sum is
sent as input to the neurons in the
hidden layer each of these neurons is
associated with a number numerical value
called the bias which is then added to
the input sum this value is then passed
through a threshold function called the
activation function the result of the
activation function determines if the
particular neuron will get activated or
not an activated neuron transmits data
to the neurons of the next layer over
the channels in this manner the data is
propagated through the network this is
called forward propagation
in the output layer the neuron with the
highest value fires and determines the
output the values are basically a
probability for example here our neuron
associated with square has the highest
probability hence that's the output
predicted by the neural network
of course just by a look at it we know
our neural network has made a wrong
prediction but how does the network
figure this out note that our network is
yet to be trained during this training
process along with the input our Network
also has the output fed to it the
predicted output is compared against the
actual output to realize the error in
prediction the magnitude of the error
indicates how wrong we are and the sign
suggests if our predicted values are
higher or lower than expected the arrows
here give an indication of the direction
and magnitude of change to reduce the
error this information is then
transferred backward through our Network
this is known as back propagation now
based on this information the weights
are adjusted this cycle of forward
propagation and back propagation is
iteratively performed with multiple
inputs this process continues until our
weights are assigned such that the
network can predict the shapes correctly
in most of the cases this brings our
training process to an end
you might wonder how long this training
process takes honestly neural networks
may take hours or even months to train
but time is a reasonable trade-off when
compared to its scope let us look at
some of the Prime applications of neural
networks facial recognition cameras on
smartphones these days can estimate the
age of the person based on their facial
features this is neural networks at play
first differentiating the face from the
background and then correlating the
lines and spots on your face to a
possible age forecasting neural networks
are trained to understand the patterns
and detect the possibility of rainfall
or rise in stock prices with high
accuracy music composition neural
networks can even learn patterns in
music and train itself enough to compose
a fresh tune with deep learning and
neural networks we are still taking baby
steps the growth in this field has been
foreseen by the big names companies such
as Google Amazon and Nvidia have
invested in developing products such as
libraries predictive models and
intuitive gpus that support the
implementation of neural networks the
question dividing the vision Aries is on
the reach of neural networks to what
extent can we replicate the human brain
we'd have to wait a few more years to
give a definite answer here's an example
of artificial intelligence in today's
world Amazon Echo Amazon Echo is a
wonderful tool we can go in there and
you can say hey Alexa what's the
temperature in Chicago and the Amazon
Echo then translates that into zeros and
ones and something the computer
understands then it comes in and
processes that information to identify
what you're asking and what you need and
where to get that information and then
it comes back and says the current
temperature in Chicago is six degrees
Fahrenheit or whatever it is at the
moment
so this is a wonderful example of
artificial intelligence that we're using
right now today where that's at as far
as a commercial deployment
machine learning a machine learning
example out there is Google you're on
your Google search engine it comes up
you spend a lot of time on the first
link you come into and you read the page
and Google looks at that and says okay
he spent five minutes on this let's give
it a thumbs up and then you go to the
second page and the third page you just
kind of skip over them and glance at
them for a couple seconds and Google
says I wasn't interested in those pages
let's give them a thumbs down so this is
a good example machine learning as it
starts guessing what you like and what
you don't like so it gives you more
information along what you're going to
read and actually use
and then we have an example of deep
learning in this example we have a black
and white image it comes into in this
case a neural network some people like
to call it a magic box because you
really it's hard to follow all that's
going on in there there's all these
different weights and connections and
nodes and then it comes out and colors
the beach ball the people the background
what's going on in here is the black and
white image goes into this neural
network and before it's ever gone in the
neural network has looked at all these
different pictures on the web or
wherever it pulls the data from and it's
already itemized them and kind of
separated them that we have some that
look like beach balls we have some that
look like people and it programs that so
that when the black and white image
comes in and goes okay that piece right
there resembles this and all these other
photos
and the neural network is able to
identify that and then color the beach
ball with the colors that you see on
there so it gives it this is really
wonderful because they did a wonderful
job coloring this picture
and that's the full setup is where the
Deep learning example comes in and they
usually center around neural networks
human versus artificial intelligence
humans are amazing let's just face it
we're amazing creatures we're all over
the planet we're exploring every Nick
chanuk we've gone to the Moon uh we've
got into outer space we're just amazing
creatures we're able to use the
available information to make decisions
to communicate with other people
identify patterns and data remember what
people have said adapt to new situations
so let's take a look at this so you can
get a picture you're a human being so
you know it's like to be human let's
take a look at artificial intelligence
versus the human artificial intelligence
develops computer systems that can
accomplish texts that require human
intelligence
so we're looking at this one of the
things that computers can do is they can
provide more accurate results this is
very important recently I did a project
on cancer whereas identifying markers
and as a human being you look at that
and you might be looking at all the
different images in the data that comes
off of them and say I like this person
so I want to give them a very good
outlook and the next person you might
not like so you want to give them a bad
Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolve it's very quick and easy and
affordable to do this
what is machine learning and deep
learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world
and you're sitting by the road and you
have a whole lot and a whole lot of time
passes by there's a few hours
and suddenly you wonder
how many cars buses trucks and so on
passed by in the six hours now chances
are you're not going to sit by the road
for six hours and count buses cars and
trucks unless you're working for the
city and you're trying to do City
Planning and you want to know hey do we
need to add a new truck route maybe we
need a bicycle length we have a lot of
bicyclists here that kind of thing so
maybe City Planning would be great for
this
machine learning well the way machine
Learning Works is we have labeled data
with features okay so you have a truck
or a car a motorcycle a bus or a bicycle
and each one of those are labeled it
comes in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle this is a little bit more
in depth on this
and the model here it actually the
features we're looking at would be like
the tires someone sits there and figures
out what a tire looks like it takes a
lot of work if you try to try to figure
the difference between a car tire a
bicycle tire a motorcycle tire uh so in
the machine learning field this could
take a long time if you're going to do
each individual
aspect of a car and try to get a result
on there and that's what they did do
that was a very this is still used on
smaller amounts of data we figure out
what those features are and then you
label them
deep learning so with deep learning one
of our Solutions is to take a very large
unlabeled data set
and we put that into a training model
using artificial neural networks and
then that goes into the neural network
itself when we create a neural network
and you'll see the arrows are actually
kind of backward but which actually is a
nice point because when we train the
neural network
we put the bicycle in and then it comes
back and says if it's a truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns
once you've trained the neural network
you then put the new data in and they
call this testing the models you need to
have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
as a truck a motorcycle as a motorcycle
let's just take a little closer look at
that
determining what objects are present in
the data so how does deep learning do
this and here we have the image of the
bike it's 28 by 28 pixels that's a lot
of information there could you imagine
trying to guess that this is a bicycle
image by looking at each one of those
pixels and trying to figure out what's
around it and we actually do that as
human beings it's pretty amazing we know
what a bicycle is and even though it
comes in as all this information and
what this looks like is the image comes
in
it converts it into a bunch of different
nodes in this case there's a lot more
than what they show here and it goes
through these different layers and
outcomes and says okay this is a bicycle
a lot of times they call this the magic
Black Box why because as we watch it go
across here all these weights and all
the math behind this and it's not it's a
little complicated on the math side you
really don't need to know that when
you're programming or doing working with
the Deep learning but it's like magic
you don't know you really can't figure
out what's going to come out by looking
what's in each one of those dots and
each one of those lines are firing and
what's going in between them so we like
to call it the magic box so that's where
deep learning comes in
and in the end it comes up and you have
this whole neural network it comes up
and it says okay we fire all these
different pixels and we connect all
these different dots and gives them
different weights and it says okay this
is a bicycle and that's how we determine
what the object is present in the data
with deep learning
machine learning we're going to take a
step into machine learning here and
you'll see how these fit together in a
minute the system is able to make
predictions or take decisions based on
past data that's very important for
machine learning is that we're looking
at stuff and based on what's been there
before we're creating a decision on
there we're creating something out of
there we're coloring a beach ball we're
telling you what the weather is in
Chicago
what's nice about machine learning is a
very powerful processing capability it's
quick and accurate outcomes so you get
results right away once you program the
system the results are very fast
and the decisions and predictions are
better they're more accurate they're
consistent you can analyze very large
amounts of data some of these data
things that they're analyzing now are
petabytes and terabytes of data it would
take hundreds of people hundreds of
years to go through some of this data
and do the same thing that the machine
learning can do in a very short period
of time and it's inexpensive compared to
hiring hundreds of people so because a
very affordable way to move into the
future is to apply the machine learning
to whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do
problem solved in an end-to-end method
so instead of having to break it apart
and you have the first piece coming in
and you identify tires and the second
piece is identifying labeling handlebars
and then you bring that together that if
it has handlebars and tires it's a
bicycle and if it has something that
looks like a large Square it's probably
a truck the neural networks does this
all in one network you don't really know
what's going on in all those weights and
all those little bubbles but it does it
pretty much in one package that's why
the neural network systems are so big
nowadays and coming into their own
best features are selected by the system
and it this is important they kind of
put it it's on a bullet on the side here
it's a subset of machine learning this
is important we talk about deep learning
it is a form of machine learning there's
lots of other forms of machine learning
data analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test you put in your information
you then have your group of tests and
then you held some aside you see how
does it do it's very quick to test it
and see what's going on with your deep
learning and your neural network
are they really all that different
AI versus machine learning versus deep
learning concepts of AI
so we have a concepts of II you'll see
natural language processing machine
learning and approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep learning
so we look at this we have ai with
machine learning and deep learning and
so we're going to put them all together
we find out that AI is a big picture we
have a collection of books it goes
through some deep learning the Digital
Data is analyzed text mining comes
through the particular book you're
looking for maybe it's a genre of books
is identified and in this case we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens so
we'll look at some of the pieces here
this information goes into uh there's as
far as this example the translation of
the handwritten printed data to digital
form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we use the Deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the book
so here's a nice summation of what we
were just talking about AI with machine
learning and deep learning deep learning
is a subset of machine learning which is
a subset of artificial intelligence so
you can look at artificial intelligence
as the big picture how does this compare
to The Human Experience in either doing
the same thing as a human we do or it
does it better than us and machine
learning which has a lot of tools is
something that learns from data past
experiences it's programmed it comes in
there and it says hey we already had
these five things happen the sixth one
should be about the same and then
there's a lot of tools in machine
learning but deep learning then is a
very specific tool in machine learning
it's the artificial neural network which
handles large amounts of data and is
able to take huge pools of experiences
pictures and ideas and bring them
together
real life examples
artificial intelligence news generation
very common nowadays as it goes through
there and finds the news articles or
generates the news based upon the news
feeds or the back end coming in and says
okay let's give you the actual news
based on this there's all the different
things Amazon Echo they have a number of
different Prime music on there of course
there's also the Google command and
there's also Cortana there's tons of
smart home devices now where we can ask
it to turn the TV on or play music for
us that's all artificial intelligence
from front to back you're having a human
experience with these computers and
these objects that are connected to the
processing machine learning spam
detection very common machine learning
doesn't really have the human
interaction part
so this is the part where it goes and
says okay that's a Spam that's not a
Spam and it puts it in your spam folder
search engine result refining another
example of machine learning whereas it
looks at your different results and it
Go and it is able to categorize them as
far as this had the most hits this is
the least viewed this has five stars you
know however they want to weight it all
exam good examples of machine learning
and then the Deep learning deep learning
another example is as you have like a
exit sign in this case is translating it
into French sorti I hope I said that
right
neural network has been programmed with
all these different words and images and
so it's able to look at the exit in the
middle and it goes okay we want to know
what that is in French and it's able to
push that out in France French and learn
how to do that
and then we have chat Bots I remember
when Microsoft first had their little
paper clip
um boy that was like a long time ago
they came up and you would type in there
and chat with it these are growing you
know it's nice to just be able to ask a
question and it comes up and gives you
the answer and instead of it being were
you just doing a search on certain words
it's now able to start linking those
words together and form a sentence in
that chat box
types of AI and machine learning
types of artificial intelligence this in
the next few slides are really important
so one of the types of artificial
intelligence is reactive machines
systems that only react they don't form
memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to re-sitter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
short-lived when we're talking about
this and you look at like a neural
network that's been programmed to
identify cars it doesn't remember all
those pictures it has no memory as far
as hundreds of pictures you process
through it all it has is this is the
pattern I use to identify cars as a
final output for that neural network we
looked at
so when they talk about limited memory
this is what they're talking about
they're talking about I've created this
based on all these things but I'm not
going to remember anyone specifically
theory of Mind systems being able to
understand human emotions and how they
affect decision making to adjust their
behaviors according to their human
understanding
this is important because this is our
page mark this is how we know whether it
is an artificial intelligence or not is
it interacting with humans in a way that
we can understand
without that interaction is just an
object so we talk about theory of mind
we really understand how it interfaces
that whole if you're in web development
user experience would be the term I
would put in there so the theory of mine
would be user experience how's the whole
UI connected together and one of the
final things is as we get into
artificial intelligence is systems being
aware of themselves understanding their
internal States and predicting other
people's feelings and act appropriately
so as artificial intelligence continues
to progress we see ones they're trying
to understand well what makes people
happy how would they increase our
happiness how would they keep themselves
from breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based on all
that information predicting which action
would work the best what would help
people if I know that you're having a
cup of coffee first thing in the morning
is what makes you happy as a robot I
might make you a cup of coffee every
morning at the same time to help your
life and help you grow that'd be the
self-awareness is being able to know all
those different things
types of machine learning and like I
said on the last slide this is very
important this is very important if you
decide to go in and get certified in
machine learning or know more about it
these are the three primary types of
machine learning the first one is
supervised learning systems are able to
predict future outcome based on past
data requires both an input and an
output to be given to the model for it
to be trained
so in this case we're looking at
anything where you have 100 images of a
bicycle
and those hundred images you know are
bicycle so they're preset someone
already looked at all hundred images and
said these are pictures of bicycles and
so the computer learns from those and
then it's given another picture
and maybe the next picture is a bicycle
and it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says yeah it's
not a bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
it can look at an image and start
pulling the different pieces of the
image out because they aren't the same
the human all the parts of the human are
not the same as a fuzzy tree behind them
so slightly out of focus which is not
the same as the beach ball it's
unsupervised because we never told it
what a beach ball was we never told it
what the human was and we never told it
that those were trees all we told it was
hey separate this picture by things that
don't match
and things that do match and come
together
and finally there's reinforcement
learning systems are given no training
it learns on the basis of the reward
punishment it received for performing
its Last Action it helps increase the
efficiency of a tool function or a
program reinforced learning or
reinforcement learning is kind of give
it a yes or no yes you gave me the right
response no you didn't and then it looks
at that and says oh okay so based on
this data coming in what I gave you was
a wrong response so next time I'll give
you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
were grouping a ton of machine learning
tools all together linear regression
k-means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe you have like the case
of one of the things 500 points of data
and 30 different fields it starts
getting really confusing there and
artificial intelligence or machine
learning and the Deep learning aspect
really shines when you get to that
larger data that's really complex
works well on a low end systems so a lot
of the machine learning tools out there
you can run on your laptop with no
problem and do the calculations there
where with the machine learning usually
needs a higher end system to work it
takes a lot more processing power to
build those neural networks and to train
them it goes through a lot of data
we're talking about the general machine
learning tools most features need to be
identified in advanced and manually
coded so there's a lot of human work on
here the machine learns the features
from the data it is provided so again
it's like a magic box you don't have to
know what a tire is it figures it out
for you
the problem is divided into parts and
solved individually and then combined so
machine learning you usually have all
these different tools and use different
tools for different parts
and the problem is solved in an
end-to-end manner so you only have one
neural network or two neural networks
that is bringing the data in and putting
it out it's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem I have a solving one
piece of the puzzle
with regular machine learning and most
machine learning tools out there they
take longer to test and understand how
they work
and with the Deep learning it's pretty
quick once you build that neural network
you test it and you know
so we're dealing with very crisp rules
limited resources you have to really
explain how the decision was made when
you use most machine learning tools but
when you use the Deep learning tool
inside the machine learning tools the
system takes care of it based on its own
logic and reasoning and again it's like
a magic Black Box you really don't know
how it came up with the answer you just
know it came up with the right answer a
glimpse into the future so a quick
glimpse into the future artificial
intelligence using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe it'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in healthcare
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization good so what's really
nice about the Deep learning is it's
going to start now catering to you
that'll be one of the things we see more
and more of and we'll have more of a
hyper intelligent personal assistant I'm
excited about that
brings you comprehensive artificial
intelligence boot camp that will cover a
wide range of topics that will Empower
you with the knowledge and skills needed
to excel in the field of AI to learn
more about this course you can find the
course Link in the description box below
ever get stuck while writing an email
finding the right words or stuck on that
article you can't find the final words
for well to get rid of this writer's
blog a my rate of AI tools help you
achieve your targets we are now at a
time when artificial intelligence tends
to do many jobs better and more
efficiently due to the lack of human
error it's not just content creation AI
can find use in a ton of scenarios let
us go through some of the best AI tools
available today that can boost your
productivity at number 10 you have magic
eraser by Magic Studio Magic Eraser
removes unwanted images from your
pictures using artificial intelligence
it can decipher the backgrounds and
remove unnecessary elements out of a
picture beat unwanted Birds on landscape
shots or branding on social media posts
select the element to be removed using
the Eraser and let the Tool Work its
magic next at number 9 we have the thing
translator this tool allows users to
point their camera at an object and hear
the translation in a different language
using the cloud Vision API and the
translate API of Google the translator
serves as a God send for forward and
travelers or just people trying to learn
a new language with both the Google apis
set to develop even more over time
expect the thing translator to become
more helpful than ever at number 8 we
have tab 9. tab 9 analyzes and
recommends your next blocks of code
based on the context and syntax get
lengthy accurate and creative code
completions to become a more efficient
developer cabinet's AI completions may
be executed on a developer's laptop
behind a firewall or even on the cloud
it recommends code completions based on
your coding Tendencies it supports the
most common languages such as JavaScript
Python and typescript and even more
Niche languages such as rust go and Bash
irrespective of what your Tech stack
looks like tab 9 will boost your speed
at number 7 we have Trevor AI Trevor AI
is a daily planning program that
leverages intelligent time blocking to
connect your list of activities and
calendar it supports integration and
two-way sync with services like Google
Calendar and todoist taking control of
how you spend your time based on
schedules and time frames it helps plan
your habits and work life life to create
a streamlined approach to your day using
artificial intelligence this helps you
focus on the individual task rather than
worrying about cramming your busy
schedule into the entire day at number 6
we have deep Nostalgia deep Nostalgia is
an online tool developed by a company
called myheritage and can animate faces
from old pictures my Heritage license
this unique technology for animating
photographs from did which is a firm
specializing in video reconstruction
using deep learning myheritage uses
technology to animate the faces in Old
images and generate high quality
lifelike video footage the tool Works
equally well for black and white
pictures and photographs taken in color
so needless to say the results are
pretty astonishing and can be quite
moving in many cases number five on our
list goes to durable durable helps
create sample websites in under 30
seconds for people starting a new
business or who are generally not well
versed in web development just choose
the type of business you are running and
the name of your company to get sample
designs made immediately from spaces
assigned to customer reviews to buttons
allotted for social media profiles
durables ensures the websites match the
latest trends in web development you can
restyle the colors and headings or sign
up to further customize the website
based on your requirements at number 4
we have Jasper with the growth of AI
everything it's only a matter of time
until AI is utilized to produce content
for your blog social networks websites
and other platforms Jasper AI formerly
known as Jarvis is a low cost service
that generates original materials such
as blog entries for you it's simple
enough for almost anyone to use all you
have to do is enter general information
about what you want to address and the
AI will do the rest providing over 50
templates Jasper can handle a
significant amount of the heavy lifting
for topics that are often written about
the AI can provide a solid foundation of
information to which you can
subsequently add your own unique Insight
number three on our list goes to Tetra
with Tetra you don't have to note down
your minutes of meeting every day Tetra
dials into your calls and essentially
takes notes for you allowing you to
focus on the discussion now and remember
all of it later authentic speech
recognition has yet to reach the level
where it can function as a viable
alternative Jetta also has a notes
viewer with each audio byte synced with
the notes this level of detail allows
you to focus on the meetings discussion
without making individual notes
throughout the call at number 2 we have
the dolly 2. the latest groundbreaking
text to picture generator from openai is
called to only do it enables users to
produce visuals in response to text cues
users can use this generator to
transform the imaginative thoughts into
lifelike Graphics if you scroll through
Twitter and look for the hashtag dolly2
you will be greeted with many images and
artworks shared by many people based on
different topics from Victorian era
style pictures of aliens to just plain
portrait style images Dolly 2 has all
its bases covered regarding variety the
AI is taught by a deep learning how to
make text to image associations which
the model later uses to recreate the
relevant images finally we have chat GPT
at number one openai has unveiled chat
GPT a powerful new chatbot that use just
an improved version of its AI technology
to Converse in plain English well gbt
has existed for a while this model has
reached a critical point it is
beneficial for a broad range of
activities from developing software to
generating company ideas to composing a
wedding toast while prior iterations of
the program could potentially execute
these tasks the output quality was far
worse than that of a typical person
since its debut Twitter has been
bombarded with people using it for
unusual and bizarre purposes such as
creating weight loss strategies and
children's novels and providing
instructions on retrieving a peanut
butter sandwich from a VCR it seems chat
GPT will serve as an important Monument
for artificial intelligence to be
welcomed in the consumer sphere ever
wondered how Google Maps can provide the
fastest route almost instantly have you
ever considered the speed and accuracy
of virtual Bots like Google assistant or
Siri
all of them are powered by artificial
technology AI for short
writing on the data science hype train
the global AI Market is projected to go
from around 87 billion dollars in 2021
to nearly 1600 billion dollars by 2030.
artificial intelligence has impacted
almost all sectors of the it domain but
its significance in the robotics
industry is unmatched even Tesla has
announced a new humanoid robot named
Optimus recently built to perform daily
activities and help in Tesla factories
in today's video let's go through some
of the most advanced artificial
intelligence-based robots Paving the way
for a faster and smarter future in 2023
before we start here's a question for
the AI Enthusiast watching this video
which one of the following is not an
example of AI is it a computer vision is
it be web design is it C voice
recognition or D robotic
please leave your answers in the
comments below and that's a cue to get
started with the top 10 AI robots for
2023
number 10 on our list is IBO
IBO is a channel neutral robot dog
created by Sony this robot was retired
nearly a decade ago but has been brought
back with even greater human operation
capabilities
this artificial puppy responds to words
of Praise or head scratches can learn
tricks and seeks out owners
a very important feature of IBO is that
it has an app that owners can use to
alter system settings or add new tricks
and store Memories by connecting to the
cloud AI system
next on number nine we have spot
form a spot robot also known as spot
mini is really a four-legged robot
produced by Boston Dynamics and American
robotics company spot is 83 centimeters
tall and weighs only 25 kg
sport can perform various tasks
including walking stairs and navigating
rugged terrain it's small enough to be
used indoors as well Spock's unique
selling point is that it can go places
other wheeled robots cannot all while
carrying carcos of up to 14 kilogram of
examination gear number eight we have
Serena 4 Serena 4 is a humanoid robot
created by scientists at the University
of Tehran the fourth generation of Iran
Serena robot can imitate a human
position grip a water bottle and write
its name on a whiteboard the robot which
took four years to develop can track
objects properly and its new hands
permitted to manipulate various objects
from delicate gestures to using Power
Equipment
however the improvements are not only
internal Serena Force exterior has been
upgraded with new plastic panels making
the robot appear sleek and just slightly
threatening
next we have Aqua knot at number seven
Aquanaut is a shape-shifting underwater
robot developed by Houston mechatronics
Incorporated in Texas
The Equinox design allows it to mop from
a sleek and small submarine to a
slightly humanoid form with two extended
arms allowing it to perform various jobs
underwater was created to address a
long-standing problem in offshore oil
and gas which was the cost of
maintaining offshore sites with no
similar device on the market The Equinox
might have a significant Advantage
because it can accomplish this task
without the inconvenience of being tied
to a support vehicle and with only
minimal oversight from a human operator
at number six we have strontronic
chantronics are simple animatronics tank
topples each strandtronic employs
onboard sensing to perform numerous
flips twists and postures with
repeatability and position combining
modern robotic technology with the
investigation of untethered Dynamic
movement the initiative to investigate
stunt double animatronics began then
attempt to manage the landing of a
figure heard from a window by Walt
Disney Studios meanwhile Disney research
experts worked hard to utilize angular
momentum conservation to enable fine
Airborne motion control a union of the
two projects allowed the team to
construct the standtronics platform
which is ready to bring a wide range of
dynamic characters to life
at number 5 we have flipping flippy is
an autonomous robotic kitchen assistant
that can assist chefs in preparing
freshly cooked burgers and fried foods
such as crispy chicken tenders and tater
tots
for example while cooking at the grill
it can automatically detect when the raw
burger patties are placed monitor each
Patty in real time and switch between
spatulas for raw and grooming rippy's
brain is powered by Cloud connected
artificial intelligence allowing you to
learn from its environment and acquire
new abilities over time next we have
Sofia at number four Sophia is regarded
to be the most intelligent humanoid
robot she made her dpu in 2016 and an
interaction with humans was the most
extraordinary thing you'd ever witness
in a machine
Hanson robotics based in Hong Kong
developed her to integrate as an elderly
companion in healthcare facilities or
crowd management at events Sofia is the
world's first robot Citizen and the UN
development program's first robot
Innovation Ambassador Sophia has
integrated neural network models and
artificial intelligence to recognize
human feces and interpret the gestures
and emotions at number three we have
Atlas Atlas is a humanoid robot created
by Boston Dynamics and renowns for its
unmatched ability to jump over obstacles
do backflips and dance Atlas completes a
parkour track in one of the company's
most recent videos its Evolution has
been nothing short of remarkable aside
from the spectacular acrobatics it
demonstrates some interesting
foundational abilities such as awkwardly
shifting its equilibrium after landing
atlas's Behavior has also been extended
by providing it with a collection of
template behaviors such as sleeping and
vaulting and allowing it to adapt those
behaviors to the new scenarios it finds
next we have pepper in the second
position the pepper robot is among
businesses most sophistically
commercially available social robots
today
SoftBank robotics designed it to be
friendly and engaging as well as to put
people at ease and is equally at home
with infants as it is with Adolf pepper
can hold conversations and incorporate
Knowledge from those stocks into his
words it may be trained to recognize
items pick them out in a room and make
decisions to Enlighten or amuse finally
at number one we have Amika engineered
Arts describes Amica as the world's most
advanced human-shaped robot reflecting
The Cutting Edge of human robotics
technology the robot can mimic
Expressions fairly well in response to
all questions articulately engineered
Arts Amica Hardware is based on the
research into human at Robotics and is
built on the proprietary Mesmer
technology
amica's artificial limbs ligaments
Motors and sensor Aries are made with
Cutting Edge technology as shown in
movies the rise of air usually results
in chaos amica's developers on the other
hand argue that the scientific
breakthrough will be used for a
different purpose claiming that Amica
will serve as a future phase of robots
face detection system it is a form of of
biometric recognition a method for
identifying or confirming someone's
identity by glancing at their face is
called facial recognition people can be
identified by securing a match on facial
ID using this technique real-time
visuals videos and photos can be the
sources to run face detection this
technology is mostly employed in
security and law enforcement opencv is
the best technology to create it a
python package called opencv is made
specifically to address computer vision
jobs computer vision is a process used
in the processing of images by computers
it is concerned with the in-depth
comprehension of digital photos or films
it is necessary to automate operations
that can be performed by human visual
systems therefore a computer should be
able to identify items like a statue or
a Lamppost or even the face of a human
being second one is chatbot it is the
best idea if you have chosen chatbot as
your project topic this will make your
resume more attractive in case you are
looking for a job I and Oracle survey
suggests that 80 percent of the
businesses uses chatbot you can use
Python Java Ruby C plus plus or PHP as a
programming language to develop a
chatbot designing and building NLP
chatbots that accept speech and Text
data is made easier by dialog flow you
can go through many projects to get the
rough idea there are many platforms
which will help you to build a chatbot
regardless of how excellent your chatbot
is there is always room for development
the finest chatbot developers constantly
enhance their Bots over the time using
Ai and machine learning social media
recommendation system the rise of web
services like Netflix Amazon and YouTube
has increased the use of recommender
systems in our daily lives they are
algorithms that insist users in finding
information that is pertinent to them
recommender systems are important in
some forms since they can generate a lot
of income or allow you to set yourself
apart from rivals in order to provide
recommendations it evaluates the
relationship between the user and the
object as well as the parallels between
users and positions coming to predicting
stock this application is widely
applicable everywhere as AI career
aspirants one will love to develop stop
prediction applications as it is full of
data this project would be ideal for
students who want to work in the finance
industry because it it can provide I
repeat because it can provide them a
better understanding of very various
aspects of the field coming to medical
diagnosis this project is advantageous
from a medical standpoint AI projects
can be created to detect heart-based
diseases and also detect cancer it is
intended to offer patients with heart
illness online medical advice and
guidance after processing the data this
system will search the database for any
illnesses that might be connected to the
given details using data mining
techniques this intelligent system
determines the disease that the
patient's information most closely
resembles
based on the system diagnosis losers can
then speak with qualified medical
professionals users of the system can
also view information about various
doctors coming to an important project
that is search engine search engines are
utilized by all we look for information
on the greatest product to buy a nice
area to hang out or solutions to any
questions we have NLP is quite
significant in modern search engines
because a lot of language processing
takes place there python is the widely
used language to develop any search
engine search engine is mainly confined
with lots and lots of data it is helpful
for any AI career as parents next is
virtual assistants here the challenge is
to build a virtual assistant to assist
user why do you need virtual assistant
in your devices when you are building
your own it is also interesting for a ml
developer to build a virtual assistant
it involves NLP and data mining voice
based virtual assistants are popular
today because they make life easier for
users NLP is utilized to comprehend
human language in order to construct
this system when a voice command is
received the system will translate it
into machine language and store the
commands in its database hate speech
detection in social media automated hate
speech detection is a crucial weapon in
the fight against hate speech
propagation especially on social media
for the job many techniques have been
developed including a recent explosion
of deep learning based system
for the objective of detecting hate
speech a number of techniques have been
investigated including conventional
classifiers classifiers based on deep
learning and combination of both of them
on the other hand a number of data set
benchmarks including Twitter sentiment
analysis have been introduced and made
available for the evolution of the
performance of these algorithms
and the last one is predicting house
price you will need to estimate the sale
price of a brand new home in any place
for this assignment the data set for
this project includes information on the
cost of homes in various City
neighborhoods the UCI machine learning
repository is the place where you may
find the data set needed for This
research you will also receive other
data set with information on the age of
the population the city's crime rate and
the location of non-retail Enterprises
in addition to the pricing of various
residences it's a x project to test your
knowledge if you are a new
as we mentioned there are many different
branches of AI each employing a
different set of methods and technique
this is a good thing for a beginner
since you can select a project that is
interesting to you and not worry about
being able to implement all of the
different techniques and methods popular
market research projects that the market
for artificial intelligence will grow
from 59.7 billion dollars in 2021 to
422.4 billion dollars in 2028.
artificial intelligence Automation and
Robotics are disrupting almost every
business
companies that don't invest in AI
Services risk becoming obsolete whether
in machine learning smart applications
Appliance digital assistants or
autonomous vehicles
numerous companies stand to benefit from
AI but a handful of them have proven to
be the game changers for 2023 and Beyond
so hey everyone you are already watching
simply learn and here we are with the
list of top 10 AI companies in 2023 if
you love watching videos like these do
hit the Subscribe button to never miss
an update from Simply learn so let's
explore and learn about them one by one
starting with the 10th position which is
uipath robotic process automation or RPA
is a technology developed by uipath that
helps businesses to boost productivity
and reduce cost by automating time
consuming and repetitive operations
software robots can be equipped with
uipath AI Technologies to carry out
duties including reading documents and
emails interpreting language and visual
cues and comprehending the content and
the intent of the communications the RPA
Market is estimated to be worth 38
billion and uipath offers distinct
advantages over the competitors thanks
to its patent computer vision technology
and wide range of Bot Technologies in
the ninth position we have diameteries
the surface is offered by dynatories
include officer availability and
infrastructure monitoring the Davis AI
engine developed by the business can
analyze 368 trillion dependencies each
second Davis can quickly locate problems
in a company's digital ecosystem explain
what went wrong and evaluate and rate
likely commercial ramifications the
company estimated a market Worth to
285.42 million and is poised to grow
Revenue by roughly 20 percent annually
throughout at least 2025 then at the
eighth position we have talented
Technologies data analytics software
startup talented Technologies uses AI to
analyze data according to revenue and
market share parenting was ranked as the
top AI software platform globally by IDC
in September 2022 the United States
intelligence and defense Industries
account for a sizable portion of
volunteers Revenue the business just won
a new 85.1 million contract from the US
Army Marshall command to assist with the
developing Ai and machine learning
Technologies to enhance equipment
reliability improve predictive
maintenance and improve Supply chains
parent team may deliver revenues closer
to the higher end of analysis estimates
roughly 2.5 billion in 2023 then and the
seventh position is workday workday is a
provider of cloud-based applications
with an emphasis on human resources
management with the systems of workdays
distinctive AI based optimize station
engine businesses can manage challenges
with hiring and Staffing fluctuation
labor demand shift scheduling and
prioritizing and more
in April 2022 workday Incorporated
Barrister and AI powered virtual
assistant from espressive into their
platform workday is Raising 2023 markets
Worth to almost 5.53 billion
representing year-over-year growth of 22
percent now in the sixth position we
have intuitive surgical The DaVinci
surgical system offered by intuitive
surgical performs minimally invasive
procedures using Cutting Edge draw bolts
and computerized visualization
technology with the help of a big data
and AI intuitive is developing tools
that will help surgeons by improving
their training and offering them
real-time Direction despite the fact
that the DaVinci surgical system has
been approved by usfda for 22 years
intuitive reported a 13 percent
year-over-year increase is installed in
the system most recent quarter the
global intuitive surgical Market is
projected to reach 6.5 billion dollars
by 2023. moving towards the fifth
position we have IBM for years IBM has
been developing strategies to use its AI
supercomputer to alter the legal banking
academic and Healthcare Industries IBM's
clients harness the power of AI to stay
current on average changing regulations
by providing end-to-end risk and
compliance management IBM continues to
dominate the market for the other
breakthroughs in Ai and its Auto Ai and
auto email products can help data
scientists build and improve Ai and
machine learning models the market worth
of IBM in 2023 will range from 14.4
billion to 16 billion following that in
the fourth position is nvidium high-end
chipmaker Nvidia provides the
significant computing power needed for
advanced AI applications
in reality one of the world's fastest
supercomputers Leonardo is powered by
Nvidia Graphics processing units the
parent company of Facebook meta
platforms is building the biggest
artificial intelligence supercomputer in
the world meta also owns nvidia's
Quantum infibe and networking system and
6080a 100 nvda Graphics processing
processors Nvidia is a significant
supplier for fast growing Industries
like high-end gaming business Graphics
cloud computing artificial intelligence
and advanced automative technology
nvidia's outlook for 2023 revenue is
expected to be around 6 billion here
comes the top three in the list and in
the third position we have Amazon AI is
integrated into every aspect of Amazon's
business including e-commerce search
engines targeted advertising and Amazon
web services Amazon Alexa one of the
most popular virtual assistant is
already present in many American Homes
Amazon said in August that it would pay
1.7 billion to acquire robots Corp RBT a
maker of homework this action can allow
Amazon to boost the use of AI in
household worldwide Amazon expects a 491
billion Market worth between 2023 and
2027. in the second position is alphabet
alphabet the company that owns Google
and YouTube uses Ai and Automation in
practically every part of its business
including ad pricing Gmail spam filters
and content promotion in addition it is
the parent company of the autonomous car
manufacturer waymo and the AI software
business deepmind which created history
in 2022 when it became the first
completely autonomous commercial taxi
service to operate on public routes the
market worth of alphabet in 2023 will be
around 70 billion dollars and the first
position on the list is Microsoft in
2020 Microsoft announced the
construction of a new supercomputer
hosted on Azure Microsoft's cloud
computing Network the supercomputer was
created in collaboration with open AI to
train AI models producing substantial AI
models and related infrastructure for
programmers and other businesses in
October Microsoft released Microsoft
designer a graphic design tool that
makes original social media post
invitations and other Graphics using AI
technology Microsoft the tech giant is
expected to earn a market worth of
49.73 billion and with that we have come
to the end of this list of the top 10
artificial intelligence companies of
2023. opportunities in AI have increased
significantly in recent years the search
in jobs related to AI is legitimate due
to its widespread involvement in crucial
Fields according to indeed the pay range
for jobs involving artificial
intelligence is between 160 thousand
dollars to three thousand fifty thousand
dollars and go even higher so
come up with a list of leading career
opportunities in artificial intelligence
along with companies hiring for those
roles but before we begin here is a
question for you who do you think is
currently leading the AI Market let us
know in the comment section below so
without any further delay let's get
started starting with our list we have
robotics engineer an engineer in
robotics create prototypes constructs
and test machines and updates the
software that manages them additionally
they investigate the most affordable and
secure way to make their robotic system
aspirin should have a bachelor degree in
computer science and pursue a career in
robotics with additional training or
certification in automation is
advantages robotics engineers are in
high demand with 9000 plus job openings
in India and 8 000 plus openings in the
US top companies like Amazon Bosch and
flybase are hiring robotics engineers
but salary is as high as high hundred
sixty thousand dollars in the US and
rupees 27 lakhs in India next up on our
list we have product manager the role of
a product manager is to recognize and
express user requirements market
research and creating competitive
evaluations and also creating a
product's vision and putting emphasis on
a product strengths and qualities
freshers cannot join as a product
manager it requires basic experience of
two to three years in the field of
product and software development
according to indeed.com an average of
2000 plus jobs are currently available
in India and 20 000 plus job vacancies
are there in the US with top companies
hiring are Tesla Amazon TCS sprinkler
cognizant Etc with a salary as high as
160
000 per year in the US and rupees 20
lakhs per annum in India up next we have
data scientists they mainly deal with
lots of data extraction of Knowledge
from all collected data is the subject
matter of data science it was also voted
the sexiest job of the 21st century good
news is there is no need for any special
degree as a data scientist you need to
have knowledge of machine learning
programming in Python are Java and
mathematical modeling being a fresher
you can join as a data scientist too
data scientists are in demand with 17
000 plus vacancies in the US and 3000
plus vacancies in India and big
businesses like accent enter TCS IBM
Google JP Morgan Deloitte Bank of
America recruiting data employees with
salary going as high as two hundred
thousand dollars per year in the US and
rupees 15 lakhs per annum in India
moving forward in our list we have ai
data analyst data mining data cleaning
data interpretation are the three main
tasks of an AI data analyst data
cleaning allows for the Gathering of the
necessary information for data analysis
and AI data analyst make inferences from
the data using statisticals tools and
techniques you need a bachelor degree in
computer science or mathematics to work
as an AI data analyst to get this job
you must have a thorough understanding
of regression and be able to use Ms
Excel according to indeed.com there are
currently 700 plus AI data analyst jobs
available in the United States and 16
000 plus job vacancies in India with top
companies like Accenture IB am we Pro
TCS and capgemini recruiting them and
salaries going as high as 100 000
dollars per year in the US and rupees
8.5 lakhs per annum in India the next
job opportunity in AI is business
intelligence engineer a business
intelligence developer's main duty is to
take both business ends and AI into
account they evaluate complex data sets
to identify various business Trends they
assist in boosting a company's earnings
by planning creating and sustaining
business intelligence solutions to
become a business intelligence engineer
you are required to have a bachelor
degree in mathematics and computer
science according to indeed.com there
are 8 000 plus job available in India
and 5000 plus job vacancies in United
States companies hiring are Amazon
Development Center IBM AWS Etc with this
salary going as high as hundred thousand
dollars per year in the USA and rupees
15 lakhs per year in in India coming to
the top three jobs available in AI at
number 3 we have machine learning
engineer artificial intelligence is
known to include machine learning it
runs simulations using the varied data
provided and produces precise results
they are constantly in demand from the
businesses and their position seldom
goes unfilled they handle enormous
amount of data and have exceptional data
management skills the ability to code
use computers and understand mathematics
a requirement for success as a machine
learning engineer a master's degree in
computer science or mathematics is
highly preferred the necessary
technological Stacks include python are
Scala and Java a deep understanding of
neural networks deep learning and
machine learning algorithm is very
helpful coming to job opportunities
lastdoor.com predicts 13 000 plus job
vacancies in the US and 27 000 plus jobs
available in India with with companies
like mathwork Google Amazon Microsoft
Azure IBM Etc hiring them with salary
going as high as 150 000 per annum in
the US and rupees 10 lakhs per annum in
India coming at runner-up position we
have data architect data Architects are
it Specialists who utilize their
knowledge of computer science and
database architecture to assist and
analyze an organization's data
infrastructure fresher graduates cannot
become data Architects a bachelor degree
in computer science and computer
engineering or related subject is the
absolute minimum requirement to become a
data architect data Architects are in
high demand with 52 000 job vacancies in
the United States and 7000 plus jobs in
India and top companies hiring data
Architects are Amazon IBM Tesla Intel
Wipro with the average salary quite high
as two hundred thousand dollars per per
year in the United States and rupees 21
lakhs per annum in India and Topping our
list of job opportunities in aies AI
engineer AI Engineers are problem
solvers who create test and use various
artificial intelligence models they
manage the AI infrastructure well to
create practical AI models we use neural
network knowledge and machine learning
method these models enable one to gain
business insights which AIDS the
organization in making wise business
decisions
ug or PG degree in the field of AI or
computer science is required in addition
to that you can hold certifications from
any reputed organization which will add
up to your resume please check out and
enroll to the AI program by simply learn
in collaboration with Purdue University
the link is provided in the description
below job openings for an AI engineer
are quite high with 35 000 plus job
vacancies in the US and 4000 plus jobs
in India and Tech giants like Accenture
TCS IBM Google JP Morgan Beloit Bank of
America recruiting them with salary
going as high as two hundred thousand
dollars per year in the US and rupees 10
lakhs per annum in India and this was
all for this AI bootcamp hope you guys
found it informative and helpful if you
like the session then like share and
subscribe if you have any questions then
you can drop them in the comment section
below thanks for watching and stay tuned
for more from Simply learn
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos turn it up and get certified
click here