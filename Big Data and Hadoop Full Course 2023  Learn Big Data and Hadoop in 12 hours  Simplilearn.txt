foreign
the global big data and analytics Market
is worth 274 billion dollars per year
and the start
of this that around 2.5 Queen billion
bytes of
service generated every day but how is
it stored how can it be utilized forever
and research this video will answer all
your questions what are we going to
discuss today we are going to highlight
the fundamentals of Big Data what is
Hadoop installation of Hadoop Hadoop and
its architecture followed by which we
will cover in-depth understanding of
hadoopyan hdfs mapreduce and its example
post which we will understand another
important processing layer Apache spark
once we have decent understanding of the
fundamentals we will dive into big data
analytics with python and finally to
make things more interesting we will
cover the most frequently Asked Hadoop
interview questions which should help
you stand on the safer side in your
upcoming interview but big data is an
ocean of topics if you are an aspiring
Big Data developer and Keen to get your
certification training then we have the
best offer for you
now over to our training experts we all
use smartphones but have you ever
wondered how much data it generates in
the form of texts phone calls emails
photos videos searches and music
approximately 40 exabytes of data gets
generated every month by a single
smartphone user now imagine this number
multiplied by 5 billion smartphone users
that's a lot for our mind to even
process isn't it in fact this amount of
data is quite a lot for traditional
Computing systems to handle and this
massive amount of data is what we term
as Big Data let's have a look at the
data generated per minute on the
internet
2.1 million Snaps are shared on Snapchat
3.8 million search queries are made on
Google one million people log on to
Facebook 4.5 million videos are watched
on YouTube
188 million emails are sent that's a lot
of data so how do you classify any data
as Big Data this is possible with the
concept of five V's volume velocity
variety veracity and value
let us understand this with an example
from the healthcare industry Hospitals
and Clinics across the world generate
massive volumes of data
2314 exabytes of data are collected
annually in the form of patient records
and test results all this data is
generated at a very high speed which
attributes to the velocity of Big Data
variety refers to the various data types
such as structured semi-structured and
unstructured data examples include Excel
records log files and x-ray images
accuracy and trustworthiness of the
generated data is termed as veracity
analyzing all this data will benefit the
medical sector by enabling faster
disease detection better treatment and
reduced cost this is known as the value
of big data but how do we store and
process this big data to do this job we
have various Frameworks such as
Cassandra Hadoop and Spark let us take
Hadoop as an example
and see how Hadoop stores and processes
Big Data
Hadoop uses a distributed file system
known as Hadoop distributed file system
to store Big Data if you have a huge
file your file will be broken down into
smaller chunks and stored in various
machines not only that when you break
the file you also make copies of it
which goes into different nodes this way
you store your big data in a distributed
way and make sure that even if one
machine fails your data is safe on
another
mapreduce technique is used to process
Big Data a lengthy task a is broken into
smaller tasks
b c and d now instead of one machine
three machines take up each task and
complete it in a parallel fashion and
assemble the results at the end thanks
to this the processing becomes easy and
fast this is known as parallel
processing
now that we have stored and processed
our big data we can analyze this data
for numerous applications in games like
Halo 3 and Call of Duty designers
analyze user data to understand at which
stage most of the users pause restart or
quit playing this Insight can help them
rework on the storyline of the game and
improve the user experience which in
turn reduces the customer churn rate
similarly Big Data also helped with
disaster management during Hurricane
Sandy in 2012. it was used to gain a
better understanding of the storm's
effect on the east coast of the U.S and
necessary measures were taken it could
predict the Hurricane's landfall five
days in advance which wasn't possible
earlier these are some of the clear
indications of how valuable big data can
be once it is accurately processed and
analyzed let's rewind to the days before
the world turn digital back then
minuscule amounts of data were generated
at a relatively sluggish Pace all the
data was mostly documents and in the
form of rows and columns storing or
processing this data wasn't much trouble
as a single storage unit and processor
combination would do the job but as
years passed by the internet took the
World by storm giving rise to tons of
data generated in a multitude of forms
and formats every microsecond
semi-structured and unstructured data
was available now in the form of email
files images audio and video to name a
few all this data became collectively
known as Big Data although fascinating
it became nearly impossible to handle
this big data and a storage unit
processor combination was obviously not
enough
so what was the solution
multiple storage units and processors
were undoubtedly the need of the hour
this concept was incorporated in the
framework of Hadoop that could store and
process vast amounts of any data
efficiently using a cluster of commodity
Hardware Hadoop consisted of three
components that were specifically
designed to work on big data in order to
capitalize on data the first step is
storing it the first component of Hadoop
is its storage unit the Hadoop
distributed file system or hdfs storing
massive data on one computer is
unfeasible hence data is distributed
amongst many computers and stored in
blocks so if you have 600 megabytes of
data to be stored hdfs splits the data
into multiple blocks of data that are
then stored on several data nodes in the
cluster 128 megabytes is the default
size of each block hence 600 megabytes
will be split into four blocks a b c and
d of 128 megabytes each and there
remaining 88 megabytes in the last block
e so now you might be wondering what if
one data node crashes do we lose that
specific piece of data well no that's
the beauty of hdfs hdfs makes copies of
the data and stores it across multiple
systems for example when block a is
created it is replicated with a
replication factor of 3 and stored on
different data nodes this is termed the
replication method by doing so data is
not lost at any cost even if one data
node crashes making hdfs fault tolerant
after storing the data successfully it
needs to be processed this is where the
second component of Hadoop mapreduce
comes into play in the traditional data
processing method entire data would be
processed on a single machine having a
single processor this consumed time and
was inefficient especially when
processing large volumes of a variety of
data to overcome this mapreduce splits
data into parts and processes each of
them separately on different data nodes
the individual results are then
aggregated to give the final output
let's try to count the number of
occurrences of words taking this example
first the input is split into five
separate parts based on full stops the
next step is the mapper phase where the
occurrence of each word is counted and
allocated a number after that depending
on the words similar words are shuffled
sorted and grouped following which in
the reducer phase all the grouped words
are given a count finally the output is
displayed by aggregating the results all
this is done by writing a simple program
similarly mapreduce processes each part
of Big Data individually and then sums
the result at the end this improves load
balancing and saves a considerable
amount of time
now that we have our mapreduce job ready
it is time for us to run it on the
Hadoop cluster this is done with the
help of a set of resources such as RAM
Network bandwidth and CPU multiple jobs
are run on Hadoop simultaneously and
each of them needs some resources to
complete the task successfully to
efficiently manage these resources we
have the third component of Hadoop which
is yarn yet another resource negotiator
or yarn consists of a resource manager
node Manager application master and
containers the resource manager assigns
resources node managers handle the nodes
and monitor the resource usage in the
node the containers hold a collection of
physical resources suppose we want to
process the mapreduce job we had created
first the application Master requests
the container from the node manager once
the node manager gets the resources it
sends them to the resource manager this
way yarn processes job requests and
manages cluster resources in Hadoop in
addition to these components Hadoop also
has various Big Data tools and
Frameworks dedicated to managing
processing and analyzing data the Hadoop
ecosystem comprises several other
components like Hive Pig Apache spark
Flume and scoop to name a few the Hadoop
ecosystem works together on big data
management you can download the Cloudera
quick start VM so you can just type in
download cloud era quick start VM and
you can search for package now this can
be used to set up a quick start VM which
would be a single node cloud
cluster so you can click on this link
and then basically based on the platform
which you would be choosing to install
such as using a VM box or which version
of Cloudera you would in also here I can
select a platform so I can choose
virtual box and then you can click on
get it now so give your details and
basically then it should allow you to
download the quick start VM which would
look something like this and once you
have the zip file which is downloaded
you can unzip it which can then be used
to set up a single node Cloudera cluster
so once you have downloaded the zip file
that would look something like this so
you would have a quick start virtual box
and then a virtual boss disk now this
can be used to set up a cluster ignore
these files which are related to Amazon
machines and we you don't need to have
this can be used to
use to solve for this to be set up you
can click on file import Appliance and
here you can choose your quick start VM
by looking into downloads quick start VM
select this and click on open now you
can click on next and that shows you the
specifications of CPU Ram which we can
then change later and click on import
this will start importing virtual disk
image dot vmdk file into your VM box
once this is done we will have to change
the specifications or machines to use
two CPU cores minimum and give a little
more RAM because clouded a quick start
VM is very CPU intensive and it needs
good amount of ram so to survive I will
give two CPU cores and 5gb RAM and that
should be enough for us to bring up a
quick start VM which gives us a Cloudera
distribution of Hadoop in a single node
cluster setup which can be used for
working learning about different
distributions in Cloudera cluster
working with stfs and other Hadoop
ecosystem components let's just wait for
this importing to finish and then we
will go ahead and set up a quick start
VM for our practice here the importing
of Appliance is done and we see Cloudera
quick start machine is added to my list
of machines I can click on this and
click on settings as mentioned I would
like to give it more RAM and more CPU
cores so click on system and here let's
increase the ram to at least five and
click on processor and let's give it two
CPU cores which would at least be better
than using one CPU core Network it goes
for Nat and that's fine click on OK and
we would want to start this machine so
that it uses two CPU cores 5gb RAM and
it should bring up my Cloudera quick
start VM now let's go ahead and start
this machine which has our quick start
VM it might take initially some time to
start up because internally there will
be various Cloudera Services which will
be starting up and those Services need
to be up for our Cloudera quick start VM
to be accessible so unlike your Apache
Hadoop cluster where we start our
cluster and we will be starting all our
processes in case of Cloudera it is your
Cloudera SCM server and agents which
take care of starting up of your
services and starting up of your
different roles for those Services I
explained in my previous session that
for a Cloudera cluster it would be these
Services let me show you that so in case
of Apache cluster we start our services
that is that is start our cluster by
running script and then basically those
scripts will individually start the
different processes on different nodes
in in case of cloud data we would always
have a Cloudera SCM server which would
be running on one machine and then
including that machine we would have
cloud and ICM agents which would be
running on multiple machines similarly
if we had a hortonworks cluster we would
have ambari server starting up on the
first machine and the number agents
running on other machines so your server
component knows what are the services
which are set up what are their
configurations
agents running on every node are
responsible to send heartbeats to the
server receive instructions and then
take care of starting and stopping off
of individual roles on different
machines in case of our single node
cluster setup in quick start VM we would
just have one SCM server and one SCM
agent which will start on the machine
which will then take care of all the
roles which need to be started for your
different services so we will just wait
for our machine to come up and basically
have clouded SCM server and agent
running and once we have that we need to
follow few steps so that we can have the
Cloudera admin console accessible which
allows you to browse the cluster look at
different Services look at the roles for
different services and also work with
your cluster either using command line
or using the web interface that is Hue
now that my machine has come up and it
already is connected to the
things so that we can have
hand console
so at this point of time you can click
on Terminal and check if you have access
to the cluster so here type in hostname
and that shows you your host name which
is quickstart.cloud error data we can
also type in hdfs command to see if we
have access and if my cluster is working
these commands are same as you would
give them in a Apache Hadoop cluster or
in any other distribution of Hadoop
sometimes when your cluster is up and
you have access to the terminal it might
take few seconds or few minutes before
there is a connection established
between Cloudera CM server and Cloud era
CM agent running background which takes
care of your cluster cluster I have
given hdfs DFS list command which
basically should show me what by default
exists on my sdfs let's just give it a
couple of seconds before it shows us the
output we can also check by giving a
service Cloud error SCM server status
and here it tells me that if you would
want to use Cloudera Express free run
this command it needs 8GB of RAM and it
gives to Virtual CPU cores and it also
mentions it may take several minutes
before cloud manager has
I can login as root here and then give
the command service Cloudera SCM server
status remember the password for root is
so it basically says that if you would
want to check the settings it is good to
have Express Edition running so we can
close this my sdfs access is working
fine let's close the terminal and here
we have launch Cloudera Express click on
this and that will give you that you
need to give a command which is Force
let's copy this command let's open a
different terminal and let's give this
command like this which will then go
ahead and shut down your Cloudera Based
Services and then it will restart it
only after which you will be able to
access your admin console so let's just
give it a couple of minutes before it
does this and then we will have access
to our admin console here if you see it
is starting the Cloudera manager server
again it is waiting for Cloudera manager
API then starting the Cloudera manager
agents and then configuring the
deployment as per the new settings which
we have given as to use the express
edition of Cloudera once all this is
done it will say the cluster has been
restarted and the admin console can be
accessed by ID and password as Cloud
error give it a couple of more minutes
and once this is done we are ready to
use our admin
admin now that deployment has been
configured client configurations have
also been deployed and it has restarted
the Cloudera Management Service
it gives you an access to Quick Start
admin console using username and
password as Cloud error let's try
accessing it so we can open up the
browser here and let's change this to
7180 that's the default port and that
shows the admin console which is coming
up now here we can
look at a cloud and then let's click on
login now as I said Cloudera is very CPU
intensive and memory intensive so it
would slow down since we have not given
enough GB GB Ram to our Cloud error
cluster and thus it will be advisable to
stop or even remove the services which
we don't need now as of now if we look
at the services all of them look in a
stop status and that's good in one way
because we can then go ahead and remove
the services
will not use in the beginning and later
we can anytime add services to the
cluster so for example I can click on
key value store here and then I can
scroll down where it says delete to
remove this service from the admin
console now anytime you are removing a
particular service it will only remove
the service from the management by Cloud
attack manager all the role groups under
this service will be removed from host
templates so we can click on delete now
if this service was depending on some
other service it would have prompted me
with a message that removed the relevant
services on which this particular
service depends if the service was
already running then it would have given
me a message that the service has to be
stopped before it can be deleted from
the cloud error at
now this is my admin console which
allows you to click on Services look at
the different roles and processes which
are running for this service we anyways
have access to our Cloud era cluster
from the terminal using our regular sdfs
or yarn or mapred commands now I removed
a service I will also remove solar which
we will not be using for the beginning
but then it depends on your choice so we
can here scroll down to delete it and
that says that before deleting the solar
service you must remove the dependencies
on the service from the configuration of
following services that is Hue now Hue
is a web interface which allows you to
work with your sdfs and that is
depending on this so click on configure
service dependency and here we can make
sure that our Hue service does not
depend on a particular service we are
removing so that then we can have a
clean removal of the service so I'll
click on none and I will say save
changes once this is done then then we
can go ahead and try removing the solar
service from our admin console which
will reduce some load on my Management
console which will also allow me to work
faster on my cluster now here we have
removed the dependency of hue on solar
so we can click on this and then we can
delete
remember I am only doing this so that my
cluster becomes little lighter and I can
work on my focus services at any point
of time if you want to add more services
to your cluster you can anytime do that
you can fix different configuration
issues like what we see here with
different warning messages and here we
have these Services which are already
existing now if we don't need any of the
service I can click on the drop down and
click on delete again this says that's
group 2 also has relevance to Hue so Hue
as a web interface also depends on scope
too as of now we'll make it none at any
point of time later you can add the
services by clicking the add service
option now this is a cluster to which
you have admin access and this is a
quick start VM which gives you a single
node Cloud error cluster which you can
use for Learning and practicing so here
we'll click on scope 2 and then we will
say delete as we have configured the
dependency now and we will remove scope
2 also from the list of services which
your admin console is managing right so
once this is done we have removed three
services which we did not need we can
even remove scoop as a client and if we
need we can add that later now there are
various other alerts which your cloud
data admin console shows and we can
always fix them by clicking on the
health issues or configuration issues we
can click here and see what is the
health issue it is pointing to if that
is a critical one or if that can be
ignored so it says there is an issue
with a clock offset which basically
relates to an ntp service network time
protocol which makes sure that one or
multiple machines are in the same time
zone and are in sync so for now we can
click on suppress and we can just say
suppress for all hosts and we can say
look into it later and confirm so now we
will not have that health issue reported
that probably the ntp service and the
machines might not be in sync now that
does not have an impact for our use case
as of now but if we have a Kerberos kind
of setup which is for security then
basically offset and time zone becomes
important so we can ignore this message
and we are still good to use the cluster
we also have other configuration issues
and you can click on this which might
talk about the Heap size or the ram
which is available for machines it talks
about zookeeper should be in odd numbers
Hue does not have a load balancer sdfs
only has one data node but all of these
issues are not to be worried upon
because this is a single node cluster
setup so if you want to avoid all of
these warnings you can always click on
suppress and you can avoid and let your
cluster be in
but that's nothing to worry so we can
click on cluster and basically we can
look at the services so we have removed
some Services which we don't intend to
use now I have also suppressed a offset
warning which is not very critical for
my use case and basically I am good to
start the cluster at any point of time
as I said if you would want to add
services this is the actions button
button which you can
eat services
just say restart my cluster which will
restart all the services one by one
starting from zookeeper as the first
service to come up we can always click
on this Arrow Mark and see what is
happening in the services what services
are coming up and in which order if you
have any issues you can always click on
the link next to it which will take you
to the logs and we can click on close to
let it happen in
so this will basically let my services
restart one by one and my cluster will
then become completely accessible either
using Hue as a web interface or quick
start terminal which allows you to give
your commands now while my machines are
coming up you can click on host and you
can have a look at all the hosts we have
as of now only one which will also tell
you how many roles or processes are
running on this machine so that is 25
roles it tells you what is the disk
usage it tells you what is the physical
memory being used and using this host
tab we can add new host to the cluster
we can check the configuration we can
check all the hosts in Diagnostics you
can look at the logs which will give you
access to all the logs you can even
select the sources from which you would
want to have the logs or you can give
the host name you can click on search
you can build your own charts you can
also do the admin stuff by adding
different users or enabling security
using the administration tab so since we
have clicked on restart of a cluster we
will slowly start seeing all the
services one by one coming up starting
with zookeeper to begin with and once we
have our cluster up and running whether
that is showing all services in green or
in a different status we still should be
able to access the service now as we saw
in Apache Hadoop cluster even here we
can click on hdfs and we can access the
web UI once our hdfs service is up by
clicking on quick links so the service
is not yet up once it is up we should be
able to see the web UI link which will
allow you to check things from sdfs web
interface similarly yarn as a service
also has a web interface so as soon as
the service comes up under your quick
links we will have access to the yarn UI
and similarly once the service comes up
we will have access to Hue which will
give you the web interface which allows
you to work with your stfs which allows
you to work with your different other
components within the cluster without
even using the command line tools
command line
so we will have to give it some time
while the Cloudera SCM agent on every
machine will
not the role
roles possible for your
output we can always click here which
tells that there are some running
commands in the background which are
trying to start
and go to the terminal and we can switch
sstfs user remember sdfs user is the
admin user and it does it does not have
a password unless you have set one so
you can just log in as sdfs which might
ask you for password initially which we
do not have so the best way to do this
is by logging in as root where the
password is cloud error and then you can
log in as hdfs so that then onwards you
can give your sdfs commands to work with
your file system now since my services
are coming up right now when I try to
give a sdfsdfs command it might not work
or it might also say that it is trying
to connect name node which is
which is yet so we will have to give it
some time and only once the name node is
up we will be
will be access our sdfs using commands
so this is how you can quickly set up
your quick start and then you can be
working using the command line options
from the terminal like what you would do
in Apache Hadoop cluster you could use
the web interfaces which allow you to
work with your cluster now
usually takes more time so you will have
to give it some time before your
services are up and running and for any
reason if you have issues it might
require you
you start your cluster several times in
the beginning before it gets accustomed
to the settings what you have given and
it starts up the services at any point
of time if you have any error message
then you can always go back and look in
logs and see what is happening and try
starting your cluster so this is how we
set up a quick start VM and you can be
using this to work with your
today we're covering the Hadoop
ecosystem at least the very fundamentals
of all the different parts that are in
the Hadoop ecosystem and it's very
robust it's grown over the years with
different things added in there's a lot
of overlapping a lot of these tools but
we're just going to cover these basic
tools you can see what's available in
the Hadoop ecosystem the question is why
Hadoop now let's
that with an analysis just imagine in a
town far away Tim sells food grains in a
shop the customers were happy as Tim was
very quick with the orders and Tim sends
the good demand for other products so he
thought of expanding his business but it
wasn't easy as he expected it to be the
number of customers were increasing and
he was not able to cater to their needs
on time he had to look into assisting
his customers with each of their orders
and billing it was too difficult for him
to manage alone so what does he do to
start delivering orders on time and to
manage the customer's demand Tim hired
three more people to work with him so it
was Tim it was Matt Luke and Anne now
there was still a problem the problem
was with storage area so Solutions which
Tim was looking for was that he should
have enough space in the shop for all
the items so that he can cater to
different customers for their different
needs and as it seems storage was a
bottleneck since storing and accessing
became more and more difficult with
increased supply and demand so what does
Tim do so Tim came up with an idea to
overcome this issue he basically decided
to expand the storage area and
distribute each category of product on
different floors now that's a smart
thinking customers were happy and after
picking up their products from the
respective sections the products could
be built now if we compare this simple
story of Tim to what we call as Big Data
let's talk in technical terms now so we
know that data has been critical and
important for organizations however in
past data was generated at a moderate
rate and all the data was structured in
nature so when I say structured data can
have different types data can be
structured it can be semi-structured and
it can be unstructured now when we talk
about structure semi-structure or
unstructured the main differences
presence or absence of schema so if you
can have schema defined well before the
data comes in and then your data fits
into that schema we would call it as
structured if you have intelligently
find your schema expecting your data to
get stored within that schema however
the data pattern breaks it then we are
talking about unstructured data or
absence of schema in the data you could
have semi-structured data which
basically means that you could have a
mix of structured and unstructured data
now in past data was mainly structured
and pretty much One processor was enough
to process all of it now as the data
generation increased there were
different types of data which were
getting generated or I would say
different type of data sets which were
getting generated and that too with a
high speed so it was evidently becoming
difficult for a single processor or a
single machine to store to cure to
process to mine to analyze different
types of data and data sets now with
different types of data existing and
they're getting generated in massive
amount it posed a challenge that this
data could not be processed and stored
and the known Storage Solutions were
traditional databases to overcome this
issue multiple processes were used so
this is where we started seeing a shift
in Paradigm from single processor to
multiple processors so I would say from
a vertical scalable solution to a
horizontal scalable solution So when you
say vertical scalability we talk about
increasing the capacity of a machine
when we talk about scalability we mean a
capability to scale capability to expand
and when we talk about a single
processor that is vertical scalability
where you would have one machine or one
server with limited amount of ram
limited amount of storage and you could
think of expanding that by adding more
RAM adding more storage but that would
have some limits you could not go beyond
a particular limit you probably would
have to be logged in based on the vendor
because you cannot change your internal
com components to suit your requirements
and if we talk about horizontal
scalability that is multiple processors
in this case we are talking about adding
more machines so that you could have a
horizontal scalable architecture now the
problem which we were talking about to
overcome that issue multiple processes
were used to process each type of data
and here we see a similarity in Tim's
approach with the technical approach of
going for a distributed platform which
could handle huge amount of data data
coming in in different formats data
getting generated with lot of speed and
thus helping the organization now what
happens is when you think of having
multiple Solutions or having multiple
systems which can be used to store or
process your data one of the major
challenge is one storage system being
accessed by all the machines becomes a
bottleneck and that is again a problem
similar to what Tim was having when he
decided to cater to multiple customers
looking for different products but then
he was limited with the storage space he
had and to keep all the products in one
place so just like how Tim adopted the
distributed approach the storage system
which was a bottleneck was also then
distributed so in this way your data
could be stored in individual databases
or in individual machines on their disks
however it could be accessed from one
place so if we look at this story we see
two approaches and these two approaches
are the ones which are used by Hadoop
which is a framework and these
approaches are your storage layer that
is your sdfs and your processing
Paradigm that is mapreduce now what is
sdfs let's understand this sdfs stands
for Hadoop distributed file system so
that's a logical storage layer it's a
distributed storage space just like our
example of Tim having distribute did the
storage space amongst various sections
so each person was taking care of a
separate section and at the end the
customers went to the cashier for the
final bidding this sorted the process
and make it easier or made it easier now
this is how similar or this is how
Hadoop mapreduce works now that was a
rough story of Big Data generation and
why Hadoop is required right let me
explain what Hadoop is so if we talk
about what is Hadoop it's basically a
framework it's not a programming
language it's not an architecture it's
not just a hardware it's not just a
software it is a framework which allows
you to store and process big data in a
distributed and parallel fashion now
there are a lot of users which have made
Big Data case study a famous and a
popular one so here are some companies
say Facebook or Instagram Google Twitter
LinkedIn Skype YouTube and many and
these and many more organizations across
the globe are using Hadoop for their
storage and processing requirements
however it would not be just Hadoop
Hadoop also comes with its own ecosystem
which has various set of Open Source
Technologies and packages all packaged
together to help an organization in
various facets of its data life so the
question is how does Hadoop store and
process all of this data or I would say
this big data or before we even talk
about Hadoop which has individual
components which are responsible for
storing and processing big data we also
need to understand what is Big Data so
big data is just a term data has been
exponentially growing in past decade and
now we don't talk about gigabytes or
terabytes we already are talking about
petabytes and exabytes or even
zettabytes that is 10 to the power 21
bytes however in known factors any
organization is not processing zatabase
of data but yes there are organizations
which are processing petabytes and
exabytes of data so how does Hadoop help
them and what is this big data so big
data is a term coined to explain the
data which is very complex which is huge
which is getting generated with lot of
speed which has a lot of variety and
then it becomes a challenge to store to
capture to analyze process this data and
derive useful information out of it now
we are talking about huge amount of data
we are talking about different ways that
is V for volume v for velocity V for
variety B for velocity and other aspects
like viscosity validity virality and
many more so when your data comes in
with all these characteristics it can be
easily categorized as big data and
organizations in recent past have
realized that they cannot ignore any
data considering it as an economical so
they would want to work on all of it and
when I say all of it that is one of the
drawbacks in terms of rdbms so in rdbms
usually you will never have 100 percent
of data online you would still have your
data it would be consistent you could do
transactions but you would not have
access to all 100 of it because that
would be a costlier option so in rdbms
you would have 10 or 20 or 30 percent of
data which would be online and rest of
the data would be archived which
basically means that if you would want
to process that data you would have to
bring it from the storage layer to the
processing layer process it and then
store it back now how does Hadoop help
in this approach or this requirement so
when we talk about components of Hadoop
we have a storage layer that is sdfs now
you can compare this with your own
laptop or PC every laptop on a PC has
two layers it has a storage layer that
is your disk or your external hard disk
which is attached to your machine and
your machine also has the processing
layer which is your RAM and CPU cores so
sdfs is the storage layer of Hadoop so
it's a logical storage layer which
technically speaking does not exist but
then yes it exists because it depends on
the individual file system of multiple
machines we'll learn about this in a few
minutes the other layer is the
processing layer so when you talk about
mapreduce it is a programming model
which allows you to process the data in
one particular way now mapreduce has
been oldest and the most mature so when
I say most mature it has evolved over
time since the time Hadoop came into
existence map reduce exists it is still
being used by organizations for doing
processing and when we talk about
processing here we are not talking about
processing on one single machine but we
are talking about processing on multiple
number of machines where each machine's
RAM and
course contribute to the processing so
we have two main components one is your
sdfs one is your map reduce and for your
map reduce or basically any other
processing framework mapreduce is one of
them so for any other kind of processing
say massive parallel processing say
graph processing say map reduce
processing say structured query language
being processing for all of these
processing needs there is a processing
layer called yarn now yarn stands for
yet another resource negotiator so
basically yarn takes care of your
resource management that is when the
processing happens across the machines
who would take care of RAM and CPU core
which would be required for processing
and that would be urea for a Hadoop
cluster sdfs is Fault tolerant is
reliable and is scalable so let's
understand and learn on the working of
sdfs that is The Logical storage layer
for a Hadoop cluster sdfs is a
distributed file system which is
designed to store large volumes of data
on commodity machines it is designed
within low cost hardware and that is
what I mean when I say commodity
machines so we could have a cluster of
tens and hundreds and thousands of nodes
which can still scale and accommodate
huge volume of data it provides access
to data across multiple Hadoop clusters
it has a high fault tolerance and
throughput and sdfs stores data in
multiple servers instead of a central
server now earlier when we were
discussing on how storing data on a
single machine can lead to more read or
processing times and basically lead to
high seek time that can be taken care by
a distributed file system such as sdfs
which would rely on file systems of
multiple machines which form your Hadoop
cluster now let's understand how SDF has
works for example we have a large file
which is to be stored on sdfs now that
file would be broken down into blocks
and the block size by default is 128 MB
these blocks would then be stored across
different slave machines or different
nodes which are responsible for storing
data sdfs has a default block size of
128 MB which can be increased as per
requirement now that could be in
megabytes gigabytes terabytes or even
bigger than that the default block size
is 128 MB from Hadoop version 2 onwards
and your previous versions of Hadoop had
the block size of 64 MB here the point
to be remembered is block is a logical
entity which basically takes the data
and stores it on the underlying machines
in their disks now this is how it would
look in a normal Hadoop cluster we would
have one single Master unless it is a
high availability cluster where we could
have multiple Masters that has two
masters one being active and one being
standby in a normal Hadoop cluster you
would have one master and multiple slave
nodes where these data blocks are
getting stored now before we understand
the mapreduce algorithm let's also
understand the working of sdfs and how
data gets stored overall how does it
work so let me just open up a notepad
file and I would explain about sdfs now
let's imagine that this is I would like
to have say five machines which could be
forming my cluster so I would have
machine one machine two machine in three
machine four and machine 5. now if I say
my Hadoop cluster is running on these
machines which could be of a Apache
Hadoop Hadoop core distribution or
having a vendor specific distribution
such as Cloud era or hortonworks or
mapper or IBM big Insight Etc so these
are some vendor specific distributions
of a Hadoop cluster now here we can
discuss later on how or what processes
would be running on these machines now
in case of Apache Hadoop cluster in case
of Apache Hadoop cluster to have a
Hadoop cluster running we would have to
download the Hadoop related packages we
would have to untar then we would have
to edit config files to specify which
processes would run on which nodes and
then basically we would be formatting
hdfs and then onwards we can go ahead
and start our cluster so it looks like
in Apache Hadoop cluster we would have
to manually do various things such as
downloading the Hadoop related tar file
untiring it editing the config files
formatting sdfs and then going ahead and
starting your cluster in case of a
vendor specific distribution such as
Cloudera or hotel works let me split it
here so you would have basically a
cluster management solution that would
help you in doing various things to set
up your cluster in case of a Cloudera
distribution of Hadoop you would have a
Cloudera installer file and then you
would have a Cloudera manager which will
help in deploying a Cloudera cluster so
downloading Hadoop related Parcels or
packages untiring them editing config
files formatting sdfs and basically
starting your cluster would be taken
care automatically by cloud manager in
case of hortonworks data platform it
would be ambari software that would be
used for deploying your cluster now no
matter which distribution of Hadoop you
winning you would still have the same
roles or processes which would be
running let me just briefly discuss here
about the terminologies so let me bring
up a file here which could explain these
details so let's look at this now here I
have documented how it would look in
different distributions of Hadoop so we
have Hadoop version 1 and 2 and let me
just look for files yeah so this is the
one now if you would look in version one
Apache Hadoop we would call demons or
processes and if you look at the names
we have the name node we have data nodes
we have job tracker we have secondary
name node and we have task records in
case of version 2 you would still call
them demons you could have one or
multiple name nodes if it was a
federation or a high availability kind
of setup you would have data nodes
instead of job tracker you would have
resource manager and then there is a
temporary demand called Mr App Master if
you had a high availability then
secondary name node would not be allowed
and if there was a federation kind of
setup secondary day node would still be
existing instead of task trackers you
would have known managers now in case of
vendor specific distribution such as
Cloud era whether you are talking about
older version or newer version which is
five point something you would have what
we call as roles so in Apache Hadoop we
call them demons in Cloud enough we call
them roles in hortonworks we call them
components but we are talking about the
same thing in case of Cloudera you would
have Services which would be part of the
cluster such as Impala Uzi spark Hive
hbase storm Hue SDS surface map reduce
yarn and so on similarly in Cotton works
also you would have these Services which
would be running in case of Apache
Hadoop we would not have so-called
Services seen but then the functionality
still exists and how is that that is
depending on these demons so basically
if you look in Apache Hadoop or Cloudera
or hortonworks everywhere we have these
demons and we have these would be
running on one or multiple nodes so we
could have different kind of setups and
just to briefly discuss on that so we
could have a name node running here a
resource manager running here and you
could have a secondary name node now
don't go by name it's not a second name
node we'll discuss about that later a
secondary name node and then we could
have data nodes running in machines so
this is one kind of setup where I am
talking about data node node manager and
here we have a data node node manager
and similarly we would have that on one
more machine now this is one kind of
setup this is not the only setup
possible you could have a different
setup where on the same machines you
could decide to have data node and node
manager running on every node however
that would not be preferred because you
the slaves and the Masters to run on the
same machine
will you could have a high availability
kind of setup which basically is good
for cluster and for an organization
where you could have name Node 1 which
would be running here and say we can
have a name node 2 running here we could
have a resource manager to running here
and then we could have a resource
manager one running here and I could
still have my data node and node manager
running in the same fashion now this
would not be allowed in a high
availability cluster because we have a
standby name node so this is one more
setup which is possible and then you
could have a single node setup which
could be everything having on the same
machine that is name node resource
manager secondary name node data node
and node manager so these are different
setups and if somebody asks which is a
preferable setup then I would still go
for either this one which has multiple
data nodes and node managers no not this
one or your high availability setup now
a single node or a pseudo distributed
cluster is basically good for testing or
some development kind of activities and
these are your distributed clusters now
if we closely see we basically have the
roles or components or demons running on
these machines to take care of mainly
two functionalities or two services that
is your hdfs which is your storage layer
and yarn which is your processing layer
so in a Hadoop cluster we have a storage
layer and we have a processing layer for
storage layer that is sdfs you basically
have one single master and you have
multiple slave processes running so I
could say for hdfs to work fine I should
have a name node running multiple data
nodes running and then a secondary name
node running for on as a service that is
my processing layer I would need minimum
one resource manager and multiple node
managers running on one or multiple
machines these are the two processing
these are the two layers one being the
storage layer and one being the
processing layer and for your storage
layer you have these processes wherein
your name node is the master and your
data nodes and secondary name node can
be considered as the slaves you have
your resource manager as the master for
your processing layer and node managers
are the Slate now this is briefly about
the setup which could be in a particular
environment now say the Hadoop related
packages or Parcels have been downloaded
whether it is a vendor-specific
distribution or Apache Hadoop cluster
say untiring of that is done editing of
config file system formatting is done
and your cluster is started in that case
your cluster would be up and running and
then we could basically have and once
your cluster is up we would have the
sdfs up and running ready to allow to
store data now let's imagine that this
is my sdfs which is up and running or
sdfs as a Services up and running let's
imagine that these are my nodes which is
M1 M2 M3 M4 and M4 now while the config
files were edited for each of these
nodes it would have been defined which
processes will run and we would have
also given path where these processes
will store the data so let's say that
this is my M1 this is M2 M3 M4 and M5 M3
M4 and M5 have data nodes running so we
would have defined a path let's call it
say ABC slash data ABC slash again data
or D and ABC so this is the path which
has been defined on every node to store
Hadoop related data and on M1 where we
would have name node running we would
have still defined a path let's call it
ABC slash n where name node kit storage
data so once your cluster is up and
running and in your sdfs is available we
can have our data stored on sdfs now
let's understand how does that work so
any client or an application which would
want to write data to the cluster needs
to interact with the master to begin
with say let's say client machine or an
API or an application
we interest Ed in the data now how would
name node know about available data
nodes so as soon as the cluster comes up
the first thing which happens is these
data nodes would be sending in their
heartbeat to the name node every three
seconds so data nodes send the heartbeat
to name node every three seconds to let
the name node know to let the master
know that they are available what does
name node do with this name node is
building a metadata in its Ram so it
already has some metadata in the disk
which was created as a result of
formatting sdfs and now we would have
metadata in Ram which is getting built
so initially name node would have
information of which data nodes are
available in the cluster now if a client
or an application wants to write data to
the cluster it gets in touch with the
name node inquiring about the slave
machines or inquiring about the data
nodes where the data can actually be
written your name no node refers to the
metadata collected so far and responds
back mentioning the data nodes or the
slave machines which are available for
data storage based on this response your
client takes the file to sdfs intending
to write it to the cluster now in Hadoop
we basically have two things one is the
default block size which we discussed
earlier is 128 MB so you have 128 MB as
the default block size which is
customizable and that can be changed
depending on the average data size and
you have the default replication which
is three now imagine the application or
API or client would want to write a file
which is 256 MB which is bigger than the
block size and this file has to be
broken down into what we call as blocks
which are The Logical entities so the
file is taken to the framework framework
will split the file into blocks so let's
imagine that this file is 256 MB it will
be broken down into two blocks B1 and B2
and these blocks will then be randomly
distributed to the slave machines that
is the data nodes so we would have block
one say getting stored here now you
could also have block one getting stored
on any of these data nodes so we would
have block one and then we could have
block 2 which could be written here and
then you would also have their replicas
sitting on the machine so the rule of
replication is you will never have two
identical blocks sitting on the same
node so if I have block 1 here then I
will never have a replica of block 1
again on the same machine so we will
have a replica of block 2 here block one
here and you have a replica of block to
here there is no differentiation between
which one is the original and which one
is the replica for the cluster all the
blocks are same and what also happens is
after the blocks are written your data
nodes are repeatedly sending their block
report to name node every 10 seconds and
this information is again updated in the
name nodes metadata so now name node
will have information what are the files
the files are broken down into what
blocks the blocks are residing on which
data nodes and this metadata is getting
built in the name node as data nodes
respond with their heartbeats and also
every 10 seconds with their block report
now we need to remember one point here
that name node is basically a master and
data nodes are the slave Hadoop as a
framework has been quoted in Java which
basically means that every directory
every file and file related block is
considered as an object which is getting
stored and for every object which name
node is tracking 150 bytes of name nodes
Ram is utilized which also tells us that
if the block sizes were smaller than 128
MB then for the same file we would have
more number of blocks which would mean
that name node Sanam would be over
utilized and overload name node so block
size has to be chosen with lot of
consideration on the Master's RAM and
what is the average size of the data now
this is how the data is distributed in
blocks and distributed across the data
nodes thus sdfs is very fault tolerant
because if any of the data nodes fail
then you still have other data nodes
which have the replica of data thus
making it possible to read the data and
access the data even if multiple data
nodes go down there are other internals
that is how data nodes are placed in
racks or what has rack awareness to do
in Block placement but we will learn on
all that later so this is how your sdfs
works now there are many other things
such as the read and write process which
is taken care by your methods such as
open and create which internally are
playing a role when a
also facts like replication is a
sequential process but right activity is
parallel so we can learn more
functionality of sdfs in later sessions
but this is in brief on how sdfs Works
how your file is split into blocks and
the blocks are getting stored on the
disks note that the block contains the
data so even if you have a file which is
one kilobyte and if this file is to be
stored on sdfs it will still use a block
of 128 MB which does not mean that there
is a disk space getting wasted when the
block resides on the data nodes disk it
will only occupy one kilobyte of space
for the data it has which means that
block is a logical entity to take your
data to the disk and get it stored on
the disk this is how your sdfs works and
basically takes care of reliable data
storage across machines in distributed
fashion now that we have set up a two
node cluster which has two data nodes
two node managers one Master machine
let's try working with hdfs let's look
at quick commands which allow you to
write data to the cluster although there
are different ways the first way is
using commands the second is using an
IDE and where you could write your code
to hadoops file system API and the third
is a graphical user interface called Hue
which normally does not come with Apache
although it can work with Apache it
comes as a service with cloud data and
other distributions to work with hdfs we
need some sample data now there are
various websites from where you can get
some sample data so you can open up your
browser and let me
sample places or sample data set links
from where you can download bigger or
smaller files which then can be used to
write data to your sdfs although you
could just pick up a local file file or
you could create your own files and
write them to sdfs there are various
data set sites which can be used so for
example you can type in machine learning
UCI data sets and this is one link which
has more than 200 data sets which can be
used for even data science kind of
activities
for testing
test rhythms and you can download data
from here onto your local machine and
then from your local machine we will be
writing the data to hdfs which then
based on the size of the data splits the
data into blocks and your blocks would
get
underlying machine wherever data
node so I can click on this use UCI
machine learning repository or even just
on data sets to be very specific all we
need is some sample data sets you can
get Kegel data sets you can get data
sets from Yelp website which are bigger
data sets zipped and you can be using
them to write data to see how your file
split into multiple blocks now here we
have lot of data we can click on any one
link to begin with and here I have the
data folder shows me the sum file and I
can
just right click on this save link as
and save the file this is a simple text
file and we can say and you can get
various data sets from here you can even
go to Yelp data set challenge
it has bigger files which are zipped and
they can be downloaded so you can click
on download data set although I will not
start the downloading here you can just
given your name here just just some
email ID so some initials and you can
click on download
so this is one more website where you
can download the Json data sets which
are 3.6 gigabytes in compressed format
and then you photos which are in 7.2
gigabytes you can also say Gutenberg
download books and
in a website from where you can
the free data sets which are not very
huge so try to get some variety of data
sets in different formats which then you
can write to hdfs you can even use your
sdfs log files which already exist on
your machines and you don't need to
download anything from internet now
let's close this for now so I already
have some file let's copy this from home
HTC
downloads that was my file let's copy to
home HTC and here here file I can check
the permission so that's owned by SDC I
can even look into user local Hadoop
logs and there are some already log
files so you can even copy some of these
files using copy user local Hadoop logs
and let's take Hadoop data node related
log and let's copy to home HTC for our
testing and now we have some other files
now to work with hdfs as I said you can
always type in hdfs and that shows you
various options now we already used the
first option which is name node hyphen
format we also used HD DF admin to look
at how many nodes we have and to work
with your hdfs file system the option is
DFS so you would say hdfs DFS and then
you can always hit on enter which shows
you various options to work with your
distributed file system like creating a
directory copying some files getting
data from unsdfs to your local machine
changing the permissions changing the
replication and any other options so
here let's click on hdfs EFS let's
create a directory and let's call it my
data so this is a directory I am
creating on hdfs remember this directory
will not be seen on your local machine
it is a hdfs directory now if I look in
my route I don't see my data because
that's a sdfs directory which is
technically getting created on hdfs and
once you start writing data to hdfs the
relevant blocks will start getting
stored under ABC slash data so as of now
we can always keep a browser open to see
as we work with sdfs how does the data
propagate or come to your distributed
file system using the web interface
so here let's say HTTP slash slash
m150070 and that's my browser interface
let's click on utilities and let's click
on browse the file system which should
show the directory which we have created
although that does not have any data as
of now let's go in here and now we know
that we have some files here so I will
say hdfs DFS and this has two options
either I can say put or I can say copy
from local both of these do the same
thing and both of these you can find
here so we have put and we should have
copy from local that is you are copying
the data from your local machine to your
hdfs so now I will say copy from local I
will say take the ABA directory or file
and put it in my data so now I am
technically copying a file from my local
file system to my hdfs which is then
using again my local file systems path
so this is done let's quickly check by
clicking on my data if my file is
written now if you see this file is 187
kilobytes however this will still use a
block of 128 and in my previous sessions
I already explained why a block of 128
but when it goes to the disk only occupy
the space for the data the block has we
can quickly go in here and we can do a
hdfs DFS listing of my directory which
should have my file now and we can
quickly check if my file is replicated
either using the web interface which
says replication is to that means the
file related blocks should be getting
on the underlying data nodes we can
always click on overview if we would
want to browse and scroll down which
says there are three files and a
directory the file has one block so
totally you have four file system
objects we can scroll down to show or
see we have two live nodes and if you
see each node has one block we can
always look from our data path by
looking into ABC slash data slash
current that has block pool directory
that has current that has finalized that
has subdirectory subdirectory and the
block look at the size of the block it
is 191 which relates to the kilobyte
size which we are talking about
similarly we can check on M2 machine
which also has a data node running to
make sure our file related block is
replicated so look in blog pool current
finalized subdirectory subdirectory and
that shows me the same block which is
replicated with its meta file and with
the same size this ensures that my data
is getting replicated as per whatever
Factor we have set and my file is
written to hdfs and now I can also do a
complete listing I can just say a
recursive listing to show me all the
directories and all the files which I
have in my hdfs I think we are
forgetting something we are logged in as
root which is not right so we need to be
logged in as HTC and then we can try our
commands that is hpfs DFS iPhone LS
slash which should show me my directory
but then if you would want to do a
recursive listing of showing all the
directories and files you can just add
hyphen r so we have written a file to
hdfs in my data directory similarly I
can write using a put command last time
we used a copy from local I will use a
put command I will say let's take this
log file and let us write it to the my
data directory instead of copy from
local I am using a put command which
also allows me to write the file to my
htfs once this is done let's go to the
web UI and let's go to browse the file
system let's look in our directory and
that shows me that I have a log file
which is even smaller than the first
file which we wrote it is in kilobytes
however it is replicated it is using
still the block size and we can always
go back and look at the terminal to see
if for my new file I have a new block
which should be in my data path
you see this
I block which will also have an
application on machine into so this is
how you write one file or you could
write multiple files you could write
multiple files let's create some files
here so I have these log files let's say
Hadoop blog and let's give it a name
like this so now if you see I just
created multiple files and I can also
say sdfs DFS make directory let's say my
data too so I'm creating a new directory
on my sdfs and then I can even copy
multiple files or a complete directory
so all I need to do here is give a
matching pattern or a directory so I'll
say sdfs DFS put and and then I can say
take all the directories files and
to my data too remember this one file or
multiple files are following the
replication what has been set at a
cluster level now if we do a recursive
listing Well I would want to see all the
directories and the files which are
within my hdfs I see that I have a my
data directory and I also have a my data
2 directory which has multiple files and
all of the files within the directory
are following the replication set at a
cluster level so all of them are having
their blocks replicated twice we can
always go back here we can just do a
back and that shows me what I have in my
data too everything is replicated twice
what I have in my data is also
replicated twice now sdfs also has a
functionality where you can write data
with a different replication so let's
create one more directory and now let's
write data with a different replication
so the way I my files I follow the same
thing however now I will say hyphen DFS
dot replication and I will specify it as
one so I am passing in as argument and I
am specifying that I would want to write
data with the replication of just one so
my blocks will not be considered under
replicated and I am choosing to write
these files in this directory with the
replication of one we can check from our
browser interface by just doing a quick
refresh and here if I click on my data 3
it shows it follows an application of
one there is also a command to check
your replication status using hdfs so we
can say stfs fsck slash files slash
blocks and if I would want to see only
blocks or only files I could choose one
of them now here I have given a complete
sdfs file system I can replace that with
Slash my data so I can say my data three
this is what I would want to see and
this will show me within this directory
how many files I have how many blocks
does each file use and is it replicated
correctly so my fsck command gives me an
information that in my data 3 I have one
file which is 24 kilobytes uses one
block because it's very smaller than 1
only once so there is no situation of
over under misreplicated blocks we have
our files correctly replicated as they
were specified while writing same thing
can be seen for other files say my data
2 and that will tell me that all the
files within my data 2 were replicated
as per the default replication which was
at a cluster level and if you see here
everything is replicated twice we don't
have any over under misreplicated blocks
and my file system status is healthy so
this is the hdfs command to make sure
that your files are replicated correctly
so we have seen how your files are
written with default replication we have
seen how your files follow a dynamic
replication what you specify on the
command line now we can also see a
command which allows us to change
replication after the data has been
written so I can say set rep and I can
say path that is my data 3 where I
initially had all my file related blocks
replicated only once and now I'll say my
data 3 and I can specify that this
should follow a replication of 2 which
basically means that it will look into
the directory it will check all the
files which had only one block and now
it will create a replica of those blocks
across other data nodes we can always go
back and look into the data node logs to
see
happening we can also look from the
browser interface which initially was
showing an application of one and now it
shows a replication of 2 of 2 this is
the command to change the replication
after the data has been written so our
rights can have default replication we
can write files with a different
replication we can change the
replication after the data is written
not only that we can even get the data
from our hdfs to my local
and for that I'll give hdfs get command
or copy to local so I can say my data 3
and then I can bring it to my low
my home HDC using a get command so this
is to get data from your sdfs to your
local machine using a hdfs command now
if you see I should have a my data 3
directory which is in my local path and
we can look in my data 3 it has all the
files which we had on hdfs so we were
not only putting data from our local
machine to sdfs we also successfully
brought data from our hdfs to local
machine we have written data with
different replication we have checked
the status of nodes we have checked the
status of blocks and if we would want to
delete some data sdfs also has options
for that so I can say hdfs DFS remove
and I could also do a recursive remove
to remove everything including the
directory from my data 3. so this will
delete data from my hdfs however if
trash is enabled now thrash basically
means recycle bin which depends on a
property in the core hyphen site file we
have not enabled it if hash is enabled
then your data will be deleted from hdfs
however it will not be permanently
deleted it will sit in trash for a time
interval what admin has specified in the
regulation file so right now I deleted
thrash and this seems to be gone from
sdfs because we have not set up any
thresh however if a thrash is set up
then either you can decide to have your
data deleted and still be in trash so
that it could be restored or you could
delete it permanently using a skip
thrash option however remember this skip
thrash option makes more sense if the
thrash is enabled in your cluster and
you do not want your deleted data to be
string in trash even for whatever time
interval has been set because even if
the data sits in Flash and your thrash
is a directory structure on hdfs all the
data will still be replicated so we have
seen how you can write data to the
cluster how you can get data from
cluster to your local machine how you
can write it with different replications
how you can delete it from your cluster
and do a recursive listing of your
directories and files now there are
various other commands which can be used
for example something like copy or move
or change permissions and many more
thanks Ajay now we have Richard and Ajay
taking us through the Hadoop ecosystem
we will look into hdfs map reduce yarn
scoop height big and H base suppose you
have a library that has a collection of
huge number of books on each floor and
you want to count the total number of
books present on each floor what would
be your approach you could say I will do
it myself but then don't you think that
will take a lot of time and that's
obviously not an efficient way of
counting the number of books in this
huge collection on every floor by
yourself now there could be a different
approach or an alternative to that you
could think of asking three of your
friends or three of your colleagues and
you could then say if each friend could
count the books on every floor then
obviously that would make your work
faster and easier to count the books on
every floor now this is what we mean by
parallel processing So when you say
parallel processing in technical terms
you're talking about about using
multiple machines and each machine would
be contributing its RAM and CPU cores
for processing and your data would be
processed on multiple machines at the
same time now this type of process
involves parallel processing in our case
or in our library example where you
would have person one who would be
taking care of books on floor one and
Counting them person two on floor 2 then
you have someone on floor 3 and on floor
four so every individual would be
counting the books on every floor in
parallel so that reduces the time
consumed for this activity and then
there should be some mechanism where all
these Counts from every floor can be
aggregated so what is each person doing
here each person is mapping the data of
a particular floor or you can say each
person is doing a kind of activity or
basically a task on every floor and the
task is counting the books on every
floor now then you could have some
aggregation mechanism that could
basically reduce or summarize this total
count and in terms of map reduce we
would say that's the work of reducer so
when you talk about Hadoop map reduce it
processes data on different node
machines now this is the whole concept
of Hadoop framework right that you not
only have your data stored across
machines but you would also want to
process the data locally so instead of
transferring the data from one machine
to
you know bringing all the data together
into some central processing unit and
then processing it you would rather have
the data processed on the machines
wherever that is stored so we know in
case of Hadoop cluster we would have our
data stored on multiple data nodes on
their multiple disks and that is the
data which needs to be processed but the
requirement is that we want to process
this data as fast as possible and that
could be achieved by using parallel
processing now in case of mapreduce we
basically have the first phase which is
your mapping phase so in case of
mapreduce programming model you
basically have two phases one is mapping
and one is reducing now who takes care
of things in mapping phase it is a
mapper class and this mapper class has
the function which is provided by the
developer which takes care of these
individual map tasks which will work on
multiple nodes in parallel your reducer
class belongs to the reducing phase so a
reducing phase basically uses a reducer
class which provides a function that
will Aggregate and reduce the output of
different data nodes to generate the
final output now that's how your map
reduce Works using mapping and then
obviously reducing now you could have
some other kind of jobs which are map
only jobs wherein there is no reducing
required but we are not talking about
those we are talking about our
requirement where we would want to
process the data using mapping and
reducing especially when data is huge
when data is stored across multiple
machines and you would want to process
the data in parallel so when you talk
about mapreduce you could say it's a
programming model you could say
internally it's a processing engine of
Hadoop that allows you to process and
compute huge volumes of data and when we
say huge volumes of data we can talk
about terabytes we can talk about
petabytes exabytes and that amount of
data which may needs to be processed on
a huge cluster we could also use
mapreduce programming model and run a
produce algorithm in a local mode but
what does that mean if you would go for
a local mode it basically means it would
do all the mapping and reducing on the
same node using the processing capacity
that is RAM and CPU cores on the same
machine which is not really efficient in
fact we would want to have our map
reduce work on multiple nodes which
would obviously have mapping phase
followed by a reducing phase and
intermittently there would be data
generated there would be different other
phases which help this whole processing
so when you talk about Hadoop map reduce
you are mainly talking about two main
components or two main phases that is
mapping and reducing mapping taking care
of map tasks reducing taking care of
reduced tasks so you would have your
data which would be stored on multiple
machines now when we talk about data
data could be in different formats we
could or the developer could specify
what is the format which needs to be
used to understand the data which is
coming in that data then goes through
the mapping internally there would be
some shuffling sorting and then reducing
which gives you your final output so the
way we Access Data from sdfs or the way
our data is getting stored on sdfs we
have our input data which would have one
or multiple files in one or multiple
directories and your final output is
also stored on sdfs to be accessed to be
looked into and to see if the processing
was done correctly so this is how it
looks so you have the input data which
would then be worked upon by multiple
map tasks now how many map tasks that
basically depends on the file that
depends on the input format so normally
we know that in a Hadoop cluster you
would have a file which is broken down
into blocks depending on its size so the
default block size is 128 MB which can
then still be customized based on your
average size of data which is getting
stored on the cluster so if I have
really huge files which are getting
stored on the cluster I would certainly
set a higher block size so that my every
file does not have huge number of blocks
creating a load on name nodes Ram
because that's tracking the number of
elements in your cluster or number of
objects in your cluster so depending on
your file size your file would be split
into multiple chunks and for every Chunk
we would have a map task running now
what is this map task doing that is
specified within the mapper class so
within the mapper class you have the
mapper function which basically says
what each of these map tasks has to do
on each of the chunks which has to be
processed this data intermittently is
written to stfs where it is sorted and
software old and then you have internal
phases such as partitioner which which
decides how many reduce Stars would be
used or what data goes to which reducer
you could also have a combiner phase
which is like a mini reducer doing the
same reduce operation before it reaches
reduce then you have your reducing phase
which is taken care by a reducer class
and internally the reducer function
provided by developers which would have
reduced task running on the data which
comes as an output from map task
finally your output is then generated
which is stored on sdfs now in case of
Hadoop it accepts data in different
formats your data could be in compressed
format your data could be in parquet
your data could be in Avro text CSV tsv
binary format and all of these formats
are supported however remember if you
are talking about data being compressed
then you have to also look into what
kind of splitability the compression
mechanism supports otherwise when
mapreduce processing happens it would
take the complete file as one chunk to
be processed so sdfs accepts input data
in different formats this data is stored
in sdfs and that is basically our input
which is then passing through the
mapping phase now what is mapping phase
doing as I said it reads record by
record depending on the input format it
reads the data so we have multiple map
tasks running on multiple chunks once
this data is being read this is broken
down into individual elements when I say
individual element I could say this is
my list of key value pairs so your
records based on some kind of delimiter
or without delimiter are broken down
into individual elements and thus your
Mac creates key value pairs now these
key value pairs are not my final output
these these key value pairs are
basically a list of elements which will
then be subjected to further processing
so you would have internally shuffling
and sorting of data so that all the
relevant key value pairs are brought
together which basically benefits the
processing and then you have your
reducing which Aggregates the key value
pairs into set of smaller tuples or
tuples as you finally your output is
getting stored in the designated
directory as a list of aggregated key
value pairs which gives you your output
so when we talk about mapreduce one of
the key factors here is the parallel
processing which it can offer so we know
that we our data is getting stored
across multiple data nodes and you would
have huge volume of data which is split
and randomly distributed across data
nodes and this is the data which needs
to be processed and the best way would
be parallel processing so you could have
your data getting stored on multiple
data nodes or multiple slave nodes in
each slave node would have again one or
multiple disks to process this data
basically we have to go for parallel
processing approach we
have mapreduce now let's look at the
mapreduce workflow to understand how it
works so basically you have your input
data stored on sdfs now this is the data
which needs to be processed it is stored
in input files and the processing which
you want can be done on one single file
or it can be done on a directory which
has multiple files you could also later
have multiple outputs merged which we
achieve by using something called as
chaining of mappers so here you have
your data getting stored on sdfs now
input format is basically something to
define the input specification and how
the input files will be split so there
are various input formats now we can
search for that so we can go to Google
and we can basically search for Hadoop
map reduce Yahoo tutorial this is one of
the good links and if I look into this
link I can search for different input
formats and output formats so let's
search for input format so when we talk
about input format you basically have
something to Define how input files are
split so input files are split up and
read based on what input format is
specified so this is a class that
provides following functionality it
selects the files or other objects that
should be used for input it defines the
input split that break a file into tasks
provides a factory for record reader
objects that read the file so there are
different formats if you look in the
table here and you can see that the text
input format is the default format which
reads lines of a text file and each line
is considered as a record here the key
is the byte offset of the line and the
value is the line content it says you
can have key value input format which
parses lines into key value pairs
everything up to the first tab character
is the key and the remainder is the line
you could also have sequence file input
format which basically works on binary
format so you have input format and in
the same way you can also also search
for output format which takes care of
how the data is handled after the
processing is done so the key value
pairs provided to this output collector
are then returned to Output files the
way they are written is governed by
output format so it functions pretty
much like input format as described in
earlier right so we could set what is
the output format to be followed and
again you have text output sequence file
output format null output format and so
on so these are different classes which
take care of how your data is handled
when it is being read for processing or
how is the data being written when the
processing is done so based on the input
format the file is broken down into
splits and this logically represents the
data to be processed by individual map
tasks or you could say individual mapper
functions so you could have one or
multiple splits which need to be
processed depending on the file size
depending on what properties have been
set now once this is done you have your
input splits which are subjected to
mapping phase internally you have a
record reader which communicates with
the input split and converts the data
into key value pairs suitable to be read
by mapper and what is mapper doing it is
basically working on these key value
pairs the map task giving you an
intermittent output which would then be
forwarded for further processing now
once that is done and we have these key
value pairs which is being worked upon
my map your map task as a part of your
mapper function are generating your key
value pairs which are your intermediate
outputs to be processed further now you
could have as I said a combiner phase or
internally a mini reducer phase Now
combiner does not have its own class so
combiner basically uses the same class
as the reducer class provided by the
developer and its main work is to do the
reducing or its main work is to do some
kind of mini aggregation on the key
value pairs which were generated by map
so once the data is coming in from the
combiner then we have internally a
partitioner phase which decides how
outputs from combiners are sent to the
reducers or you could also say that even
if I did not have a combiner partitioner
would decide based on the keys and
values based on the type of keys how
many reducers would be required or how
many reduced tasks would be required to
work on
output which was generated by map task
now once partitioner has decided that
then your data would be then sorted and
shuffled which is then fed into the
reducer so when you talk about your
reducer it would basically have one or
multiple reduced tasks now that depends
on what or what partitioner decided or
determined for your data to be processed
it can also depend on the configuration
properties which have been set to decide
how many radio Stars should be used now
internally all this data is obviously
going through sorting and shuffling so
that you're reducing your aggregation
becomes an easier task once we have this
done we basically have the reducer which
is the code for the reducer is provided
by the developer and all the
intermediate data has then to be agree
to give you a final output which would
then be stored on sdfs and who does this
you have an internal record writer which
writes these output key value pairs from
reducer to the output files now this is
how your map reduce Works wherein the
final output data can be not only stored
but then read or accessed from hdfs or
even used as an input for further
mapreduce kind of processing so this is
how it overall looks so you basically
have your data stored on sdfs based on
input format you have the splits then
you have record reader which gives your
data to the mapping phase which is then
taken care by your mapper function and
mapper function basically means one or
multiple map tasks working on your
chunks of data you could have a combiner
phase which is optional which is not
mandatory then you have a partitional
phase which decides on how many reduce
tasks or how many reducers would be used
to work on your data internally there is
sorting and shuffling of data happening
and then basically based on your output
format your record reader will write the
output
sdfs directory internally you could also
remember that data is being
improv the output of each task which is
being worked upon stored locally however
we do not access the data directly from
data nodes we access it from sdfs so our
output is stored on sdfs so that is your
map reduce workflow when you talk about
mapreduce architecture now this is how
it would look so you would have
basically a edge node or a client
program or an API which intends to
process some data so it submits the job
to the job tracker or you can say
resource manager in case of Hadoop yarn
framework right now before this step we
can also say that an interaction with
name node would have already happened
which would have given information of
data nodes which have the relevant data
stored then your master processor so in
Hadoop version one we had job tracker
and then the slaves were called task
trackers in Hadoop version 2 instead of
job tracker you have resource manager
answer of task trackers you have node
managers so basically your resource
manager has to assign the job to the
task trackers or node managers so your
node managers as we discussed in yarn
are basically taking care of processing
which happens on every node so
internally there is all of this work
Happening by resource manager node
managers and application Master then you
can refer to the yarn based tutorial to
understand more about that so here your
processing Master is basically breaking
down the application into tasks what it
does internally is once your application
is submitted your application to be run
on yarn processing framework is handled
by resource manager now forget about the
yarn part as of now I mean who does the
negotiating of resources who allocates
them how does the process happen on the
nodes right so that's all to do with how
yarn handles the processing request so
you have your data which is stored in
sdfs broken down into one or multiple
splits depending on the input format
which has been specified by the
developer your input splits are to be
worked upon by your one or multiple map
tasks which will be running within the
container on the nodes basically you
have the resources which are being
utilized so for each map task you would
have some amount of ram which will be
utilized and then further the same data
which has to go through reducing phase
that is your reduced task will also be
utilizing some RAM and CPU cores now
internally you have these functions
which take care of deciding on number of
reducers doing a mini reduce and
basically reading and processing the
data from multiple data nodes now this
is how your mapreduce programming model
makes parallel processing work or
processes your data which is stored
across multiple machines finally you
have your output which is getting stored
on hdfs
foreign
so let's have a quick demo on mapreduce
and see how it works on a Hadoop cluster
now we have discussed briefly about
mapreduce which contains mainly two
phases that is your mapping phase and
your reducing phase and mapping phase is
taken care by your mapper function and
your reducing phase is taken care by
your reducer function now in between we
also have sorting and shuffling and then
you have other phases which is
partitioner and combiner and we will
discuss about all those in detail in
later sessions but let's have a quick
demo on how we can run a mapreduce which
is already existing as a package jar
file within your Apache Hadoop cluster
or even in your cloud data cluster now
we can build our own mapreduce programs
we can package them as jar transfer them
to the cluster and then run it on a
Hadoop cluster on yarn or we could be
using already provided default program
so let's see where they are now these
are my two machines which I have brought
up and basically this would have my
Apache Hadoop cluster running now we can
just do a Simple Start hyphen all dot SH
now I know that this script is
deprecated and it says instead you start
DFS and start yarn but then it will
still take care of static of my cluster
on these two nodes where I would have
one single name node two data nodes one
secondary name node one resource manager
and two node managers now if you have
any doubt in how this cluster came up
you can always look at the previous
sessions where we had a walkthrough in
setting up a cluster on Apache and then
you could also have your cluster running
using Less Than 3 GB of your total
machine RAM and you could have a Apache
cluster running on your machine now once
this cluster comes up we will also have
a look at the web UI which is available
for name node and resource manager now
based on the settings what we have given
our UI will show us details of our
cluster but remember the UI is only to
browse now here my cluster has come up I
can just do a JPS to look at Java
related processes and that will show me
what are the processes which are running
on C1 which is your data node resource
manager node manager and name node and
on my M1 machine which is my second
machine which I have configured here I
can always do a JP s and that shows me
the processes running which also means
that my cluster is up with two data
nodes with two node managers and here I
can have a look at my web UI so I can
just do a refresh and the same thing
with this one just do a refresh so I had
already opened the web pages so you can
always access the web UI using your name
nodes host name and 570 Port it tells me
what is my cluster ID what is my block
pool ID it gives you information of what
is the space usage how many live nodes
you have and you can even browse your
file system so I have put in a lot of
data here I can click on browse the file
system and this basically shows me
multiple directories and these
directories have one or multiple files
which we will use for our mapreduce
example now if you see here these are my
directories which have some sample files
although these files are very small like
8.7 kilobytes if you look into this
directory if you look into this I have
just pulled in some of my Hadoop logs
and I have put it on my sdfs these are a
little bigger files and then we also
have some other data which we can see
here and this is data which I've
downloaded from web now we can either
run a map reduce on a single file or in
a directory which contains multiple
files let's look at that before looking
at demo on map reduce also remember map
reduce will create a output directory
and we need to have that directory
created plus we need to have the
permissions to run the mapreduce job so
by default since I'm running it using
admin ID I should not have any problem
but then if you intend to run mapreduce
with a different user then obviously you
will have to ask the admin or you will
have to give the user permission to read
and write from sdfs so this is the
directory which I've created which will
contain my output once the map reduce
job finishes and this is my cluster file
system if you look on this UI this shows
me about my yarn which is available for
taking care of any processing it as of
now shows that I have total of a GB
memory and I have 8v cores now that can
be depending on what configuration we
have set or how many nodes are available
we can look at nodes which are available
and that shows me I have two node
managers running each has 8 GB memory
and 8 V cores now that's not true
actually but then we have not set the
configurations for node managers and
that's why it takes the default
properties that is 8GB RAM and 8B cores
now this is my yarn UI we can also look
at scheduler which basically shows me
the different cues if they have been
configured where you will have to run
the jobs we'll discuss about all these
in later in detail now let's go back to
our terminal and let's see where we can
find some sample applications which we
can run on the cluster so once I go to
the terminal I can well submit the
mapreduce job from any terminal now here
I know that my Hadoop
directory is here and within Hadoop you
have various directories we have
discussed that in binaries you have the
commands which you can run in s bin you
basically have the startup scripts and
here you also notice there is a share
directory in the end if you look in the
Shell directory you would find Hadoop
and within Hadoop you have various sub
directories in which we will look for
map reduce now this map reduce directory
has some sample jar files which we can
use to run a mapreduce on the cluster
similarly if you are working on a cloud
error cluster you would have to go into
opt Cloudera parcel CDH slash lib and in
that you would have directories for sdfs
mapreduce or sdfs yarn where you can
still find the same jars it is basically
a package which contains your multiple
applications now how do we run a map
reduce we can just type in Hadoop and
hit enter and that shows me that I have
an option called jar which can be used
to run a jar file now at any point of
time if you would want to see what are
the different classes which are
available in a particular particular jar
you could always do a jar minus xvf for
example I could say jar x v f and I
could say user local Hadoop group share
Hadoop map reduce and then list down
your jar file so I'll say Hadoop
mapreduce examples and if I do this this
should basically unpack it to show me
what classes are available within this
particular jar and it has done this it
has created a meta file and it has
created a org directory we can see that
by doing a LS and here if you look in LS
org since I ran the command from your
phone directory I can look into org
patchy Hadoop examples which shows me
the classes which I have and those
classes contain which mapper or reducer
class
so it might not be just mapper and
reducer but you can always have a look
so for example I am targeting to use
word count program which does a word
count on files and gives me a list of
words and how many times they occur in a
particular file or in set of files and
this shows me that what are the classes
which belong to word count so we have a
in some reducer so this is my reducer
class I have tokenizer mapper that is my
mapper class right and basically this is
what is used these classes are used if
you run a word count now there are many
other programs which are part of this
jar file and we can expand and see that
so I can say Hadoop jar and give your
path so I'll say Hadoop jar user local
Hadoop share Hadoop map reduce Hadoop
map reduce examples and if I hit on
enter that will show me what are the
inbuilt classes which are already
available now these are certain things
which we can use now there are other
files also for example I can look at
Hadoop and here we can look at the jar
files which we have in this particular
path so this is one Hadoop map reduce
examples which you can use you can
always look in other jar files like you
can look for Hadoop mapreduce client job
client and then you can look at the
tests one so that is also an interesting
one so you can always look into Hadoop
map reduce client job client and then
you have something ending with tests so
if I would have tried this one using my
Hadoop jar command so in my previous
example when we did this it was showing
me all the classes which are available
and that already has a word count now
there are other good programs which you
can try like terrajen to generate dummy
data Terra saw to check your sorting
performance and so on and tell a
validate to validate the results
similarly we can also do a Hadoop jar as
I said on Hadoop mapreduce I think that
was client and then we have job client
and then test STAR now this has a lot of
other classes which can be used or
programs which can be used for doing a
stress testing or checking your cluster
status and so on one of them interesting
one is test the fsio but let's not get
into all the details and first instance
let's see how we can run a map reduce
now if I would want to run a map reduce
I need to give Hadoop jar and then my
jar file and if I hit on enter it would
say it needs the input and output it
needs which class you want to run so for
example I would say word count and again
if I hit on enter it tells me that you
need to give me some input and output to
process and obviously this processing
will be happening on cluster that is our
yarn processing framework unless and
until you would want to run this job in
a local mode so there is a possibility
that you can run the job in a local mode
but let's first try how it runs on the
cluster so how do we do that now here I
can do a hdfs LS slash command to see
what I have on my sdfs now through my UI
I was already showing you that we have
set of files and directories which we
can use to process now we can take up
one single file so for example if I pick
up new data and I can look into the
files here what we have and we can
basically run a map reduce on a single
file or multiple files so let's take
this file whatever that contains and I
would like to do a word count so that I
get a list of words and their occurrence
in this file so let me just copy this
now I also need my output to be written
and that will be written here so here if
I want to run a map reduce I can say
Hadoop which we can pull out from
history so Hadoop jar word count now I
need to give my input so that will be
new data and then I will give this file
which we just copied now I am going to
run the word count only on a single file
and I will basically have my output
which will be stored in this directory
the directory which I have created
already Mr output so let's do this
output and this is fair enough now you
can give many other properties you can
specify how many map jobs you want to
run how many reduce jobs you want to run
do you want your output to be compressed
do you want your output to be merged or
many other properties can be defined
when you are specifying word count and
then you can pass in an argument to pass
properties from the command line which
will affect your output now once I go
ahead and submit this this is basically
running a simple inbuilt mapreduce job
on our Hadoop cluster now obviously
internally it will be looking for name
node now we have some issue here and it
says the output already exists what does
that mean so it basically means that
Hadoop will create an output for you you
just need to give a name but then you
don't need to create it so let's give
let's append the output with number one
and then let's go ahead and run this so
I've submitted this command now this can
also be done in background if you would
want to run multiple jobs on your
cluster at the same time so it takes
total input paths to processes one so
that is there is only one split on which
your job has to work now it will
internally try to contact your resource
manager and basically this is done so
here we can have a look and we can see
some counters here now what I also see
is for some property which it is missing
it has run the job but it has run in a
local mode it has run in a local mode so
although we have submitted so this might
be related to my yarn settings and we
can check that so if I do a refresh when
I have run my application it has
completed it would have created an
output but the only thing is it did not
interact with your yarn it did not
interact with your resource manager we
can check those properties and here if
we look into the job it basically tells
me that it went for mapping and reducing
it would have created an output it was
on my file but then it ran in a local
mode it ran in a local mode so mapreduce
remember is a programming model right
now now if you run it on yarn you get
the facilities of running it on a
cluster where yarn takes care of
resource management if you don't run it
on yarn and run it on a local mode it
will use your machine 's RAM and CPU
cores for processing but then we can
quickly look at the output and then we
can also try running this on yarn so if
I look into my hdfs and if I look into
my output Mr output that's the directory
which was not used actually let's look
into the other directory which is ending
with one and that should show me the
output created by this map reduce
although it ran in a local mode it
fetched an input file from your sdfs and
it would have created output in sdfs now
that's my part file which is created and
if you look at part minus r minus these
zeros if you would have more than one
reducer running then you would have
multiple such files created we can look
into this what does this file contain
which should have my word count and here
I can say cat which basically shows me
what is the output created by my map
reduce let's have a look into this so
the file which we gave for processing
has been broken down and now we have the
list of words which occur in the file
plus a count of those words so if there
is some word which is in is more then it
shows me the count so this is a list of
my words and the count for that so this
is how we run a sample mapreduce job I
will also show you how we can run it on
yeah now let's run mapreduce on yarn and
initially when we tried running a map
reduce it did not hit yarn but it ran in
a local mode and that was because there
was a property which had to be changed
in mapreduce hyphen site file so
basically if you look into this file the
error was that I had given a property
which says
mapred.framework.name and that was not
the right property name and it was
ignored and that's why it ran local mode
so I change the property to mapreduce
dot framework.name restarted my cluster
and everything should be fine now and
that mapred hyphen site file has also
been copied across the nodes now to run
a map reduce on a Hadoop cluster so that
it uses yarn and yarn takes care of
resource allocation on one or multiple
machines so I'm just changing the output
here and now I will submit this job
which should first connect to the
resource manager and if it connects to
the resource manager that means our job
will be run using yarn on the Clusters
rather than in a local mode so now we
have to wait for this application to
internally connect to resource manager
and once it starts there we can always
go back to the web UI and check if our
application has reached yarn so it shows
me that there is one input part to be
processed that's my job ID that's my
application ID which you can even
monitor status from the command line now
here the job has been submitted so let's
go back here and just do a Refresh on my
yarn UI which should show me the new
application which is submitted it tells
me that it is an accepted State
application master has already started
and if you click on this link it will
also give you more details of how many
map and reduced tasks would run so as of
now it says the application Master is
running it would
this node which is m one we can always
look into the logs we can see then there
is a one task attempt which is being
made and now if I go back to my terminal
I will see that it is waiting to get
some resources from the cluster and once
it gets the resources it will first
start with the mapping phase where the
mapper function runs it does the map
tasks one or multiple depending on the
splits so right now we have one file and
one split so we will have just one map
task running and once the mapping phase
completes then it will get into reducing
which will finally give me my output so
we can be toggling through these
sessions so here I can just do a refresh
to see what is happening with my
application is it proceeding is it still
waiting for resource manager to allocate
some resources now just couple of
minutes back I tested this application
on yarn and we can see that my first
application completed successfully and
here we will have to give some time so
that yarn can allocate the resources now
if the resources were used by some other
application they will have to be freed
up now internally yarn takes care of all
that which we will learn more detail in
yarn or you might have already followed
the yarn based session now here we will
have to just give it some more time and
let's see if my application proceeds
with the resources what Yan can allocate
to it sometimes you can also see a
slowness in what web UI shows up and
that can be related to the amount of
memory you have allocated to your notes
now for Apache we can have less amount
of memory and we can still run the
cluster and as I said the memory which
shows up here 16 GB and 16 cores is not
the true one those are the default
settings right but then my yarn should
be able to facilitate running of this
application let's just give it couple of
seconds and then let's look into the
output here again I had to make some
change changes in the settings because
our application was not getting enough
resources and then basically I restarted
my cluster now let's submit the
application again to the cluster which
first should contact the resource
manager and then basically the map and
reduce process should start so here I
have submitted an application it is
connecting to the resource manager and
then basically it will start internally
an app master that is application Master
it is looking for the number of splits
which is one it's getting the
application ID and it basically then
starts running the job it also gives you
a tracking URL to look at the output and
now we show go back and look at our yarn
UI if our application shows up here and
we will have to give it a couple of
seconds when it can get the final status
change to running and that's where my
application will be getting resources
now if you closely notice here I have
allocated specific amount of memory that
is 1.5 GB for node manager on every node
and I have basically given two cores
each which my machines also have and my
yarn should be utilizing these resources
rather than going for default now the
application has started moving and we
can see the progress bar here which
basically will show what is happening
and if we go back to the terminal it
will show that first it went in deciding
map and reduce it goes for map once the
mapping phase completes then the
reducing phase will come into existence
and here my job has completed so now it
has basically used we can always look at
how many map and reduced SAS were run it
shows me that there was one map and one
reduced task now with the number of map
tasks depends on the number of splits
and we had just one file which is less
than 128 MB so that was one split to be
processed and reduced task is internally
decided by the reducer or depending on
what kind of property has been set in
Hadoop config files now it also tells me
how many input records were read which
basically means these were the number of
lines in the file it tells me output
records which gives me the number of
total words in the file now there might
be duplicates and that which is
processed by internal combiner further
processing or forwarding that
information to reducer and basically
reducer works on 335 records gives us a
list of words and their count now if I
do a refresh here this would obviously
show my application is completed it says
succeeded you can always click on
application to look for more information
it tells me where it ran now we do not
have a history server running as of now
otherwise we can always access more
information so this leads to history
server where all your applications are
stored but I can click on this attempt
tasks and this will basically show me
the history URL or you can always look
into the logs so this is how you can
submit a sample application which is
inbuilt which is available in the jar on
your Hadoop cluster and that will
utilize your cluster to run now you
could always as I said when you are
running a particular job remember to
change the output directory and if you
would not want it to be processing a
single individual file you could also
point it to a directory that basically
means it will have multiple files and
depending on the file sizes there would
be multiple splits and according to that
multiple map tasks will be selected so
if I click on this this would submit my
second application to the a cluster
which should first connect to resource
manager then resource manager has to
start an application Master now here we
are targeting 10 splits now you have to
sometimes give couple of seconds in your
machines so that the resources which
were used are internally already freed
up so that your cluster can pick it up
and then your yarn can take care of
resources so right now my application is
an undefined status but then as soon as
my yarn provides it the resources we
will have the application running on our
yarn cluster so it has already started
if you see it is going further then it
would launch 10 map tasks and it would
the number of reduced tasks would be
decided on either the way your data is
or based on the properties which have
been set at your cluster level let's
just do a quick refresh here on my yarn
UI to show me the progress also take
care that when you are submitting your
application you need to have have the
output directory mentioned however to
not create it Hadoop will create that
for you now this is how you run a map
reduce without specifying properties but
then you can specify more properties you
can look into what are the things which
can be changed for your mapper and
reducer or basically having a combiner
class which can do a mini reducing and
all those things can be done so we will
learn about that in the later sessions
now we will compare Hadoop version one
that is with mapreduce version one we
will understand and learn about the
limitations of Hadoop version 1 what is
the need of yarn what is yarn what kind
of workloads can be running on yarn what
are yarn components what is yarn
architecture and finally we will see a
demo on yarn so Hadoop version 1 or map
reduce version one well that's outdated
now and nobody is using Hadoop version
one but it would be good to understand
what was in Hadoop version 1 and what
were the limitations of Hadoop version 1
which brought in the thought for the
future processing layer that is yarn now
when we talk about Hadoop we already
know that Hadoop is a framework and
Hadoop has two layers one is your
storage layer that is your hdfs Hadoop
distributed file system which allows for
distributed storage and processing which
allows fault tolerance by inbuilt
replication and which basically allows
you to store huge amount of data across
multiple commodity machines when we talk
about processing we know that map reduce
is the oldest and the most mature
processing programming model which
basically takes care of your data
processing on your distributed file
system so in Hadoop version 1 mapreduce
performed both data processing and
resource management and that's how it
was problematic in mapreduce we had
basically when we talk about the
processing layer we had the master which
was called job tracker and then you had
the slaves which were the task records
so your job tracker was taking care of
allocating resources it was performing
scheduling and even monitoring the jobs
it basically was taking care of a
signing map and reduced tasks to the
jobs running on task trackers and task
trackers which were co-located with data
nodes were response useful for
processing the jobs so task trackers
were the slaves for the processing layer
which reported their progress to the job
tracker so this is what was happening in
Hadoop version one now when we talk
about Hadoop version 1 we would have say
client machines or an API or an
application which basically submits the
job to the master that is job tracker
now obviously we cannot forget that
there would already be an involvement
from name node which basically tells
which are the machines or which are the
data nodes where the data is already
stored now once the job submission
happens to the job tracker job tracker
being the master demon for taking care
of your processing request and also
resource management job scheduling would
then be interacting with your multiple
task trackers which would be running on
multiple machines so each machine would
have a task tracker running and that
task tracker which is a processing slave
would be co-located with the data nodes
now we know that in case of Hadoop you
have the concept of moving the
processing to wherever the data is
stored rather than moving the data to
the processing layer so we would have
task trackers which would be running on
multiple machines and these task
trackers would be responsible for
handling the tasks what are these tasks
these are the application which is
broken down into smaller tasks which
would work on the data which is
respectively stored on that particular
node now these were your slave demons
right so your job tracker was not only
tracking the resources so your task
trackers were sending heartbeats they
were sending in packets and information
to the job tracker which would then be
knowing how many resources and when we
talk about resources we are talking
about the CPU course we are talking
about the ram which would be available
on every node so task trackers would be
sending in their resource information to
job tracker and your job tracker would
be already aware of what amount of
resources are available on a particular
node how loaded a particular node is
what kind of work could be given to the
task tracker so job tracker was taking
care of resource management and it was
also breaking the application into tasks
and doing the job scheduling part assign
different tasks to these slave domains
that is your task records so job tracker
was eventually overburdened right
because it was managing jobs it was
tracking the resources from multiple
task trackers and basically it was
taking care of job scheduling so job
tracker would be overburdened and in a
case if job tracker would fail then it
would affect the overall processing so
if the master is skilled if the master
demand dies then the processing cannot
proceed now this was one of the
limitations of Hadoop person one so when
you talk about scalability that is the
capability to scale due to a single job
tracker scalability would be hitting a
bottleneck you cannot have a cluster
size of more than 4000 nodes and cannot
run more than 40 000 concurrent tasks
now that's just a number we could always
look into the individual resources which
each machine was having and then we can
come up with an appropriate number
however with a single job tracker there
was no horizontal scalability for the
processing layer because we had single
processing Master now when we talk about
availability job tracker as I mentioned
would be a single point of failure now
any failure kills all the queued and
running jobs and jobs would have to be
resubmitted now why would we want that
in a distributed platform in a cluster
which has hundreds and thousands of
machines we would want a processing
layer which can handle huge amount of
processing which could be more scalable
which could be more available and could
handle different kind of workloads when
it comes to resource utilization now if
you would have a predefined number of
map and reduced slots for each task
tracker you would have issues which
would relate to resource utilization and
that again is putting a burden on the
master which is tracking these resources
which has to assign jobs which can run
on multiple machines in parallel so
limitations in running non-map reduce
applications now that was one more
limitation of Hadoop version 1 and
mapreduce that the only kind of
processing you could do is mapreduce and
mapreduce programming model although it
is good it is oldest it has matured over
a period of time but then it is very
rigid you will have to go for mapping
and reducing approach and that was the
only kind of processing which could be
done in Hadoop version one so when it
comes to doing a real-time analysis or
doing ad hoc query or doing a graph
based processing or massive parallel
processing there were limitations
because that could not be done in Hadoop
version 1 which was having mapreduce
version 1 as the processing component
now that brings us to the execution so
your client submits an application to
resource manager resource manager
allocates a container or I would say
this is at a high level right resource
manager is negotiating the resources and
internally who is negotiating the
resources it is your applications
manager who is granting this request it
is node manager and that's how we can
say resource manager allocates a
container application Master basically
contacts the related node manager
because it needs to use the containers
node manager is the one which launches
the container or basically gives those
resources within which an application
can run an application Master itself
will then accommodate itself in one of
the containers and then use other
containers for the processing and it is
within these containers the actual
execution happens now that could be a
map does that could be a reduced task
that could be a spark executor taking
care of smart tasks and many other
processing
all
so before we look into the demo on how
yarn works I would suggest looking into
one of the blogs from Cloudera so you
can just look for yarn untangling and
this is really a good blog which
basically talks about the overall
functionality which I explained just now
so as we mentioned here so you basically
have the master process you have the
worker process which basically takes
care of your processing your resource
manager being the master and node
manager being the slave this also talks
about the resources which each node
manager has it talks about the yarn
configuration file where you give all
these properties it basically shows you
node manager which reports the amount of
resources it has to resource manager now
remember if worker node shows 18 to 8
CPU cores and 128 GB RAM and if your
node manager says 64 V cores and RAM 128
GB then that's not the total capacity
city of your node it has some portion of
your node which is allocated to node
manager now once your node manager
reports that your resource manager is
requesting for containers based on the
application what is a container it is
basically a logical name given to a
combination of vcore and RAM it is
within this container where you would
have basically the process running so
once your application starts and once
node manager is guaranteed these
containers your application or your
resource manager has basically already
started an application Master within the
container and what does that application
Master do it uses the other containers
where the tasks would run so this is a
very good blog which you can refer to
and this also talks about mapreduce if
you have already followed the mapreduce
tutorials in past then you would know
about the different kind of tasks that
is map and reduce and these map and
reduced tasks could be running within
the container in one or multiple as said
it could be map task it could be reduced
task it could be a spark based task
which would be running within the
container now once the task finishes
basically that resources can be freed up
so the container is released and the
resources are given back to yarn so that
it can take care of further processing
if you'll further look in this blog you
can also look into the part 2 of it
where you talk mainly about
configuration settings you can always
look into this which talks about why and
how much resources are allocated to the
node manager it basically talks about
your operating system overhead it talks
about other services it talks about
Cloudera or hortonworks related Services
running and other processes which might
be running and based on that some
portion of RAM and CPU cores would be
allocated to node manager so that's how
it would be done in the yarn hyphen site
file and this basically shows you what
is the total amount of memory and CPU
course which is allocated to node
then within every machine where you have
a node manager running on every machine
in the yarn hyphen site file you would
have such properties which would say
what is the minimum container size what
is the maximum container size in terms
of ram what is the minimum for CPU cores
what is the maximum for CPU cores and
what is the incremental size in where
RAM and CPU cores can increment so these
are some of the properties which Define
how containers are allocated for your
application request so have a look at
this and this could be a good
information which talks about different
properties now you can look further
which talks about scheduling if you look
in this particular blog which also talks
about scheduling where it talks about
scheduling in yarn which talks about
Fair scheduler or you basically having
different cues in which allocations can
be done you also have different ways in
which queues can be managed and
different schedulers can be used so you
can always look at the series of blog
you can also be checking for yarn
schedulers and then search for uh Hadoop
definitive guide and that could give you
some information on how it looks when
you look for Hadoop definitive guide so
if you look into this book which talks
about the different resources as I
mentioned so you could have a fee for
scheduler that is first in first out
which basically means if a long running
application is submitted to the cluster
all other small running applications
will have to wait there is no other way
but that would not be a preferred option
if you look in V4 scheduler if you look
for capacity scheduler which basically
means that you could have different
queues created and those queues would
have resources allocated so then you
could have a production queue where
production jobs are running in a
particular queue which has fixed amount
of resources allocated you could have a
development queue where development jobs
are running and both of them are running
in parallel you could then also look
into Fair scheduler which basically
means again multiple applications could
be running on the cluster however they
would have a fair share so when I say
fair share in brief what it means is if
I had given 50 percent of resources to a
queue for production and 50 of resources
for a queue of development and if both
of them are running in parallel then
they would have access to 50 percent of
cluster resources however if one of the
queue is unutilized then second queue
can utilize all cluster resources so
look into the fair scheduling part it
also shows you about how allocations can
be given and you can learn more about
schedulers and how queues can be used
for managing multiple applications now
we will spend some time in looking into
few ways or few quick ways in
interacting with yarn in the form of a
demo to understand and learn on how yarn
works we can look into a particular
cluster now here we have a designated
cluster which can be used you could be
using the similar kind of commands on
your Apache based cluster or a Cloudera
quick start VM if you already have or if
you have a Cloudera or a hortonworks
cluster running there are different ways
in which we can interact with yarn and
we can look at the information one is
basically looking into the admin console
so if I would look into Cloud error
manager which is basically an admin
console for a cloudera's distribution of
Hadoop similarly you could have a
hortonworks cluster then access to the
admin console so if you have even read
access for your cluster and if you have
the admin console then you can search
for yarn as a service which is running
you can click on yarn as a service and
that gives you different tabs so you
have the instances which tells basically
what are the different roles for your
yarn service running so we have here
multiple node managers now some of them
show in stop status but that's nothing
to worry so we have three and six node
managers we have resource manager which
is one but then that can also be in a
high availability where you can have
active and standby you also have a job
history server which would show you the
applications once they have completed
now you can look at the yarn
configurations and as I was explaining
you can always look for the properties
which are related to the allocation so
you can here search for course and that
should show you the properties which
talk about allocations so here if we see
we can be looking for yarn App mapreduce
application Master resource CPU course
what is the CPU course allocated for map
reduce map task reduce task you can be
looking at yarn node manager resource
CPU course which basically says every
node manager on every node would be
allocated with six CPU cores and the
container sizing is with minimum
allocation of one CPU core and the
maximum could be two CPU cores similarly
you could also be searching for memory
allocation and here you could then
scroll down to see what kind of memory
allocation has been done for the node
manager so if we look further it should
give me information of node manager
which basically says here that the
container minimum allocation is 2GB the
maximum is 3 GB and we can look at node
manager which has been given 25 GB per
node so it's a combination of this
memory and CPU cores which is the total
amount of resources which have been
allocated to every node manager now we
can always look into applications tab
that would show us different
applications which are submitted on yarn
for example right now we see there is a
spark application running which is
basically a user who is using spark
shell which has triggered a application
on spark and that is running on yarn you
can look at different applications
workload information you can always do a
search based on the number of days how
many applications have run and so on you
can always go to the web UI and you can
be searching for the resource manager
web UI and if you have access to that it
will give you overall information of
your cluster so this basically says that
here we have 100 GB memory allocated so
that could be say 25 GB per node and if
we have four node managers running and
we have 24 cores which is six cores per
node if we look further here into nodes
I could get more information so this
tells me that I have four node managers
running and node managers basically have
25 GB memory allocated per node and 6
cores out of which some portion is being
utilized we can always look at the
scheduler here which can give us
information what kind of scheduler has
been allocated so we basically see that
there is just a root q and within root
you have default queue and you have
basically users queue based on different
users we can always scroll here and that
can give us information if it is a fair
share so here we see that my root dot
default has 50 of resources and the
other queue also has 50 percent of
resources which also gives me an idea
that a fair scheduler is being used we
can always confirm that if we are using
a fair scheduler or a capacity scheduler
which takes care of allocation so search
for scheduler and that should give you
some understanding of what kind of
scheduler is being used and what are the
locations given for that particular
scheduler so here we have Fair scheduler
it shows me you have under root you have
the root queue which has been given 100
capacity and then you have within that
default which also takes 100 so this is
how you can understand about yarn by
looking into the yarn web UI you can be
looking into the configurations you can
look at application Asians you can
always look at different actions now
since we do not have admin access the
only information we have is to download
the client configuration we can always
look at the history server which can
give us information of all the
applications which have successfully
completed now this is from your yarn UI
what I can also do is I can be going
into Hue which is the web interface and
your web interface also basically allows
you to look into the jobs so you can
click on Hue web UI and if you have
access to that it should show up or you
should have a way to get to your Hue
which is a graphical user interface
mainly comes with your Cloud era you can
also configure that with Apache
hortonworks has a different way of
giving you the web UI access you can
click and get into Hue and that is also
one way where you can look at yarn you
can look at the jobs which are running
if there are some issues with it and
these these are your web interfaces so
either you look from yarn web UI or here
in Hue you have something called as job
browser which can also give you
information of your different
applications which might have run so
here I can just remove this one which
should basically give me a list of all
the different kind of jobs or workflows
which were run so either it was a spark
based application or it was a map reduce
or it was coming from hive so here I
have list of all the applications and it
says this was a map reduce this was a
spark something was killed something was
successful and this was basically a
probably a hive query which triggered a
mapreduce job you can click on the
application and that tells you how many
tasks were run for it so there was a map
task which ran for it you can get into
the metadata information which you can
obviously you can also look from the
yarn UI to look into your applications
which can give you a detailed
information of if it was a map reduce
how many map and reduced tasks were run
what were the different counters if it
was a spark application it can let you
follow through spark history server or
job history server so you can always use
the web UI to look into the jobs you can
be finding in a lot of useful
information here you can also be looking
at how many resources were used and what
happened to the job was it successful
did it fail and what was the job status
now apart from web UI which always you
might not have access to so in a
particular cluster in a production
cluster there might be restrictions and
the organization might not have access
given to all the users to graphical user
interface like you or might be you would
not have access to the Cloudera manager
or admin console because probably
organization is managing multiple
clusters using this admin console so the
one way which you would have access is
is your web console or basically your
Edge node or client machine from where
you can connect to the cluster and then
you can be working so let's login here
and now here we can give different
commands so this is the command line
from where you can have access to
different details you can always check
by just typing in mapred which gives you
different options where you can look at
the mapreduce related jobs you can look
at different queues if there are queues
configured you can look at the history
server or you can also be doing some
admin stuff provided you have access so
for example if I just say mapred and
queue here this basically gives me an
option says what would you want to do
would you want to list all the queues do
you want information on a particular
queue so let's try a list and that
should give you different queues which
were being used now here we know that
per user a queue dynamically gets
created which is under root dot users
and that gives me what is the status of
the queue what is the capacity has there
been any kind of maximum capacity or
capping done so we get to see a huge
list of cues which dynamically get
configured in this environment and then
you also look at your root dot default I
could have also picked up one particular
queue and I could have said show me the
jobs so I could do that now here we can
also give a yarn command so let me just
clear the screen and I will say yarn and
that shows me different options so apart
from your web interface something like
web UI apart from your Yarns web UI you
could also be looking for information
using yarn commands here so these are
some list of commands which we can check
now you can just type in yarn and
version if you would want to see the
version which basically gives you
information of what is the Hadoop
version being used and what what is the
vendor specific distribution version so
here we see we are working on cloud
errors distribution 5.14 which is
internally using Hadoop 2.6 now
similarly you can be doing a yarn
application list so if you give this
that could be an exhaustive list of all
the applications which are running or
applications which have completed so
here we don't see any applications
because right now probably there are no
applications which are running it also
shows you you could be pulling out
different status such as submitted
accepted or running now you could also
say I would want to see the services
that have finished running so I could
say yarn application list and app States
as finished so here we could be using
our Command so I could say yarn
application list and then I would want
to see the app states which gives me the
applications which have finished and we
would want to list all the applications
which finished now they that might be
applications which succeeded right and
there is a huge list of application
which is coming in from the history
server which is basically showing you
the huge list of applications which have
completed so this is one way and then
you could also be searching for one
particular application if you would want
to search a particular application if
you have the application ID you could
always be doing a grip that's a simple
way I could say basically let's pick up
this one and if I would want to search
for this if I would want more details on
this I could obviously do that by
calling in my previous command and you
could do a grip if that's what you want
to do and if you would want to search is
there any application which is in the
list of my applications that shows my
application I could pull out more
information about my application so I
could look at the log files for a
particular application by giving the
application ID so I could say yarn law
logs now that's an option and every time
anytime you have a doubt just hit enter
it will always give you options what you
need to give with a particular command
so I can say yarn logs application ID
now we copied an application ID and we
could just give it here we could give
other options like app owner or if you
would want to get into the Container
details or if you would want to check on
a particular node now here I'm giving
yarn logs and then I'm pointing it to an
application ID and it says the log
aggregation has not completed might be
this was might be this was an
application which was triggered based on
a particular interactive cell or based
on a particular query so there is no log
existing for this particular application
you can always look at the status of an
application you can kill an application
so here you can be saying yarn yarn
application and then what would you want
to do with an application hit and enter
it shows you the different options so we
just tried app States you could always
look at the last one which says status
and then for my status I could be giving
my application ID so that tells me what
is the status of this application it
connects to the resource manager it
tells me what's the application ID what
kind of application it was who ran it
which was the queue where the job was
running what was the start and end time
what is the progress the status of it if
it is finished or if it has succeeded
and then it basically gives me also an
information of where the application
master was running it gives me the
information where you can find this job
details in history server if you are
interested in looking into it also gives
you a aggregate resource allocation
which tells how much GB memory and how
many core seconds it used so this is
basically looking out at the application
details now I could kill an application
if that application was already running
I could always do a yarn application
minus skill and then I could be giving
my application now I could try killing
this however it would say the
application is already finished if I had
an application running and if my
application was already given an
application ID by The Source manager I
Could Just Kill it I can also say yarn
node list which would give me a list of
the node managers now this is what we
were looking from the yarn web UI and we
were pulling out the information so we
can get this and kind of information
from your command line always remember
and always try to be well accustomed
with the command line so you can do
various things from the command line and
then obviously you have the web uis
which can help you with a graphical
interface easily able to access things
now you could be also starting the
resource manager which we would not be
doing here because we are already
running in a cluster so you could give a
yarn resource manager you could get the
logs or of resource manager if you would
want by giving yarn demin so we can try
that so you can say yarn and then demon
so it says it does not find the demon so
you can give something like this get
level and here I will have to give the
node and the IP address where you want
to check the logs of resource manager so
you could be giving this for which we
will have to then get into Cloudera
manager to look into the nodes and the
IP address you could be giving a command
something like this which basically
gives you the level of the log which you
have and I got this resource manager
address from the web UI now I can be
giving in this command to look into the
demon log and it basically says you
would want to look at the resource
manager related log and you have the log
4J which is being used for logging the
kind of level which has been said is
info which can again be changed in the
way you are logging the information now
you can try any other commands also from
yarn for example looking at the yarn RM
admin so you can always do a yarn RM
admin and this basically gives you a lot
of other informations like refreshing
the queues or refreshing the nodes or
basically looking at the admin ACLS or
getting groups so you could always get
group names for a particular user now we
could search for a particular user such
as yarn or hdfs itself so I could just
say here I would want get groups and
then I could be searching for say
username hdfs so that tells me sdfs
belongs to a Hadoop group similarly you
could search for say mapred or you could
search for yarn so these are service
related users which automatically get
created and you can pull out information
related to these you can always do a
refresh nodes kind of command and that
is mainly done internally this can be
useful when you are doing commissioning
decommissioning but then in case of
Cloudera or hortonworks kind of cluster
you would not be manually giving this
command because if you are doing a
commissioning decommissioning from an
admin console and if you are an
administrator then you could just
restart the services which are affected
and that will take care of this but if
you were working in an Apache cluster
and if you are doing commissioning
decommissioning then you would be using
in two commands refresh notes and
basically that's for refreshing the
nodes which should not be used for
processing and similarly you could have
a command refresh notes which comes with
stfs so these are different options
which you can use with your yarn on the
command line you could also be using
curl commands to get more information
about your cluster by giving curl minus
X and then basically pointing out to
your resource manager web UI address now
here I would like to print out the
cluster related metrics and I could just
simply do this which basically gives me
a high level information of how many
applications were submitted how many are
pending what is the reserved resources
what is the available amount of memory
or CPU cores and all the information
similarly you can be using the same curl
commands to get more information like
scheduler information so you would just
replace the metrics with scheduler and
you could get the information of the
different queues now that's a huge list
we can cancel this and that would give
me a list of all the queues which are
allocated and what are the resources
allocated for each queue you could also
get cluster information on application
IDs and Status running of applications
running in yarn so you would have to
replace the last bit of it and you would
say I would want to look at the
applications and that gives me a huge
list of applications then you can do a
grip and you can be filtering out
specific application related information
similarly you can be looking at the
notes so you can always be looking at
node specific information which gives
you how many nodes you have but this
could be mainly used when you have an
application which wants to or a web
application which wants to use a curl
command and would want to get
information about your cluster from an
HTTP interface now when it comes to
application we can basically try running
a simple or a sample mapreduce job which
could then be triggered on yarn and it
would use the resources now I can look
at my application here and I can be
looking into my specific directory which
is this one which should have lot of
files and directory is which we have
here now I could pick up one of these
and I could be using a simple example to
do some processing let's take up this
file so there is a file and I could run
a simple word count or I could be
running a hive query which triggers a
mapreduce job I could even run a spark
application which would then show that
the application is running on the
cluster so for example if I would say
spark to Shell now I know that this is
an interactive way of working with spark
but this internally triggers a spark
submit and this runs an application so
here when you do a spark 2 Shell by
default it will contact yarn so it gets
an application ID it is running on yarn
with the master being yarn and now I
have access to the interactive way of
working with spark now if I go and look
into applications I should be able to
see my application which has been
started here and it shows up here so
this is my application 3827
which has been started on yarn and as of
now we can also look into the yarn UI
and that shows me the application which
has been started which basically has one
running container which has one CPU core
allocated 2GB RAM and it's in progress
although we are not doing anything there
so we can always look at our
applications from the yarn UI or as I
mentioned from your applications tab
within yarn Services which gives us the
information and you can even click on
this application to follow and see more
information but you should be given
access to that now this is just a simple
application which I triggered using
spark shell similarly I can basically be
running a map reduce now to run a map
reduce I can say Hadoop jar and that
basically needs a class so we can look
for the default path which is opt
Cloudera Parcels CDH lib Hadoop map
produce Hadoop map reduce examples and
then we can look at this particular jar
file and if I hit on enter it shows me
the different classes which are part of
this jar and here I would like to use
word count so I could just give this I
could say word count now remember I
could run the job in a particular queue
by giving in an argument here so I could
say minus d mapred dot job dot Q dot
name and then I can point my job to a
particular queue I can even give
different arguments in saying I would
want my mapreduce output to be
compressed or I want it to be stored in
a particular directory and so on so here
I have the word count and then basically
what I can be doing is I can be pointing
into a particular input path and then I
can have my output which can be getting
stored here again a directory which we
need to choose and I will say output new
and I can submit my job now once I have
submitted my job it connects to resource
manager it basically Gets a Job ID it
gets an application ID it shows you from
where you can track your application you
can always go to the yarn UI and you can
be looking at your application and the
resources it is using so my application
was not a big one and it has already
completed it triggered one map task it
launched one reduce task it was working
on around 12 466 records where you have
then the output of map which is these
many number of output records which was
then taken by a combiner and finally by
a reducer which basically gives you the
output so this is my yarn application
which has completed now I could be
looking into the yarn UI and if my job
has completed you might not see your
application here so as of now it shows
up here the word count which I ran it
also shows me my previous Sparks job it
shows me my application is completed and
if you would want further information on
this you can click and go to the history
server if you have been given access to
it or directly go to the history server
web UI where your application shows up
it shows how many map and reduced tasks
it was running you can click on this
particular application which basically
gives you information of your map and
reduce tasks you can look at different
counters for your application right you
can always look at map specific tasks
you can always look into one particular
task what it did on which node it was
running or you can be looking at the
complete application log so you can
always click on the logs and here you
have click here for full log which gives
you the information and you can always
look for your application which can give
you information of App Master being
launched or you could have search for
the word container so you could see a
job which needs one or multiple
containers and then you could say
container is being requested then you
could see container is being allocated
then you can see what is the container
size and then basically your task moves
from initializing to running in the
container and finally you can even
search for release which will tell you
that the container was released so you
can always look into the log for more
information so this is how you can
interact with yarn this is how you can
interact with your command line to look
for more information or using your yarn
web UI my name is Richard kirschner I am
with the simply learned team that's
www.simplylearn.com get certified get
ahead today we're going to dive in on
scoop one of the many features of the
Hadoop ecosystem for the Hadoop file
system what's in it for you today we're
going to cover the need for scoop what
is scoop scoop features scoop
architecture scoop imp Port scoop export
scoop processing and then finally we'll
have a little Hands-On demo on Scoops
you can see what it looks like so where
does the need for scoop come in in our
big data Hadoop file system processing
huge volumes of data requires loading
data from diverse sources into Hadoop
cluster you can see here we have our
data processing and this process of
loading data from the heterogeneous
sources comes with a set of challenges
so what are the challenges maintaining
data consistency ensuring efficient
utilization of resources especially when
you're talking about big data we can
certainly use up the resources when
importing terabytes and petabytes of
data over the course of time loading
bulk data to Hadoop was not possible
it's one of the big challenges that came
up when they first had the Hadoop file
system going and loading data using
script was very slow in other words
you'd write a script in whatever
language you're in and then it would
very slowly load each piece and parse it
in so the solution scoop scoop helped in
overcoming all the challenges to
traditional approach and could lead bulk
data from rdbms to Hadoop very easily so
think your Enterprise server you want to
take the from MySQL or your SQL and you
want to bring that data into your Hadoop
Warehouse your data filing system and
that's where scoob comes in so what
exactly is scoop scoop is a tool used to
transfer bulk of data between Hadoop and
external data stores such as relational
databases and MySQL server or the
Microsoft SQL server or MySQL server so
scoop equals SQL plus Hadoop and you can
see here we have our rdbms all the data
we have stored on there and then your
scoop is the middle ground and brings
the import into the Hadoop file system
it also is one of the features that goes
out and grabs the data from Hadoop and
exports it back out into an rdbms let's
take a look at scoop features scoop
features has parallel Import and Export
it has import results of SQL query
connect vectors for all major rdbms
databases Kerberos security integration
provides full and incremental load so we
look at parallel Import and Export scoop
uses yarn yet another resource
negotiator framework to Import and
Export data this provides fault
Tolerance on a top of parallelism scoop
allows us to import the result return
from an SQL carry into the Hadoop file
system or the hdfs and you can see here
where the import results of SQL query
come in school provides connectors for
multiple relational database management
system rdbms's databases such as MySQL
and Microsoft SQL server and it has
connectors for all major rdbms databases
scoop supports Kerberos computer network
Authentication Protocol that allows
nodes communicating over a non-secure
network to prove their identity to one
another in a secure manner scoop can
load the whole table or parts of the
table by a single command hence it
supports full and incremental load this
stick a little deeper into the scoop
architecture we have our client in this
case a hooded wizard behind his laptop
you never know who's going to be
accessing the Hadoop cluster and the
client comes in and sends their command
which goes into scoop the client submits
the import export command to import or
export data data from different
databases is fetched by scoop and so we
have our Enterprise data warehouse
document based systems you have connect
connector for your data warehouse a
connector for document based systems
which reaches out to those two entities
and we have our connector for the rdbms
so connectors help in working with a
range of popular databases multiple
mappers perform map tasks to load the
data onto hdfs the Hadoop file system
and you can see here we have the map
task if you remember from Hadoop Hadoop
is based on map reduce because we're not
reducing the data we're just mapping it
over it only accesses the mappers and it
opens up multiple mappers to do parallel
processing and you can see here the HD
DFS hbase Hive is where the target is
for this particular one similarly
multiple map tests will export the data
from hdfs onto rdbms using scoop export
command so just like you can import it
you can now export it using the multiple
map routines scoop import so here we
have our rdbms data store and we have
the folders on there so maybe it's your
company's database maybe it's an archive
at Google with all the searches going on
whatever it is usually you think with
scoop you think SQL you think MySQL
server or Microsoft SQL Server that kind
of setup so it gathers the metadata and
you see the scoop import so introspect
database to gather metadata primary key
information and then it submits so you
can see submits map only job remember we
talked about mapreduce it only needs the
map side of it because we're not
reducing the data we're just mapping it
over scoop device the input data set
into splits and uses individual map
tests to push the splits into hdfs so
right into the Hadoop file system and
you can see down on the right is kind of
a small depiction of a Hadoop cluster
and then you have scoop export so we're
going to go the other direction and with
the other direction you have your Hadoop
file system storage which is your Hadoop
cluster you have your scoop job and each
one of those clusters then gets a map
mapper comes out to each one of the
computers it has data on it so the first
step is you've got to gather the
metadata so step one you gather the
metadata step two submits map only job
introspect database to gather metadata
primary key information scoop divides
the input data set into splits and uses
individual map tests to push the splits
to rdbms scoop will export Hadoop files
back to rdms tables and you can think of
this in a number of different manners
one of them would be if you're restoring
a backup from the Hadoop file system
into your Enterprise machines there's
certainly many others as far as
exploring data and data science so as we
dig a little deeper into scoop input we
have our connect our jdbc and our URL so
specify the jdbc connect string
connecting manager we specify The
Connection Manager class to use you can
see here driver with the class name
manually specify the jdbc driver class
to use Hadoop map reduce home directory
override Hadoop mapped home username set
authentication username and of course
help print uses instructions and with
the export you'll see that we can
specify the jdbc connect string specify
The Connection Manager class to use
manually specify jdbc driver class to
use you do have to let it know to
override the Hadoop map reduce home and
that's true on both of these and set
authentication username and finally you
can print out all your help setups so
you can see the format for scoop is
pretty straightforward both Import and
Export so let's continue on our path and
look at scoop processing and what the
computer goes through for that and we
talk about school processing first scoop
runs in the Hadoop cluster it Imports
data from the rdbms the nosql database
to the Hadoop file system so remember it
might not be importing the data from a
rdbms it might actually be coming from a
nosql and there's many out there it uses
mappers to slice the incoming data into
multiple formats and load the data into
hdfs it exports data back into an rdbms
while making sure that the schema of the
data in the database is maintained so
now that we've looked at the basic
commands in our scoop in the scoop
processing or at least the basics as far
as theory is concerned let's just jump
in and take a look at a demo on scoop
[Music]
for this demo I'm going to use our
Cloudera quick start if you've been
watching our other demos we've done
you'll see that we've been using that
pretty consistently certainly this will
work in any of your your Horton sandbox
which is also a single node testing
machine Cloudera is one of um there's a
Docker version instead of virtual box
and you can also set up your own Hadoop
cluster plan a little extra time if
you're not an admin it's actually a
pretty significant Endeavor for an admin
if you've been admitting Linux machines
for a very long time and you know a lot
of the commands I find for most admins
it takes them about two to four hours
the first time they go in and create a
virtual machine and set up their own
Hadoop in this case though I mean you're
just learning and getting set up best to
start with Cloudera Cloudera also
includes an install version of MySQL
that way you don't have to worry install
the the SQL version for importing data
from and two once you're in the Cloudera
quick start you'll see it opens a nice
Centos Linux interface and it has a
desktop setup on there this is really
nice for learnings here not just looking
at command lines and from in here it
should open up by default to Hue if not
you can click on Hue here's a kind of a
fun little web-based interface under Hue
I can go under query I can pick an
editor and we'll go right down to scoop
so now I'm just going to load the scoop
editor in our Hue now I'm going to
switch over and do this all in command
line I just want to show that you can
actually do this in a hue through the
web-based interface the reason I like to
do the command line is specifically on
my computer it runs much quicker or if I
do the command line here and I run it it
tends to have an extra lag or an added
layer in it so for this we're going to
go ahead and open our command line the
second reason I do this is we're going
to need to go ahead and edit our MySQL
so we have something to scoop in other
words I don't have anything going in
there and of course we zoom in we'll
zoom in this and increase the size of
our screen so for this demo our Hands-On
I'm going to use Oracle virtualbox
manager and the Cloudera quick start if
you're not familiar with this we do have
another tutorial we put out and you can
send a note in the YouTube video below
and let our team know and they'll send
you a link or come visit
www.simplylearn.com now this creates a
Linux box on my Windows computer so
we're going to be in Linux and it'll be
the Cloudera version with scoop it will
also be using MySQL MySQL server once
inside the Cloudera virtual box we'll go
under the Hue editor now we're going to
do everything in terminal window I just
want you to be aware that under the Hue
editor you can go under query editor and
you'll see as we come down here here's
our scoop on this so you can run your
Scoop from in here now before we do this
we have to do a little exploration in my
SQL and MySQL server that way we know
what data is coming in so let me go
ahead and open up a terminal window in
Cloudera you have a terminal window at
the top here that you can just click on
and open it up and let me just go ahead
and zoom in on here we'll go View and
zoom in now to get into MySQL server you
typically type in MySQL and this part
will depend on your setup now the
Cloudera quick start comes up that the
username is root and the password is
Cloudera kind of a strange Quirk is that
you can put a space between the minus U
and the root but not between the minus p
and the Cloudera usually you'd put in a
minus capital P and then it prompts you
for your password on here for this demo
I don't worry too much about you knowing
the password on that so we'll just go
right into MySQL server since this is
the standard password for this quick
start and you can see we're now into
MySQL and we're going to do just a
couple of quick commands in here there's
show databases and you follow by the
semicolon that's standard in most of
these shell commands so it knows it's
the end of your shell command and you'll
see in here in the quick start Cloudera
quick start the MySQL comes with a
standard set of databases these are just
some of these have to do like with the
uzi which is the uzi part of Hadoop
where others of these like customers and
employee fees and stuff like that those
are just for demo purposes they come as
a standard setup in there so that people
going in for the first time have a
database to play with which is really
good for us so we don't have to recreate
those databases and you will see in the
list here we have a retail underscore DB
and then we can simply do uh use retail
underscore DB this will set that as a
default in MySQL and then we want to go
ahead and show the tables and if we show
the tables you can see under the
database the retail DB database we have
categories customers departments order
items orders products so there's a
number of tables in here and we're going
to go ahead and just use a standard SQL
command and if you did our Hive language
you'll note remember it's the same for
hql also on this we're just going to
select star everything from departments
so there's our departments table and
we're going to list everything on the
Departments table and you'll see we've
got six lines in here and it has a
department ID and a department name two
for Fitness three for Footwear so on and
so forth now at this point I can just go
ahead and exit but it's kind of nice to
have this data up here so we can look at
it and flip back and forth between the
screens so I'm going to open up another
terminal window and we'll go ahead and
zoom in on this also and it isn't too
important for this particular setup but
it's always kind of fun to know what
your setup you're working with what is
your host name and so we'll go ahead and
just type that in this is a Linux
command and it's uh hostname minus F and
you see we're on quick start Cloudera no
surprise there now this next command is
going to be a little bit longer because
we're going to be doing our first scoop
command and I want to do two of them
we're going to list databases and list
tables it's going to take just a moment
to get through this because there's a
bunch of stuff going on here so we have
scoop we have list databases we have
connect and under the connect command we
need to let it know how we're connecting
we're going to use the jdbc this is a
very standard one jdbc MySQL so you'll
see that if you're doing an SQL database
that's how you start it off with and
then the next part this is where you
have to go look it up it's however it
was created so if your admin created a
MySQL server with a certain setup that's
what you have to go by and you'll see
that usually they list this as localhost
so you'll see something like localhost
sometimes there's a lot of different
formats but the most common is either
localhost or the actual connection so so
in this case we want to go ahead and do
quick start
3306 and so quick start is the name of
the localhost database and how it's
hosted on here and when you set up the
quick start for for Hadoop under
Cloudera it's Port 3306 is where that's
coming in so that's where all that's
coming from and so there's our path for
that and then we have to put in our
password we typically typed password if
you look it up password on the cloud era
quick start is Cloudera and we have to
also let it know the username and again
if you're doing this you'd probably put
in a minus Capital you can actually just
do it for a prompt Cloud for the
password so if you leave that out it'll
prompt you but for this doesn't really
matter I don't care if you see my
password it's the default one for
Cloudera quick start and then the
username on here is simply root and then
we're going to put our semicolon at the
end and so we have here our full setup
and we go ahead and list the databases
and you'll see you may get some warnings
on here I haven't run the updates on the
quick start I suggest you're not running
the updates either if you're doing this
for the first time because it'll do some
reformatting on there and it quickly
pops up and you can see here's all of
our the tables we went in there and if
we go back to on the previous window we
should see that these tables match so
here we come in and here we have our
databases and you can see back up here
where we had the CM customers employees
and so on so the databases match and
then we want to go ahead and list the
tables for a specific database so let's
go ahead and do that I'm a very lazy
typist so I'll put the up arrow in and
you can see here scoop list databases
we're just going to go back and change
this from databases to list tables so we
want to list the tables in here same
connection so most the connection is the
same except we need to know which tables
we're listing an interesting fact is you
can create a table without being under a
database so if you left this blank it
will show the open tables that aren't
connected directly to a database or
under a database but what we want to do
is right past this last slash on the 33
306 we want to put that retail
underscore DB because that's the
database we're going to be working with
and this will go in there and show the
tables listed under that database and
here we go we got categories customers
departments order items and products if
we flip back here real quick there it is
the same thing we had we had categories
customers departments order items and so
on and so let's go ahead and run our
first import command and again I'm that
lazy typer so we're going to do scoop
and instead of list tables we want to go
ahead and import so there's our import
command and so once we have our import
command in there then we need to tell it
exactly what we're going to import so
everything else is the same we're
importing from the retail DB so we keep
that and then at the very end we're
going to tag on dash dash table that
tells it so we can tell it what table
we're importing from and we're going to
import departments
there we go so this is pretty
straightforward because what's nice
about this is you can see the commands
are the same I got the same connection
um I change it for the whatever database
I'm in then I come in here our password
and the username are going to be the
same that's all under the MySQL server
setup and then we let it know what table
we're entering in we run this and this
is going to actually go through the
mapper process in Hadoop so this is a
mapping process it takes the data and it
Maps it up to different parts in the
setup in Hadoop on there and then saves
that data into the Hadoop file system
and it does take it a moment to zip
through which I kind of skipped over for
you since it is running a you know it's
designed to run across the cluster not
on a single node so when you're running
on a single node it's going to run slow
even if you dedicate a couple cores to
it I think I put dedicated four cores to
this one and so you can see right down
here we get to the end it's now mapped
in that information and then we can go
in here we can go under and flip back to
our Hue and under Hue on the top I have
there's databases the second icon over
is your Hadoop file system and we can go
in here and look at the Hadoop file
system and you'll see it show up
underneath our documents there it is
departments Cloudera departments and you
can see there's always a delay when I'm
working in Hue which I don't like and
that's the quick start issue that's not
necessarily running out on a server when
I'm running it on a server you pretty
much have to run through some kind of
server interface I still prefer the
terminal window it still runs a lot
quicker but we'll flip back on over here
to the command line and we can do the
Hadoop type in the Hadoop fs and then
list minus LS and if we run this you'll
see underneath our Hadoop file system
there is our departments which has been
added in and we can also do Hadoop fs
and this is kind of interesting for
those who've gone through the Hadoop
file system everything you'll you'll
recognize this on here I'm going to list
it the contents of departments and
you'll see underneath departments we
have part part m0001
002003 and so this is interesting
because this is how Hadoop saves these
files and this is in the file system
this is not in Hive so we didn't
directly import this into Hive we put
this in the Hadoop file system depending
on what you're doing you would then
write the schema for Hive to look at the
Hadoop file system certainly visit our
Hive tutorial for more information on
hive specific uh so you can see in here
are different files that it forms that
are part of departments and we can do
something like this we can look at the
contents of one of these files FS minus
LS or a number of the files and we'll
simply do the full path which is user
Cloudera and then we already know the
next one is departments and then after
departments we're going to put slash
part star so this is going to see
anything that has part in it so we have
part Dash m000 and so on we can go ahead
and Cat music cat command or that list
command to bring those up and then we
can use the cat command to actually
display the contents and that's a a
Linux command Hadoop Linux command the
cat catenate not to be confused with
catatonic catastrophic there's a lot of
cat got your tongue and we see here
Fitness Footwear apparel that should
look really familiar because that's what
we had in our MySQL server when we went
in here we did a select all on here
there it is Fitness Footwear apparel
golf outdoors and fan shop and then of
course it's really important slip back
on over here to be able to tell it where
to put the data so we go back to our
import command so here's our scoop
import we have our connect we have the
DB underneath our connection our MySQL
server we have our password our username
the table going where it's going to I
mean the table where it's coming from
and then we can add a Target on here we
can put in Target Dash directory and you
do have to put the full path that's a
Hadoop thing it's a good practice to be
in and we're going to add it to
Department we'll just do Department one
and so here we now add a Target
directory in here in user Cloudera
this will take just a moment before so
I'll go ahead and skip over the process
since it's going to run very slowly it's
only running on like I said a couple
cores and it's also on a single node and
now we can do the uh Hadoop let's just
do the up Arrow file system list we want
just straight list and when we do the
Hadoop file system minus LS or list
you'll see that we now have Department
one and we can of course do a list
Department one and you can see we have
the files inside Department one and they
mirrored what we saw before with the
same files in there and the part mm0 and
so on if we were to look at it would be
the same thing we did before with the
cat so except instead of departments
we'd be Department one there we go
something that's going to come up with
the same data we had before now one of
the important things when you're
importing data and it's always a
question to ask is do you fill through
the data before it comes in do we want
to filter this data as it comes in so
we're not storing everything in our file
system you would think Hadoop Big Data
put it on all in there I know from
experience that putting it all in there
can turn a couple hundred terabytes into
a petabyte very rapidly and suddenly
you're having to really add on to that
data store and you're storing duplicate
data sometimes so you really need to be
able to filter your data out and so
let's go ahead and use our up Arrow to
go to our last import since it's still a
lot the same stuff so we have all of our
commands under import we have the target
we're going to change this to Department
two so we're going to create a new
directory for this one and then after
departments there's another command that
we didn't really slide in here and
that's our mapping and I'll show you
what this looks like in a minute we're
going to put M3 in there that does have
nothing to do with the filtering I'll
show you that in a second though what
that's for and we just want to put in
where so where and what is the where in
this case we want to know where
Department ID and if you want to know
where that came from we can flip back on
over here we have Department underscore
IDs this is where that coming from
that's just the name of the column on
here so we come in here to Department ID
is greater than four simple logic there
you can see where you'd use that for
maybe creating buckets for ages uh you
know age from 10 to 15 20 to 30. you
might be looking for I mean there's all
kinds of reasons why you could use the
where command on here in filter
information out maybe you're doing word
counting and you want to know words that
are used less than a hundred times you
want to get rid of the and is and and
all the stuff that's used over and over
again so we'll go ahead and put the
where and then Department ID is greater
than four we'll go ahead and hit enter
on here and this will create our
department to set up on this and I'll go
ahead and skip over some of the runtime
again it runs really slow on a single
node a real quick page through our
commands
here we go our list and we should see
underneath the list the department two
on here now and there it is Department
two and then I can go ahead and do list
Department two you'll see the contents
in here and you'll see that there is
only three maps and it could be that the
data created three Master remember I set
it up to only use three mappers so
there's zero one and two and we can go
ahead and do a cat on there remember
this Department two so we want to look
at all the contents of these three
different files and there it is it's
greater than four so we have golf is
five outdoor six fan shop is seven so
we've effectively filtered out our data
and just storing the data we want on our
file system so if you're going to store
data on here the next stage is to export
the data remember a lot of times you
have MySQL server and we're continually
dumping that data into our long-term
storage and access a Hadoop file system
but what happens when you need to pull
that data out and restore a database or
uh maybe you have um you just merged
with a new company a favorite topic
merging companies emerging databases
that's listed under Nightmare and how
many different names for company can you
have so you can see where being able to
export is also equally important and
let's go ahead and do that and I'm going
to flip back over to my SQL Server here
and we'll need to go ahead and create
our database we're going to export into
now I'm not going to go too much in
detail on this command we're simply
creating a table and the table is going
to have it's pretty much the same table
we already have in here from departments
but in this case we're going to create a
table called Dept so it's the same setup
but it's it's just gonna we're just
giving a different name a different
schema and so we've done that and we'll
go ahead and do a select star from Dept
there we go and it's empty that's what
we expect a new database a new data
table and it's empty in there so now we
need to go ahead and Export our data
that we just filtered out into there so
let's flip back on over here to our
scoop setup which is just our Linux
terminal window and let's go back up to
one of our commands here's scoop Import
in this case instead of import we're
going to take the scoop and we're going
to export so we're going to just change
that export and the connection is going
to remain the same so same connect same
database we're also we're still doing
the retail DB we have the same password
so none of that changes the big change
here is going to be the table instead of
departments remember we changed it and
gave it a new name and so we want to
change it here also d-e-p-t so
Department we're not going to worry
about the mapper count and the where was
part of our import there we go and then
finally it needs to know where to export
from so instead of Target directory we
have an export directory that's where
it's coming from still user Cloudera and
we'll keep it as Department two just so
you can see how that data is coming back
with the that we've filtered in and
let's go ahead and run this
it'll take it just a moment to go
through in steps and again because it's
slow I'm just going to go ahead and skip
this so you don't have to sit through it
and once we've wrapped up our export
we'll flip back on over here to mySQL
use the up arrow and this time we're
going to select star from department and
we can see that there it is it exported
the golf outdoors and fan shop and you
can imagine also that you might have to
use the where command in your export
also so there's a lot of mixing the
command line for scoop is pretty
straightforward you're changing the
different variables in there whether
you're creating a table listing a table
listing databases very powerful tool for
bringing your data into the Hadoop file
system and exporting it so now that
we've wrapped up our demo on scoop and
gone through a lot of basic commands
that wraps it up for us today first
we're going to start with the history of
Hive what is Hive architecture of Hive
data flow And Hive Hive data modeling
Hive data types different modes of Hive
and difference between High 5 and rdbms
finally we're going to look into the
features of Hive and do a quick Hands-On
demo on Hive in the Cloudera Hadoop file
system let's dive in with a brief
history of Hive so the history of Hive
begins with Facebook Facebook began
using Hadoop as a solution to handle the
growing big data and we're not talking
about a data that fits on one or two or
even five computers we're talking due to
the fifth Sun if you've looked at any of
our other Hadoop tutorials you'll know
we're talking about very big data and
data pools and Facebook certainly has a
lot of data it tracks as we know the
Hadoop uses map reduce for processing
data mapreduce required users to write
long codes and so you'd have these
really extensive Java codes very
complicated for the average person to
use not all users reversed in Java and
other coding languages this proved to be
a disadvantage for them users were
comfortable with writing queries in SQL
SQL has been around for a long time the
standard SQL query language High was
developed with the vision to incorporate
the concepts of tables columns just like
SQL so why Hive well the problem was for
processing and analyzing data users
found it difficult to code as not all of
them were well versed with the coding
languages you have your processing you
have your analyzing and so the solution
was required a language similar to SQL
which was well known to all the users
and thus the hive or hql language
evolved what is Hive Hive is a data
warehouse system which is used for
querying and analyzing large data sets
stored in the hdfs or the Hadoop file
system Hive uses a query language that
we call Hive ql or hql which is similar
to SQL so if we take our user the user
sends out their hive queries and then
that is converted into a mapreduce tasks
and then accesses the Hadoop mapreduce
system let's take a look at the
architecture of Hive architecture of
Hive we have the hive client so that
could be the pro programmer
or the manager who knows enough SQL to
do a basic query to look up the data
they need the hive client supports
different types of client applications
in different languages prefer for
performing queries and so we have our
Thrift application in the hive Thrift
client Thrift is a software framework
Hive server is based on Thrift so it can
serve the request from all programming
language that support Thrift and then we
have our jdbc application and the hive
jdbc driver jdbc Java database
connectivity jdbc application is
connected through the jdbc driver and
then you have the odbc application or
the hive odbc driver the odbc or open
database connectivity the odbc
application is connected through the
odbc driver with the growing development
of all of our different scripting
languages python C plus plus spark Java
you can find just about any connection
in any of the main scripting languages
and so we have our high Services as we
look at deeper into the architecture
Hive supports various services so you
have your Hive server basically your
Thrift application or your hive Thrift
client or your jdbc or your hive jdbc
driver your odbc application or your
hive odbc driver they all Connect into
The Hive server and you have your hive
web interface you also have your CLI now
the hive web interface is a GUI is
provided to execute Hive queries and
we'll actually be using that later on
today so you can see kind of what that
looks like and get a feel for what that
means commands are executed directly in
CLI and then the CLI is a direct
terminal window and I'll also show you
that too so you can see how those two
different interfaces work these then
push the code into the hive driver Hive
driver is responsible for all the
queries submitted so everything goes
through that driver let's take a closer
look at the hive driver The Hive driver
now performs three steps internally one
is a compiler Hive driver passes query
to compiler where it is checked and
analyzed then the optimizer kicks in and
the optimized logical plan in the form
of a graph of mapreduce and hdfs tasks
is obtained and then finally in the
executor in the final step the tasks are
executed when we look at the
architecture we also have to note the
metastore metastore is a repository for
five metadata stores metadata for Hive
tables and you can think of this as your
schema and where is it located and it's
stored on the Apache Derby DB processing
and resource management is all handled
by the map reduce V1 you'll see
mapreduce V2 the yarn and the Tes these
are all different ways of managing these
resources depending on what version of
Hadoop you're in Hive uses mapreduce
framework to process queries and then we
have our distributed storage which is
the hdfs and if you looked at our Hadoop
tutorials you'll know that these are on
commodity machines and are linearly
scalable that means they're very
affordable a lot of time when you're
talking about Big Data you're talking
about a tenth of the price of storing it
on Enterprise computers and then we look
at the data flow And Hive so in our data
flow And Hive we have our Hive in the
Hadoop system and underneath the user
interface or the UI we have our driver
our compiler our execution engine and
our metastore that all goes into the map
reduce and the Hadoop file system so
when we execute a query you can see it
coming in here it goes into the driver
step one step two we get a plan what are
we going to do refers to the query
execution uh then we go to the metadata
it's like well what kind of metadata are
we actually looking at where is this
data located what is the schema on it uh
then this that comes back with the
metadata into the compiler then the
compiler takes all that information and
the send plan and returns it to the
driver the driver then sends the execute
plan to the execution engine once it's
in the execution engine the execution in
it acts as a bridge between Hive and
Hadoop to process the query and that's
going into your map reduce and your
Hadoop file system or your hdfs and then
we come back with the metadata
operations it goes back into the
metastore to update or let it know
what's going on which also goes to the
between it's a communication between the
execution engine and the metastore
execution engine Communications is
bi-directionally with the metastore to
perform operations like create drop
tables metastore stores information
about tables and columns so again we're
talking about the schema of your
database and once we have that we have a
bi-directional send results
communication back into the driver and
then we have the fetch results which
goes back to the client so let's take a
little bit look at the hive data
modeling Hive data modeling so you have
your high data modeling you have your
tables you have your partitions and you
have buckets the tables in however
created the same way it is done in rdbms
so when you're looking at your
traditional SQL server or MySQL server
where you might have Enterprise
equipment and a lot of people pulling
and moving stuff off of there the tables
are going to look very similar and this
makes it very easy to take that
information and let's say you need to
keep current information but you need to
store all of your years of transactions
back into the Hadoop Hive so you match
those those all kind of look the same
the tables are the same your databases
look very similar and you can easily
import them but you can easily store
them into the hive system partitions
here tables are organized into
partitions for grouping same type of
data based on partition key this can
become very important for speeding up
the process of doing queries so if
you're looking at at dates as far as
like your employment dates of employees
if that's what you're tracking you might
add a partition there because that might
be one of the key things that you're
always looking up as far as employees
are concerned and finally we have
buckets data present in partitions can
be further divided into buckets for
efficient querying again there's that
efficiency at this level a lot of times
you're you're working with the
programmer and the admin of your Hadoop
file system to maximize the efficiency
of that file system so it's usually a
two-person job and we're talking about
Hive data modeling you want to make sure
that they work together and you're
maximizing your resources Hive data
types so we're talking about Hive data
types we have our primitive data types
and our complex data types A lot of this
would look familiar because it mirrors a
lot of stuff in SQL in our primitive
data types we have the numerical data
types string data type date time data
type and Melissa miscellaneous data type
and these should be very they're kind of
selfish explanatory but just in case
numerical data is your floats your
integers your short integers all of that
numerical data comes in as a number a
string of course is characters and
numbers and then you have your date time
step and then we have kind of a general
way of pulling your own created data
types in there that's your miscellaneous
data type and we have complex data types
so you can store arrays you can store
maps you can store structures and even
units in there as we dig into Hive data
types and we have the primitive data
types and the complex data types so we
look at primitive data types and we're
looking at numeric data types data types
like an integer a float a decimal those
are all stored as numbers in the hive
data system a string data type data
types like characters and strings you
store the name of the person you're
working with you know John Doe the city
um
State Tennessee maybe it's Boulder
Colorado USA or maybe it's hyperbad
India that's all going to be string it's
stored as a string character and of
course we have our date time data type
data types like time stamp date interval
those are very common as far as tracking
sales anything like that so you just
think if you can type a stamp of time on
it or maybe you're dealing with the race
and you want to know the interval how
long did the person take to complete
whatever task it was all that is date
time data type and then we talked
miscellaneous data type these are like
Boolean in binary and when you get into
Boolean and binary you can actually
almost create anything in there but your
yes nose zero one now let's take a look
at complex data types a little closer we
have arrays so your syntax is of data
type and it's an array and you can just
think of an array as a collection of
same entities one two three four if
they're all numbers and you have Maps
this is a collection of key value pairs
so understanding so Central to Hadoop so
we store Maps you have a key which is a
set you can only have one key per mapped
value and so you in Hadoop of course you
collect the same keys and you can add
them all up or do something with all the
contents of the same key but this is our
map as a primitive type data type in our
collection of key value Pairs and then
collection of complex data with comment
so we can have a structure we have the
column name data type comment column
comment so you can get very complicated
structures in here with your collection
of data and your commented setup and
then we have units and this is a
collection of heterogeneous data types
so the syntax for this is Union type
data type data type and so on so it's
all going to be the same a little bit
different than the arrays where you can
actually mix and match different modes
of Hive Hive operates in two modes
depending on the number and size of data
nodes we have our local mode and our map
reduce mode when we talk about the local
mode it is used when had is having one
data node and the data is small
processing will be very fast on a
smaller data sets which are present in
local machine and this might be that you
have a local file stuff you're uploading
into the hive and you need to do some
processes in there you can go ahead and
run those High processes and queries on
it usually you don't see much in the way
of a single node Hadoop system if you're
going to do that you might as well just
use like an SQL database or even a Java
sqlite or something python is sqlite so
you don't really see a lot of single
node Hadoop databases but you do see the
local mode in Hive where you're working
with a small amount of data that's going
to be integrated into the larger
database and then we have the map reduce
mode this is used when Hadoop is having
multiple data nodes and the data is
spread across various data nodes
processing large data sets can be more
efficient using this mode and this you
can think of instead of it being one two
three or even five computers we're
usually talking with the Hadoop file
system we're looking at 10 computers 15
100 where this data is spread across all
those different Hadoop nodes difference
between Hive and rdbms remember rdbms
stands for the relational database
manage
let's take a look at the difference
between Hive and the rdbms with Hive
Hive enforces schema on read and it's
very important that whatever is coming
in that's when hive's looking at it and
making sure that it fits the model the
rdbms enforces a schema when it actually
writes the data into the database so
it's read the data and then once it
starts to write it that's where it's
going to give you the error or tell you
something's incorrect about your scheme
Hive data size is in petabytes that is
hard to imagine you know when we're
looking at your personal computer on
your desk maybe you have 10 terabytes if
it's a high-end computer we're talking
petabytes so that's hundreds of
computers grouped together when rdbms
data size is in terabytes very rarely do
you see an rdbms system that's spread
over more than five computers and
there's a lot of reasons for that with
the rdbms it actually has a high-end
amount of rights to the hard drive
there's a lot more going on there your
writing and pulling stuff so you really
don't want to get too big with an
rdbmaster you're going to run into a lot
of problems with Hive you can take it as
big as you want Hive is based on the
notion of right once and read many times
this is so important and they call it
worm which is right W10 read R many
times M they refer to it as warm and
that's true of any of you a lot of your
Hadoop setup it's it's altered a little
bit but in general we're looking at
archiving data that you want to do data
analysis on we're looking at pulling all
that stuff off your rdbms from years and
years and years of business or whatever
your company does or scientific research
and putting that into a huge data pool
so that you can now do queries on it and
get that information out of it with the
rdbms it's based on the notion of read
and write many times so you're
continually updating this database
you're continually bringing up new stuff
new sales the account changes because
they have a different licensing now
whatever software you're selling all
that kind of stuff where the data is
continually fluctuating and then Hive
resembles a traditional database by
supporting SQL but it is not a database
it is a data warehouse this is very
important it goes with all the other
stuff we've talked about that we're not
looking at a database but a data
warehouse to store the data and still
have fast and easy access to it for
doing queries you can think of Twitter
and Facebook they have so many posts
that are archived back historically
those posts aren't going to change they
made the post they're posted they're
there and they're in their database but
they have to store it in a warehouse in
case they want to pull it back up with
the rdbms it's a type of database
management system which is based on the
relational model of data and then with
Hive easily scalable at a low cost again
we're talking maybe a thousand dollars
per terabyte the rdbms is not scalable
at a low cost when you first start on
the lower end you're talking about ten
thousand per terabyte of data including
all the backup on the models and and all
the added Necessities to support it as
you scale it up you have to scale those
computers and Hardware up so you might
start off with a basic server and then
you upgrade to a sun computer to run it
and you spend you know tens of thousands
of dollars for that Hardware upgrade
with Hive you just put another computer
into your Hadoop file system so let's
look at some of the features of Hive
when we're looking at the features of
Hive we're talking about the use of ql
like language called Hive ql a lot of
times you'll see that as hql which is
easier than long codes this is nice if
you're working with your shareholders
you come to them and you say Hey you can
do a basic SQL query on here and pull up
the information you need this way you
don't have to take I'll have your
programmers jump in every time they want
to look up something in the database
they actually now can easily do that if
they're not skilled in programming and
script writing tables are used which are
similar to The rdbms hence easier to
understand and one of the things I like
about this is when I'm bringing tables
in from a MySQL server or SQL Server
there's almost a direct reflection
between the two so when you're looking
at one which is a data which is
continually changing and then you're
going into the archive database it's not
this huge jump where you have to learn a
whole new language you mirror that same
schema into the hdfs into the hive
making it very easy to go between the
two and then using Hive ql multiple
users can simultaneously query data so
again you have multiple clients in there
and they send in their query that's also
true with the rdbms which kind of cues
them up because it's running so fast you
don't notice the lag time well you get
that also with the hql as you add more
computers in the query can go very
quickly depending on
Pewters and how much resources each
machine has to pull the information and
Hive supports a variety of data types so
with Hive it's designed to be on the
Hadoop system which you can put almost
anything into the Hadoop file system so
with all that let's take a look at a
demo on hive ql or hql before I dive
into the Hands-On demo let's take a look
at the website
hive.apache.org that's the main website
since Apache it's an Apache open source
software this is the main software or
the main site for the build and if you
go in here you'll see that they're
slowly migrating Hive into beehive and
so if you see beehive versus Hive note
The Beehive is a new release is coming
out that's all it is it reflects a lot
of the same functionality of Hive it's
the same thing and then we like to pull
up some kind of documentation on
commands and for this I'm actually going
to go to hortonworks Hive cheat sheet
and that's because hortonworks and
Cloudera two of the most common used
builds for Hadoop and for which include
Hive and all the different Tools in
there and so hortonworks has a pretty
good PDF you can download cheat sheet on
there I believe Cloudera does too but
we'll go ahead and just look at the
Horton one because it's the one that
comes up really good and you can see
when we look at the query language it
Compares my SQL Server to Hive ql or hql
and you can see the basic select we
select from columns from table where
conditions exist you know most basic
command on there and they have different
things you can do with it just like you
do with your SQL and if you scroll down
you'll see data types so here's your
integer your flow your binary double
string timestamp and all the different
data types you can use some different
semantics different Keys features
functions for running a hive query
command line setup and of course the
hive shell setup in here so you can see
right here if we Loop through it it has
a lot of your basic stuff in it is we're
basically looking at SQL across a Horton
database we're going to go ahead and run
our Hadoop cluster Hive demo and I'm
going to go ahead and use the Cloudera
quick start this is in the virtual box
so again we have an oracle virtual box
which is open source and then we have
our Cloudera quick start which is the
Hadoop setup on a single node now
obviously Hadoop And Hive are designed
to run across a cluster of computers so
we talk about a single node is for
Education testing that kind of thing and
if you have a chance you can always go
back and look at our demo we had on
setting up a Hadoop system in a single
cluster to set a note Down Below in the
YouTube video and our team will get in
contact with you and send you that link
if you don't already have it or you can
contact us at the
www.simplylearn.com now in here it's
always important to note that you do
need on your computer if you're running
on Windows because I'm on a Windows
machine you're going to need probably
about 12 gigabytes to actually run this
it used to be goodbye with a lot less
but as things have evolved they take up
more and more resources and you need the
professional version if you have the
home version I was able to get that to
run but boy did it take a lot of extra
work to get the home version to let me
use the virtual setup on there and we'll
simply click on the Cloudera quick start
and I'm going to go and just start that
up and this is starting up our Linux so
we have our Windows 10 which is a
computer I'm on and then I have the
virtual box which is going to have a
Linux operating system in it and we'll
skip ahead so you don't have to watch
the whole install something interesting
to know about the Cloudera is that it's
running on Linux Santos and for whatever
reason I've always had to click on it
and hit the escape button for it to spin
up and then you'll see the Dos come in
here now that our Cloudera is spun up on
our virtual machine with the Linux on we
can see here we have our it uses the
Thunderbird browser on here by default
and automatically opens up a number of
of different tabs for us and a quick
note because I mentioned like the
restrictions on getting set up on your
own computer if you have a home edition
computer and you're worried about
setting it up on there you can also go
in there and spin up a one month free
service on Amazon web service to play
with this so there's other options
you're not stuck with just doing it on
the quick start menu you can spin this
up in many other ways now the first
thing we want to note is that we've come
in here into Cloudera and I'm going to
access this in two ways the first one is
we're going to use Hue and I'm going to
open up Hue and it'll take it a moment
to load from the setup on here and Hue
is nice if I go in and use Hue as an
editor into Hive or into the Hadoop
setup usually I'm doing it as a from an
admin side because it has a lot more
information a lot of visuals less to do
with you know actually diving in there
and just executing code and you can also
write this code into files and scripts
and there's other things you can other
ways you can upload it into high live
but today we're going to look at the
command lines and we'll upload it into
Hue and then we'll go into and actually
do our work in a terminal window Under
The Hive shell now in the Hue browser
window if you go under query and click
on the pull down menu and then you go
under editor and you'll see Hive there
we go there's our Hive setup I go and
click on hive and this will open up our
query down here and now it has a nice
little B that shows our Hive going and
we can go something very simple down
here like show databases and we follow
it with the semicolon and that's the
standard in Hive as you always add our
punctuation at the end there and I'll go
ahead and run this and the query will
show up underneath and you'll see down
here since this is a new quick start I
just put on here you'll see it has the
default down here for the databases
that's the database name I haven't
actually created any databases on here
and then there's a lot of other like
assistant function tables your databases
up here there's all kinds of things you
can research you can look at through Hue
as far as a bigger picture the downside
of this is it always seems to lag for me
whenever I'm doing this I always seem to
run slow so if you're in Cloudera you
can open up a terminal window they
actually have an icon at the top you can
also go under applications and under
applications system tools and terminal
either one will work it's just a regular
terminal window and this terminal window
is now running underneath our Linux so
this is a Linux terminal window or on
our virtual machine which is resting on
our regular Windows 10 machine and we'll
go ahead and zoom this in so you can see
the text better on your own video and I
simply just clicked on view and zoom in
and then all we have to do is type in
Hive and this will open up the shell on
here and it takes it just a moment to
load when starting up Hive I also want
to note that depending on your rights on
the computer you're on in your action
you might have to do pseudohive and put
in your password and username most
computers are usually set up with the
hive login again it just depends on how
you're accessing the Linux system and
the hive shell once we're in here we can
go ahead and do a simple hql command
show databases and if we do that we'll
see here that we don't have any
databases so we can go ahead and create
a database and we'll just call it office
for today for this moment now if I do
show we'll just do the up Arrow up arrow
is a hotkey that works in both Linux and
in Hive so I can go back and paste
through all the commands I've typed in
and we can see now that I have my
there's of course a default database and
then there's the office database so now
we've created a database it's pretty
quick and easy and we go ahead and drop
the database we can do drop Database
Office now this will work on this
database because it's empty if your
database was not empty you would have to
do Cascade and that drops all the tables
in the database and the database itself
now if we do show database and we'll go
ahead and recreate our database because
we're going to use the office database
for the rest of this Hands-On demo a
really handy command to Now set with the
SQL or hql is to use office and what
that does is that sets office as the
default database so instead of having to
reference the database every time we
work with a table we now automatically
assumes that's the database being used
whatever tables we're working on the
difference is you put the database name
period table and I'll show you in just a
minute what that looks like and how
that's different if we're going to have
a table and a database we should
probably load some data into it so let
me go ahead and switch gears here and
open up a terminal window you can just
open another terminal window and it'll
open up right on top of the one that you
have Hive shell running in and when
we're in this terminal window first
we're going to go ahead and just do a
list which is of course a Linux command
you can see all the files I have in here
this is the default load we can change
directory to documents we can list in
documents and we're actually going to be
looking at
employee.csv a Linux command is the cat
you can use this actually to combine
documents there's all kinds of things
that cat does but if we want to just
display the the contents of our
employee.csv file we can simply do cat
employee CSV and when we're looking at
this we want to know a couple things one
there's a line at the top okay so the
very first thing we notice is that we
have a header line the next thing we
notice is that the data is comma
separated and in this particular case
you'll see a space here generally with
these you've got to be real careful with
spaces there's all kinds of things you
got to watch out for because it can
cause issues these spaces won't because
these are all strings that the space is
connected to if this was a space next to
the integer you would get a null value
that comes into the database without
doing something extra in there now with
most of Hadoop that's important to know
that you're writing the data once
reading it many times and that's true of
almost all your Hadoop things coming in
so you really want to process the data
before it gets into the database and for
those who of you have studied data
transformation that's the adult where
you extract transfer form and then load
the data so you really want to extract
and transform before putting it into the
hive then you load it into the hive with
the transform data and of course we also
want to note the schema we have an
integer string string integer integer so
we kept it pretty simple in here as far
as the way the data is set up the last
thing that you're going to want to look
up is the source since we're doing local
uploads we want to know what the path is
we have the whole path in this case it's
home slash Cloudera documents and these
are just text documents we're working
with right now we're not doing anything
fancy so we can do a simple git edit
employee.csv and you'll see it comes up
here it's just a text document so I can
easily remove these added spaces there
we go and then we go and just save it
and so now it has the new setup in there
we've edited it the G edit is usually
one of the default that loads into Linux
so any text editor will do back to the
hive shell so let's go ahead and create
a table employee and what I want you to
note here is I did not put the semicolon
on the end here a semicolon tells it to
execute that line so this is kind of
nice if you're you can actually just
paste it in if you have it written on
another sheet and you can see right here
where I have create table employee and
it goes in to the next line on there so
I can do all of my commands at once now
just I don't have any typo errors I went
ahead and just pasted the next three
lines in and the next one is our schema
if you remember correctly from the other
side we had the different values in here
which was ID name Department year of
joining and salary and the ID is an
integer name is a string department
string you're joining energy salary an
integer and they're in Brackets we put
close brackets around them and you could
do this all as one line and then we have
row format delimited Fields terminated
by comma and this is important because
the default is tabs so if I do it now it
won't find any terminated Fields so
you'll get a bunch of null values loaded
into your table and then finally our
table properties we want to skip the
header line count equals one now this is
a lot of work for uploading a single
file it's kind of goofy when you're
uploading a single file that you have to
put all this in here but keep in mind
Hive and Hadoop is designed for writing
many files into the database you write
them all in there and then you can
they're saved it's an archive it's a
data warehouse and then you're able to
do all your queries on them so a lot of
times we're not looking at just the one
file coming up we're loading hundreds of
files you have your reports coming off
of your main database all those reports
are being loaded and you have your log
files you have I mean all this different
data is being dumped into Hadoop and in
this case Hive on top of Hadoop and so
we need to let it know hey how do I
handle these files coming in and then we
have the semicolon at the end which lets
us know to go ahead and run this line
and so we'll go ahead and run that and
now if we do a show tables you can see
there's our employee on there we can
also describe if we do describe employee
you can see that we have our ID integer
named string department string year of
joining integer and salary integer and
then finally let's just do a select star
from employee very basic
sqlnhql command selecting data and it's
going to come up and we haven't put
anything in it so as we expect there's
no data in it so if we flip back to our
Linux terminal window you can see where
we did the cat
employee.csv and you can see all the
data we expect to come into it and we
also did our PWD and right here you see
the path you need that full path when
you are loading data you know you can do
a browse and if I did it right now with
just the employee.csv is the name it
will work but that is a really bad habit
in general when you're loading data
because it's you don't know what else is
going on on the computer you want to do
the full path almost in all your data
loads so let's go ahead and flip back
over here to our Hive shell we're
working in and the command for this is
load data so that says hey we're loading
data that's a high of command hql and we
want local data so you got to put down
local and path so now it needs to know
where the path is now to make this more
legible I'm just going to go ahead and
hit enter then we'll just paste the full
path in there which I have stored over
on the side like a good prepared demo
and you'll see here we have home
Cloudera documents employee.csv so it's
a whole path for this text document in
here and we go ahead and hit enter in
there and then we have to let it know
where the data is going so now we have a
source and we need a destination and
it's going to go into the table and
we'll just call it employee we'll just
match the table in there and because I
wanted to execute we put the semicolon
on the end it goes ahead and executes
all three lines now if we go back if you
remember we did the select star from
employee just using the up Arrow to page
through my different commands I've
already typed in you can see right here
we have as we expect we have Rose Sam
Mike and Nick and we have all their
information showing in our four rows and
then let's go ahead and do select and
count we'll just look at a couple of
these different select options you can
do we're going to count everything from
employee now this is kind of interesting
because the first one just pops up with
the basic select because it doesn't need
to go through the full map reduce phase
but when you start doing a count it does
go through the full map reduce setup in
the hive in Hadoop and because I'm doing
this demo on a single node Cloudera
virtual box on top of a Windows 10. all
the benefits of running it on a cluster
are gone and instead is now going
through all those added layers so it
takes longer to run you know like I said
when you do a single node as I said
earlier it doesn't do any good as an
actual distribution because you're only
running it on one computer and then
you've added all these different layers
to run it and we see it comes up with
four and that's what we expect we have
four rows we expect four at the end and
if you remember from our cheat sheet
which we brought up here from Hortons
it's a pretty good one there's all these
different commands we can do we'll look
at one more command where we do the what
they call sub queries right down here
because that's really common to do a lot
of sub queries and so we'll do select
star or all different columns from
employee now if we weren't using the
office database it would look like this
from Office dot employee and either one
will work on this particular one because
we have office set as a default on there
so from office employee and then the
command where creates a subset and in
this case we want to know where the
salary is greater than 25
000. there we go and of course we end
with our semicolon if we run this query
you can see it pops up and there's our
salaries the people top earners we have
Rose and it and Mike and HR kudos to
them of course they're fictitional I
don't actually we don't actually have a
rows and a mic in those positions or
maybe we do so finally we want to go
ahead and do is we're done with this
table remember you're dealing with the
data warehouse so you usually don't do a
lot of dropping of tables and
databases but we're going to go ahead
and drop this table here before we drop
it one more quick note is we can change
it so what we're going to do is we're
going to alter table office employee and
we want to go ahead and rename it
there's some other commands you can do
in here but rename is pretty common and
we're going to rename it to and it's
going to stay in office and it turns out
one of our shareholders really doesn't
like the word employee he wants
employees plural it's a big deal to him
so let's go ahead and change that name
for the table it's that easy because
it's just changing the meta data on
there and now if we do show tables
you'll see we now have employees not
employee and then at this point maybe
we're doing some house cleaning because
this is all practice so we're going to
go ahead and drop the table and we'll
drop table employees because we changed
the name in there so if we did employee
just give us an error and now if we do
show tables you'll see how the tables
are gone now the next thing we want to
go and take a look at and we're going to
walk back through the loading of data uh
just real quick because we're going to
load two tables in here and let me just
float back to our terminal window so we
can see what those tables are that we're
loading and so up here we have customer
we have a customer file and we have an
order file we want to go ahead and put
the customers and the orders into here
so those are the two we're doing and of
course it's always nice to see what
you're working with so let's do our cat
customer dot CSV we could always do G
edit but we don't really need to edit
these we just want to take a look at the
data in customer and important in here
is again we have a header so we have to
skip a line comma separated nothing odd
with the data we have our schema which
is integer string integer string integer
so you know you'd want to take that note
that down or flip back and forth when
you're doing it and then let's go ahead
and do cat
order.csv and we can see we have oid
which I'm guessing is the order ID we
have a date up something new we've done
integers and strings but we haven't done
date when you're importing knew and you
never worked with the date dates always
one of the more trickier fields to port
in and that's true of just about any
scripting language I've worked with all
of them have their own idea of how dates
supposed to be formatted what the
default is this particular format or its
year and it has all four digits Dash
month two digits Dash day is the
standard import for the hive so you'll
have to look up and see what the
different formats are if you're going to
do a different format in there coming in
or you're not able to pre-process the
data but this would be a pre-processing
of the data thing coming in if you
remember correctly from our edel which
is uh e just in case you weren't able to
hear me last time
ETL which stands for extract transform
then load so you want to make sure
you're transforming this data before it
gets into here and so we're going to go
ahead and bring both this data in here
and really we're doing this so we can
show you the basic join there is if you
remember from our setup merge join all
kinds of different things you can do but
joining different data sets is so common
so it's really important to know how to
do this we need to go ahead and bring in
these two data sets and you can see
where I just created a table customer
here's our schema the integer name age
address salary here's our delimited by
commas and our table properties where we
skip a line well let's go ahead and load
the data first and then we'll do that
with our order and let's go ahead and
put that in here and I've got it split
into three lines you can see it easily
we've got load data local in path so we
know we're loading data we know it's
local and we have the path here's the
complete path for oops this is supposed
to be order CSV grab the wrong one of
course it's going to give me errors
because you can't recreate the same
table on there and here we go create
table here's our integer date customer
the basic setup that we had coming in
here for our schema row format commas
table properties skip header line and
then finally let's load the data into
our order table load data local in path
home Cloudera document ordered at CSV
into table order now if we did
everything right we should be able to do
select star from customer and you can
see we have all seven customers and then
we can do select star from order and we
have four orders so this is just like a
quick frame we have you know a lot of
times when you have your customer
databases in business you have thousands
of customers from years and years and
some of them you know they move they
close their business they change names
all kinds of things happen so we want to
do is we want to go ahead and find just
the information connected to these
orders and who's connected to them and
so let's go ahead and do it's a select
because we're going to display
information so select and this is kind
of interesting we're going to do c dot
ID and I'm going to Define c as customer
as a customer table in just a minute
then we're going to do c dot name and
again we're going to define the c c dot
age so this means from the customer we
want to know their ID their name their
age and then and you know I'd also like
to know the order amount so let's do o
for DOT amount and then this is where we
need to go ahead and Define what we're
doing and I'll go and capitalize from
customer so we're going to take the
customer table in here and we're going
to name it C that's where the C comes
from so that's the customer table C and
we want to join order as o that's where
our o comes from so the O DOT amount is
what we're joining in there and then we
want to do this on we've got to tell it
how to connect the two tables C dot ID
equals o Dot customer underscore ID so
now we know how they're joined and now
remember we have seven customers in here
we have four orders and as it processes
we should get a return of four different
names joined together and they're joined
based on of course the orders on there
and once we're done we now have the
order number the person who made the
order their age and the amount of the
order which came from the order table so
you have your different information you
can see how the join works here very
common use of tables and hql and SQL and
let's do one more thing with our
database and then I'll show you a couple
other Hive commands and let's go ahead
and do a drop and we're going to drop
Database Office and if you're looking at
this and you remember from earlier this
will give me an error and this is see
what that looks like it says fail to
execute exception one or more tables
exist so if you remember from before you
can't just drop a database unless you
tell it to Cascade that lets it know I
don't care how many tables are in it
let's get rid of it and in Hadoop since
it's an R it's a warehouse a data
warehouse you usually don't do a lot of
dropping maybe at the beginning when
you're developing the schemas and you
realize you messed up you might drop
some stuff but down the road you're
really just adding commodity machines to
take up so you can store more stuff on
it so you usually don't do a lot of
database dropping and some other fun
commands to know is you can do so round
2.3 is round value you can do a round
off in Hive we can do as floor value
which is going to give us a 2 so it
turns it into an integer versus a float
it goes down you know basically
truncates it but it goes down and we can
also do ceiling which is going to round
it up so we're looking for the next
integer above there's a few commands we
didn't show in here because we're on a
single node as as an admin to help
speediate the process you usually add in
partitions for the data and buckets you
can't do that on a single node because
the when you add a partition it
partitions it across separate nodes but
beyond that you can see that it's very
straightforward we have SQL coming in
and all your basic queries that are in
SQL are very similar to hql let's get
started with pig why Pig what is pig map
reduce versus Hive versus pig hopefully
you've had a chance to do our Hive
tutorial in our map reduce tutorial if
you haven't send a note over to Simply
learn and we'll follow up with a link to
you we'll look at Pig architecture
working a pig pig latin data model Pig
execution modes a use case Twitter and
features a pick and then we'll tag on a
short demo so you can see Pig In Action
so why pig as we all know Hadoop uses
mapreduce to analyze and process big
data processing Big Data consumed more
time so before we had the Hadoop system
they'd have to spend a lot of money on a
huge set of computers and Enterprise
machines so we introduced the Hadoop
mapreduce and so afterwards processing
Big Data was faster using mapreduce then
what is the problem with map reduce
prior to 2006 all mapreduce programs
were written in Java non-programmers
found it difficult to write lengthy Java
codes they faced issues in incorporating
map sort reduce to fundamentals of
mapreduce while creating a program you
can see here map face Shuffle and sort
reduce phase eventually it became a
difficult test to maintain and optimize
a code due to which the processing time
increased you can imagine a manager
trying to go in there and needed in a
simple query to find out data and he has
to go talk to the programmers anytime he
wants anything so that was a big problem
not everybody wants to have a on-call
programmer for every manager on their
team Yahoo faced problems to process and
analyze large data sets using Java as
the codes were complex and lengthy there
was a necessity to develop an easier way
to analyze large data sets without using
time-consuming complex Java modes and
codes and scripts and all that fun stuff
Apache Pig was developed by Yahoo it was
developed with a vision to analyze and
process large data sets without using
complex Java codes Pig was developed
especially for non-programmers pig used
simple steps to analyze data sets which
was time efficient so what exactly is
pick pig is a scripting platform that
runs on Hadoop clusters designed to
process and analyze large data sets and
so you you have your pig which uses SQL
like queries they're definitely not SQL
but some of them resemble SQL queries
and then we use that to analyze our data
Pig operates on various types of data
like structured semi-structured and
unstructured data let's take a closer
look at mapreduce versus Hive versus pig
so we start with a compiled language
your map reduce and we have Hive which
is your SQL like query and then we have
pig which is a scripting language it has
some similarities to SQL but it has a
lot of its own stuff remember SQL like
query which is what Hive is based off
looks for structured data and so when we
get into scripting languages like Pig
now we're dealing more with
semi-structured and even unstructured
data with a Hadoop map reduce we have a
need to write long complex codes with
Hive no need to write complex codes you
could just put it in a simple SQL query
or hql hql and in pig no need to write
complex codes as we have pig lat now
remember in the map reduce it can
produce structured my structured and
unstructured data and as I mentioned
before Hive can process only structured
data think rows and columns where Pig
can process structured semi-structured
and unstructured data you can think of
structured data as rows and columns
semi-structured as your HTML XML
documents that you have on your web
pages and unstructured could be anything
from groups of documents and written
format Twitter tweets any of those
things come in as very unstructured data
and with our Hadoop mapreduce we have a
lower level of abstraction with both
Hive and pig we have a higher level
abstraction so it's much more easy for
someone to use without having to dive in
deep and write a very lengthy map reduce
code and those map and reduce codes can
take 70 80 lines of code when you can do
the same thing in one or two lines with
high over Pig this is the advantage Pig
has over Hive it can process only
structured data in Hive while in pig it
can process structured semi-structured
and unstructured data some other
features to note that separates the
different query languages is we look at
map and reduce map reduce supports
partitioning features as does Hive Pig
no concept of partitioning in pigs it
doesn't support your partitioning
feature your partitioning features allow
you to partition the data in such a way
that it can be queried quicker you're
not able to do that in pig mapreduce
uses Java in Python while Hive uses an
SQL like query language known as Hive ql
or hql Pig Latin is used which is a
procedural data flow language mapreduce
is used by programmers pretty much as
straightforward on Java Hive is used by
data analysts pig is used by researchers
and programmers certainly there's a lot
of mix between all three programmers
have been known to go in and use a hive
for quick query and anybody's been able
to use Pig for a quick query or research
under map and reduce code performance is
really good under Hive code performance
is lesser than map and reduce and pick
under Pig Code performance is lesser
than mapreduce but better than Hive so
if we're going to look at speed and time
the map reduce is going to be the
fastest performance on all of those
where Pig will have second and high
follows in the back let's look at
components of pig pig has two main
components we have pig Latin Pig Latin
is the procedural data flow language
used in pig to analyze data it is easy
to program using Piglet and it is
similar to SQL and then we have the
runtime engine runtime engine represents
the execution environment created to run
and pig latin programs it is also a
compiler that produces mapreduce
programs uses hdfs or your Hadoop file
system for storing and retrieving data
and as we dig deeper into the pig
architecture we'll see that we have pig
latin scripts programmers write a script
in pig latin to analyze data using Pig
then you have the grunt shell and it
actually says grunt when we start it up
and we'll show you that here in a little
bit which goes into the pig server and
this this is where we have our parser
parser checks the syntax of the pig
script after checking the output will be
a dag directed acelic graph and then we
have an Optimizer which optimizes after
your dag your logical plan is passed
through the logical Optimizer where an
optimization takes place finally the
compiler converts the dag into mapreduce
jobs and then that is executed on the
map reduce under the execution engine
the results are displayed using dump
statement and stored in hdfs using store
statement and again we'll show you that
um kind of end you always want to
execute everything once you've created
it and so dump is kind of our execution
statement and you can see right here as
we were talking about earlier once we
get to the execution engine and it's
coded into mapreduce then the map reduce
processes it onto the hdfs working of
pick Pig Latin script is written by the
users so you have load data and right
Pig script and pig operations so we look
at the working of pig pig latin script
is written by the users there's step one
we load data and write pigscript and
step two in this step all the pig
operations are performed by parser
Optimizer and compiler so we go into the
pig operations and then we get to step
three execution of the plan in this
stage the results are shown on the
screen otherwise stored in the hdfs as
per the code so it might be of a small
amount of data you're reducing it to and
you want to put that on the screen or
you might be converting a huge amount of
data which you want to put back into the
Hadoop file system for other use let's
take a look at the pig latin data the
data model of pig latin helps pig to
handle various types of data for example
we have Adam Rob or 50. Adam represents
any single value of primitive data type
in pig latin like integer float string
it is stored as a string two bolts so we
go from our atom which are most basic
thing so if you look at just Rob or just
50 that's an atom that's our most basic
object we have in pig latin then you
have a tuple Tuple represents sequence
of fields that can be of any data type
it is the same as a row in rdbms for
example a set of data from a single row
and you can see here we have Rob comma
five and you can imagine with many of
our other examples we've used you might
have the ID number the name where they
live their age their date of starting
the job that would all be one row and
store it as a tuple and then we create a
bag a bag is a collection of tuples it
is the same as a table in rdbms and is
represented by brackets and you can see
here we have our table with Rob 5 Mike
10 and we also have a map a map is a set
of key value pairs key is of character
array type and a value can be of any
type it is represented by the brackets
and so we have name and age where the
key value is Mike and 10. pig latin has
a fully nestable data model that means
one data type can be nested within
another here's a diagram representation
of pig latin data model and in this
particular example we have basically an
ID number a name an age and a place and
we break this apart we look at this
model from Pig Latin perspective we
start with our field and if you remember
a field contains basically an atom it is
one particular data type and the atom is
stored as a string which it then
converts it into either an integer or
number or character string next we have
our Tuple and in this case you can see
that it represents a row so our Tuple
would be three comma Joe comma 29 comma
California and finally we have our bag
which contains three rows in it in this
particular example let's take a quick
look at Pig execution mode
Pig Works in two execution modes
depending on where the data is reciting
and where the pig script is going to run
we have local mode here the pig engine
takes input from the Linux file system
and the output is stored in the same
file system local mode local mode is
useful in analyzing small data sets
using Pig and we have the mapreduce mode
here the pig engine directly interacts
and executes in hdfs and mapreduce in
the map reduce mode queries written in
pig latin are translated into mapreduce
jobs in our run on a Hadoop cluster by
default Pig runs in this mode there are
three modes in pig depending on how a
pig latin code can be written we have
our interactive mode batch mode and
embedded mode the interactive mode means
coding and executing the script line by
line when we do our example we'll be in
the interactive mode in badge mode all
scripts are coded in a file with
extension dot Pig and the file is
directly executed and then there's
embedded mode Pig lets its users Define
their own functions udfs in a
programming language such as Java so
let's take a look and see how this works
in a use case in this case use case
Twitter users on Twitter generate about
500 million tweets on a daily basis the
Hadoop mapreduce was used to process and
analyze this data analyzing the number
of tweets created by a user in the Tweet
table was done using mapreduce and Java
programming language and you can see the
problem it was difficult to perform
mapreduce operations as users were not
well versed with written complex Java
codes so Twitter used Apache pig to
overcome these problems and let's see
how let's start with the problem
statement analyze the user table and
tweet table and find out how many tweets
are created by a person and here you can
see we have a user table we have Alice
Tim and John with their ID numbers one
two three and we have a tweet table in
the Tweet table you have your the ID of
the user and then what they tweeted
Google was a good whatever it was
tennis. spacecraft Olympics politics
whatever they're tweeting about the
following operations were perform for
analyzing given data first the Twitter
data is loaded into the pig storage
using load command and you can see here
we have our data coming in and then
that's going into Pig storage and this
data is probably on an Enterprise
computer so this is actually active
twitters going on and then it goes into
Hadoop file system remember the Hadoop
file system is a data warehouse for
storing data and so the first step is we
want to go ahead and load it into the
pig storage into our data storage system
the remaining operations performed are
shown Below in join and group operation
the tweet and user tables are joined and
grouped using co-group command and you
can see here where we add a whole column
when we go from user names and tweet to
the ID link directly to the name so
Alice was user one Tim was 2 and John 3
and so now they're listed with their
actual tweet the next operation is the
aggregation the tweets are counted
according to the names the command used
is count so it's very straightforward we
just want to count how many tweets each
user is doing and finally the result
after the count operation is joined with
the user table to find out the username
and you can see here where LS had three
Tim two and John 1. Pig reduces the
complexity of the operations which would
have been lengthy using mapreduce and
joining group operation the tweet and
user tables are joined and grouped using
co-group command the next operation is
the aggregation the tweets are counted
according to the names the command used
as count the result after the count
operation is joined with the user table
to find out the username and you can see
we're talking about three lines of
script versus a mapreduce code of about
80 lines finally we could find out the
number of tweets created by a user in a
simple way so let's go quickly over some
of the features of pig that we already
went through most of these first ease of
programming as Pig Latin is similar to
SQL lesser lines of code need to be
written short development time is the
code is simpler so we can get our
queries out rather quickly instead of
having to have a programmer spend hours
on it handles all kind of data like
structured semi-structured and
unstructured pig lets us create user
defined functions Pig offers a large set
of operators such as join filter and so
on it allows for multiple queries to
process unparallel and optimization and
compilation is easy as it is done
automatically and internally
[Music]
so enough Theory let's dive in and show
you a quick demo on some of the commands
you can do and pick today's setup will
continue as we have in the last three
demos to go ahead and use Cloudera quick
start and we'll be doing this in Virtual
box we do have a tutorial in setting
that up you can send a note to our
simply learned team and then get that
linked to you once your Cloudera quick
start is uh spun up and remember this is
virtualbox we've created a virtual
machine and this virtual machine is
Centos Linux once it's spun up you'll be
in a full Linux system here and as you
see we have Thunderbird browser which
opens up to the Hadoop basic system
browser and we can go underneath the Hue
where it comes up by default if you
click on the pull down menu and go under
editor you can see there's our Impala
our Hive uh Pig along with a bunch of
other query languages you can use and
we're going under Pig and then once
you're in pig we can go ahead and use
our command line here and just click
that little blue button to start it up
and running we will actually be working
in terminal window and so if you're in
the Cloudera quick start you can open up
the terminal window up top or if you're
in your own setup and you're logged in
you can easily use all of your commands
here in terminal window and we'll zoom
in that way you get a nice view of
what's going on there we go now for our
first command we're going to do a Hadoop
command and import some data into the
Hadoop system in this case a pig input
let's just take a look at this we have
Hadoop now let's know it's going to be a
Hadoop command DFS there's actually four
variations of DFS so if you have hdfs or
whatever that's fine all four of them
Point used to be different setups
underneath different things and now they
all do the same thing and we want to put
this file which in this case is under
home Cloudera documents and Sample and
we just want to take that and put it
into the pig input and let's take a look
at that file if I go under my document
browsers and open this up you'll see
it's got a simple ID name profession and
age we have one Jack engineer 25. and
that was in one of our earlier things we
had in there and so let's go ahead and
hit enter and execute this and now we've
uploaded that data and it's gone into
our Pig input and then a lot of the
Hadoop commands mimic the Linux commands
and so you'll see we have cat as one of
our commands or it hasn't hyphen before
it so we execute that with Hadoop DFS
some hyphen cat slash Pig input because
that's what we called it that's where we
put our sample CSV at and we execute
this you can see from our Hadoop system
it's going to go in and pull that up and
sure enough it pulls out the data file
we just put in there and then we can
simply enter the pig Latin or Pig editor
mode by typing in pick and we can see
here by our grunt I told you this hat
was going to tell you we're in pig latin
there's our grunt command line so we are
now in the pig shell and then we'll go
ahead and put our load command in here
and the way this works is I'm going to
have office equals load and here's my
load when this case is going to be Pig
input we have that in single brackets
remember that's where the data is in the
Hadoop file system where we dumped it
into there we're going to using Pig
storage our data was separated as with a
comma so there's our comma separator and
then we have as and in this case we have
an ID character array name character a
profession character array and age
character array and we're just going to
do them all as character arrays just to
keep this simple for this one and then
when I hit put this all in here you can
see that's our full command line going
in and we have our semicolon at the end
so when I hit enter it's now set office
up but it hasn't actually done anything
yet it doesn't do anything until we do
dump office so there's our Command to
execute whatever we've loaded or
whatever setup we have in here and we
run that you can see it go through the
different languages and this is going
through the map reduce remember we're
not doing this locally we're doing this
on the Hadoop setup and once we've
finished our dump you can see we have ID
name profession age and all the
information that we just dumped into our
pick oh we can now do let's say oh let's
say we have a request just for we'll
keep it simple in here but just for the
name and age and so we can go office
we'll call it each as our variable
underscore each and we'll say for each
office
generate name comma H and for each means
that we're going to do this for each row
and if you're thinking map reduce you
know that this is a map function because
it's mapping each row and generating
name and age on here and of course we
want to go ahead and close it with the
semicolon and then once we've created
our query or the command line in here
let's go ahead and dump office
underscore each and with our semicolon
and this will go through our map reduce
setup on here and if we were on a large
cluster the same processing time would
happen in fact it's really slow because
I have multiple things on this computer
in this particular virtual box is only
using a quarter of my processor it's
only dedicated to this and you can see
here there it is name and age and it
also included the top row since we
didn't delete that out of there or tell
it not to and that's fine for this
example but you need to be aware of
those things when you're processing a
significantly large amount of data or
any data and we can also do office and
we'll call this DSC for descending so
maybe the boss comes to you and says hey
can we order office by ID descending and
of course your boss you've taught them
how to your shareholder it sounds a
little derogatory to say boss you've
talked to the shareholder and you said
and you've taught him a little bit of
Pig Latin and they know that they can
now create office description and we can
order office by ID description and of
course once we do that we have to dump
office underscore description so that
it'll actually execute and there goes
into our map reduce it'll take just a
moment for it to come up because again
I'm running on only a quarter of my
processor and you can see we now have
our IDs in descending order returned
let's also look at and this is so
important with anytime you're dealing
with big data let's create office with a
limit and you can of course do any of
this instead of with office we could do
this with office descending so you get
just the top two IDs on there but we're
going to limit just to two and of course
to execute that we have to dump office
underscore limit you can just think of
dumping your garbage into the pig pen
for the pig to eat there we go dump
office limit two and that's going to
just limit our office to the top two and
for our output we get our first row
which had our ID name profession and age
and our second row which is Jack who's
an engineer let's do a filter we'll call
it office underscore filter you guessed
it equals filter office by profession
equals and keep note this is uh similar
to how python does it with the double
equal signs for equal for doing a true
false statement so for your logic
statement remember to use two equal
signs in Pig and we're going to say it
equals doctor so we want to find out how
many doctors do we have on our list and
we'll go ahead and do our dump we're
dumping all our garbage into the pig pen
and we're letting Pig take over and see
what it can find out and see who's a
doctor on our list and we find uh
employee ID number two Bob is a doctor
30 years old for this next section I'll
we're going to cover something we see a
lot nowadays in data analysis and that's
word counting tokenization that is one
of the next big steps as we move forward
in our data analysis where we go from
say stock market analysis of highs and
lows and all the numbers to what are
people saying about companies on Twitter
what are they saying on the web pages
and on Facebook suddenly you need to
start counting words and finding out how
many words are totals I mean in the
first part of the document and so on
we're going to cover a very basic word
count example and in this case I've
created a document called wordrows.txt
and you can see here we have simply
learned as a company supporting online
learning simply learn helps people
attain their certifications simply learn
as an online community I love simply
learn I love programming I love data
analysis if I go ahead and saved this
into my documents folder so we could use
it and let me go ahead and open up a new
terminal window for our word count let
me go ahead and close the old one so
we're going to go in here and instead of
doing this as Pig we're going to do pig
minus X local and what I'm doing is I'm
telling the pig to start the pig shell
but we're going to be looking at files
local to our virtual box or the Centos
machine and let me go ahead and hit
enter on there just map
this up there we go and it will load Pig
up and it's going to look just the same
as the pig we were doing which was
defaulted to high to our Hadoop system
to our hdfs this is now defaulted to the
local system now we're going to create
lines we're going to load it straight
from the file remember last time we took
the hdfs and loaded it into there and
then loaded it into Pig since we've gone
the local we're just going to run a
local script we have lines equals load
home the actual full path home Cloudera
documents and I called it wordrows.txt
and as line is a character array so each
line and I've actually you can change
this to read each document I certainly
have done a lot of document analysis and
then you go through and do word counts
and different kind of counts in there so
once we go ahead and create our line
instead of doing the dump we're going to
go ahead and start entering all of our
different setups for each of our steps
we want to go through and let's just
take a look at this next one because the
load is straightforward we're loading
from this particular file since we're
locals loading it directly from here
instead of going into the Hadoop file
system and it says as and then each line
is read as a character array now we're
going to do words equal for each of the
lines generate Flat tokenize Line space
as word now there's a lot of ways to do
this this is if you're a programmer
you're just splitting the line up by
spaces there's actual ways to tokenize
it you gotta look for periods
capitalization there's all kinds of
other things you play with with this but
for the most basic word count we're just
going to separate it by spaces the
flattened takes the line and just
creates a it flattens each of the words
out so this is we're just going to
generate a bunch of words for each line
and then each each of those words is as
a word a little confusing in there but
if you really think about it we're just
going down each line separating it out
and we're generating a list of words one
thing to note is the default for
tokenize you can just do tokenized line
without the space in there if you do
that it'll automatically tokenize it by
space you can do either one and then
we're going to do group we're going to
group it by words so we're going to
group words by word so when we we split
it up each token is a word and it's a
list of words and so and so we're going
to group equals group words by word so
we're going to group all the same words
together and if we're going to group
them then we want to go ahead and count
them and so for count we'll go ahead and
create a word count variable and here's
our four each so for each grouped
grouped is our line where we group all
the words in the line that are similar
we're going to generate a group and then
we're going to count the words for each
grouped so for each line we group the
words together we're going to generate a
group and that's going to count the
words we want to know the word count in
each of those and that comes back in our
word count and finally we want to take
this and we want to go ahead and dump
word count and this is a little bit more
what you see when you start looking at
run scripts you'll see right here these
these lines right here we have each of
the steps you take to get there so we
load our file for each of our lines
we're going to generate and tokenize it
into words then we're going to take the
words and we're going to group them by
same words for each grouped we're going
to generate a group and we're just going
to count the words so we're going to
summarize all the words in here and
let's go ahead and do our dump word
count which executes all this and it
goes through our mapreduce it's actually
a local Runner you'll see down here you
start seeing where they still have
mapreduce but as a special Runner we're
mapping it that's a part of each row
being counted and grouped and then when
we do the word count that's the reducer
the reducer creates these keys and you
can see I is used three times a came up
once and came up once is to continue on
down here to attain online people
company analysis simply learn they took
the top rating with four certification
so all these things are then counted in
the how many words are used and in data
analysis this is probably the very the
beginnings of data analysis where you
might look at it and say oh they
mentioned love three times so whatever's
going on in this post it's about love
and what do they love you might attach
that to the different objects in here so
you can see that pig latin is fairly
easy to use there's nothing really you
know it may it takes a little bit to
learn the script depending on how good
your memory is as I get older my memory
leaks a little bit more so I don't
memorize it as much but that was pretty
straightforward the script we put in
there and then it goes through the full
map reduce localized run comes out and
like I said it's very easy to use that's
why people like Pig Latin is because
it's intuitive one of the things I like
about Pig Latin is when I'm
troubleshooting when we're
troubleshooting a lot of times you're
working with a small amount of data and
you start doing one line at a time and
so I can go lines equal load and there's
my loaded text and maybe I'll just dump
lines and then it's going to run it's
going to show me all the lines that I'm
working on in the small amount of data
and that way I can test that if I got an
error on there that said oh this isn't
working maybe I'll be like oh my gosh
I'm in mapreduce or I'm in the basic
grunt shell instead of the local path
current this start with an introduction
to hbase back in the days data used to
be less and was mostly structured we can
see we have structured data here we
usually had it like in a database where
you had every field was exactly the
correct length so if you had a name
field there's exactly 32 characters I
remember the old access database in
Microsoft the files were small if we had
you know hundreds of people in one
database that was considered Big Data
this data could be easily stored in
relational database or rdbms and we talk
about relational database you might
think of Oracle you might think of SQL
Microsoft SQL MySQL all of these have
evolved even from back then to do a lot
more today than they did but they still
fall short in a lot of ways and they're
all examples of an rdms or relationship
database then internet evolved and he
huge volumes of structured and
semi-structured data got generated and
you can see here with the
semi-structured data we have email if
you look at my spam filter you know
we're talking about all the HTML Pages
XML which is a lot of time is displayed
on our HTML and help desk Pages Json all
of this really has just even in the last
each year it almost doubles from the
year before how much of this is
generated so storing and processing this
data on an rdbms has become a major
problem and so the solution is we use
Apache hbase Apache hbase was the
solution for this let's take a look at
the history the hbase history and we
look at the hbase history we're going to
start back in 2006 November Google
released a paper on big table and then
in 2017 just a few months later hbase
prototype was created as a Hadoop
contribution later on in the Year 2007
in October first usable hbase along with
the Hadoop .15 was released and then in
January 2008 hbase became the
sub-project of Hadoop and later on that
year in October all the way into
September the next year hbase was
released to 0.81 version the 0.19
version and 0.20 and finally in May of
2010 hbase became Apache top level
project and so you can see in the course
of about four years hbase started off as
just an idea on paper and has evolved
all the way till 2010 as a solid project
under the Apache and since 2010 it's
continued to evolve and grow as a major
source for storing data in
semi-structured data so what is hbase
hbase is a column oriented database
management system derived from Google's
no SQL database bigtable that runs on
top of the Hadoop file system or the
hdfs it's an open source project that is
horizontally scalable and that's very
important to understand that you don't
have to buy a bunch of huge expensive
computers you're expanding it by
continually adding commodity machines
and so it's a linear cost expansion as
opposed to being exponential no SQL
database written in Java which permits
faster querying so job is to back in for
the hbase setup and it's well suited for
sparse data sets so it can contain
missing or n a values and this doesn't
Boggle it down like it would in other
database companies using hbase so let's
take a look and see who is using this no
SQL database for their servers and for
storing their data and we have
hortonworks which isn't a surprise
because they're one of the like Cloudera
hortonworks they are behind Hadoop and
one of the big developments and backing
of it and of course Apache hbase is the
open source behind it and we have
Capital One as Banks you also see Bank
of America where they're collecting
information on people and tracking it so
their information might be very sparse
they might have one Bank way back when
they collected information as far as the
person's family and what their income
for the whole family is and their
personal income and maybe another one
doesn't collect the family income as you
start seeing where you have data that is
very difficult to store where it's
missing a bunch of data how spots using
it Facebook certainly all of your
Facebook Twitter most of your social
medias are using it and then of course
there's JPMorgan Chase and Company
another bank that uses the hbase as
their data warehouse for nose SQL let's
take a look at an hbase use case so we
can dig a little bit more into it to see
how it functions telecommunication
company that provides mobile voice and
multimedia Services across China the
China mobile and China mobile they
generate billions of call detailed
records or CDR and so these cdrs and all
these records of these calls and how
long they are and different aspects of
the call maybe the tower they're
broadcasted from all of that is being
recorded so they can track it a
traditional database systems were unable
to scale up to the vast volumes of data
and provide a cost-effective solution no
good so storing in real-time analysis of
billions of call records was a major
problem for this company solution Apache
hbase hbase stores billions of rows of
detailed call records hp's perform forms
fast processing of Records using SQL
queries so you can mix your SQL and
nosql queries and usually just say no
SQL queries because of the way the query
Works applications of hbase one of them
would be in the medical industry hbase
is used for storing genome sequences
storing disease history of people of an
area and you can imagine how sparsat is
as far as both of those a genome
sequence might be only have pieces to it
that each person is unique or is unique
to different people and the same thing
with disease you really don't need a
column for every possible disease a
person could get you just want to know
what those diseases those people have
had to deal with in that area e-commerce
hbase is used for storing logs about
customer search history performs
analytics and Target advertisement for
Better Business insights sports hbase
stores match details in the history of
each match uses this data for better
prediction so when we look at age base
we all want to know what's the
difference between hbase versus rdbms
that is a relational database base
management system hbase versus rdbms so
the hbase does not have a fixed schema a
schema less defines only column families
and we'll show you what that means later
on an rdbms has a fixed schema which
describes the structure of the tables
and you can think of this as you have a
row and you have columns and each column
is a very specific structure how much
data can go in there and what it does
with the age base it works well with
structured and semi-structured data with
the rdbms it works only well with
structured data with the AIDS base it
can have denormalized data it can
contain missing or null values with the
rdbms it can store only normalized data
now you can still store a null value in
the rdbms but it still takes up the same
space as if we're storing a regular
value in many cases and it also for the
hbase is built for y tables it can be
scaled horizontally for instance if you
were doing a tokenizer of words and word
clusters you might have of 1.4 million
different words that you're pulling up
and combinations of words so with an
rdbms it's built for thin tables that
are hard to scale you don't want to
store 1.4 million columns in your SQL
it's going to crash and it's going to be
very hard to do searches with the age
base it only stores that data which is
part of whatever row you're working on
let's look at some of the features of
the hbase it's scalable data can be
scaled across various nodes as it is
stored in the hdfs and I always think
about this it's a linear add-on for each
terabyte of data I'm adding on roughly a
thousand dollars in commodity Computing
with an Enterprise machine we're looking
at about 10 000 at the lower end for
each terabyte of data and that includes
all your backup and redundancy so it's a
big difference it's like a tenth of the
cost to store it across the hbase it has
automatic failure support right ahead
log across clusters which provides
automatic support against failure
consistent read and write hbase provides
consistent read and write of the data
it's a Java API for client access
provides easy to use Java API for
clients block cache and Bloom filters so
the hbase supports block caching and
Bloom filters for high volume query
optimization let's dig a little deeper
into the hbase storage a space column
oriented storage and I told you we're
going to look into this to see how it
stores the data and here you can see you
have a row key and this really one of
the important references is each row has
to have its own key or your row ID and
then you have your column family and in
here you can see we have column family
one column family two column family
three and you have your column
qualifiers so you can have in column
family one you can have three columns in
there and there might not be any data in
that so when you go into column family
one and do a query for every column that
contains a certain thing that row might
not have anything in there and not be
queried where in column family two maybe
you have column one filled out and
column three filled out and so on and so
forth and then each cell is connected to
the row where the data is actually
stored let's take a look at this and
what it looks like when you fill the
data in so in here we have a row key
with a row ID and we have our employee
ID one two three that's pretty
straightforward you probably would even
have that on an SQL server and then you
have your column family this is where it
starts really separating out your column
family might have personal data and
under personal data you would have name
City age you might have a lot more than
just that you might have number of
children you might have degree all those
kinds of different things that go under
personal data and some of them might be
missing you might only have the name and
the age of an employee you might only
have the name the city and how many
children and not the age and so you can
see with the personal data you can now
collect a large variety of data and
stored in the hbase very easily and then
maybe you have a family of professional
data your designation your salary all
the stuff that the employee is doing for
you in that company let's dig a little
deeper into the hbase architecture and
so you can see here what looks to be a
complicated chart it's not as
complicated as you think from the Apache
a space we have the Zookeeper which is
used for monitoring what's going on and
you have your age Master this is the
hbase master of science regions and load
balancing and then underneath the region
or the hbase master then under the H
master or H base Master you have your
reader server serves data for read and
write and the region server which is all
your different computers you have in
your Hadoop cluster he'll have a region
an H log you'll have a store memory
store and then you have your different
files for H file that are stored on
there and those are separated across the
different computers and that's all part
of the hdfs storage system so we look at
the Architectural Components or regions
and we're looking at we're drilling down
a little bit hbase tables are divided
horizontally by a row so you have a key
range into regions so each of those IDs
you might have IDs one to twenty twenty
one to 50 or whatever they are regions
are assigned to the nodes in the cluster
called region servers a region contains
all rows in the table between the region
start key and the End Key again 1 to 10
11 to 20 and so forth these servers
serve data for read and write and you
can see here we have the client and the
get and the git sends it out and it
finds out where that startup is between
which start keys and in keys and then it
pulls the data from that different
region server and so the region sign
data definition language operation
create Elite are handled by the H master
so the H Master is telling it what are
we doing with this data what's going out
there assigning and reassigning regions
for Recovery or load balancing and
monitoring all servers so that's also
part of it so you know if your IDs if
you have 500 IDs across three servers
you're not going to put 400 IDs on
server 1 and 100 on the server 2 and
leaves Region 3 and Region 4 empty
you're going to split that up and that's
all handled by the H master and you can
see here it monitors region servers
assigns regions to region servers
assigns regions to Regions servers and
so forth and so forth hbase has a
distributed environment where age Master
alone is not sufficient to manage
everything hence zookeeper was
introduced it works with h master so you
have an active h Master which sends a
heartbeat signal to zookeeper indicating
that it's active and the Zookeeper also
has a heartbeat to the region server so
the region servers send their status to
zookeeper indicating they are ready for
read and write operation in active
server acts as a backup if the active h
Master fails it will come to the rescue
active Ace master and region servers
connect with a session to zookeeper so
you see your activation Master selection
region server session they're all
looking at the Zookeeper keeping that
pulse an active hmas and region server
connects with a session to the Zookeeper
and you can see here where we have
ephemeral nodes for active sessions via
heartbeats to indicate that the region
servers are up and running so let's take
a look at hbase read or write going on
there's a special hbase catalog table
called The Meta table which holds a
location of the regions in the cluster
here's what happens the first time a
client reads or writes data to hbase the
client gets the region server the host
the meta table from zookeeper and you
can see right here the client has a
request for your region server and goes
hey zookeeper can you handle this the
Zookeeper takes a look at it and goes ah
metal location is stored in Zookeeper so
it looks at its metadata on there and
then the metadata table location is sent
back to to the client the client will
query The Meta server to get the region
server corresponding to the row key if
it wants to access the client caches
this information along with the meta
table location and you can see here the
client going back and forth to the
region server with the information and
it might be going across multiple region
servers depending on what you're
querying so we get the region server for
row key from The Meta table that's where
that row key comes in and says hey this
is where we're going with this and so
once it gets the row key from the
corresponding region server we can now
put row or get Row from that region
server let's take a look at the hbase
meta table special hbase catalog table
that maintains a list of all the region
servers in the hbase storage system so
you see here we have the meta table we
have a row key and a value table key
region region server so the meta table
is used to find the region for the given
table key and you can see down here you
know our meta table comes in is going to
fire out where it's going with the
region server and we look a little
closer at the write mechanism in hbase
we have have write a head law or wall as
you abbreviate it kind of a way to
remember wall is right ahead log is a
file used to store new data that is yet
to be put on permanent storage it is
used for Recovery in the case of failure
so you can see here where the client
comes in and it literally puts the new
data coming in into this kind of
temporary storage or the wall on there
once it's gone into the wall then the
memory store mem store is the right
cache that stores a new data that has
not yet been written to disk there is
one mem store per column family per
region and once we've done that we have
three ack once the data is placed in mem
store the client then receives the
acknowledgment when the minister reaches
the threshold it dumps or commits the
data into H file and so you can see
right here we've taken our or has gone
into the wall the wall then Source it
into the different memory stores and
then the memory stores it says Hey we've
reached we're ready to dump that into
our H files and then it moves it into
the age files age files store the Roses
data as stored key value on risk so here
we've done a lot of theory let's dive in
and just take a look and see what some
of these commands look like and what
happens in our age base when we're
manipulating a no SQL setup
[Music]
foreign
so if you're learning a new setup it's
always good to start with where is this
coming from it's open source by Apache
and you can go to
hbase.apache.org and you'll see that it
has a lot of information you can
actually download the hbase separate
from the Hadoop although most people
just install the Hadoop because it's
bundled with it and if you go in here
you'll find a reference guide and so you
can go through the Apache reference
guide and there's a number of things to
look at but we're going to be going
through Apache H based shell that's what
we're going to be working with and
there's a lot of other interfaces on the
setup and you can look up a lot of the
different commands on here so we go into
the Apache hbase reference guide we go
down to read hbase shell commands from a
command file you can see here where it
gives you different options of formats
for putting the data in and listing the
data certainly you can also create files
and scripts to do this too but we're
going to look at the basics we're going
to go through this on a basic hbase
shell and one last thing to look at is
of course if you continue down the setup
you can see here where they have more
detail tell as far as how to create and
how to get to your data on your hbase
now I will be working in a virtual box
and this is by Oracle you can download
the Oracle virtual box you can put a
note in below for the YouTube as we did
have a previous session on setting up
virtual setup to run your Hadoop system
in there I'm using the Cloudera quick
start installed in here there's Hortons
you can also use the Amazon web service
there's a number of options for trying
this out in this case we have Cloudera
on the Oracle virtual box the virtual
box has Linux Centos installed on it and
then the Hadoop it has all the different
Hadoop flavors including hbase and I
bring this up because my computer is a
Windows 10 the operating system of the
virtual box is Linux and we're looking
at the hbase data warehouse and so we
have three very different entities all
running on my computer and that can be
confusing if it's a first time in and
working with this kind of setup now
you'll notice in our Cloudera setup they
actually have some hbase monitoring so I
can go underneath here and click on
hbase and master and it'll tell me
what's going on with my region servers
it'll tell me what's going on with our
backup tables right now I don't have any
user tables because we haven't created
any and this is only a single node and a
single hbase tour so you're not going to
expect anything too extensive in here
since this is for practice and education
and perhaps testing out package you're
working on it's not for really you can
deploy Cloudera of course but when you
talk about a quick start or a single
node setup that's what it's really for
so we can go through all the different
hbase and you'll see all kinds of
different information with zookeeper if
you saw it flash by down here what
version we're working in some zookeepers
part of the hbase setup where we want to
go is we want to open up a terminal
window and in Cloudera it happens to be
up at the top and when you click on here
you'll see your Cloudera terminal window
open and let me just expand this so we
have a nice full screen and then I'm
also going to zoom in that way I have a
nice big picture and you can see what
I'm typing and what's going out on and
to open up your H base shell simply type
hbase shell to get in and hit enter and
you'll see it takes just a moment to
load and we'll be in our age based shell
for doing hbase commands once we've
gotten into our H base shell you'll see
you'll have the hbase prompt information
ahead of it we could do something simple
like list this is going to list whatever
tables we have it so happens that
there's a base table that comes with
hbase now we can go ahead and create and
I'm going to type in just create what's
nice about this is it's going to throw
me kind of a it's going to say hey
there's no just straight create but it
does come up and tell me all these
different formats we can use for create
so we can create our table and one of
our families you can add splits names
versions all kinds of things you can do
with this let's just start with a very
basic one on here and let's go ahead and
create and we'll call it new table now
this is to call it new TBL for table new
table and then we also want to do let's
do knowledge so let's take a look at
this I'm creating a new table and it's
going to have a family of knowledge
ginet and let me hit enter it's going to
come up it's going to take it a second
to go ahead and create it now we have
our new table in here so if I go list
you'll now see table and new table so
you can now see that we have the new
table and of course the default table
that's set up in here and we can do
something like uh describe we can
describe and then we're going to do new
TBL and when we describe it it's going
to come up it's going to say hey name I
have knowledge data block encoding none
Bloom filter row or replications Go
version all the different information
you need new we have minimum version 0
forever deleted cells false block size
in memory and you can look this stuff up
on apache.org to really track it down
one of the things that's important to
note is versions so you have your
different versions of the data that's
stored and that's always important to
understand that we might talk about that
a little bit later on and then we have
to describe it we can also do a status
the status says I have one active Master
going on that's our hbase as a whole we
can do status
summary I should do the same thing as
status so we got the same thing coming
up and now that we've created let's go
ahead and put something in it so we're
going to put new TBL and then we want
Row one you know what before I even do
this let's just type input and you can
see when I type in put it gives us like
a lot of different options of how it
works and different ways of formatting
our data as it goes in and all of them
usually begin with the new table new TBL
then we have in this case we'll call it
Row one and then we'll have knowledge if
you remember we created knowledge
already and we'll do knowledge Sports
and then in knowledge and sports we're
going to set that equal to Cricut so
we're going to put underneath this our
knowledge setup that we have a thing
called Sports in there and we'll see
what this looks like in just a second
let's go ahead and put in we'll do a
couple of these let's see let's do
another row one and this time set of
sports Let's Do Science you know this
person not only you know we have Row one
which is both knowledgeable and Cricut
and also in chemistry so it's a chemist
who plays Cricket in row one and let's
see if we have let's do another row one
just to keep it going and we'll do
science in this case let's do physics
not only in chemistry but also physicist
I have quite a joy in physics myself so
here we go we have uh Row one there we
go and then let's do row two let's see
what that looks like when we start
putting in row two and in row two this
person is has knowledge in economics
this is a master of business and how or
maybe it's Global economics maybe it's
just for the business and how it fits in
with the country's economics and we call
it macroeconomics so I guess it is for
the whole country there so we have
knowledge economics macroeconomics and
then let's just do one more we'll keep
it as row two and this time our
Economist is also a musician so we'll
put music and they happen to have
knowledge and they enjoy oh let's do pop
music they're into the current pop music
going on so we've loaded our database
and you'll see we have two rows Row one
and row two in here and we can do is we
can list the contents of our database by
simply doing scan scan and then let's
just do scan by itself so you can see
how that looks you can always just type
in there and it tells you all the
different setups you can do with scan
and how it works in this case we want to
do scan new TBL and in our scan new TBL
we have Row one row one row two row two
and you'll see Row 1 has a column called
knowledge science time step value
crickets value physics so it has
information is when it was created when
the timestamp is Row one also has
knowledge Sports and a value of Cricut
so we have sports and Science and this
is interesting because if you remember
up here we also gave it originally we
told it to come in here and have
chemistry we had science chemistry and
science physics and we come down here I
don't see the chemistry why because
we've now replaced chemistry with
physics so the new value is physics on
here let me go ahead and clear down a
little bit and in this we're going to
ask the question is enabled new table
when I hit enter in here you're going to
see it comes out true and then we'll go
ahead and disable it let's go ahead and
disable new table make sure I have our
quotes around it and now that we've
disabled it what happens when we do the
scan we do the scan new table and hit
enter you're going to see that we get an
error coming up so once it's disabled
you can't do anything with it until we
re-enable it now before we enable the
table Let's do an alteration on it and
here's our new table and this should
look a little familiar because it's very
similar to create we'll call this test
info we'll hit enter in there it'll take
just a moment for updating and then we
want to go ahead and enable it so let's
go ahead and enable our new table so
it's back up and running and then we
want to describe describe new table and
we come in here you'll now see we have
name knowledge and under there we have
our data encoding and all the
information under knowledge and then we
also have have down below test info so
now we have the name test info and all
the information concerning the test info
on here and we'll simply enable it new
table so now it's enabled oops I already
did that I guess we'll enable it twice
and so let's start looking at well we
had scan new table and you can see here
where it brings up the information like
this but what if we want to go ahead and
get a row so we'll do R1 and when we do
hbase R1 you can see we have knowledge
science and it has a timestamp value
physics and we have knowledge Sports and
it has a time stamp on it and value
Cricut and then let's see what happens
when we put into our new table and in
here we want Row one and if you can
guess from earlier because we did
something similar we're going to do
knowledge economics
and then it's going to be instead of I
think it was what macroeconomics is now
market economics and we'll go back and
do our git command and now see what it
looks like and we can see here where we
have knowledge economics it has a time
stamp value market economics physics and
Cricut and this is because we have
economic science and sports those are
the three different columns that we have
and then each one has different
information in it and so if you've
managed to go through all these commands
and look at Basics on here you'll now
have the ability to create a very basic
hbase setup no SQL setup based on your
columns and your rows thank you guys now
that we're done with Hadoop ecosystem we
have Shruti who will take us through Big
Data applications
before we move on to the applications
let's have a quick look at the big data
market revenue forecast worldwide from
2011 to 2027. so here's a graph in which
the y-axis represents the revenue in
billion US Dollars and the x-axis
represents the years as it is seen
clearly from the graph big data has
grown until 2019 and statistics predict
that this growth will continue even in
the future this growth is made possible
as numerous companies use big data in
various domains to boost their revenue
we will look into few of such
applications the first Big Data
application we will look into is weather
forecast imagine there is a sudden storm
and you're not even prepared that would
be a terrifying situation isn't it
dealing with any calamities such as
hurricane storms floods would be very
inconvenient if we are caught off guard
the solution is to have a tool that
predicts the weather of the coming days
well in advance this tool needs to be
accurate and to make such a tool big
data is used so how does Big Data help
here well it allows us to gather all the
information required to predict the
weather information such as the climate
change details wind direction
precipitation previous weather reports
and so on after all this data is
collected it becomes easier for us to
spot a trend and identify what's going
to happen next by analyzing all of this
big data a weather prediction engine
works on this analysis it predicts the
weather of every region across the world
for any given time by using such a tool
we can be well prepared to face any
climate change or any natural Calamity
let's take an example of a landslide and
try to understand how big data is used
to tackle such a situation predicting a
landslide is very difficult with just
the basic warning signs lack of this
prediction can cause a huge damage to
life and property this challenge was
studied by the University of Melbourne
and they developed a tool which is
capable of predicting a landslide this
tool predicts the boundary where a
landslide is likely to occur two weeks
before this magical tool works on both
big data and Applied Mathematics and
accurate prediction like this which is
made two weeks before can save lives and
help in relocating people in that
particular region it also gives us an
insight into the magnitude of the
upcoming destruction this is how big
data is used in weather forecast and in
predicting any natural calamities across
the world let us now move on to our next
application that is big data application
in the field of media and entertainment
the media and the entertainment industry
is a massive one leveraging big data
here can produce sky-high results and
boost the revenue for any company let us
see the different ways in which big data
is used in this industry have you ever
noticed that you come across relevant
advertisements in your social media
sites and in your mailboxes well this is
done by analyzing using all your data
such as your previous browsing history
and your purchase data Publishers then
display what you like in the form of ads
which will in turn catch your interest
in looking into it next up is customer
sentiment analysis customers are very
important for a company the happier the
customer the greater the company's
Revenue Big Data helps in gathering all
the emotions of a customer through their
posts messages conversations Etc these
emotions are then analyzed to arrive at
a conclusion regarding the customer
satisfaction if the customer is unhappy
the company strives to do better the
next time and provides their customers a
better experience while purchasing an
item from an e-commerce site or while
watching videos on an entertainment site
you might have noticed a segment which
says most recommended list for you this
list is a personalized list which is
made Available to You by analyzing all
the data such as your previous watch
History your subscriptions your likes
and so on recommendation engine is a
tool that filters and analyzes all this
data and provides you with a list that
you would most likely be interested in
by doing so the site is able to retain
and engage its customer for a longer
time next is customer churn analysis in
simple words customer churn happens when
a customer stops a subscription with a
service predicting and preventing this
is of Paramount importance to any
organization by analyzing the behavioral
patterns of previously churned customers
an organization can identify which of
their current customers are likely to
churn by analyzing all of this data the
organization can then Implement
effective programs for customer
retention let us now look into an use
case of Starbucks big data is
effectively used by the Starbucks app 17
million users use this app and you can
imagine how much data they generate data
in the form of of their coffee buying
habits the stores they visit and to the
time they purchase all of this data is
fed into the app so when a customer
enters a new Starbucks location the
system analyzes all their data and we
are provided with their preferred order
this app also suggests new products to
the customer in addition to this they
also provide personalized offer and
discounts on special occasions moving on
to our next sector which is Healthcare
it is one of the most important sectors
big data is widely used here to save
lives with all the available Big Data
medical researchers are done very
effectively they are performed
accurately by analyzing all the previous
medical histories and new treatments and
medicines are discovered cure can be
found out even for few of the incurable
diseases there are cases when one
medication need not be effective for
every patient hence personal care is
very important Personal Care is provided
to each patient depending on their past
medical history and individuals medical
history along with their body parameters
are analyzed and personal attention is
given to each of them as we all know
Medical Treatments are not very pocket
friendly every time a medical treatment
is taken the amount increases this can
be reduced if readmissions are brought
down analyzing all the data precisely
will deliver a long-term efficient
result which will in turn prevent a
patient's readmission frequently with
globalization came an increase in the
ease for infectious diseases to spread
widely based on geography and
demographics Big Data helps in
predicting where an outbreak of epidemic
viruses are most likely to occur an
American Healthcare Company United
Healthcare uses big data to detect any
online medical fraud activities such as
payment of unauthorized benefits
intentional misrepresentation of data
and so on the Healthcare company runs
disease management programs the success
rates of these programs are predicted
using big data depending on how patients
respond to it the next sector we will
look into is logistics Logistics looks
into the process of transportation and
storage of goods the movement of a
product from its supplier to a consumer
is very important big data is used to
make this process faster and efficient
the most important factor in logistics
is the time taken for the products to
reach their destination to achieve
minimum time sensors within the vehicle
analyze the fastest route this analysis
is based on various data such as the
weather traffic the list of orders and
so on by doing so the fastest route is
obtained and the delivery time is
reduced capacity planning is another
factor which needs to be taken into
consideration details regarding the
workforce and the number of vehicles are
analyzed thoroughly and each vehicle is
allocated a different route this is done
as there is no need for many trucks to
travel in the same direction which will
be pointless depending on the analysis
of the available Workforce and resources
this decision is taken big data
analytics also Finds Its use in managing
warehouses efficiently this analysis
along with tracking sensors provide
information regarding the underutilized
space which results in efficient
resource allocation and eventually
reduces the cost customer satisfaction
is important in logistics just like it
is in any other sector customer
reactions are analyzed from the
available data which will eventually
create an instant feedback loop a happy
customer will always help the company
gain more customer us let us now look
into a use case of UPS as you know UPS
is one of the biggest shipping company
in the world they have a huge customer
database and they work on data every
minute UPS uses big data to gather
different kinds of data regarding the
weather the traffic jams the geography
the locations and so on after collecting
all this data they analyze it to
discover the best and the fastest route
to the destination in addition to this
they also use big data to change the
routes in real time this is how
efficiently UPS leverages Big Data next
up we have a very interesting sector
that is the travel and tourism sector
the global tourism Market is expected to
grow in the near future big data is used
in various ways in this sector let us
look into a few of them hotels can
increase their revenue by adjusting the
room tariffs depending on the peak
Seasons such as holiday seasons festive
since and so on the tourism industry
uses all of this data to anticipate the
demand and maximize their revenue big
data is also used by Resorts and hotels
to analyze various details regarding
their competitors this analysis result
helps them to incorporate all the good
facilities their competitors are
providing and by doing so the hotel is
able to flourish further a customer
always comes back if they are offered
good packages which are more than just
the basic ones looking into a customer's
past travel history likes and
preferences hotels can provide its
customers with personalized experiences
which will interest them highly
investing in an area which could be the
Hub of Tourism is very wise few
countries use big data to examine the
tourism activities in their country and
this in turn helps them discover new and
fruitful investment opportunities let us
look into one of the best online home
stay networks Airbnb and see how big
data is used by them Airbnb undoubtedly
provides its customers with the best
accommodation across the world big data
is used by it to analyze the different
kinds of available properties depending
on the customer's preferences the
pricing the keywords previous customers
ratings and experiences Airbnb filters
out the best result Big Data Works its
magic yet again now we will move on to
our final sector which is the government
and law enforcement sector maintaining
Law and Order is of utmost importance to
any government it is a huge task by
itself Big Data plays an active role
here and in addition to this it also
helps governments bring in new policies
and schemes for the welfare of its
citizens the police department is able
to predict criminal activities way
before it happens by analyzing big data
information such as the previous crime
record odds in a particular region the
safety aspect in that region and so on
by analyzing these factors they are able
to predict any activity which breaks the
law and order of the region governments
are able to tackle unemployment to a
great extent by using big data by
analyzing the number of students
graduating every year to the number of
relevant job openings the government can
have an idea of the unemployment rate in
the country and then take necessary
measures to tackle it our next factor is
poverty in large countries it is
difficult to analyze which area requires
attention and development big data
analytics makes it easier for
governments to discover such areas
poverty gradually decreases once these
areas begin to develop governments have
to always be on the lookout for better
development a public survey voices the
opinion of a country's citizens
analyzing all the data collect it from
such service can help governments build
better policies and services which will
benefit its citizens let us now move on
to our use case did you know that the
New York Police Department uses big data
analytics to protect its citizens the
department prevents and identifies
Crimes by analyzing a huge amount of
data which includes fingerprints certain
emails and records from previous police
investigations and so on after analyzing
all of this data meaningful insights are
drawn from it which will help the police
in taking the required preventive
measures against crimes thank you Shruti
let us now have a look at the next data
processing framework Spark we have our
instructor Ajay who will take us through
Apache spark with installation welcome
to this tutorial on Apache spark one of
the most in-demand Technologies and
processing Frameworks in the Big Data
world and here we will learn on Apache
spark history of spark what is spark
Hadoop which is a framework again West
spark components of Apache spark that is
spark core spark SQL spark streaming
spark ml lib and Graphics then we will
learn on spark architecture applications
of spark spark use cases so let's begin
with understanding about history of
Apache spark it all started in 2009 as a
project at UC Berkeley amp Labs by mate
zaheria in 2010 it was open source under
a BST license in 2013 spark became an
Apache top level project and in 2014
used by data bricks to sort large scale
data sets and it set a new world record
so that's how Apache spark started and
today it is one of the most in-demand
processing framework or I would say in
memory Computing framework which is used
across the Big Data industry so what is
Apache spark let's learn about this
Apache spark is a open source in-memory
Computing framework or you could say
data processing engine which is used to
process data in batch and also in real
time across various cluster computers
and it has a very simple programming
language behind the scenes that is Scala
which is used although if users would
want to work on spark they can work with
python they can work with Scala they can
work with Java and so on even R for that
matter so it supports all these
programming languages and that's one of
the reasons that it is called polyglot
wherein you have good set of libraries
and support from all the programming
languages and developers and data
scientists incorporate spark into their
applications or build spark based
applications to process analyze query
and transform data at a very large scale
so these are key features of Apache
spark now if you compare Hadoop West
spark we know that Hadoop is a framework
and it basically has mapreduce which
comes with Hadoop for processing data
however processing data using mapreduce
in Hadoop is quite slow because it is a
batch oriented operation and it is time
consuming if you if you talk about spark
spark can process the same data 100
times faster than mapreduce as it is a
in-memory Computing framework well there
can always be conflicting ideas saying
what if my spark application is not
really efficiently coded and my map
reduce application has been very
efficiently coded well then it's a
different case however normally if you
talk about code which is efficiently
written for mapreduce or for spark based
processing spark will win the battle by
doing almost 100 times faster than
mapreduce so as I mentioned Hadoop
performs batch processing and that is
one of the paradigms of mapreduce
programming model which involves mapping
and reducing and that's quite rigid so
it performs batch processing the
intermittent data is written to sdfs and
written right back from sdfs and that
makes hadoops mapreduce processing
slower in case of spark it can perform
both batch and real-time processing
however a lot of use cases are based on
real-time processing take an example of
Macy's take an example of retail giant
such as Walmart and there are many use
cases who would prefer to do real-time
processing or I would say near real time
processing so when we say real time or
near real time it is about processing
the data as it comes in or you're
talking about streaming kind of data now
Hadoop or hadoop's map reduce obviously
was started to be written in Java now
you could also write it in Scala or in
Python however if you talk about
mapreduce it will have more lines of
code since it is written in Java and it
will take more times to execute you have
to manage the dependencies you have to
do the right declarations you have to
create your mapper and reducer and
Driver classes however if you compare
spark it has few lines of code as it is
implemented in Scala and Scala is a
statically typed dynamically inferred
language it's very very concise and the
benefit is it has features from both
functional programming and object
oriented language and in case of Scala
whatever code is written that is
converted into byte code codes and then
it runs in the jvm now Hadoop supports
Kerberos authentication there are
different kind of authentication
mechanisms Kerberos is one of the
well-known ones and it can really get
difficult to manage now spark supports
authentication via a shared secret it
can also run on yarn leveraging the
capability of Kerberos so what are spark
features which really makes it unique or
in demand processing framework when we
talk about spark features one of the key
features is fast processing so spark
contains resilient distributed data sets
so rdds are the building blocks for
spark and we learn more about rdds later
so spark contains rdds which saves huge
time taken in reading and writing
operations so it can be 100 times or you
can say 10 to 100 times faster than
Hadoop when we say in memory Computing
here I would like to make a note that
there is a a difference between caching
and in-memory Computing think about it
caching is mainly to support read ahead
mechanism where you have your data
pre-loaded so that it can benefit
further queries however when we say in
memory Computing we are talking about
lazy valuation we are talking about data
being loaded into memory only and only
when a specific kind of action is
invoked so data is stored in Ram so here
we can say Ram is not only used for
processing but it can also be used for
storage and we can again decide whether
we would want that Ram to be used for
persistence or just for computing so it
can access the data quickly and
accelerate the speed of analytics now
spark is quite flexible it supports
multiple languages as I already
mentioned and it allows the developers
to write applications in Java Scala r or
python it's quite fault tolerance so
spark contains these rdds or you could
say execution logic or you could say
temporary data sets which initially do
not have any data loaded and the data
will be loaded into rdds only when
execution is happening so these can be
fault tolerant as these rdds are
distributed across multiple nodes so
failure of one worker node in the
cluster will really not affect the rdds
because that portion can be recomputed
so it ensures loss of data it ensures
that there is no data loss and it is
absolutely fault tolerant it is for
better than analytics so sparked has
Rich set of SQL queries machine learning
algorithms complex analytics all of this
supported by various spark components
which we will learn in coming slides
with all these functionalities analytics
can be performed better in terms of
spark so these are some of the key
features of spark however there are many
more features which are related to
different components of spark and we
will learn about them so what are these
components of spark which I'm talking
about spark core so this is the core
component which basically has rdds which
has a core engine which takes care of
your processing now you also have spark
SQL so people who would be interested in
working on structured data or data which
can be structurized would want to prefer
using spark SQL and Spark SQL internally
has components or features like data
frames and data sets which can be used
to process your structured data in a
much much faster way you have spark
streaming now that's again an important
component of spark which allows you to
create your spark streaming applications
which not only works on data which is
being streamed in or data which is
constantly getting generated but you
would also or you could also transform
the data you could analyze or process
the data as it comes in in smaller
chunks you have Sparks mlib now this is
basically a set of libraries which
allows developers or data scientists to
build their machine learning algorithms
so that they can do Predictive Analytics
or prescriptive descriptive preemptive
analytics or they could build their
recommendation systems or bigger smarter
machine learning algorithms using these
libraries and then you have Graphics so
think about organizations like LinkedIn
or say Twitter where you have data which
naturally has a network kind of flow so
data which could be represented in the
form of graphs now here when I talk
about graphs I'm not talking about pie
charts or bar charts but I'm talking
about Network related data that is data
which can be networked together which
can have some kind of relay structure
the technique about Facebook think about
LinkedIn where you have one person
connected to other person or one company
connected to other companies so if we
have our data which can be represented
in the form of network graphs then spark
has a component called Graphics which
allows you to do graph based processing
so these are some of the components of
Apache spark spark core spark SQL spark
streaming spark mlib and Graphics so to
learn more about components of spark
let's learn here about spark core now
this is the base engine and this is used
for large scale parallel and distributed
data processing so when you work with
spark at least and the minimum you would
work with is spark core which has rdds
as the building blocks of your spark so
it is responsible for your memory
management your fault recovery
scheduling Distributing and monitoring
jobs on a cluster and interacting with
storage system so here I would like to
make a key point that Spar by itself
does not have its own storage it relies
eyes on storage now that storage could
be your sdfs that is hadoop's
distributed file system it could be a
database like nosql database such as
hbase or it could be any other database
say rdbms from where you could connect
your spark and then fetch the data
extract the data process it analyze it
so let's learn a little bit about your
rdds resilient distributed data sets now
spark core which is the base engine or
the core engine is embedded with the
building blocks of spark which is
nothing but your resilient distributed
data set so as the name says it is
resilient so it is existing for a
shorter period of time distributed so it
is distributed across nodes and it is a
data set where the data will be loaded
or where the data will be existing for
processing so it is immutable fault
tolerant distributed section of objects
so that's what your rdd is and there are
mainly two operations which can be
performed on an rdd now to take an
example of this say I want to process a
particular file now here I could write a
simple code in Scala and that would
basically mean something like this so if
I say well which is to declare the
variable I would say well X and then I
could use what we call a spark context
which is basically the most important
entry point of your application so then
I could use a method of spark context
for example that is text file and then I
could point it to a particular file so
this is just a method of your spark
context and Spark context is the entry
point of your application now here I
could just give a path in this method so
what does this step do it does not do
any evaluation so when I say a Val X I'm
creating a immutable variable and to
that variable I'm assigning a file now
what this step does is it actually
creates a rdd resilient distributed data
set so we can imagine this as a simple
execution logic a EMT data set which is
created in memory of your node so if I
would say I have multiple nodes in which
my data is split and stored I'm
imagining that your yarn your spark is
working with Hadoop so I have Hadoop
which is using say two nodes and this is
my distributed file system sdfs which
basically means my file is written to
hdfs and it also means that the file
related blocks are stored in the
underlying disk of these machines so
when I say Val x equals SC dot text file
that is using a method of spark context
now there are various other methods like
whole text files parallelized and so on
this step will create an rdd so you can
imagine this as a logical data set which
is created in memory across these nodes
because these nodes have the data
however no data is loaded here so this
is the first rdd and I can say first
step in what we call as a tag a tag
which will have series of steps which
will get executed at later stage now
later I could do further processing on
this I could say well Y and then I could
do something on X so I could say x dot
map and I would want to apply a function
to every record or every element in this
file and I could give a logic here x dot
map now this second step is again
creating an rdd a resilient distributed
data set you can say second step in my
dag okay and here you have a external Rd
one more rdd created which depends on
the first RTD so my first RTD becomes
the base rdd or parent rdd and the
resultant RTD becomes the child rdd then
we can go further and we could say well
Z and I would say okay now I would want
to do some filter on y so this filter
which I am doing here and then I could
give a logic might be I'm searching for
a word I am searching for some pattern
so I could say well Z equals y dot
filter which again creates one more rdd
a resilient distributed data set in
memory and a you can say this is nothing
but one more step in the dag so this is
my tag which is a series of steps which
will be executed now here when does the
execution happen when the data get when
will the data get loaded into these rdds
so all of this that is using a method
using a transformation like map using a
transformation like filter or flat map
or anything else these are your
Transformations so the operations such
as map filter join Union and many others
will only create rdds which basically
means it is only creating execution
Logic No data is evaluated no operation
is happening right now only and only
when you invoke an action that is might
be you want to print some result might
be you want to take some elements and
see that might be you want to do a count
so those are actions which will actually
trigger the execution of this dag right
from the beginning so if I here say Z
dot count where I would want to just
count the number of words which I am
filtering this is an action which is
invoked and this will trigger the
execution of dag right for from the
beginning so this is what happens in a
spark now if I do a z dot count again it
will start the whole execution of dag
again right from the beginning so my Z
dot count second time in action is
invoked again the data will be loaded in
the first RTD then you will have map
then you will have filter and finally
you will have result so this is the core
concept of your rdds and this is how RTD
works so mainly in spark there are two
kind of operations one is your
Transformations and one is your actions
Transformations or using a method of
spark context will always and always
create an RTD or you could say a step in
the tag actions are something which will
invoke the execution which will invoke
the execution from the first rdd till
the last rdd where you can get your
result so this is how your rdds work now
when we talk about components of spark
let's learn a little bit about spark SQL
so spark SQL is a component type
processing framework which is used for
structured and semi-structured data
processing so usually people might have
their structured data stored in rdbms or
in files where data is structured with
particular delimiters and has a pattern
and if one wants to process the
structured data if one wants to use
spark to do in memory processing and
work on the structured data they would
prefer to use spark SQL so you can work
on different data formats say CSV Json
you can even work on smarter formats
like Avro parquet even your binary files
or sequence files you could have your
data coming in from an rdbms which can
then be extracted using a jdbc
connection so at the bottom level when
you talk about Spark SQL it has a data
source API which basically allows you to
get the data in whichever format it is
now spark SQL has something called as
data frame API so what are data frames
data frames in short you can visualize
or imagine as rows and columns or if
your data can be represented in the form
of rows and columns with some column
headings so data frame API allows you to
create data frames so like my previous
example when you work on a file when you
want to process it you would convert
that into an rdd using a method of smart
context or by doing some Transformations
so in the similar way when you use data
frame so when you want to use spark SQL
you would use
Sparks context which is SQL context or
Hive context or spark which allows you
to work with data frames so like in my
earlier example we were saying Val x
equals SC dot text file now in case of
data frames instead of SC you would be
using say spark dot something so spark
context is available for your data
frames API to be used in older versions
like spark 1.6 and so on we were using
Hive context or SQL context so if you
were working with spark 1.6 you would be
saying well x equals SQL context dot
here we would be using spark dot so data
frame API basically allows you to create
data frames out of your structured data
which also lets spark know that data is
already in a particular structure it
follows a format and based on that your
Sparks back in DAC scheduler right so
when I say about dag I talk about your
sequence of steps so spark is already
aware of what are the different steps
involved in your application so your
data frame API basically allows you to
create data frames out of your data and
data frames when I say I'm talking about
rows and columns with some headings and
then you have your data frame DSL
language or you can use spark SQL or
Hive query language any of these options
can be used to work with your data
frames so to learn more about data
frames follow in the next sessions when
you talk about spark streaming now this
is very interesting for organizations
who would want to work on streaming data
imagine a store like Macy's where they
would want to have machine learning
algorithms now what would these machine
learning algorithms do suppose you have
a lot of customers walking in the store
and they are searching for particular
product or particular item so there
could be cameras placed in the store and
this is being already done there are
cameras placed in the store which will
keep monitoring in which corner of the
store there are more customers now once
camera captures this information this
information can be streamed in to be
processed by algorithms and those
algorithms will will see which product
or which series of product customers
might be interested in and if this
algorithm in real time can process based
on the number of customers based on the
available product in the store it can
come up with a attractive alternative
price so that which the price can be
displayed on the screen and probably
customers would buy the product now this
is a real-time processing where the data
comes in algorithms work on it do some
computation and give out some result and
which can then result in customers
buying a particular product so the whole
essence of this machine learning and
real-time processing will really hold
good if and when customers are in the
store or this could relate to even a
online shopping portal where there might
be machine learning algorithms which
might be doing real-time processing
based on the clicks which customer is
doing based on the clicks based on
customer history based on customer
Behavior algorithms can come up with
recommendation of products or better
altered price so that the sale happens
now in this case we would be seeing the
essence of real-time processing only in
a fixed or in a particular duration of
time and this also means that you should
have something which can process the
data as it comes in so spark streaming
is a lightweight API that allows
developers to perform batch processing
and also real-time streaming and
processing of data so it provides secure
reliable fast processing of live data
streams so what happens here in spark
streaming in brief so you have a input
data stream now that data stream could
be a file which is constantly getting
appended it could be some kind of
metrics it could be some kind of events
based on the clicks which customers are
doing or based on the products which
they are choosing in a store this input
data stream is then pushed in through a
spark streaming application now spark
streaming application will broke break
this content into smaller streams what
we call as discreticized streams or
batches of smaller data on which
processing can happen in frames so you
could say process my file every five
seconds for the latest data which has
come in now there are also some windows
based uh options like when I say windows
I mean a window of past three events
window of past three events each event
being of five seconds so your batches of
smaller data is processed by Spark
engine and this process data can then be
stored or can be used for further
processing so that's what spark
streaming does when you talk about mlib
it's a low level machine learning
library that is simple to use scalable
and compatible with various programming
languages now Hadoop also has some
libraries like you have Apache mahaut
which can be used for machine learning
algorithms however in terms of spark we
are talking about machine learning
algorithms which can be built using
mlibs libraries and then spark can be
used for processing so mlib eases the
deployment and development of scalable
machine learning algorithms I mean think
about your clustering techniques so
think about your classification where
you would want to classify the data
where you would want to do supervised or
unsupervised learning think about
collaborative filtering and many other
data science related techniques or
techniques which are required to build
your recommendation engines or machine
learning algorithms can be built using
Sparks ml lip Graphics is Spark's own
graph computation engine so this is
mainly if you are interested in doing a
graph based processing think about
Facebook think about LinkedIn where you
can have your data which can be stored
and that data has some kind of network
connections or you could say it is well
networked I could say x is connected to
y y is connected to z z is connected to
a so x y z a all of these are in terms
of graph terminologies we call as
vertices or vertex which are basically
being connected and the connection
between the
are called edges so I could say a is
friend to B so A and B are vertices and
friend a relation between them is The
Edge now if I have my data which can be
represented in the form of graphs if I
would want to do a processing in such
way this could be not only for social
media it could be for your network
devices it could be a cloud platform it
could be about different applications
which are connected in a particular
environment so if you have data which
can be represented in the form of graph
then Graphics can be used to do ETL that
is extraction transformation load to do
your data analysis and also do
interactive graph computation so graph x
is quite powerful now when you talk
about spark your spark can work with
your different clustering uh
Technologies so it can work with Apache
mesos that's how Spar came in where it
was initially to prove The credibility
of Apache mesos spark can work with yarn
which is usually you will see in
different working environments Spa can
also work as Standalone that means
without Hadoop spark can have its own
setup with master and worker processes
so usually or you can say technically
spark uses a Master Slave architecture
now that consists of a driver program
that can run on a master node it can
also run on a client node it depends on
how you have configured or what your
application is and then you have
multiple executors which can run on
worker nodes so your master node has a
driver program and this driver program
internally has the spark context so your
spark Every Spark application will have
a driver program and that's driver
program has a inbuilt or internally used
spark context which is basically your
entry point of application for any spark
functionality so your driver or your
driver program interacts with your
cluster manager now when I say interacts
with clustered manager so you have your
spark context which is the entry point
that takes your application request to
the cluster manager now as I said your
cluster manager could be say Apache
mesos it could be yarn it could be spark
Standalone Master itself so your cluster
manager in terms of yarn is your
resource manager so your spark
application internally runs as series or
set of tasks and processes your driver
program wherever that is run will have a
spark context and Spark context will
take care of your application execution
how does that do it spark context will
talk to Cluster manager so your cluster
manager could be on and in terms of when
I say cluster manager for yarn would be
resource manager so at high level we can
say a job is split into multiple tasks
and those tasks will be distributed over
the slave nodes or worker nodes so
whenever you do some kind of
transformation or you use a method of
spark context and rdd is created and
this RTD is distributed across multiple
nodes as I explained earlier worker
nodes are the slaves that run different
tasks so this is how a spark
architecture looks like now we can learn
more about spark architecture and its
interaction with yarn so usually what
happens when your spark context
interacts with the cluster manager so in
terms of yarn I could say resource
manager now we already know about yarn
so you would have say node managers
running on multiple machines and each
machine has some RAM and CPU cores
allocated for your node manager on the
same machine you have the data nodes
running which obviously are there to
have the Hadoop related data so whenever
a application wants to process the data
your application via spark contacts
contacts the cluster managers that is
resource manager now what does resource
manager do resource manager makes a
request so resource manager makes
requests to the node manager of the
machines wherever the relevant data
resides asking for containers so your
resource manager is negotiating or
asking for containers from node manager
saying hey can I have a container of 1GB
RAM and one CPU core can I have a
container of 1GB RAM and one CPU core
and your node manager based on the kind
of processing it is doing will approve
or deny it so node manager would say
fine I can give you the container and
once this container is allocated or
approved by node manager resource
manager will basically start an extra
piece of code called App Master so App
Master is responsible for execution of
your application locations whether those
are spark applications or mapreduce so
your application Master which is a piece
of code will run in one of the
containers that is it will use the RAM
and CPU core and then it will use the
other containers which were allocated by
node manager to run the tasks so it is
within this container which can take
care of execution so what is a container
a combination of RAM and CPU core so it
is within this container we will have a
executed process which would run and
this executor process is taking care of
your application related tasks so that's
how overall spark Works in integration
with yarn now let's learn about the
spark cluster managers as I said spark
can work in a standalone mode so that is
without Hadoop so by default application
submitted to spark Standalone mode
cluster will run in fifo order and each
application will try to use all the
available nodes so you could have a
spark stand in loan class faster which
basically means you could have multiple
nodes on one of the nodes you would have
the master process running and on the
other nodes you would have the spark
worker processes running so here we
would not have any distributed file
system because spark is Standalone and
it will rely on an external storage to
get the data or probably the file system
of the nodes where the data is stored
and processing will happen across the
nodes where your worker processes are
running you could have spark working
with Apache mesos now as I said Apache
mesos is a open source project to manage
your computer clusters and can also run
Hadoop applications Apache mesos was
introduced earlier and Spark came in and
as existence to prove The credibility of
Apache mesos you can have spark working
with hadoop's yarn this is something
which widely you will see in different
working environments so yarn which takes
care of your processing and can take
care of different processing Frameworks
also supports spark you could have
kubernetes now that is something which
is making a lot of news in today's world
it is a open source system for
automating deployment scaling and
management of containerized applications
so where you could have multiple Docker
based images which can be connecting to
each other so spark also works with
kubernetes now let's look at some
applications of spark so JPMorgan Chase
and company uses Spark to detect
fraudulent transactions analyze the
business spends of an individual to
suggest offers and identify patterns to
decide how much to invest and where to
invest so this this is one of the
examples of banking lot of banking
environments are using spark due to its
real-time processing capabilities and
in-memory faster processing where they
could be working on fraud detection or
credit analysis or pattern
identification and many other use cases
Alibaba group that uses also spark to
analyze large data sets of data such as
real-time transaction details now that
might be based online or in the stores
looking at the browsing history in the
form of spark jobs and then provides
recommendations to its users so Alibaba
group is using spark in its e-commerce
domain you have IQ via now this is a
leading Healthcare company that uses
Spark to analyze patients data identify
possible health issues and diagnose it
based on their medical history so there
is a lot of work happening in healthcare
industry where real-time processing is
finding a lot of importance and real
time and faster processing is what is
required so healthcare industry and iqvi
is also using spark you have Netflix
which is known and you have Riot games
so entertainment and gaming companies
like Netflix and write games use Apache
spark to Showcase relevant
advertisements to their users based on
the videos that they have watched shared
or liked so these are few domains which
find use cases of spark that is banking
e-commerce Healthcare entertainment and
then there are many more which are using
spark in their day-to-day activities for
real time in memory faster processing
now let's discuss about the Sparks use
case and let's talk about conviva which
is world's leading video streaming
companies so video streaming is a
challenge now if you talk about YouTube
which has data you could always read
about it so YouTube has data which is
worth watching 10 years so that is huge
amount of data where people are
uploading their videos or companies are
doing advertisements and this videos are
streamed in or can be watched by users
so video streaming is a challenge and
especially with increasing demand for
high quality streaming experiences
conviva collects data about video
streaming quality to give their
customers visibility into the end user
experience they are delivering now how
do they do it Apache spark again using
Apache spark convert delivers a better
quality of service to its customers by
removing the screen buffering and
learning in detail about Network
conditions in real time this information
is then stored in the video player to
manage live video traffic coming in from
4 billion video feeds every month to
ensure maximum retention Now using
Apache spark conveyor has created an
auto diagnostics alert it automatically
detects anomalies along the video
streaming Pipeline and diagnosis the
root cause of the issue now this really
makes it one of the leading video
streaming companies based on auto
diagnostic alerts it reduces waiting
time before the video starts it avoids
buffering and recovers the video from a
technical error and the whole goal is to
maximize the viewer engagement so this
is Spark's use case where conviva is
using spark in different ways to stay
ahead in video streaming related
deliveries so let's have a quick demo on
setting up spark on Windows and trying
out the Spark's interactive way of
working for this first thing is you will
have to download spark now you can just
go to Google and type spark download and
once you click on this link it shows you
a spark release a package type which
says pre-built so we will have to get a
pre-built Apache Hadoop related spark so
here I will choose say spark 2.4.3 and I
will choose pre-built for Apache 2.6 and
then I can just click and download on
the spark
2.4.3 bin Hadoop 2.6 tar file once you
click on this link takes you to the
mirror site and you can just click on
this link and download spark
2.4.3 once this is done which I have
already downloaded here I can go to my
downloads and it shows that I have a tar
file now we will have to untar it or
unzip it so I can click on this link and
I already have WinZip which allows me to
unzip spark 2.4.3 I can choose a
location so I can say unzip click on
unzip choose local and then I can choose
one of my folders so I have already
unzipped it here and that's how my spark
2.4.4 directory exists here 2.4.3 I'm
sorry now let's go and look into this
directory and what it contains so so I
can click on C drive and then spark
2.4.3 and this has the folders which are
required for us to use spark if you look
in bin we have different applications
and commands which we can use and if you
also see I would have added one of the
utilities here either we can have the
win utils because spark will need Hadoop
and we are planning to use spark on
windows so what we need is this so I
have in my desktop a Hadoop directory
which has a bin folder and here I have
downloaded a win utils executable file
which you can always search on internet
and you can say download when
utils.exe for spark 64-bit and then you
can find the link from where you can
download this for example we can click
on this and here we can search for when
utils so here we have the link I can
open this link in a different Tab and
this basically shows me when utils for
Adobe 2.7.1 similarly you can search for
Hadoop 2.6 and you can download however
2.7 will also work fine now I have
downloaded win
utils.exe and that's in this Hadoop
folder within bin I have this so we need
two things here one is spark which is
untied and in a particular location and
then your Hadoop which has been and this
has been utils now once you have these
things you can set your environment
variables by just type in envir go to
your edit system environment variables
click on environment variables and here
if you see I have added two variables so
you can just click on new and then you
can give your variable name as Hadoop
underscore home and then you can give
the path where you have your Hadoop
directory that contains bin and that
contains win
util.exe I also have added spark home
which points to my spark directory now
once you have this you are ready to use
your spark on Windows in local mode once
this is done I can open up my command
prompt and here I need to go to my spark
directory I can just say CD Spar 2.4.3
and then I can do a dir slash P which
shows me the folders where we have been
so to execute spark we can look into
what our bin has and it has different
programs such as you have spark shell to
work with Scala and in an interactive
way of working with spark you can also
use Pi spark which allows you to use
Python to work with spark and you also
have spark submit if you have packaged
your application in a jar file to submit
it on a cluster so in this case we would
be use using spark in a local mode so
let's use spark shell and test it so I
can just say spark shell in this way to
start my spark in an interactive way now
based on the path which you have set for
your win utils.exe and Spark shell you
should be able to use spark so it says
welcome to spark version 2.4.3 it also
shows that spark context which is the
entry point of your application is
available as SC wherein it is connecting
to master as local and it has an
application ID it also shows you a spark
UI which you can look at and it says
spark session is available as spark so
if you are interested in working with
spark core that is with rdds we would be
using spark context and if you are
interested in working with spark SQL
that is data frames and data sets then
we can be using spark session so let's
try it out so we can declare a variable
that is a mutable variable and now I
will use spark context with a method of
spark context so I could just do a tab
to see what are the available options
within your spark context and here we
can use one of them to read a file so I
will use SC dot text file and then I
need to point to a file so I already
know that in my spark directory I have a
file called
readme.rd and this is the method of
spark context if I enter it will create
a rdd now as I've explained about rdds
these are resilient distributed data
sets there is no evaluation happening
right now so even if this file does not
exist I would still have an rdd created
now I can go further and do some more
Transformations on this so I could say
Val Y and then I could say take X I
would like to do a map transformation on
it and I would use two upper case now
this is a inbuilt function where I would
want to convert the content of this file
into uppercase when I hit on enter it
again creates an rdd and this rdd is
basically a child rdd of the parent rdd
created by text file remember here we
don't have any evaluation happening or
any execution happening it has just
Transformations which will lead to
creation of further rdds now we can
invoke an action to see the result and
whenever you invoke an action it will
trigger the execution of your dag
starting from your first rdd till the
last transformation you did before
invoking an action so I could just say
count to see how many lines it has and
once I do this it says input path does
not exist it is not able to find a file
in this location so this clearly
explains that there was no valuation
happening when we did this when we tried
to read a file and neither when we did a
map transformation only and only when we
invoked an action it tried to search for
file in this location so let's quit this
and check if the file exists to quit you
can just do a Ctrl D or you could do a
colon quit now here if you see some
messages like this which says unable to
delete temporary file so that's a known
issue we can fix that but my spark is
working fine now let's go and check if
the file exists so I will go in here and
then I will look in my spark directory
to see if the file exists so this is the
file
readme.md however I had done readme.rd
so let's do it again and let's test if
our spark shell works fine so we can go
into the spark directory by just doing
this and then either I could be going
into bin directory and then start spark
shell or Some people prefer to do it
this way wherein you can say bin spark
shell and that should start your spark
shell but there was a error in the in
the slash so let's do it this way and
this will start your spark shell an
interactive way of working with your
spark and using spark context or spark
now once we have done this I can again
say Val X and I can say SC which is Spar
context which is already initialized
when you start your spark shell I can
say text file and I would want to read a
file which is
readme.md now we know that this file
exists however the checking of the file
existing or not that means an evaluation
does not happen when you do a
transformation or when you use a method
of spark context it just creates an rdd
click on this and this has created an
rdd of string now we can do further
other transformation on X by saying map
and then I can say I would want to
convert the content into uppercase and
that's also done now finally let's
invoke an action which will trigger the
execution of my tag which contains these
rdds so I could just say Y and I could
say take 10 and I could say for each
print Ln and this should be able to
invoke an action and it shows me the
result from the file which is converted
into uppercase remember if I do a y dot
count which is another action it will
trigger the execution of dag right from
the beginning where the first RTD was
created the transformation will be done
and then it will show me the count of
lines which exist as a result of this so
we can do this and this says me that
there are 105 lines this is a simple
example of using Spark shell and using
Spar in an interactive way on your
Windows machine now to quit we can
always do a colon quit or we could do a
control D and that basically takes you
out of your spark shell similarly we can
work on Pi Spark by just saying dot
slash bin slash Pi spark and that's
Python's way of working with spark which
brings your python shell it starts the
spark context which is available as SC
and if you are interested in working on
data frames or data sets using spark SQL
then we would be using spark now here I
can just declare a variable X and I can
just do SC dot text file like what we
did using Scala we can be referring to
the same file
readme.md and this has created an rdd
you can just type in X and that shows
the rdd which was created you can then
further do some transformation on it
using x dot map and then I could say I
would want to convert this to uppercase
now in case of python you would normally
use Lambda if you would want to do any
kind of transformation so I would say
Lambda X and then I would want to
convert this to uppercase so here we can
check if my rdd was correctly created
and as I was saying we can then do some
transformation on it so I can say again
y equals and then I would want to do
some transformation on X I would say x
dot map so in case of python as I
explained whenever we want to use
anonymous function or we want to do some
transformation then we can pass in the
function to the transformation so here I
can say Lambda and then I can just say
line or I could say X or a whatever that
is and I could say line I would use a
inbuilt function that is upper which
should convert the content of x to
uppercase now remember we are just doing
some Transformations here so if I click
on y again and enter sorry if I enter y
it will show me that that has created an
rdd to see the content I can just say y
dot collect and that should show me the
content of first few lines which is
converted in uppercase so this clearly
shows that we can even use Python to
work with Spark by using pi spark now
with this we can just say quit and that
should take you out of your Pi spark
shell so this is how you work on spark
shell or Pi spark on your Windows
machine so this has a local spark setup
or basically spark running in a local
mode and you could be using the
attractive way of working with spark now
if we have packaged our application in a
jar file then I could be using spark
submit to submit my application how do
we do that for this we can basically
bring up one of our Ides so in my case
I'm using eclipse and I can get into my
Eclipse now there are two options here
one we can be writing code in your IDE
that is Eclipse we can be compiling the
code based on the spark related jars and
then we can run our application from IDE
which would interact with your spark
that's one way the second way is we can
use packaging tools like SBT to package
our application as jar and then push the
jar to your local spark or a Hadoop base
spark and then run it using spark submit
now this is a simple sample application
for which I have set up certain things
here the first thing is when you have
downloaded eclipse for all the people
who are new they might have to add the
Scala plugin now if you see here in my
Eclipse it shows up Java perspective and
it shows Scala perspective now how did I
get this you can click on help and you
can say install new software and here
you can say add you can say Scala IDE
and then you will have to give a link
from where you can get the Scala IDE how
do I get that so I can go to my browser
and I can say Scala IDE dot org and once
you go to this link just scroll down
click on stable and that shows you
different releases for your different
Ides now for Eclipse although this shows
for oxygen and I have Eclipse neon but
that works fine you can just copy this
link and here once you have copied the
link give the location here and you can
say okay now once you do that it shows
you different options from where you can
choose color ID for Eclipse now in my
case it is already installed so these
options don't show up however if you do
not have this option for example let's
choose something else say I will say
Scala search and once I do this next
gets activated so scholar ID is already
installed in my case so I can click on
next and this should basically get
scholar search plugin also which can be
added to my Eclipse now once this is
done you will be prompted to restart
your eclipse and then you should have
your Scala perspective so for now I'll
just cancel this because I already have
my Scala ID dot Eclipse so this is done
I'll just do a cancel here now once you
have your Scala ID or Scala plugin added
to your IDE you can always click on this
icon here and here you can just click
double click on Scala that will open up
Scala perspective so that's the first
thing then you can always click on file
new and create a Scala project for
example I could say my test apps and
then I can just say finish so it has
added a project now what we need to do
is we need to make sure that our setup
is fine so first thing I would suggest
is click on this go to your build path
and here click on configure build path
so where it says color compiler it would
be good to choose for your project
settings instead of 2.12 go for 2.11 so
sometimes you might have problems with
Scala 2.12 which might not be able to
compile for your Scala 2.11 so use
latest 2.11 bundle and then you can say
apply it says compiler settings have
changed a full rebuild is required for
changes to take effect shall all
projects be clean now we can and say OK
and we can say Okay so this will build
your workspace to use Scala 2.11 bundle
now once that is done second thing what
we need is we would want to write our
code and we would want our code to
compile for that as I said if we want to
write our code using an IDE and then run
it from our IDE rather than using a
build tool like Maven or SVT I can just
add the spark related jars to my build
path so I can right click here I can say
build path and I can click on configure
build path and here where it says
libraries I can say add external charts
now this needs my spark related jars now
where do I find it so remember we have
downloaded spark so you can even find it
there so you can click on wherever your
spark is and then you can click for jars
which shows you all spark related jars
you can just do a control a and just add
it to your build path say apply and then
just say Okay so once this is done you
have already added the spark related
jars which might be good enough for you
to write an application once this is
done we need to basically go ahead and
write our code now how do I do that I
can just click on source and I can say
new and then I can say package and here
you can say main dot Scala click on
finish and that has created main dot
Scala so that would be Source slash main
slash Scala in the folders we can always
check that so I can go to C drive I can
look in users win 10 and then look in my
workspace for my project and if you see
in Source we have Main and we have Scala
now this is where we will be writing our
code so for this then I can just right
click and I can say new and I can just
create an object and I can give it a
name so we can say test app and then we
can say finish so in case of Scala and
Spark we need to create an object and
then we need to Define main class here
once that is done we also need to import
some packages and we need to initialize
the spark context now for that you can
take an example here so I have already
written the code and I can look in
Source main Scala and I have created an
app called first app.scala now here we
have the package name we are importing
some packages which are required for us
to initialize spark context and Spark
configuration so it is per context spark
conf and here is my object and for a
particular project at least one of your
object or one of your application needs
to be the main so we Define the main
here by saying def Main arbs and array
or string then we need to basically
Define our spark context so I'm creating
a configuration object here and then I
say new spark conf I am setting my
application name to hello spark and then
this is very important if you intend to
run your spark application either using
spark submit or from your IDE you need
to specify your master as local now that
could be given with one thread or
multiple threads so this is my
configuration and then I initialize my
spark context pointing to my
configuration so this is the most
important part here now if you see this
is something which I showed you in the
command line so I'm doing a Val X and
I'm pointing it to a file in my project
directory which is abc1 dot text so this
is just to make sure that you are giving
a Windows path and you are giving the
right Escape characters now if it would
be if you intend to run this application
on a Hadoop based cluster then you will
have to give the path of the file
accordingly which we can see later so
here I have Val X I am pointing it to a
file which I have created here and this
just contains three lines this is my
sample file which I want to test this is
a new file now here I have created a
variable called X further I would say
Val Y and I would use spark context
method text file like what we did in
interactive way and I'm pointing it to X
I'm also using some extra features like
caching so that I can cache the rdd
which gets created further I create a
variable called counts and then I do it
flat map transformation on Y where I
would split the content based on space I
would do a map transformation where I
would get every word from the file file
and map it with number one and finally I
do a Reduce by key operation once this
is done I do a saves as text file now I
could do a collect I could do a take I
could do a count and I could also do a
save as text file where I would want my
output to be saved in this location plus
I am using a Java utility to append a
random number to my output directory
finally I do a spark context top so this
is my application and if you see here my
application completely compiles because
I have already added the relevant jar
files in the build path now once this
application is done we need to run this
but before doing that we need to look
into our run configurations so you can
click on run configurations and here I'm
giving my project name I am specifying
my main class which is first app then
you can straight away go to environment
and here I have added two variables one
is spark underscore local and underscore
IP which points to this machine's
localhost and then I have said Hadoop
underscore home because spark even when
you're running locally would want if
Hadoop is existing so we can just set
Hadoop underscore home and I am pointing
it to my desktop Hadoop directory which
contains bin folder and that bin folder
contains a win
utils.exe once this is done we are fine
and then I can test my application by
just clicking on run now this should
trigger my application on my Windows
machine using my IDE I didn't have to
package this as jar and here my
application is completed which should
have created one additional directory
here so I can click on this I can right
click and I can try doing a refresh and
here you see a new output is created we
can look into this it has a part file
which shows my word count so this is a
simple example of setting up your IDE
and running your applications from IDE
on a local spark now what you also see
is a build file here what is that for so
in case you would want to create a
project and you would want to package
the application as a jar file and then
run it on the cluster using spark submit
now in that case we can avoid adding the
jars to my build path but what I would
need is I would need build dot SBT file
within my project folder remember if you
intend to package your application as a
jar and then run it on a cluster or on a
local setup you would not need the build
related charts however if the build
related charts which are already here
will not cause any harm let them be
there now we have a build.sbt file also
which is existing in my project folder
now just for this run I can in fact act
delete this target directory it will be
anyways recreated and if you would want
we can also clean up these just to avoid
the confusion I will delete these now I
just have my project folder and I have
the libraries I have ABC one dot text
and I have a Bill Dot SBT file what does
that contain so this basically has a
name I'm giving a version which will be
appended to my jar I'm saying the Scala
version which I'm using which is 2.11.8
and I'm saying spark version which is
2.2.0 you can replace these with the
version which you intend to use other
than that we are pointing to the
different spark components and the
relevant dependency related jars which
your SBT can get for you so I have given
spark core spark SQL ml lab spark
streaming and Spark Hive which will be
then fetched from the repository so this
is my build Dot SVT file which exists in
my project folder we have our code
written we have our build dot SBT file
which is already existing in my project
folder my code is already fine now we
need to use SBT to package my
applications into jar for which you will
have to download SBT so you can just say
download SBT windows and that should
take you to the installing SVT on
Windows page now here you can just
download the MSI installer and run
through the installer which will
basically install SBT on your machine
now I have already done that so my build
file is ready my code is ready all I
need to do is now use SBT so what I can
do is from my command line I can just
say CD and I will go into users I'll go
into win10 and I'll go into my workspace
now here I can see for my projects and I
can go into my spark apps now once I'm
in my project folder we can double check
that it has a build.sbt which is fine
SBT is already installed on this machine
and I can just say SVT if you would want
to check you can always do SBT version
to check if you have SVT and this
command is also done once you install
SBT for the first slide you can always
do SBT version and before displaying it
was displaying sbt's version it will try
to fix all your dependencies it will try
to get all the relevant dependencies so
this is just to check that SBD is
installed on my machine as I said my
code is already ready I have a build.sbd
file and I can just say SBT package
which then will refer to bill.svt will
look for your code in Source main Scala
and then if everything is fine it will
package it as jar and it will create
folders within your project folder we
can click on this we can try doing a
refresh to see now if you see here the
project was again created a Target
folder is created within scalar 2.11 I
will have my jarg created so we can see
that my SBT was fine it has packaged the
code and it has created a jar file as
simple project 2.11 minus 1 which can be
used to run on the cluster or in a local
mode let's check in Eclipse I'll click
on this and I'll just do a refresh and I
see my jar is already existing which
basically means then we can do a spark
submit so this is where we see that I am
already running a spark submit command
now this might show up some error
messages but we can see how we did this
so if you see here once my packaging was
done I went into my spark folder I did a
bin slash spark submit I mentioned my
jar file and then I said my class main
dot Scala DOT first step so this is my
app and this is how I'm using spark
submit to submit an application which is
running in local mode so we can see
further if it starts processing so here
it says created local directory then it
starts an Executor on localhost it goes
for execution and then if we see further
we could also see it will add our jar to
the class path and then finally it
should be doing the execution now if
there is any error message that might be
related to your file not being deleted
or if there is any problem with executor
so we can basically come back and check
here if there was any specific error so
it the error was that it tries to delete
a temp directory but it does not have
the permissions so we have not done that
but as per my application it should be
creating a output in this folder as
spark out so let's click on this and see
I'll do a refresh right and if you see a
new spark output has been created
previous sessions we have seen how we
can set up spark on Windows or setup our
spark related IDE so that we can run our
applications on Windows now here we have
a quick demo on setting up spark as a
standalone cluster on Ubuntu machines
and then trying out spark way of working
for which we need at least two machines
and here I have Ubuntu machines set up
um1 and um2 just to give insights on how
my machines look so if I look in my Etc
host I have the IP address of multiple
machines so I can ping one machine to
other Machine by just doing a ping um2
and that should work fine similarly I
have also set up SSH access for these
machines both of the machines have hdu
as the user and I can just do a SSH
um2 and that locks me into the second
machine without a password similarly I
can check the same thing from my second
Machine by doing a SSH um1 and that
works fine too so we need two machines
which can ping each other firewall
disabled able to SSH each other so that
we can set up a standalone cluster of
spark how do we do it so first thing is
we will have to download the spark
related tar file now I can just go to
Google and I can type in spark
2.2.1 download now that's what I'm
interested in and this takes you to
spark release 2.2 point 0.1 however here
it shows you the latest release is 2.4
and then
2.4.2 so what we can do is we can go to
archives so I can basically look at
existing versions now here I can click
on download and this shows me a spark
release and also pre-built for Apache
Hadoop now what we can also do is we can
go to release archives for an older
stable version I click on this and here
I have spark 2.2.1 now I can click on
this and we can either install something
which is not already built or we can
download this one which is spark 2.2.1
bin Hadoop 2.6 Point t g z so you click
on this link and then save this file
which I have already done and here if
you see in my machine I have home sdu
downloads which has my different package
features which I have downloaded also if
you notice I have downloaded jdk8 so you
can check on my machines so apart from
machines being able to Ping each other
and being able to do a SSH we also need
machines to have Java which is already
installed once this is done now I have
my spark related package so I go into
user local directory and then you can
give a command here that is sudo tar xvf
home hdu downloads and then give your
spark package now once you do this this
will untar the spark directory and
create a directory in my user local
location if you see here this is the
directory which is created but you also
see that there is a spark link which is
pointing to spark now that is because if
you would want to work on a newer
version of spark you could just do the
same thing for newer version and then
make your link pointing to the newer
version of spark how do you create a
link you can just say sudo Ln minus s
you can give your spark directory and
then you can create a link so I've
already created a link and my spark path
will then become this now what do we do
with this once we have done this once we
have created a link we can go into the
bash file of my user and if you
carefully see here I have given my Java
path so that I can execute Java related
commands and I have also added my spark
related path here which says user local
spark so in case you would be changing
your spark version to a latest one you
will not have to change things in your
bash file only thing you will have to do
is unlink the existing spark link and
create a new link to your newer version
so this is done on this machine and now
I can basically be using spark now
before doing that we can go and to user
local spark and this has different
directories so if you look in pin these
has a binaries programs like your Pi
spark your spark shell spark SQL and
Spark submit which we will see how we
can use here if you look in s bin it has
other startup scripts to start your
history server or to start your master
or worker processes if you look in conf
this has your config directories now
here by default you might see spark
minusdefault dot conf dot template I
have renamed that to.com slaves template
has been renamed to Slaves so let's look
into this so I can go into conf and then
I will look in spark default conf now
here based on our setup now here we
intend to set up a spark Standalone
cluster that is without Hadoop but I
would want to have a spark Standalone
distributed cluster so I have
uncommented this property with say
spark.master and I say spark and I also
mentioned that my masters will run on
this machine which is um1 spark event
log dot enabled is true because we would
want to track the events I have
mentioned a directory so this is a local
directory in user local spark and we
will have to create it it talks about
the default serializer it talks about
the driver memory which was by default 5
gigabyte I have reduced it to 2K
gigabyte based on my machine
configuration you have Java options and
then if you intend to run a history
server so that whenever your spark
application is complete you will have
your application stored in history
server I have given the log directory
which is user local spark and then
application history and this also needs
to be created we have spark history
provider for the class which takes care
of History server and the update
interval of looking for your
applications so this is what we have in
our spark default and if you look in
slaves I have given the machines where I
would want my worker processes to run so
that is um1 and um2 now once you have
made your changes in your spark default
conf and slaves file what you can simply
do is SCP assuming that whatever was
done on this machine that is untarring
your spark directory creating a link
updating your bash RC and renaming your
config files same steps need to be done
on the second machine and your second
machine would also be prepared to be
used for your spark cluster so once you
have both the machines which can ping
each other which can SSH each other both
of the machines have Java both of the
machines have the same spark version
downloaded and the basic setup done I
can just easily copy the spark related
config from here into my other machine
that is sdu
um2 and then I can give a path which is
user local spark conf and in this way I
don't need to edit my config files again
and similarly I can even copy the slaves
file this is all we need to basically
have our spark Standalone cluster now
since we have updated our bash RC we can
give our spark command from anywhere we
have set up our config files so I can
just do a start minus all dot sh and
that based on my config files based on
the directories which are mentioned it
will start my master process and my
worker process on both the machines so
here we have a spark Standalone cluster
which has two workers and one master now
we can always go to the browser
interface and we can check for the spark
UI which should be available by typing
in HTTP slash my master and then the
port is eight zero eight zero so this
shows that I have a spark Master running
I have two worker processes right now
there are no applications which are
running but my spark UI is already
available when we start using spark
either by spark shell or Pi spark or
even spark submit we will see our
applications getting populated here
additionally we can also start the start
history server by just saying start
history server dot sh and that will
start my history server once this is
done we have a history server also
running and we can go back and then we
can again pull out the history server
user interface by giving in the port
which is default 18080 so I have a spark
Standalone cluster with one master two
workers and also a history server which
is running once we have this we can try
working with spark either with spark
shell or Pi spark or if you have
packaged your application as a are using
SBT or Maven you could also use spark
submit Additionally you could also use
spark SQL so to work with spark I would
just say spark shell on one of my
machines both my machines which have
worker processes running we can start a
spark shell on any of these machines
remember here our spark will rely on the
storage of these machines that is the
file system because it does not have any
Hadoop or distributed file system we
have started our spark shell and that
shows spark context which is available
as SC spark session is available as
spark so to work with spark core and
rdds we would be using spark context and
if you intend to work with spark SQL
that is data frames and data sets you
can be using spark let's try a quick
simple example so I'll say Val X SC dot
text file and now I would want to point
it to a file so I will say the complete
path remember I am giving the spark
command from my home directory so I will
have to say user local spark and then I
can point to a file which is existing
here and this is what I would want to
use to do some processing we are using
the first step here wherein we are using
a method of spark context which will not
relate in any evaluation and it will
just create an rdt now we are using the
scholars way so let's create one more
variable and then we can do some
transformation on it by saying two upper
case I would just want to convert the
content into uppercase and this is again
a transformation so that will just
create an rdt once this is done we can
invoke an action to basically see some
results so I could just simply do a
collect which is an action and this
action will trigger the execution of dag
which starts from the farthest RTD that
is the first step where your file is
loaded into rdd now once we do this we
should be able to see our result and
that basically is execution of DAC so
from the time we have used a text file
which is a method of spark context two
we did a transformation and till we
invoked an action becomes my one job now
if I would do y dot count which is again
a different action this will trigger the
execution of tag right from the
beginning that is a c dot text file
where the data will again be loaded in
rdd a map transformation will happen and
then you will have a count now we can
always see this in the spark UI by just
going in here and just doing a refresh
so that will show me that there is an
application which is running via spark
shell it has utilized three cores it has
utilized memory per executor one
gigabyte and we can click on this
application which shows that it used
both the workers One Core and 1GB on
this worker and two course and memory on
this worker I can click on application
detail UI wherein I can see what are the
actions I invoked we also see the number
of tasks which are run and that is
because every rdd which is created by
default is having two partitions and for
each partition you would have one task
well we can change the partitions and
many more things can be done but that
you can learn in later sessions here we
have two jobs one ended with collect and
one ended with count now I can click on
collect and I can see the tag
visualization which tells me we started
the text file we did a map and finally
we did a collect it shows that
everything was done in state 0 that is
just one stage I can click on count and
that again shows me the dag
visualization which is a different stage
a different job ID which started with
text file map and then we did account if
you would want to see more details you
can always use these tabs to look at
different state Pages within your
application if you were doing some
caching looking at environment variables
and also to see how many executors were
used including the one for the driver on
which nodes these executors and how many
cores how many tasks were run was there
any shuffling involved and so on so this
is how I have used spark shell that is
an interactive way of running my
application and since we have a spark UI
we can always look at applications when
they have run and we can always drill
down and no more details now since my
application is completed I can do a
Refresh on History server but that does
not show anything because when we
started a spark shell it started an
application and that application is
still in running status if you see here
under running applications so this is
how we use spark shell on a standalone
spark cluster now we can just do a colon
quit and that takes me out of spark
shell Sim similarly if I would want to
work on Pi spark that is Python's way of
working with spark I can just type in pi
spark which should bring my python shell
and then I can continue working with
spark and again go back and look at my
UI while my Pi spark comes up I can go
here since I quit my spark shell I can
just do a refresh to see if my
application is yet coming up in history
server it might take some time and you
can always go back and look at
incomplete applications or wait till
your application is populated in history
server now if I do a refresh here on
spark UI it says this application was
completed it says finished and then now
we have started Pi spark so it has
started a new application by spark shell
and that is in running status we can
come back here and now we have our
python switch so I can do the same thing
which I did in Scala using SC dot text
file I will point it to a file which
exists in user local spark
readme.md so that's the file which I'm
interested in now this would have
created an rdt which we can confirm by
just typing in X now I can create a
different variable and I can say I would
want to do a transformation of map now
in case of python when you use anonymous
functions or when you want to apply some
functions to your Transformations you
use Lambda functions so we say Lambda
line and then I would say what I want to
do to the line I will say I will use a
inbuilt uppercase upper function which
should convert my content into uppercase
but as of now this is a transformation
so it only creates an RTD to see the
result I can do a y dot collect and that
should bring up my result now in case if
it says the file does not exist that is
because there was a typo here and we can
repeat this step so let's bring up here
again X and I can gives the right name
of the file I can do a transformation
and finally I can see the result so this
is using pi spark on a standalone spark
cluster which is working without Hadoop
which has two worker nodes and one
master node we can always come and do a
refresh here to look at more details so
we have our application we can click on
this again similarly like we did for
Scala application detail UI I see my
application ended at collect it ran two
tasks and I can look at my tag so this
is how we can use spark shell or Pi
spark logical integration of the search
results and Analysis of the data was a
nightmare
not to mention the massive efforts and
expenses that were involved
the threat of data loss challenge of
data backup and reduced scalability
resulted in the issue snowballing into a
crisis of sorts
to counter this Google introduced
mapreduce in December of 2004.
and the analysis of data sets was done
in less than 10 minutes rather than 8 to
10 days
queries could run simultaneously on
multiple servers and search results
could be logically integrated and data
could be analyzed in real time
the USPS of mapreduce are its fault
tolerance and scalability
let's look at a mapreduce analogy the
mapreduce steps the mapreduce
counting after an election as an analogy
in step one each polling Booth ballot
papers are counted by a teller this is a
pre-mapreduced step called input
splitting
in Step 2 tellers of all Boos count the
ballot papers in parallel
as multiple tellers are working on a
single job
the execution time will be faster this
is called the map method
in step 3 the ballot count of each Booth
under the assembly and Parliament seat
positions is found and the total count
for the candidates is generated
this is known as the reduce method thus
map and reduce help to execute the job
quicker than an individual counter the
mapreduce analogy vote counting is an
example to understand the use of
mapreduce
the key reason to perform mapping and
then reducing is to speed up the job
execution of a specific process this can
be done by splitting a process into a
number of tasks thus enabling
parallelism
if one person counts all of the ballot
papers and waits for others to finish
the ballot count it could take a month
to receive the election results when
many people count the ballot papers
simultaneously the results are obtained
in one or two days this is how mapreduce
works let's look at a word count example
in this screen the mapreduce operation
is explained using a real-time problem
the job is to perform a word count of
the given paragraph on the left
the sentence in input says the quick
brown fox jumps over a lazy dog
and a dog is a man's best friend we will
then take this sentence through the
corresponding steps of splitting mapping
shuffling and reducing the map reduce
process then begins with the input phase
which refers to providing data for which
the mapreduce process is to be performed
the sentence used is as input here the
next step is the splitting phase which
refers to converting a job submitted by
the client into a number of tasks in
this example the job is to split into
two tasks one for each sentence then the
mapping phase refers to generating a key
value pair for the input
since this example is about counting
words the sentence is now split into
words by using the substring method to
generate words from lines
the mapping phase will ensure that the
words generated are each converted into
keys and a default value of one is
allotted to each key or each word in the
sentence in the next step the shuffling
phase refers to sorting the data based
on those keys
as shown on screen the words sorted into
ascending order
the last phase is the reducing phase
in this phase the data is reduced based
on the repeated keys by incrementing the
value of each key where there's a
duplicate word
the word dog and letter A are repeated
therefore the reducer will delete the
key and increase the value depending on
the number of occurrences of the key
this is how the mapreduce operation is
performed map execution phases
map execution consists of five phases
the mapping phase the partition phase
the shuffle phase the sort phase and the
reduce phase
the assigned input split is read from
hdfs where split could be a file block
by default
furthermore input is parsed into records
as key value pairs the map function is
applied to each record to return zero or
more new records
these intermediate outputs are stored in
the local file system as a file
they are sorted first by bucket number
and then by a key
at the end of the map phase information
is sent to the master node after its
completion
in the partition phase each mapper must
determine which reducer will receive
each output
for any key regardless of which map or
instance generated it the destination
partition is the same
so for a single word that word would
always go to the same destination
partition
note that the number of partitions will
be equal to the number of reducers in
the shuffle phase input data is fetched
from all map tasks for the portion
corresponding to the reduced tasks
bucket in the sort phase a merge sort of
all map outputs occurs in a single run
and finally in the reduce phase A
user-defined reduced function is applied
to the merged run the arguments are a
key and the corresponding list of values
the output is written to a file in hdfs
map execution in a distributed two-node
environment the mappers on each of the
nodes are assigned to each input split a
box
based on the input format the record
reader reads the split as a key value
pair
the map function is applied to each
record to then return zero or more new
records
these intermediate outputs are stored in
the local file system
thereafter a partitioner assigns the
records to the reducer
in the shuffling phase the intermediate
key value pairs are exchanged by all
nodes
the key value pairs are then sorted by
applying the key and reduced function
again the output is stored in hdfs based
on the specified output file format
the essentials of each mapreduce phase
are shown on the screen the job input is
specified in key value pairs each job
consists of two stages first a
user-defined map function is applied to
each input record to produce a list of
intermediate key value pairs second A
user-defined reduced function is called
once for each distinct key in the map
output
then the list of intermediate values
associated with that key is passed
the essentials of each mapreduce phase
are as follows
first the number of reduced tasks can be
defined by the users second each reduced
task is assigned a set or record groups
that is intermediate records
corresponding to a group of keys
third for each group a user-defined
reduce function is applied to the
recorded values and four the reduced
tasks are read from every map task and
each read Returns the record groups for
that reduce task reduce phase cannot
start until all mappers have finished
processing
so combining your output is an important
step once all the tasks are completed
mapreduce job
a job is a mapreduce program that causes
multiple map and reduced functions to
run parallely over the life of the
program
many copies of map and many copies of
reduced functions are forked for
parallel processing across the input
data set
a task is a map or reduce function
executed on a subset of this data
with this understanding of job and task
the application master and node manager
functions become easier to comprehend
first the application Master is
responsible for the execution of a
single application or mapreduce job
it divides the job request into tasks
and assigns those tasks to node managers
running on one or more slave nodes the
node manager has a number of dynamically
created resource containers
the size of a container depends on the
amount of resources it contains such as
memory CPU disk and network i o
it executes map and reduce tasks by
launching these containers when
instructed to by the mapreduce
application master
mapreduce and Associated tasks the map
process is an initial step to process
individual input records in parallel the
reduce process is all about summating
the output with a defined goal as coded
in the business Logic the node manager
keeps track of individual map tasks and
can run in parallel
a map job runs as a part of a container
execution by node manager on a
particular data node within a cluster
the application Master keeps track of a
mapreduced job the Hadoop mapreduce job
work interaction initially a Hadoop
mapreduce job is submitted by a client
in the form of an input file or a number
of input splits of files each containing
data the mapreduce application Master
will then distribute the input split to
separate node managers the mapreduce
application Master then coordinates with
those node managers the mapreduce
application Master will now resubmit the
task to an alternate node manager if
that data node should fail
the resource manager gathers the final
output and informs the client of success
or failure status
let's look at the characteristics of
mapreduce mapreduce is designed to
handle very large scale data in the
range of petabytes and exabytes it works
well on right ones and read many data
sets also known as worm data mapreduce
allows parallelism without mutexes the
map and reduce operations are performed
by the same processor those operations
are provisioned near the data as data
locality is preferred in other words we
will move the application to the data
and not the other way around commodity
hardware and storage is leveraged and
mapreduce to keep things cost effective
and the runtime takes care of splitting
and moving data for operations
some of the real-time uses of mapreduce
are as follows simple algorithms such as
grep text indicating and reverse
indexing such things as data intensive
Computing which would include sorting
large and small sets of data stream data
and structured data data mining
operations such as Bayesian
classification which you'll study later
and search engine operations such as
keyword indicating ad rendering and Page
ranking Enterprise analytic analytics to
ensure the business is operating
smoothly and with the best decision
making data available gaussian analysis
for locating extraterrestrial objects in
astronomy which uses very large data
sets and semantic web and web 3.0
indicing and operations data types in
Hadoop
data types in Hadoop the first data type
is text
the function of this data type is stored
to string data the writable data type
stores integer data long writable as the
name suggests stores Long data similarly
other data types are a float writable
for storing float data and double
writable for storing double data there's
also Boolean writable and byte writable
data types
no writable is a placeholder when a
value is not needed
this illustration here shows a sample
data type that you can create on your
own
this data type will need you to
implement a writable interface
as you can see writable will Define a
deserialization or serialization
protocol
every data type in Hadoop is a writable
writable comparable will Define your
sort order all keys must be of this type
but not value
then it writable and long writable and
the various concrete classes that you'll
Define for your different data types
lastly sequence files refers to a binary
encoded with a sequence of key value
pairs
input formats and mapreduce
mapreduce can specify how its input is
to be read by defining an input format
the table lists some of the classes of
input formats provided by the Hadoop
framework let's look at each of them
the first class is key value text input
format which is used to create a single
key value pair per line
text input format is used to create a
program that considers a key as the line
number and a value as the line itself
inline input format is similar to text
input format except that there are n
number of lines that make an input split
multi-file input format is used to
implement an input format that
Aggregates for the class sequence style
one split format to be implemented the
input file must be a Hadoop sequence
file which can is the serialized key
value pairs
we set the environment for mapreduce
development
first let's ensure that all Hadoop
services are live and running
this can be verified in two steps
first use the command JPS as shown type
sudo JPS and then look for all five
services that you need
name node data node node manager
resource manager and secondary name node
maybe additional services that are used
by a Hadoop cluster but these are the
ones that we require as core services
next let's look at uploading big data
and small data the command to upload any
data big or small from the local file
system to hdfs is Hadoop space FS space
Dash copy from local space and then the
source file address space and the
destination file address
now let's look at the steps of building
a mapreduce program
first determine if the data can be made
parallel and solved by using mapreduce
for example you need to analyze whether
the data is right once read many or worm
type data in nature then design and
Implement a solution as a mapper and
then reducer class within your code
compile the source code with Hadoop core
and package the code as a jar executable
configure the application job as the
number of mapper and reducer tasks and
to the number of input and output
streams then load the data or use it on
previously available data and then
launch and monitor the job you can then
study the results and repeat any of the
previous steps as needed
the Hadoop mapreduce requirements
the user or developer is required to set
up the framework with the following
parameters
the locations of the job input in the
distributed file system
the location of the job output in the
distributed file system
the input format to use
the output format to use
define a class containing the map
function
and then a separate class containing the
reduce function which is optional
if a job does not need a reduced
function there is no need to specify a
reducer class in your code
the framework will partition the input
schedule and execute map tasks across
the cluster
if requested it will sort the results of
the map task and it will execute the
reduced tasks with the map output
the final output will be moved to the
output directory and the job status then
reported to the user
set of classes
this image shows the set of classes
under the user Supply and the framework
Supply the user Supply refers to the set
of java classes and the methods provided
to a Java developer for developing
Hadoop mapreduce applications
the framework Supply refers to defining
the workflow of a job which is followed
by all Hadoop services
as shown in the image the user provides
the input location and the input format
as required by the program logic
once the resource manager accepts the
input a specific job is divided into
tasks by the application master
each task is then assigned to an
individual node manager
once the assignment is complete the node
manager will start the map task
it performs a shuffling partitioning and
sorting for individual map outputs
once the Sorting is complete the reducer
starts the merging process this is also
called the reduce task
the final step is collecting the output
which is performed once across all the
individual tasks once they're completed
this reduction is based on programming
logic
let's look at mapreduce responsibilities
the basic user or developer
responsibilities of mapreduce are one
setting up the job two specifying the
input location and three ensuring that
the input is in the expected format and
location
the framework responsibilities of
mapreduce are Distributing jobs among
the application master and node manager
nodes of the cluster
running the map operation then
performing the shuffling and sorting
operations
next are the optional reducing phases
and finally placing the output in the
output directory and informing the user
of the job completion status
we create a new project
let's see how we would create a new
project
first make sure eclipse is installed on
your system
once it's installed you can create a new
project and add the essential jar files
to run a mapreduce program
then to create a new project click the
file menu select new project or
alternatively press Ctrl n to start the
wizard of a new Eclipse project
the screen shows the new project and
select wizard options for the first step
in Step number two you would select a
Java project from the list
then click the next button to continue
step 3
the newly created project has to have a
name in this case we'll type the project
name as word count and click the next
button to continue
in Step number four of your new project
you will now include jar files from the
Hadoop framework to ensure that the
programs locate the dependencies to one
location
and in Step number five you will add the
essential jar files to locate these go
to the libraries Tab and click the add
external jars button to add the
essential jar files after adding the jar
files click the Finish button to
complete the project successfully
next we'll check the Hadoop environment
to ensure we have mapreduce it is
important to check whether the machine's
setup can perform mapreduce operations
to verify this use the example jar files
that are deployed by a Hadoop
installation
this can be run
by running the command shown on the
Stream
before executing this command ensure
that the words.txt file resides in the
data first location
in the on-screen example you see the
Hadoop jar command being executed
passing it the Hadoop examples dot jar
file set
word count and then data first slash
words dot txt as the input and data
slash first slash output as the selected
output advanced mapreduce
Hadoop mapreduce uses data types to work
with user given mappers and user given
reducers
the data is read from files into the
mapper and emitted by mappers to the
reducers
the processed data is set back by the
reducers
data emitted by reducers goes into
output files at every step data is
stored in Java objects
let's Now understand the writable data
types in advanced mapreduce
in the Hadoop environment all input and
output objects across the network must
obey the writable interface which allows
Hadoop to read and write data in a
serialized form for transmission
let's look at Hadoop interfaces in some
more detail
the interfaces in Hadoop are writable
and writable comparable
as you've already seen a writable
interface allows Hadoop to read and
write data in a serialized form for
transmission
a writable interface consists of two
methods read and write fields
a writable comparable interface extends
the writable interface so that the data
can be used as a key and not as a value
as shown here the writable comparable
implements two methods compare to and
hash code output formats and mapreduce
now that you've completed the input
formats in mapreduce let's look into the
classes for the mapreduce output format
the first class is default output format
which is text output format it writes
records as lines of text each key value
pair is separated by a tab character
this can be customized by using the
mapreduce textoutput format dot
separator property
the corresponding input format is key
value text input format
sequence file output format writes
sequence files to save on output space
this represents a very Compact and
compressed version of normal data blocks
sequence file as binary output format is
responsible for writing key value pairs
that are in raw binary format into a
sequential file container
and map file output format writes map
files as the output the keys in a map
file are added in a specific order the
reducer then emits keys in that sorted
order
multiple text output format writes data
to multiple files whose names are
derived from the output keys and
multiple sequence file output format
creates output in multiple files in the
compressed form
let's look at distributed caching a
distributed cache is a Hadoop feature to
Cache files that are needed by the
applications
a distributed cache will help boost
efficiency when a map or reduced task
needs access to Common data it allows a
cluster node to read the imported files
from its local file system instead of
retrieving the files from other cluster
nodes in the environment
it allows both single files and archives
such as zip and car.gz
it copies files only to slave nodes if
there are no slave nodes in the cluster
then distributed cache copies the files
to the master node it allows access to
the cached files from mapper or reducer
applications to make sure that the
current working directory is added into
the application path
and allows referencing of the cache
files as though they were present in the
current working directory vastly
speeding up access using distributed
cache step 1.
first set up the cache by copying the
requisite files to the file system as
shown here we see a bin Hadoop FS
command using a dash copy from local of
the file lookup.dat to hcfs my out slash
lookup.dat
this shows us that there is currently no
file or directory of that name
the Hadoop FS copy from local remember
will take a file from your local file
system and place it in the Target
directory within the hdfs file system
using distributed cache Step 2
set the application's job comp as shown
in the example
in this case we're setting up a new
instance of job by creating a new
instance of job conf
we then will call a distributed cache
add cache file method and create a new
URI specifying the location in this case
my app lookup.dat with the file name of
lookup.dat
in the same way each of the different
commands here shows you creating a cache
entry for zip files jar files tar files
tgz and GZ files
in step 3 of setting up your distributed
cache you will use the cache files in
the mapper OR reducer class that you
create
once the private path and configure
information is in your program you
simply declare an instance of file
called f specifying the new file along
with the parameter
of.map.zip slash some file in zip.text
this will map your file into the
distributed cache joins and mapreduce
joins our relational constructs that can
be used to combine relations in
mapreduce Joins are applicable in
situations where you have two or more
data sets you want to combine
a join is performed either in the map
phase or later on in the reduce phase by
taking advantage of the map reduce sort
merge architecture
the various join patterns available in
mapreduce are reduce side join
replicated join composite join and
Cartesian product
A reduced side join is used for joining
two or more large data sets with the
same foreign key with any kind of join
operation a replicated join is a map
side join that works in situations where
one of these data sets is small enough
to Cache that's vastly improving its
performance
a composite join is a map side join used
on very large formatted input data sets
sorted and partitioned by a foreign key
and lastly a Cartesian product is a map
side join where every single record
paired up with another full data set
this style of join typically takes a
significantly longer period of time to
execute
A reduced side join works in the
following ways
the mapper first prepares for join
operations it takes each input record
from every data set and emits a foreign
key record pair
the reducer then performs a join
operation where it collects the values
of each input group into temporary lists
the temporary lists are then iterated
over and the records from both sets are
now joined
A reduced side join
should be used in the following
conditions
when multiple large data sets are being
joined by a foreign key
or when flexibility is needed to execute
any join operation
or when a large amount of network
bandwidth is available as will be moving
data across the network
and also when there is no limitation on
the size of data sets
the SQL analogy of a reduced side join
is given on the screen
in the output of a reduced side join the
number of part files equals the number
of reduced tasks
so if you have 10 reduced tasks you will
have 10 separate part files
replicated joins
a replicated join is a map only pattern
in other words does not use the reduce
phase and works as follows it reads all
files from the distributed cache and
then stores them in in memory lookup
tables
the mapper processes each record and
joins it with the data stored in memory
there is no data shuffled to the reduced
phase the mapper gives the final output
part this type of join is typically very
quick
replicated join should be used when all
data sets except for the largest one can
fit into the main memory of each map
task that is limited by the size of your
Java virtual machine or jvm Heap size
when there is a need for an inner join
or a left outer join with the large
input data set being the left part of
the operation
a SQL analogy of this type replicated
join is given on the screen
in the output of a replicated join the
number of part files equal the number of
map tasks
and again as this is using memory as one
side of the join it is typically much
faster
a composite join is a map only pattern
working in the following ways all data
sets are divided into the same number of
partitions each partition of dataset is
sorted by a foreign key
and all the foreign Keys reside in the
associated partition of each data set
two values are retrieved from the input
Tuple associated with each data set
based on the foreign key and the output
to the file system
this type of join is typically very
lengthy and depending on the size of
your data sets can run for a very long
time
the composite join should be used when
all data sets are sufficiently large and
when there is a need for an inner join
or a full outer join
a SQL analogy of a composite join is
displayed on the screen in the output of
a composite join the number of part
files equal the number of map tasks
the Cartesian product
a contigua product is a map only pattern
that works in the following ways data
sets are split into multiple partitions
each partition is fed to one or more
mappers for example in the image shown
here split A-1 and split a-2 are fed to
three mappers each
a record reader will read every record
of input splits associated with the
mapper
and the mapper simply pairs every record
of a data set with every record of all
other data sets
the Cartesian products should be used
when there is a need to analyze
relationships between all pairs of
individual records and when there are no
constraints on the execution time as
these can take a long time and the
output of a Cartesian product every
possible Tuple combination from the
input records is represented now I have
said
such can be either tested on your
Windows machine by running it in a local
mode or packaging your code in jar
transferring it to your cluster and then
basically running it on your cluster
nodes so we'll see both of these
examples now let's assume and let's
understand about a problem say for
example we have a telecom based company
which would want to process its data say
it would want to find out the number of
subscribers and the kind of calls they
are making or for example let's imagine
a online website or a music website
where users listen to various tracks the
data gets collected and then basically
it has to be processed so imagine you
have a company which is hosting music
online and you have users who either are
subscribers or basically create their
accounts listen to different tracks and
then basically use these services now
what if I would want to process such
data for example if I would want to find
out what is the number of unique
listeners or number of times the track
was shared with other users or number of
times the track was listened to on the
radio or in total or even was skipped on
the radio now in that case I would want
to process the data to find out
information so we will solve this kind
of problem using the mapreduce
programming model the mapreduce approach
which as I explained earlier goes for
mapping and then for goes for reducing
and in between it has different phases
that is shuffling and sorting you have
your partitioner phase you have a mini
reducer which is your combiner phase
although all of these intermittent
phases are taken care by the framework
itself the developer or the person who
is developing the solution is
responsible to work on the mapper
function and the reducer function now
for that we can basically be writing our
code to process this data by giving a
mapper and a reducer function data we
can look into example and say for
example let me pull out some data here
let's look in say some files I have and
for example in this case it would be
this kind of data now that magnitude
could be anything right now here I have
the data which the First Column talks as
the user ID or the membership ID the
second is the track ID you have then
your if the track was shared online the
fourth one is number of times the track
was listened to in total and the fifth
one is if a particular track was skipped
now I could have this data in huge
numbers and then I could be processing
it but we will take a simple small
example to understand how mapreduce does
this so this is a file which I have
already uploaded on the cluster and
which will be as my input file now let's
look at the code so what we would need
is for this kind of processing we would
first need to understand the data now
here the First Column as I said is user
ID and second one is track ID which can
then be used to basically process the
data we need to write a mapper class
which would emit the track ID from this
data so I have it here and let me also
open up a constant class for this which
is here so as I mentioned we will have
to write a mapper class which would emit
track ID from the data and user IDs with
some intermediate key value pairs now
that is what your mapping phase does it
works on the input splits it basically
does the mapping and it gives you a list
of data Elements which are nothing but
your intermediate key value pairs to
explain it in a simple way I would say
just let's work on this data sequence so
let's create a constant class as shown
here so this is my package now I create
a class called Last FM constants and
then I am here declaring some variables
with some constant values for each of
the field which I just showed in the
file you have user ID you have track ID
you have is scrabbled or shared radio
and is skipped now these are my constant
which will be used while I process then
we will have to work on a mapper class
which could or which would immediate uh
emit intermediate key value pairs that
is your track ID and user ID now how do
we do that so here is an example so we
basically would have a mapper class as
it's seen here this is my mapper class
which will be used to process or do the
mapping phase now here we have the class
name which is unique listener and unique
listeners mapper which extends the
mapper class that means I would have to
import the relevant packages for my code
which I have done on the top here then I
have variables that is track ID user ID
which use in try table so Hadoop has its
own data types say intratable long
writeable Boolean writable which are
similar to what you have in Java as int
Boolean long but these data types which
are used by Hadoop are basically an
implementation of writable and writable
comparable interfaces so those are two
interfaces which are then responsible
for these classes which we are using as
data types in our code here we start our
map task which would then consider key
which is an object value as in text and
then you have your data types that is
incredible text and object we are also
handling exception and then what we do
is we work on the file so we would split
it we based on the delimiter now in my
case the delimiter is a pipe then what
we do is we set for the track ID using
our constants that is what we have seen
here which is for your user ID track ID
and so on and we do the same thing we
for our user ID we look into the values
and then we basically emit out just the
track ID and user ID that would be my
intermediate key value pairs now here I
am also using a counter named invalid
record count now that is mainly to count
if there are any invalid records which
do not exist in the file in an expected
format now you know we don't do this in
case of invalid records if we don't do
this what would happen our if we had
invalid records in our file then our
program might fail and that's the reason
I'm using
[Music]
counters.invalidrecord.count and I I am
going to use it for my mapper once we
have our mapper written which basically
will look into the file and emit the
track ID and user ID then I would write
my reducer class now this is how my
reducer class looks where I am saying
public static class unique listener
reducer which extends reducer class and
then the data types which it would need
so in try table for the values which it
works on so A reduced task will work on
track ID and this reducer classes to
aggregate the results so we cannot
directly use some reducer as the records
we are getting are not unique the
records might be duplicates it might be
the same user with the same membership
ID listening to a different track or
sharing it and we cannot be just using a
generic reducer so here we work on the
reducer and this is how my code would
look so what my reducer does it
basically would help me in solving the
problems such as basically aggregating
and Counting the number of tracks which
were shared or listened to or overall
the number of tracks and then you have
your tracks which were skipped one of
the main tasks of your reducer class is
on working on the intermediate data
which is created by your mapper or you
can say multiple map tasks which would
be running on multiple splits on one or
multiple nodes and your reducer's work
is to aggregate the result based on the
keys here we are using set to eliminate
duplicate user IDs now once we have
written our reducer class which
basically works on the user ID set it
eliminates the duplicate user IDs and
then it basically gives me a count of
the track IDs in an aggregated way to
solve my use case once I have done that
I also need a driver class which
basically tells about my overall all
code so here we are having a job
configuration where I say my name of the
job could be unique listeners per track
I am setting the jar by class calling it
as unique listeners that's my class I'm
mentioning my mapper I'm mentioning my
reducer now if you notice I am not using
a combiner class for this use case we
would not need that so combiner class is
a mini reducer which basically helps in
reducing the data or aggregating the
data before it reaches the reducer class
I am also talking about the key which
would be using inwritable and then the
value which would also be in writable as
all my values are in numeric if I had my
key in something else or if I had names
instead of user ID then I could have
used text class I also specify what
would be my input and output path which
will be specified as an argument and
then basically I am printing out number
of invalid records so this is my
mapreduce program to work on solving our
business use case also notice I am using
the relevant packages which are being
used in the code and for these packages
and for my this code to compile we need
to make sure that we have added the
right dependencies now you could have
created your code with pom and using
Maven or based on the classes what you
are using you can always add the jars
the relevant jars to the build path so
here we can look at the build path what
I have and we can see what jars have
used so the main ones are your comments
I have Hadoop core now you can ignore
Hive or Json Json which I have here
those are for different applications so
the main is common CLI and Hadoop core
depending on what you are using within
your application so that has to be added
to your build path also we are having
the Java 1.8 now if I wanted to run this
application or test it on my local
machine I could have also set up the
configurations however it is preferred
to run the code on a yarn-based cluster
if you would want to try it on Windows
then you can always go to the Run
configuration you can basically specify
your application which you would want to
run you can then say what is your class
name you can be passing in your
arguments here and in your environment
you can specify Hadoop as a local IP
however I suggest package it as jar and
run it on a cluster so what you could do
is you could select your class now my
unique listeners dot Java class is also
getting the constants from Last FM
constants so both of them belong to the
same project and I'm using that here for
my mapper and for my reducer so in
mapper I refer to Last FM constants
where I have given constant values for
all of these fields once you have your
code ready you can always package it as
jar by saying export and select the jar
option in Java click on next and and
then you can select your project and
give it a name so I have I'm pointing
into a different jar file but I can do a
browse I have already packaged it as jar
and you can see I have a jar here so I
can select this I can say save and that
would be my jar if I'm making some
changes it will anyways overwrite it
once you have your jar ready what you
need to do is place it on your cluster
now I have already done that I have a
cluster here let me bring it up and what
we can do is you can use different ways
you can have a Apache cluster you could
have a Cloudera cluster wherein you need
to place so let's get into this now here
I'm using a already set up lab
environment you can be doing an FTP you
can upload your code using Hue I have
done an FTP from my local machine and
uploaded it to the cluster I could click
on web console which basically gives me
a console to try out things you could as
I said you need the jar and then you
need to push your jar to your cluster
let me log in here and once I'm logged
in I am on a edge node which allows me
to connect to a cluster which is cloud
eras based and now I can check on my
hdfs path in my user directory I should
have multiple files and directories and
in one of the directories I have placed
in this file so we can look in here
which is sample Mr in and that's my
directory where I have the input so I
can look in Sample Mr in and the same
file which I showed in the beginning I
have uploaded it here which is the
listeners file which we can even check
so I can say listeners Dot txt and I can
even do a cat so this is the file which
is already existing here which has the
content which we spoke about although
not huge content but it is more than
enough for us to test our code and
obviously we can upload huge data and
then have the same same map reduce
program helping us in answering our
questions we also need the jar file and
the jar file should be here which I have
uploaded so I need to run this now to
run your map reduce you can say Hadoop
jar you will give your jar name which is
this one now I need to give my package
name with my class name so I'll say or
example Hadoop codes and then I need to
give my class which is unique listeners
now if I hit enter here it will ask me
to give an input and output as my code
says to pass in the arguments so we will
pass in the arguments here so I'll save
my directory where I have the data now I
could be picking up one file or multiple
files to process this so I will say just
one file which is
here and that's my file which is sample
Mr in that is what I want to use so I'll
say sample Mr in and I need to give my
file name so I can say listeners Dot txt
and then I can give an output which will
get created when you run your map reduce
so I can say sample Mr and then I can
say output 1 and this is the directory
which will get created which will have
my output now you can run this job in
this way you can even run it in a local
mode by specifying arguments on the
command line you can give different kind
of properties if you want your mapreduce
jobs output to be in a compressed format
you can do all that so let's run this
and this should connect to the resource
manager the job is processing one input
file it will start with the mapping
phase first now it looks that the job
has failed due to some reason let's have
a look I just looked into the job and an
idealized why it failed the main reason
was I was trying to write an output in
the directory to which I don't have
access I need to have access to write
the output in a directory to which my
user id has permissions for this we will
have to submit the job again with the
right path where it can write the output
so we can do this this is my class I
need input so I'll say and then we can
say sample Mr in and I'm looking for
listeners Dot txt and I need to give an
output which should be in this path and
then we can say sample Mr output 1 and
then submit the job now once I have
submitted the job as I said it has one
input part to process it is working on
one split because the file which we are
trying to work on is smaller than the
block size it goes for the mapping phase
and then it goes for reducing and it's
done so if you carefully look at the map
reducer here it says the number of
outputs it had to process was just four
so every line by default is considered
as a record and we had totally four
lines the output is basically working on
again giving you four records right
because we are doing a split based on
the delimiter then this would become an
input to combiner if we had a combiner
class but we don't use that so the
output from your map goes to the reduce
which will basically work on the input
records and create output records now we
can have a look at this the output of
this job which is stored in hpfs and I
can either be giving a complete path or
I can just mention the directory which I
used and that should by default take me
to the directory which should have my
output now based on how many reduced
tasks were used you will have number of
part files which show up here so we have
one map and one reduce task that would
have controlled or that would have been
controlled by giving things on the
command line so here I have run it in a
yarn mode where yarn was used to
allocate resources for my applications
and we can always check that on the yarn
UI or also by going to say cloud error
manager if that is available if you're
not using Cloud eras cluster then you
can always in Apache cluster go to your
yarn and you can go into the UI wherever
your resource manager is running so that
is one option or if you have a cloud
error cluster you can look in
applications which will show all the
applications which were recently run so
this was the thing which failed earlier
because of permissions and this is the
one which succeeded now I have a job ID
which can always take me to the yarn UI
the history server where I see how many
map and jobs were run and for further
information you can always go and look
into the map and reduce tasks so this
was a map task which ran within a
container which was allocated by your
Beyond and we can also look on the Node
and logs what really happened you can
also look into the reduce task you can
also look at the counters which we were
seeing on the command line on how many
map and reduced Stars how many bytes
were read what was the garbage
collection time how much CPU time was
spent in milliseconds and you get all
the counters here you can specifically
be looking at counters for map or for
reduce so your yarn UI helps you to look
into for more information now if I go
back to my console and I have the output
file I can basically look into it by
selecting the output file and I can do a
cat which basically will show me what is
my output so this is my map reduce and
basically what it has done has it has
aggregated and pulled out the
information for your different listeners
based on the data of what we have
provided let's look at one more example
of using mapreduce programming model to
solve a real-time business case or a
problem now the problem in hand is that
we have some data collected from a
telecom Giant and basically we have some
sample data here which lists the list of
subscribers which are using the services
now if I look into the file I have
called Data records which basically
gives me information about the data
collected by a telecom company so this
Telecom company keeps records of its
subscribers and it calls this particular
file as call data records now what does
this call data records contain it
basically has a from phone number as the
first field it has the two phone number
that is call being made to a particular
number then we have the call start time
following a timestamp you have a call
end time and then we have finally if
this call was a distance call or what we
call as STD call now what we need to do
is we need need to use our knowledge of
mapreduce we need to use the mapreduce
programming model and we need to come up
with a code to find out all the phone
numbers for example who were making more
than 60 Minutes of STD calls now here
the flag one basically means it was an
STD call and 0 means it was a local call
how do we do that using mapreduce so
this is again a problem where you would
want to count through the data and you
would want to aggregate the values to
come up with result so let's look at a
sample code for this one so here I have
my eclipse and again like a previous
example we are having a class which
declares some constants for the fields
so you have phone numbers with 0 phone
number with one call start time call end
time and an STD flag so these are the
constants which I would be using in my
code and here is my main class which
will use the map reduce basically to
solve this problem now I'm importing the
packages and as I explained earlier
these packages are mainly to make sure
that we can use the classes in our code
in the right way so here now the order
of your driver class or your mapper and
reducer does not matter right the first
thing what we need is we need a mapper
which will do the map task so here I am
creating a mapper which is tokenizer
mapper and that extends your mapper
class it accepts the data types that is
object text and long writeable as
explained earlier these are your data
types which are Hadoop specific and data
types like in writable long writeable
Boolean writable and many others depend
on the writable and writable comparable
interfaces now here we start with taking
up our phone number we basically also
take duration in minutes so these are
the variables and I'm using the
appropriate data type for those then I
start writing my map task now what does
my laptops do it basically works on the
string which we have and it breaks it
into individual elements based on a
delimiter once that is done then we are
also checking if that particular string
which we are looking for has a STD flag
because we would want to find out how
many customers or how many of them were
making phone calls which were more than
60 minutes and if these calls were STD
calls then basically I work on the phone
number and I set it using the CDR
constants for which we have already
created a class using the phone number
called end time call start time and I
also compute or calculate the long due
ratio because we need to find out the
calls which were more than 60 Minutes
finally we extract the phone numbers and
the duration of the call made by a
particular phone number so this would be
my mapper giving an intermediate output
which will would then have to be sorted
and shuffled and then handed over to
reducer what does my reducer do it uses
the right data types it basically starts
with a sum as 0 it would look into all
the keys and values and wherever it
would find Keys which are repeating and
the duration is more than 60 it would
give me an aggregated result this is my
reducer and finally we have a driver as
I explained earlier which has job
configuration your mapper reducer and
also we are using just as an example a
combiner class which does not have a
class of its own but it is basically
using the same reducer class once your
code is ready check if it compiles check
if you have all the dependencies met and
package it as jar transfer it to the
cluster and run it with or on top of
yarn so what we can do is I have already
moved it here and we can look into this
machine I already have my jar file which
basically also needs a input which I
have already pushed to my cluster so we
can look in for the file which should be
existing in Sample Mr in directory which
is within my user and my name directory
and I have the call data records which
basically shows the data which I was
showing in the notepad file now I can
say Hadoop jar and point it to my jar
which is STD subscribers jar we need to
use the package so I'll say auth example
Hadoop codes that's my package and my
class name my class name is STD
subscribers so I will say STD
subscribers Dot and then you need to
give your input so we will say sample Mr
in and I will give in a particular file
which is called data
records.txt and then we can point it to
an output directory which will get
created automatically so I will say
output 2 and that should have my result
and like we ran our first example now
I'm again running it on one simple file
you can imagine this as you can replace
one single file with a directory which
has multiple files where it might be the
Telecom company was already collecting
data for couple of months and they would
like to analyze this and you can run it
in this way so once this is done we have
our output which can be seen by going
into the directory saying sample Mr
output 2 and again this would have gone
for one single reducer so we can look at
whatever we gave as an output so it was
Sample Mr output 2 and that is
my directory so since this is a
directory my cat command does not work
on it you need to give the file so the
command is by giving the specific file
not by giving the directory so it is cat
sample Mr output to slash part minus r
and then you can give five zeros and
that should be my out which should give
me an information of which were the
users or which were the phone numbers
which made calls greater than 16 minutes
and if these calls were STD calls and
what was the duration of the call so
this is how you can use mapreduce
programming model to solve your
real-time business cases always remember
you have to code and create your mapper
and reducer function and rest will be
done by the frame what is htfs hcfs is a
distributed file system that provides
access to data across Hadoop clusters a
cluster is a group of computers that
work together
like other Hadoop related Technologies
hdfs is a key tool that manages and
supports analysis of very large volumes
petabytes and even zettabytes of data
some of the challenges of traditional
systems are cost speed and reliability
the traditional file system costs
approximately ten thousand to fourteen
thousand dollars per terabyte
searching and analyzing data was time
consuming and expensive also if search
components were saved in different
servers fetching data was difficult need
for hdfs
hcfs resolves all the three major issues
of the traditional file system let's
discuss how it does that
one cost
htfs is an open source software package
so it can be used with zero licensing
and support costs it is designed to run
on a regular computer
speed
large Hadoop clusters can read or write
more than a terabyte of data per second
a cluster comprises multiple systems
logically interconnected in the same
network
hdfs can easily deliver more than two
gigabytes of data per second per
computer to mapreduce which is a data
processing framework of Hadoop
reliability hdfs copies the data
multiple times and distributes the
copies to individual nodes a node is a
commodity server which is interconnected
through a network device
hdfs then places at least one copy of
data on a different server
in case any of the data is deleted from
any of the nodes it can be found within
the cluster
let's understand how hdfs stores files
with an example a patron gifted a
collection of popular books to a college
library
the librarian decided to arrange the
books on a small rack and then
distribute multiple copies of each book
on other racks
this way the students could easily pick
a book from any of the racks similarly
hdfs creates multiple copies of a data
block and keeps them in separate systems
for easy access we see that a regular
file system data size is 51 bytes a
small block of data whereas hdfs when it
writes a block is 128 megabytes which is
a large block of data the small regular
file system blocks
suffer from disk i o problems primarily
because of multiple seek operations it
takes to retrieve so many small blocks
the hcfs file system has a very large
block which requires many viewers reads
and rights so the reads are huge data
sequentially in a single seek operation
thus dramatically speeding up the read
of the hcfs file system
hdfs storage
let's take a look at hcfs storage and
how it works acfs stores files in a
number of blocks
each block is replicated to a few
separate computers however the count can
be modified by the administrator data is
divided into 128 megabytes per block and
then replicated across local disks of
the cluster nodes
hdfs is a storage system for both input
and output of mapreduce jobs
the metadata controls the physical
location of a block and its replication
within the cluster
the metadata is stored in something
called a name node so what are the
characteristics of hdfs
we have fault tolerance scalability Rock
awareness support for heterogeneous
clusters and built for large data sets
so hdfs does provide this scalable fault
tolerant rockware storage designed to be
deployed on commodity Hardware let's
look at the first characteristic fault
tolerance hdfs is designed with Hardware
failure in mind data in a Hadoop cluster
is broken down into smaller units called
blocks these are then duplicated and
distributed throughout the cluster since
the data has been replicated it is
highly available and fault tolerant the
default replication factor is three this
can be changed upwards or downwards by
an administrator
number two is scalability when
requirements increases we can scale the
existing cluster there are two
scalability mechanisms available
vertical which is larger hardware and
horizontal which provides an unlimited
supply of computing added to the cluster
three is Rock aware Iraq is a collection
of machines typically around 40 to 50
which are connected using the same
network switch if that entire network
goes down then all machines in that rack
will be out of service
rack awareness is a concept introduced
by Apache Hadoop to overcome this issue
rack awareness achieves two things
one by increasing the availability of a
data block across multiple racks and two
better cluster performance by segmenting
the read operations across multiple
racks communication between two data
nodes on the same rack is more efficient
than the same between two nodes on
different racks so information about the
location of each data node distributed
across the racks in Hadoop cluster is
stored in the name node we'll be
learning more about the name node which
holds our metadata when we discuss the
hcfs architecture
before HDs supports a variety of storage
types to optimize data usage and lower
costs Based on data usage frequency you
can configure each data directory with a
storage type such as ssds for improved
performance archival storage for very
dense and rarely access data and so on
hdfs is meant to handle large files I.E
in terabytes and petabytes of data
hdfs splits huge files into smaller
chunks known as data blocks and requests
for these blocks to go across a network
and come without a lot of overhead
the size of the data block is very
important concerned in such scenarios
having a small block size can result in
more requests to figure out where that
block can be found thereby increasing
the network traffic and having a very
large block size can result in longer
data processing times to overcome both
these scenarios the default size of the
htfs block is 128 megabytes which can
also be configured up or down per your
requirements
hdfs architecture and components
hdfs is a high availability architecture
broadly hdfs has a Master Slave
architecture so how does it work
in a typical HCA cluster two or more
separate machines are configured as name
nodes at any point in time exactly one
of the name nodes is in an active State
and all the others are in a standby
state
the active name node is responsible for
all client operations in the cluster
while the standby is simply acting as a
slave maintaining enough state to
provide a fast failover if necessary
the master node that is the name node
ensures that the data required for the
operation is loaded and segregated into
chunks of data blocks in slave nodes as
data nodes a data node serves read or
write requests it also creates deletes
and replicates blocks based on the
instructions from the name node
the standby name node and the active
name node keep in sync with each other
through shared edit logs or metadata
the active name node updates the edit
logs which is the metadata information
about the storage
this is done with namespace
modifications like Block locations
status of the name node and so on
the standby node reads the changes made
to the edit logs and applies it to its
own namespace in a consistent manner
in the event of a failover the standby
will ensure that it has read all the
edits before promoting itself to the
active state
note that this is a manual failed over
process which has to be performed by an
admin
for an automatic failover we will need
to utilize Services of event patchy
zookeeper which maintains small amounts
of coordination data
The Zookeeper failover controller or
zkfc keeps an open session with the
active name node by periodically pinging
it with a health check command
if the node has crashed Frozen or
otherwise entered an unhealthy State the
health monitor will mark it as unhealthy
and elect a new name node
a key component of hdfs is the file
system namespace itself in other words
the hierarchy of where data is stored
acfs exposes a file system namespace and
allows user data to be stored in files
much like we're used to In traditional
operating systems the hcfs does use a
hierarchical file system that does
include directories and files within
those directories
the name node manages the file system
namespace allowing clients to work with
files and directories
a file system supports operations like
create remove and rename
the name node apart from maintaining the
file system namespace records any change
to metadata information
name node operation
the name node maintains two persistent
files a transaction log called an edit
log and a namespace image called an FS
image
the edit log records every change that
occurs in the file system metadata such
as creating a new file
the name knows local file system stores
the edit log
the entire file system namespace
including mapping of blocks files and
file system properties is stored in FS
image
this is also stored in the name node's
local file system
when new data nodes join a cluster
metadata loads the blocks that reside on
a specific data node into its memory at
startup
metadata then periodically loads the
data at user defined or default
intervals
when the name node starts up it
retrieves the edit log and Fs image from
its local file system
it then updates the fs image with edit
log information and stores a copy of the
fs image on the file system as a
checkpoint
the metadata size is limited to the RAM
available on the name node
a large number of small files would
require more metadata than a small
number of large files hence the
in-memory metadata management issue
explains why hdfs favors a small number
of large files
if a name node runs out of ram it will
crash and the applications will not be
able to use hdfs until the name node is
operational again
data block splitting
each file is split into one or more
blocks which are stored in replicated in
data nodes the data block split is an
important process of the hcfs
architecture as discussed earlier each
file is split into one or more blocks
stored and replicated in data nodes
data nodes manage names and locations of
file blocks by default each file block
is 128 megabytes
however this potentially reduces the
amount of parallelism that can be
achieved as a number of blocks proof
file decreases
each map task operates on one block so
if tasks are lesser than nodes in the
cluster the jobs will run slowly
however this issue is smaller when the
average mapreduce job involves more
files or larger individual files
let's look at some of the benefits of
the data block approach
the data block approach provides
simplified replication
fault tolerance and reliability
it also helps by shielding users from
Storage subsystem details
the block replication architecture
block replication refers to creating
copies of a block in multiple data nodes
usually the data is split in the form of
Parts such as part-0 and part dash one
hdfs performs block replication on
multiple data nodes so that if an error
exists on one of the data node servers
the job tracker service will resubmit
the job to another data node server
the job tracker service is present in
the name node server let's now look at
the replication method used in the
replication method each file is split
into a sequence of blocks all blocks
except the last one in the file are of
the same size blocks are replicated for
fault tolerance
the block replication factor is usually
configured at the cluster level but it
can be configured at the file level
the name node receives a heartbeat and a
block report from each data node in the
cluster
the heartbeat denotes that the data node
is functioning properly
a block report lists the blocks on a
data node we also have a data
replication topology
the topology of the replica is critical
to ensure the reliability of hdfs
usually each data is replicated three
times where the suggested replication
topology is as follows
one place the first replica on the same
node as out of the client
two place a second replica on a
different rack from that of the first
replica
and three place the third replica on the
same rack as that of the second one but
on a different node
all this achieves High accessibility and
fault tolerance
hdfs access
hdfs provides various access mechanisms
a Java API can be used for applications
there's also a python and a c language
wrapper for non-java applications
a web GUI can also be utilized through
an HTTP browser
and an FSL is available for executing
commands at the file system level on
hdfs
let's look at the commands for hcfs in
the command line interface
falling are a few basic command lines of
hdfs
in order to copy the file
simplylearn.text from the local disk to
the user's directory type the command
line shown on the screen
and here you see the command hdfs
DFS Dash put
simplylearn.text to simplylearn.text
this will take the file and copy it to
slash user
slash username slash simply learn.text
to get a directory listing of the user's
home directory
type the command line shown on the
screen
in this case we type in hdfs DFS Dash LS
slash user slash simple slash test
in order to create a directory called
testing under the user's home directory
type the command line hdfs DFS Dash
maker slash user simpl
slash test
to delete the directory testing and all
of its contents type the command line
hdfs Dash RM
Dash r
and testing
remember that when you are copying files
you're copying a file from your local
file system into the hdfs storage
cluster
the hcfs DFS command is what does this
let's look at the Hue file browser
the file browser in Hue lets you view
and manage your hcfs directories and
files
Additionally you can create move rename
modify upload download and delete
directories and files
you can also view the file contents all
from within a convenient browser-based
utility
use of common hdfs commands in this demo
you will learn how to use commonly used
hdfs commands in simply learn Cloud lab
web console so let's start go to
www.simplylearn.com
and click on login at the top right
corner it will navigate to https colon
slash slash
lms.simplylearn.com on this page enter
your email ID and password to log into
your learner account on successful login
you will be able to see a list of
courses on the home page click on the
big data Hadoop course to view its
content and to access its practice lab
on clicking the practice lab tab you'll
be able to see Cloudera manager Hue FTP
and web console click on web console and
access the launch lab to use the Linux
terminal of the system on which Hadoop
is installed and to use various hdfs
commands you can log into the web
console by simply clicking on the launch
tab at the bottom right corner and
copying and pasting the credentials from
the practice Labs tab now we will show
you how to use common commands for hdfs
use hdfs help if you want to view the
help content event for common hdfs
commands
use hdfs version to print the version of
Hadoop install
then we have hdfs hyphen version which
prints the version of java and jvm
use hdfs
dfs-ls to list home directory contents
of the current user then we have hdfs
DFS Dash mkdir followed by the directory
name let's say demo 1 is going to be
used to create a new directory demo 1 in
the current path now let's verify if the
directory has been successfully created
type hdfs DFS Dash LS it will show new
directory demo 1 has been created
successfully
to remove directories we can use hdfs
DFS Dash RMR followed by the directory
name let's say demo one and then it will
remove demo 1 in the current path
you can verify if it has been removed
successfully by using hdfs DFS Dash LS
here you can see that demo 1 has been
removed successfully use hcfs
dfs-ls Dash capital r
to recursively list subdirectories
encountered in the current directory
use hdfs dfs.du
to display sizes of files and
directories contained in the given
directory or the length of a file in
case it's just a file
use hdfs DFS Dash count directory name
let's say slash test to count the number
of files in the specified directory
yarn is the acronym for yet another
resource negotiator
yarn is a resource manager created by
separating the processing engine and the
management function of mapreduce
it monitors and manages workloads
maintains a multi-tenant environment
manages the high availability features
of Hadoop and implements security
controls
before 2012 users could write mapreduce
programs using scripting languages such
as Java Python and Ruby
they could also use Pig a language used
to transform data
no matter what language was used its
implementation depended on the mapreduce
processing model
in May 2012 during the release of Hadoop
version 2.0 yarn was introduced you are
no longer limited to working with the
mapreduce framework anymore as yarn
supports multiple processing models in
addition to mapreduce such as Spark
other features of yarn include
significant performance Improvement and
a flexible execution engine
now let's discuss yarn with the help of
an example
Yahoo was the first company to embrace
Hadoop and this became a trendsetter
within the Hadoop ecosystem
in late 2012 Yahoo struggled to handle
iterative and stream processing of data
on the Hadoop infrastructure due to
mapreduce limitations
both iterative and stream processing
were important to Yahoo in facilitating
its move from batch Computing to
continuous Computing after implementing
yarn in the first quarter of 2013
Yahoo installed more than 30 000
production nodes on spark for iterative
processing
storm for stream processing and Hadoop
for batch processing allowing it to
handle more than 100 billion events such
as clicks Impressions email content
metadata and so on per day
this was possible only after yarn was
introduced and multiple processing
Frameworks were implemented the single
cluster approach provides a number of
advantages including
higher cluster utilization where
Resources unutilized by a framework can
be consumed by another
lower operational costs because only one
do it all cluster needs to be managed
reduced data motion as there's no need
to move data between Hadoop yarn and
systems running on different clusters of
computers
the yarn infrastructure is responsible
providing computational resources such
as CPU or memory needed for application
executions
yarn infrastructure and hdfs are
completely independent
the former provides resources for
running an application while the latter
provides storage
the mapreduce framework is only one of
the many possible Frameworks that run on
yarn
the fundamental idea of mapreduce
version 2 is to split the two major
functionalities of resource management
and job scheduling and monitoring into
separate demons
yarn and its architecture
in this topic we will discuss yarn and
its architecture
three elements of yarn architecture
the three important elements of the yarn
architecture are resource manager
application master and node managers
the resource manager or RM which is
usually one per cluster is the master
server
resource manager knows the location of
the data node and how many resources
they have
this information is referred to as rack
awareness
the RM runs several Services the most
important of which is the resource
scheduler that decides how to assign the
resources
the application Master is a framework
specific process that negotiates
resources for a single application that
is a single job or a directed acyclic
graph of jobs which runs in the first
container allocated for the purpose
each application Master requests
resources from the resource manager and
then works with the containers provided
by node managers
that node managers can be many in one
cluster
they are the slaves of the
infrastructure
when it starts it announces itself to
the RM and periodically sends a
heartbeat to the RM
each node manager offers resources to
the cluster
the resource capacity is the amount of
memory and the number of V cores short
for virtual core
at runtime the resource scheduler
decides how to use this capacity
a container is a fraction of the node
manager capacity and it is used by the
client to run a program
each node manager takes instructions
from the resource manager and reports
and handles containers on a single node
in the next few screens you will see a
detailed explanation of the three
elements
the first element of yarn architecture
is resource manager
the RM mediates the available resources
in the cluster among competing
applications with the goal of Maximum
cluster utilization
it includes a plugable scheduler called
the yarn scheduler which allows
different policies for managing
constraints such as capacity fairness
and service level agreements
the resource manager has two main
components scheduler and applications
manager the scheduler is responsible for
allocating resources to various running
applications depending on the common
constraints of capacities cues and so on
the scheduler does not monitor or track
the status of the application
also it does not restart the tasks in
case of any application or Hardware
failures
the scheduler performs its function
based on the resource requirements of
the applications
it does so based on the abstract notion
of a resource container that
incorporates elements such as memory CPU
disk and Network
the scheduler has a policy plugin which
is responsible for partitioning the
cluster resources among various cues and
applications
the current mapreduce schedulers such as
capacity scheduler and the fare
scheduler are some examples of the
plugin
the capacity scheduler supports
hierarchical cues to enable a more
predictable sharing of cluster resources
the application manager is an interface
which maintains a list of applications
that have been submitted currently
running or completed
the applications manager is responsible
for accepting job submissions
negotiating the first container for
executing the application specific
application master and restarting the
application Master container on failure
let's discuss how each component of the
resource manager work together
the resource manager communicates with
the clients through an interface called
The Client Service
a client can submit or terminate an
application and gain information about
the scheduling queue or cluster
statistics through the client service
administrative requests are served by a
separate interface called the admin
service through which operators can get
updated information about the cluster
operation
in parallel the resource tracker service
receives node heartbeats from the node
manager to track new or decommissioned
nodes
the NM liveliness Monitor and nodes list
manager keep an updated status of which
nodes are healthy so that the scheduler
and the resource tracker service can
allocate work appropriately
the application Master service manages
application Masters on all nodes keeping
the scheduler informed
the am liveliness monitor keeps a list
of application managers and their last
heartbeat times to let the resource
manager know what applications are
healthy on the cluster
any application master that does not
send a heartbeat within a certain
interval is marked as dead and
rescheduled to run on a new container
before Hadoop 2.4 the resource manager
was the single point of failure in a
yarn cluster
the high availability or ha feature adds
redundancy in the form of an active
standby resource manager pair to remove
the single point of failure
resource met ha is realized through the
active standby architecture
at any point of time one of the RMS is
active and one or more RMS are in
standby mode waiting to take over should
anything happen to the active
the Trigger 2 transition to active comes
from either the admin through the
command line interface or through the
integrated failover controller the RMS
have an option to embed the
zookeeper-based active standby elector
to decide which RM should be active
when the active goes down or becomes
unresponsive another RM is automatically
elected to be the active
note that there is no need to run a
separate zkfc Daemon like in hdfs
because the active standby elector
embedded in RMS act as a failure
detector and a leader elector the second
element of yarn architecture is the
application master
the application master in yarn is a
framework specific Library which
negotiates resources from the RM and
works with the node manager or managers
to execute and monitor containers and
their resource consumption
while an application is running the
application manager manages the
application lifecycle Dynamic
adjustments to Resource consumption
execution flow faults and it provides
status and metrics
the application Master is architected to
support a specific framework and can be
written in any language
it uses extensible communication
protocols with the resource manager and
the node manager
the application Master can be customized
to extend the framework or run any other
code
because of this the application Master
is not considered trustworthy and is not
run as a trusted service
in reality every application has its own
instance of an application master
however it's feasible to implement an
application Master to manage a set of
applications for example
an application Master for pig or Hive to
manage a set of mapreduce jobs
the third element of yarn architecture
is the node manager
when a container is leased to an
application the node manager sets up the
container's environment
the environment includes the resource
constraints specified in the lease and
any kind of dependencies such as data or
executable files the node manager
monitors the health of the node
reporting to the resource manager when a
hardware or software issue occurs so
that the scheduler can divert resource
allocations to healthy nodes until the
issue is resolved
the node manager also offers a number of
services to Containers running on the
Node such as a log aggregation service
the node manager runs on each node and
manages the activities such as container
lifecycle management container
dependencies container leases node and
container resource usage node health and
log management and reports node and
container status to the resource manager
a yarn container is a collection of
specific set of resources to use in
certain amounts on a specific node
it is allocated by the resource manager
on the basis of the application
the application manager presents the
container to the node manager on the
Node where the container has been
allocated thereby granting access to the
resources now let's discuss how to
launch the container
the application manager must provide a
container launch context or CLC
this includes information such as
environment variables dependencies on
the requirement of data files or shared
objects brought to the launch
security tokens and the command to
create the process to launch the
application
the CLC supports the application Master
to use containers
this helps to run a variety of different
kinds of work from simple shell scripts
to applications to Virtual operating
system
owing to yarn's generic approach a
Hadoop yarn cluster runs various
workloads
this means a single Hadoop cluster in
your data center can run mapreduce storm
spark Impala and more
broadly there are five steps involved in
yarn to run an application
first the client submits an application
to the resource manager then the
resource manager allocates a container
then the application Master contacts the
related node manager
then the related node manager launches
the container and finally the container
executes the application master
in the next few screens you will learn
about each step in detail
users submit applications to the
resource manager by typing the Hadoop
jar command
the resource manager maintains the list
of applications on the cluster and
available resources on the Node manager
the resource manager determines the next
application that receives a portion of
the cluster resource
the decision is subject to many
constraints such as Q capacity Access
Control lists and fairness
when the resource manager accepts a new
application submission one of the first
decisions the scheduler makes is
selecting a container
then the application Master is started
and is responsible for the entire life
cycle of that particular application
first it sends resource requests to the
resource manager to ask for containers
to run the application's tasks
a resource request is simply a request
for a number of containers that satisfy
resource requirements such as the
following
amount of resources expressed as
megabytes of memory and CPU shares
preferred location specified by hostname
or rack name
priority within this application and not
across multiple applications
the resource manager allocates a
container by providing a container ID
and a hostname which satisfies the
requirements of the application Master
after a container is allocated the
application Master asks the node manager
managing the host on which the container
was allocated to use these resources to
launch an application-specific task
this task can be any process written in
any framework such as a mapreduce task
the node manager does not monitor tasks
it only monitors the resource usage in
the containers
for example it kills a container if it
consumes more memory than initially
allocated
throughout its life the application
Master negotiates containers to launch
all the tasks needed to complete its
application
it also monitors the progress of an
application and its tasks restarts
failed tasks in newly requested
containers and reports progress back to
the client that submitted the
application
after the application is complete the
application Master shuts itself and
releases its own container
though the resource manager does not
monitor the tasks within an application
it checks the health of the application
master
if the application Master fails it can
be restarted by the new resource manager
in a new container
thus the resource manager looks after
the application Master while the
application Master looks after the tasks
Hadoop includes three tools for yarn
Developers
yarn web UI
Hue job browser
yarn command line
these tools enable developers to submit
Monitor and manage jobs on the yarn
cluster
yarn web UI runs on 8088 Port by default
it also provides a better view than Hue
however you can't control or configure
from yarn web UI
you will see a demonstration on yarn web
UI in the later screen the huge job
browser allows you to monitor the status
of a job kill a running job and view
logs you will understand this by viewing
a demonstration in the later screen
most of the yarn commands are for the
administrator rather than the developer
a few useful commands for developers are
as follows
to list all commands of yarn type the
command line shown on screen
to print the version
type the command line shown on the
screen
to view logs of a specified application
ID type the command line shown on your
screen
using yarn web UI Hue job browser and
yarn command line
in this demo you will learn how to work
on yarn web UI Hue job browser and yarn
command line
you will use the file word count dot py
which is a python file
this file will calculate the number of
times each word appears
you will view how to execute this file
with the help of yarn
type spark hyphen submit
let's define the master as yarn hyphen
client followed by the name of the
Python file
you need each word count which is
present in the word directory loudacre
KB
click job browser
while the program is running on the
terminal let's see how the steps
executing in yarn appear in hue
you will notice the python word count
status as running
it will also show you the status of map
and reducer
click ID
you can also click on ID option to view
more details of a job
you can also click on kill option to end
this job from UI
once you enter the application ID you
can view the running metadata of that
yarn
in this page you can also view the
information such as start time end time
and the amount of memory per second that
a running program consumes
let's go back to the terminal
you can check similar details in web
user interface which you just viewed
from yarn resource manager
you need to navigate to localhost colon
8088 cluster you will again notice the
running ID
click this ID to view more details
you will be able to view information
such as user name of the program
application type current state of the
running program and a few additional
information you can also scroll through
the accepted jobs currently running jobs
finished jobs and a few more details
so this is another web UI page in which
you can monitor your yarn progress
once this program is complete you can
view the output in the terminal
it displays the number of times each
word appears
this brings you to the end of this demo
in this demo you learned the steps to
calculate the word count for a file you
have also learned the steps to monitor
your yarn progress in a web user
interface history of
it in 2009 as a project at UC Berkeley
amp Labs by mate zaheria in 2010 it was
open source under the BST license in
2013 spark became an Apache top level
project and in 2014 used by data bricks
to sort large-scale data sets and it set
a new world record so that's how Apache
spark started and today it is one of the
most in demand processing framework or I
would say in memory Computing framework
which is used across the Big Data
industry so what is Apache spark let's
learn about this Apache spark is a open
source in-memory Computing framework or
you could say data processing engine
which is used to process data in batch
and also in real time across various
cluster computers and it has a very
simple programming language behind the
scenes that is Scala which is used
although if users would want to work on
spark they can work with python they can
work with Scala they can work with Java
and so on even R for that matter so it
supports all these programming languages
and that's one of the reasons that it is
called polyglot wherein you have good
set of libraries and support from all
the programming languages and developers
and data scientists incorporate spark
into their applications or build spark
based applications to process analyze
query and transform data at a very large
scale so these are key features of
Apache spark now if you compare Hadoop
West spark we know that Hadoop is a
framework and it basically has mapreduce
which comes with Hadoop for processing
data however processing data using
mapreduce in Hadoop is quite slow
because it is a batch oriented operation
and it is time assuming if you if you
talk about spark spark can process the
same data 100 times faster than
mapreduce as it is a in memory Computing
framework well there can always be
conflicting ideas saying what if my
spark application is not really
efficiently coded and my mapreduce
application has been very efficiently
coded well then it's a different case
however normally if you talk about code
which is efficiently written format
reduce or for spark based processing
spark will win the battle by doing
almost 100 times faster than mapreduce
so as I mentioned Hadoop performs batch
processing and that is one of the
paradigms of mapreduce programming model
which involves mapping and reducing and
that's quite rigid so it performs batch
processing the intermittent data is
written to sdfs and written right back
from sdfs and that makes hadoops
mapreduce processing slower in case of
spark it can perform both batch and
real-time processing however a lot of
use cases are based on real time
processing take an example of Macy's
take an example of retail giant such as
Walmart and there are many use cases who
would prefer to do real time processing
or I would say near real-time processing
so when we say real time or near real
time it is about processing the data as
it comes in or you're talking about
streaming kind of data now Hadoop or
hadoop's map reduce obviously was
started to be written in Java now you
could also write it in Scala or in
Python however if you talk about
mapreduce it will have more lines of
code since it is written in Java and it
will take more times to execute you have
to manage the dependencies you have to
do the right declarations you have to
create your mapper and reducer and
Driver classes however if you compare
spark it has few lines of code as it is
implemented in is a statically typed
dynamically inferred language it's very
very concise and the benefit is it has
features from both functional
programming and object oriented language
and in case of Scala whatever code is
written that is converted into byte
codes and then it runs in the jvm now
Hadoop supports Kerberos authentication
there are different kind of
authentication mechanisms Kerberos is
one of the well-known ones and it can
really get difficult to manage now spark
supports authentication via a shared
secret it can also run on yarn
leveraging the capability of Kerberos so
what are spark features which really
makes it unique or in demand processing
framework when we talk about spark
features one of the key features is fast
processing so spark contains resilient
distributed data sets so rdds are the
building blocks for spark and we learn
more about rdds later so spark contains
rdds which saves huge time taken in
reading and writing operations so it can
be 100 times or you can say 10 to 100
times faster than Hadoop when we say in
memory Computing here I would like to
make a note that there is a difference
between caching and in-memory Computing
think about it caching is mainly to
support read ahead mechanism where you
have your data pre-loaded so that it can
benefit further queries however when we
say in memory Computing we are talking
about lazy evaluation we are talking
about data being loaded into memory only
and only when a specific kind of action
is invoked so data is stored in Ram so
here we can say Ram is not only used for
processing but it can also be used for
storage and we can again decide whether
we would want that Ram to be used for
persistence or just for computing so it
can access the data quickly and
accelerate the speed of analytics now
spark is quite flexible it supports
multiple languages as I already
mentioned and it allows the developers
to write applications in Java Scala r or
python it's quite fault tolerance so
spark contains these rdds or you could
say execution logic or you could say
temporary data sets which initially do
not have any data loaded and the data
will be loaded into rdds only when
execution is happening so these can be
fault tolerant as these rdds are
distributed across multiple nodes so
failure of one worker node in the
cluster will really not affect the rdds
because that portion can be recomputed
so it ensures loss of data it ensures
that there is no data loss and it is
absolutely fault tolerant it is for
better analytics so sparked has Rich set
of SQL queries machine learning
algorithms complex analytics all of this
supported by various spark components
which we will learn in coming slides
with all these functionalities analytics
can be performed better in terms of
spark so these are some of the key
features of spark however there are many
more features which are related to
different components of spark and we
will learn about them so what are these
components of spark which I'm talking
about spark core so this is the core
component which basically has rdds which
has a core engine which takes care of
your processing now you also have spark
SQL so people who would be interested in
working on structured data or data which
can be structurized would want to prefer
using spark SQL and Spark SQL internally
has components or features like data
frames and data sets which can be used
to process your structured data in a
much much faster way you have spark
streaming now that's again an important
component of spark which allows you to
create your spark streaming applications
which not only works on data which is
being streamed in or data which is
constantly getting generated but you
would also or you could also transform
the data you could analyze or process
the data as it comes in in smaller
chunks you have Sparks mlib now this is
basically a set of libraries which
allows developers or data scientists to
build their machine learning algorithms
so that they can do Predictive Analytics
or prescriptive descriptive preemptive
analytics or they could build their
recommendation systems or bigger smarter
machine learning algorithms using these
libraries and then you have Graphics so
think about organizations like LinkedIn
or say Twitter where you have data which
naturally has a network kind of flow so
data which could be represented in the
form of graphs now here when I talk
about graphs I'm not talking about pie
charts or bar charts but I'm talking
about Network related data that is data
which can be networked together which
can have some kind of relationship think
about Facebook think about LinkedIn
where you have one person connected to
other person or one company connected to
other companies so if we have our data
which can be represented in the form of
network graphs then spark has a
component called Graphics which allows
you to do graph based processing so
these are some of the components of
Apache spark spark core spark SQL spark
streaming spark mlib and Graphics so to
learn more about components of spark
let's learn here about spark core now
this is the base engine and this is used
for large scale parallel and distributed
data processing so when you work with
spark at least and the minimum you would
work with is spark core which has rdds
as the building block box of your spark
so it is responsible for your memory
management your fault recovery
scheduling Distributing and monitoring
jobs on a cluster and interacting with
storage systems so here I would like to
make a key point that Spar by itself
does not have its own storage it relies
on storage now that storage could be
your sdfs that is hadoop's distributed
file system it could be a database like
nosql database such as hbase or it could
be any other database say rdbms from
where you could connect your spark and
then fetch the data extract the data
process it analyze it so let's learn a
little bit about your rdds resilient
distributed data sets now spark core
which is the base engine or the core
engine is embedded with the building
blocks of spark which is nothing but
your resilient distributed data set so
as the name say is it is resilient so it
is existing for a shorter period of time
distributed so it is distributed across
nodes and it is a data set where the
data will be loaded or where the data
will be existing for processing so it is
immutable fault tolerant distributed
collection of objects so that's what
your rdd is and there are mainly two
operations which can be performed on an
rdd now to take an example of this say I
want to process a particular file now
here I could write a simple code in
Scala and that would basically mean
something like this so if I say well
which is to declare the variable I would
say well X and then I could use what we
call as spark context which is basically
the most important entry point of your
application so then I could use a method
of spark context for example that is
text file and and then I could point it
to a particular file so this is just a
method of your spark context and Spark
context is the entry point of your
application now here I could just give a
path in this method so what does this
step do it does not do any evaluation so
when I say Val X I'm creating a
immutable variable and to that variable
I'm assigning a file now what this step
does is it actually creates a rdd
resilient distributed data set so we can
imagine this as a simple execution logic
a EMT data set which is created in
memory of your node so if I would say I
have multiple nodes in which my data is
split and stored I'm imagining that your
yarn your spark is working with Hadoop
so I have Hadoop which is using say two
nodes and this is my distributed file
system sdfs which basically means my
file is written to hdfs and it also
means that the file related blocks are
stored in the underlying disk of these
machines so when I say Val x equals SC
dot text file that is using a method of
spark context now there are various
other methods like whole text files
parallelize and so on this step will
create an rdt so you can imagine this as
a logical data set which is created in
memory across these nodes because these
nodes have the data however no data is
loaded here so this is the first rdd and
I can say first step in what we call as
a dag a dag which will have series of
steps which will get executed at later
stage now later I could do further
processing on this I could say well Y
and then I could do something on X so I
could say x dot map and I would want to
apply a function to every record order
every element in this file and I could
give a logic here x dot map now this
second step is again creating an rdd a
resilient distributed data set you can
say second step in my dag okay and here
you have a external rdd one more rdd
created which depends on the first RTD
so my first RTD becomes the base rdd or
parent rdd and the resultant rdd becomes
the child rdd then we can go further and
we could say well Z and I would say okay
now I would want to do some filter on y
so this filter which I am doing here and
then I could give a logic might be I'm
searching for a word I am searching for
some pattern so I could say well Z
equals y dot filter which again creates
one more rdd a resilient distributed
data set in memory and a you can say
this is nothing but one more step in the
dag so this is my tag which is a series
of steps which will be executed now here
when does the execution happen when the
data get when will the data get loaded
into these rdds so all of this that is
using a method using a transformation
like map using a transformation like
filter or flat map or anything else
these are your Transformations so the
operations such as map filter join Union
and many others will only create rdds
which basically means it is only
creating execution Logic No data is
evaluated no operation is happening
right now only and only when you invoke
an action that is might be you want to
print some result might be you want to
take some elements and see that might be
you want to do a count so those are
actions which will actually trigger the
execution of this dag right from the
beginning so if I hear say Z dot count
where I would want to just count the
number of words which I am filtering
this is an action which is invoked and
this will trigger the execution of dag
right from the beginning so this is what
happens in a spark now if I do a z dot
count again it will start the whole
execution of dag again right from the
beginning so my Z dot count second time
in action is invoked again the data will
be loaded in the first RTD then you will
have map then you will have filter and
finally you will have result so this is
the core concept of your rdds and this
is how RTD works so mainly in spark
there are two kind of operations one is
your Transformations and one is your
actions Transformations or using a
method of spark content text Will Always
and always create an rdd or you could
say a step in the tag actions are
something which will invoke the
execution which will invoke the
execution from the first rdd till the
last rdd where you can get your result
so this is how your rdds work now when
we talk about components of spark let's
learn a little bit about spark SQL so
spark SQL is a component type processing
framework which is used for structured
and semi-structured data processing so
usually people might have their
structured data stored in rdbms or in
files where data is structured with
particular delimiters and has a pattern
and if one wants to process the
structured data if one wants to use
spark to do in-memory processing and
work on the structured data they would
prefer to use spark SQL so you can work
on different data form formats say CSV
Json you can even work on smarter
formats like Avro parquet even your
binary files or sequence files you could
have your data coming in from an rdbms
which can then be extracted using a jdbc
connection so at the bottom level when
you talk about spark SQL it has a data
source API which basically allows you to
get the data in whichever format it is
now spark SQL has something called as
data frame API so what are data frames
data frames in short you can visualize
or imagine as rows and columns or if
your data can be represented in the form
of rows and columns with some column
headings so data frame API allows you to
create data frames so like my previous
example when you work on a file when you
want to process it you would convert
that into an rdd using a method of smart
context or by doing some Transformations
so in the similar way when you use data
frame so when you want to use spark SQL
you would use
Sparks context which is SQL context or
Hive context or spark which allows you
to work with data frames so like in my
earlier example we were saying Val x
equals SC dot text file now in case of
data frames instead of SC you would be
using say spark dot something so spark
context is available for your data
frames API to be used in older versions
like spark 1.6 and so on we were using
Hive context or SQL context so if you
were working with spark 1.6 you would be
saying well x equals SQL context dot
here we would be using spark dot so data
frame API basically allows you to create
data frames out of your structured data
which also lets spark know that data is
already in a particular structure it
follows a format and based on that your
Sparks backend dag scheduler right so
when I say about dag I talk about your
sequence of steps so spark is already
aware of what are the different steps
involved in your application so your
data frame API basically allows you to
create data frames out of your data and
data frames when I say I'm talking about
rows and columns with some headings and
then you have your data frame DSL
language or you can use spark SQL or
Hive query language any of these options
can be used to work with your data
frames so to learn more about data
frames follow in the next sessions when
you talk about spark streaming now this
is very interesting for organizations
who would want to work on streaming data
imagine a store like Macy's where they
would want to have machine learning
algorithms now what would these machine
learning algorithms do suppose you have
a lot of customers walking in the store
and they are searching for particular
product or particular item so there
could be cameras placed in the store and
this is being already done there are
cameras placed in the store which will
keep monitoring in which corner of the
store there are more customers now once
camera captures this information this
information can be streamed in to be
processed by algorithms and those
algorithms will see which product or
which series of product customers might
be interested in and if this algorithm
in real time can process based on the
number of customers based on the
available product in the store it can
come up with a attractive alternative
price so that which the price can be
displayed on the screen and probably
customers would buy the product now this
is a real-time processing where the data
comes in algorithms work on it do some
computation and give out some result and
which can then result in customers
buying a particular product so the whole
essence of this machine learning and
real-time processing will really hold
good if and when customers are in the
store or this could relate to even a
online shopping portal where there might
be machine learning algorithms which
might be doing real-time processing
based on the clicks which customer is
doing based on the clicks based on
customer history based on customer
Behavior algorithms can come up with
recommendation of products or better
altered price so that the sale happens
now in this case we would be seeing the
essence of real-time processing only in
a fixed or in a particular duration of
time and this also means that you should
have something which can process the
data as it comes in so spark streaming
is a lightweight API that allows
developers to perform batch processing
and also real-time streaming and
processing of data so it provides secure
reliable fast processing of live data
streams so what happens here in spark
streaming in brief so you have a input
data stream now that data stream could
be a file which is constantly getting
appended it could be some kind of
metrics it could be some kind of events
based on the clicks which customers are
doing or based on the products which
they are choosing in a store this input
data stream is then pushed in through a
spark streaming application now spark
streaming application will broke break
this content into smaller streams what
we call as discreticized streams or
batches of smaller data on which
processing can happen in frames so you
could say process my file every five
seconds for the latest data which has
come in now there are also some windows
based uh options like when I say windows
I mean a window of past three events
window of past three events each event
being of 5 Seconds so your batches of
smaller data is processed by Spark
engine and this process data can then be
stored or can be used for further
processing so that's what spark
streaming does when you talk about mlib
it's a low level machine learning
library that is simple to use scalable
and compatible with various programming
languages now Hadoop also has some
libraries like you have Apache mahaut
which can be used for machine learning
algorithm times however in terms of
spark we are talking about machine
learning algorithms which can be built
using mlibs libraries and then spark can
be used for processing so mlib eases the
deployment and development of scalable
machine learning algorithms I mean think
about your clustering techniques so
think about your classification where
you would want to classify the data
where you would want to do supervised or
unsupervised learning think about
collaborative filtering and many other
data science related techniques or
techniques which are required to build
your recommendation engines or machine
learning algorithms can be built using
Sparks ml lip Graphics is Spark's own
graph computation engine so this is
mainly if you are interested in doing a
graph based processing think about
Facebook think about LinkedIn where you
can have your data which can be stored
and that data has some kind of network
connections or you could say it is well
networked I could say x is connected to
y y is connected to z z is connected to
a so x y z a all of these are in terms
of graph terminologies we call as
vertices or vertex which are basically
being connected and the connection
between these are called edges so I
could say a is friend to B so A and B
are vertices and friend a relation
between them is The Edge now if I have
my data which can be represented in the
form of graphs if I would want to do a
processing in such way this could be not
only for social media it could be for
your network devices it could be a cloud
platform it could be about different
applications which are connected in a
particular environment so if you have
data which can be represented in the
form of graph then Graphics can be used
to do ETA L that is extraction
transformation load to do your data
analysis and also do interactive graph
computation so Graphics is quite
powerful now when you talk about spark
your spark can work with your different
clustering Technologies so it can work
with Apache mesos that's how Spar came
in where it was initially to prove The
credibility of Apache mesos spark can
work with yarn which is usually you will
see in different working environments
spark can also work as Standalone that
means without Hadoop spark can have its
own setup with master and workout
processes so usually or you can say
technically spark uses a Master Slave
architecture now that consists of a
driver program that can run on a master
node it can also run on a client node it
depends on how you have configured or
what your application is and then you
have multiple executors which can run on
work nodes so your master node has a
driver program and this driver program
internally has the spark context so your
spark Every Spark application will have
a driver program and that's driver
program has a inbuilt or internally used
spark context which is basically your
entry point of application for any spark
functionality so your driver or your
driver program interacts with your
cluster manager now when I say interacts
with clustered manager so you have your
spark context which is the entry point
that takes your application request to
the cluster manager now as I said your
cluster manager could be say Apache
mesos it could be yarn it could be spark
Standalone Master itself so your cluster
manager in terms of yarn is your
resource manager so your spark
application internally runs as series or
set of tasks and processes your driver
program wherever that is run will have a
spark context and Spark context will
take care of your application execution
how does that do it spark context will
talk to Cluster manager so your cluster
manager could be on and in terms of when
I say cluster manager for yarn would be
resource manager so at high level we can
say a job is split into multiple tasks
and those tasks will be distributed over
the slave nodes or worker nodes so
whenever you do some kind of
transformation or you use a method of
spark context and rdd is created and
this rdd is distributed across multiple
nodes as I explained earlier worker
nodes are the slaves that run different
tasks so this is how a spark
architecture looks like now we can learn
more about spark architecture and its
interaction with yarn so usually what
happens when your spark context
interacts with the cluster manager so in
terms of yarn I could say resource
manager now we already know about yarn
so you would have say node managers
running on multiple machines and each
machine has some RAM and CPU cores
allocated for your node manager on the
same machine you have the data nodes
running which obviously are there to
have the Hadoop related data so whenever
a application wants to process the data
your application via spark contacts
contacts the cluster managers that is
resource manager now what does resource
manager do resource manager makes a
request so resource manager makes
request to the node manager of the
machines wherever the relevant data
resides asking for containers so your
resource manager is negotiating or
asking for containers from node manager
saying hey can I have a container of 1GB
RAM and one CPU core can I have a
container of 1GB RAM and one CPU core
and your node manager based on the kind
of processing it is doing will approve
or deny it so node manager would say
fine I can give you the container and
once this container is allocated or
approved by node manager resource
manager will basically start an extra
piece of code called App Master so App
Master is responsible for execution of
your applications whether those are
spark applications or mapreduce so your
application Master which is a piece of
code will run in one of the containers
that is it will use the RAM and CPU core
and then it will use the other
containers which were allocated by node
manager to run the tasks so it is within
this container which can take care of
execution so what is a container a
combination of RAM and CPU core so it is
within this container we will have a
executed process which would run and
this executed process is taking care of
your application related tasks so that's
how overall Spa Park Works in
integration with yarn now let's learn
about this spark cluster managers as I
said spark can work in a standalone mode
so that is without Hadoop so by default
application submitted to spark
Standalone mode cluster will run in fifo
order and each application will try to
use all the available nodes so you could
have a spark Standalone cluster which
basically means you could have multiple
nodes on one of the nodes you would have
the master process running and on the
other nodes you would have the spark
worker processes running so here we
would not have any distributed file
system because spark is Standalone and
it will rely on an external storage to
get the data or probably the file system
of the nodes where the data is stored
and processing will happen across the
nodes where your worker processes are
running you could have spark working
with Apache mesos now as I said Apache
my source is a open source project to
manage your are computer clusters and
can also run Hadoop applications Apache
mesos was introduced earlier and Spark
came in and as existence to prove The
credibility of Apache missiles you can
have spark working with hadoop's yarn
this is something which widely you will
see in different working environments so
yarn which takes care of your processing
and can take care of different
processing Frameworks also supports
spark you could have kubernetes now that
is something which is making a lot of
news in today's world it is a open
source system for automating deployment
scaling and management of containerized
applications so where you could have
multiple Docker based images which can
be connecting to each other so spark
also works with kubernetes now let's
look at some applications of spark so
JPMorgan Chase and company uses Spark to
detect fraudulent transactions analyze
the business expense of an individual to
suggest offers and identify patterns to
decide how much to invest and where to
invest so this this is one of the
examples of banking a lot of banking
environments are using spark due to its
real-time processing capabilities and in
memory faster processing where they
could be working on fraud detection or
credit analysis or pattern
identification and many other use cases
Alibaba group that uses also spark to
analyze large data sets of data such as
real-time transaction details now that
might be based online or in the stores
looking at the browsing history in the
form of spark jobs and then provides
recommendations to its users so Alibaba
group is using spark in its e-commerce
domain you have IQ here now this is a
leading Healthcare company that uses
Spark to analyze patients data identify
possible health issues and diagnose it
based on the medical history so there is
a lot of work happening in healthcare
industry where real-time processing is
finding a lot of importance and real
time and faster processing is what is
required so healthcare industry and iqvi
is also using spark you have Netflix
which is known and you have Riot games
so entertainment and gaming companies
like Netflix and ride games use Apache
spark to Showcase relevant
advertisements to their users based on
the videos that they have watched shared
or liked so these are few domains which
find use cases of spark that is banking
e-commerce Healthcare entertainment and
then there are many more which are using
spark in their day-to-day activities for
real time in memory faster processing
now let's discuss about the Spark's use
case and let's talk about conviva which
is world's leading video streaming
companies so video streaming is a
challenge now if you talk about YouTube
which has data you could always read
about it so YouTube has data which is
worth watching 10 years so that is huge
amount of data where people are
uploading their videos or companies are
doing advertisements and this videos are
streamed in or can be watched by users
so video streaming is a challenge and
especially with increasing demand for
high quality streaming experiences
conviva collects data about video
streaming quality to give their
customers visibility into the end user
experience they are delivering now how
do they do it Apache spark again using
Apache spark convert delivers a better
quality of service to its customers by
removing the screen buffering and
learning in detail about Network
conditions in real time this information
is then stored in the video player to
manage live video traffic effect coming
in from 4 billion video feeds every
month to ensure maximum retention Now
using Apache spark conver has created an
auto diagnostics alert it automatically
detects anomalies along the video
streaming Pipeline and diagnosis the
root cause of the issue now this really
makes it one of the leading video
streaming companies based on auto
diagnostic alerts it reduces waiting
time before the video starts it avoids
buffering and recovers the video from a
technical error and the whole goal is to
maximize the viewer engagement so this
is Spark's use case where conviva is
using spark in different ways to stay
ahead in video streaming related
deliveries what is big data analytics in
simple terms big data analytics is
defined as the process which is used to
extract meaning thankful information
from Big Data this information could be
your hidden patterns unknown
correlations market trends and so on by
using big data analytics there are many
advantages it can be used for better
decision making to prevent fraudulent
activities and many others we will look
into four of them step by step I will
first start off with big data analytics
which is used for risk management BDO
which is a Philippine banking company
uses big data analytics for risk
management risk management is an
important aspect for any organization
especially in the field of banking risk
management analysis comprises a series
of measures which are employed to
prevent any sort of unauthorized
activities identifying fraudulent
activities with a main concern for BDO
it was difficult for the bank to
identify the fraud still from a long
list of suspects BDO adopted big data
analytics which helped the bank to
narrow down the entire list of specs
thus the organization was able to
identify the fraudster in a very short
time this is how big data analytics is
used in the field of banking for risk
management let us now see how big data
analytics is used for product
development and Innovations with an
example all of you are aware of
Rolls-Royce cars right do you also know
that they manufacture jet engines which
are used across the world what is more
interesting is that they use big data
analytics for developing and innovating
this engine a new product is always
developed by trial and error method big
data analytics is used here to analyze
if an engine design is good or bad it is
also used to analyze if there can be
more scope for improvement based on the
previous models and on the future
demands this way big data analytics is
used in designing a product which is of
higher quality using big data analytics
the company can save a lot of time time
if the team is struggling to arrive at
the right conclusion big data can be
used to zero in on the right data which
have to be studied and thus the time
spent on the product development is less
big data analytics helps in quicker and
better decision making in organizations
the process of selecting a course of
action from various other Alternatives
is known as decision making lot of
organizations take important decisions
based on the data that they have data
driven business decisions can make or
break a company hence it is important to
analyze all the possibilities thoroughly
and quickly before making important
decisions let us now try to understand
this with an use case Starbucks uses big
data analytics for making important
decisions they decide the location of
their new outlet using big data
analytics choosing the right location is
an important factor for any organization
the wrong location will not be able to
attract the required amount of customers
positioning of a new outlet a few miles
here or there can always make a huge
difference for an outlet especially a
one like Starbucks various factors are
involved in choosing the right location
for a new outlet for example parking at
accuracy has to be taken into
consideration it would be inconvenient
for people to go to a store which has no
parking facility similarly the other
factors that have to be considered are
the visibility of the location the
accessibility the economic factors the
population of that particular location
and also we would have to look into the
competition in the vicinity all these
factors have to be thoroughly analyzed
before making a decision as to where the
new outlet must be started without
analyzing these factors it would be
impossible for us to make a wise
decision using big data analytics we can
consider all these factors and analyze
them quickly and thoroughly thus
Starbucks makes use of big data
analytics to understand if their new
location would be fruitful or not
finally we will look into how big data
analytics is used to improve customer
experience using an example Delta and
American Airline uses big data analytics
to improve its customer experiences with
the increase in global air travel it is
necessary that an airline does
everything they can in order to provide
good service and experience to its
customers Delta Airlines improves its
customer experience by making use of big
data analytics this is done by
monitoring tweets which will give them
an idea as to how their customers
Journey was if the airline comes across
a negative tweet and if it is found to
be the airline's fault the airline goes
ahead and upgrades that particular
customers ticket when this happens the
customer is able to trust the Airlines
and without a doubt the customer will
choose Delta for their next journey by
doing so the customer is happy and the
airlines will be able to build a good
brand recognition thus we see here that
by using analysis Delta Airlines was
able to improve its customer experience
moving on to our next topic that is life
cycle of big data analytics here we will
look into the various stages as to how
data is analyzed from scratch the first
stage is the business case evaluation
stage here the motive behind the
analysis is identified we need to
understand why we are analyzing so that
we know how to do it and what are the
different parameters that have to be
looked into once this is done it is
clear for us and it becomes much easier
for us to proceed with the rest after
which we will look into the various data
sources from where we can gather all the
data which will be required for analysis
once we get the required data we will
have to see if the data that we received
is fit for analysis or not not all the
data that we receive will have
meaningful information some of it will
surely just be corrupt data to remove
this corrupt data we will pass this
entire data through a filtering stage in
this stage all the corrupt data will be
removed now we have the data minus the
corrupt data do you think our data is
now fit for analysis well it is not we
still have to figure out which data will
be compatible with the tool that we will
be using for analysis if we find data
which is incompatible we first extract
it and then transform it to a compatible
form depending on the tool that we use
in the next stage all the data with the
same Fields will be integrated this is
known as the data aggregation stage the
next stage which is the analysis stage
is a very important stage in the life
cycle of big data analytics right here
in this step the entire process of
evaluating your data using various
analytical and statistical tools to
discover meaningful information is done
like we have discussed before the entire
process of deriving meaningful
information from data which is known as
analysis is done here in this stage the
result of the data analysis stage is
then graphically communicated using
tools like Tableau power bi click view
this analysis result will then be made
available to different business
stakeholders for various decision making
this was the entire life cycle of big
data analytics we just saw how data is
analyzed from scratch now we will move
on to a very important topic that is the
different types of big data analytics
well we have four different types of big
data analytics as you see here these are
the types and below this are the
question options that each type tries to
answer we have descriptive analytics
which asks the question what has
happened then we have diagnostic
analytics which asks why did it happen
Predictive Analytics asking what will
happen and prescriptive analytics which
questions by asking what is the solution
we will look into all these four one by
one with the news case for reach we will
first start off with descriptive
analytics as mentioned earlier
descriptive analytics asks the question
what has happened it can be defined as
the type that summarizes past data into
a form that can be understood by humans
in this type we will look into the past
data and arrive at various conclusions
for example an organization can review
its performance using descriptive
analytics that is it analyzes its past
data such as Revenue over the years and
arrives at a conclusion with the profit
by looking at this graph we can
understand if the company is running it
a profit or not thus descriptive
analytics helps us understand this
easily we can simply say that
descriptive analytics is used for
creating various reports for companies
and also for tabulating various social
media metrics like Facebook likes tweets
Etc now that we have seen what is
descriptive analytics let us look into
and use case of descriptive analytics
the Dow Chemical Company analyzed all
its past data using descriptive
analytics and by doing so they were able
to identify the underutilized space in
their facility descriptive analytics
help them in this space consolidation on
the whole the company was able to save
nearly 4 million dollars annually so we
now see here that descriptive analytics
not only helps us derive meaningful
information from the past data but it
can also help companies in cost
reduction if if it is used wisely let us
now move on to our next type that is
diagnostic analytics diagnostic
analytics asks the question why a
particular problem has occurred as you
can see it will always ask the question
why did it happen it will look into the
root cause of a problem and try to
understand why it has occurred
diagnostic analytics makes use of
various techniques such as data mining
data Discovery and drill down companies
benefit from this type of analytics
because it helps them look into the root
cause of a problem by doing so the next
time the same problem will not arise as
the company already knows why it has
happened and they will arrive at a
particular solution for it einet solves
bi query tool is an example of
diagnostic analytics we can use Query
tool for Diagnostic analytics now that
you know why diagnostic analytics is
required and what diagnostic analytics
is I will run you through an example
which shows where technology analytics
can be used all of us shop on e-commerce
sites right have you ever added items to
your cart but ended up not buying it yes
all of us might have done that at some
point an organization tries to
understand why its customers don't end
up buying their products although it has
been added to their cuts and this
understanding is done with the help of
diagnostic analytics an e-commerce site
wonders why they have made few online
sales although they have had a very good
marketing strategy there could have been
various factors as to why this has
happened factors like the shipping fee
which was too high or the page that
didn't load correctly not enough payment
options available and so on all these
factors are analyzed using diagnostic
analytics and the company comes to a
conclusion as to why this has happened
thus we see here that the root cost is
identified so that in future the same
problem doesn't occur again let us now
move on to the Third that is Predictive
Analytics as the meme suggests
Predictive Analytics makes predictions
of the Future IT analyzes the current
and historical facts to make predictions
about future it always asks the question
what will happen next Predictive
Analytics is used with the help of
artificial intelligence machine learning
data mining to analyze the data it can
be used for predicting customer Trends
market trends customer Behavior Etc it
solely works on probability it always
tries to understand what can happen next
with the help of all the past and
current information a company like
PayPal which has 260 plus million
accounts always has the need to ensure
that their online fraudulent and
unauthorized activities are brought down
to nil fear of constant fraudulent
activities have always been a major
concern for PayPal when a fraudulent
activity occurs people lose trust in the
company and this this brings in a very
bad name for the brand it is inevitable
that fraudulent activities will happen
in a company like PayPal which is one of
the largest online payment processes in
the world but PayPal uses analytics
wisely here to prevent such fraudulent
activities and to minimize them it uses
Predictive Analytics to do so the
organization is able to analyze past
data which includes a customer's
historical payment data a customer's
Behavior Trend and then it builds an
algorithm which works on predicting what
is likely to happen next with respect to
their transaction with the use of big
data and algorithms the system can gauge
which of the transactions are valid and
which could be potentially a fraudulent
activity by doing so PayPal is always
ready with precautions that they have to
take to protect all their clients
against fraudulent transactions we will
now move on to our last type that is
descriptive analytics prescriptive
analytics as the name suggests always
prescribes a solution to a particular
problem the problem can be something
which is happening currently hence it
can be termed as the type that always
asks a question what is the solution
prescriptive analytics is related to
both predictive and descriptive
analytics as we saw earlier descriptive
analytics always asks the question what
has happened and Predictive Analytics
helps you understand what can happen
next with the help of artificial
intelligence and machine learning
prescriptive analytics helps you arrive
at the best solution for a particular
problem various business rules
algorithms and computational modeling
procedures are used in prescriptive
analytics let us now have a look at how
and where prescriptive analytics is used
with an example here we will understand
how prescriptive analytics is used by an
airline for its profit do you know that
when you book a flight ticket the price
of it depends on various factors both
internal and external factors apart from
taxes seed selection there are other
factors like oil prices customer demand
which are all taken into consideration
before the flights fare is displayed
prices change due to availability and
demand holiday seasons are a time when
the rates are much higher than the
normal days Seasons like Christmas and
school vacations also beacons the rates
will be much higher than weekdays
another factor which determines a
flight's fare is your destination
depending on the place where you are
traveling to the flight Fair will be
adjusted accordingly this is because
there are quite a few places where the
air traffic is less and in such places
the flights fare will also be less so
prescriptive analytics analyzes all
these factors that are discussed and it
builds an algorithm which will
automatically adjust a flight's fare by
doing so the airline is able to maximum
its profit these were the four types of
analytics now let us understand how we
achieve these with the use of Big Data
tools our next topic will be the various
tools used in big data analytics these
are few of the tools that I will be
talking about today we have Hadoop
mongodb talindi Kafka Cassandra spark
and storm we will look into each one of
them one by one we will first start off
with Hadoop when you speak of Big Data
the first framework that comes into
everyone's mind is Hadoop isn't it as I
mentioned earlier Apache Hadoop is used
to store and process big data in a
distributed and parallel fashion it
allows us to process data very fast
Hadoop uses map reduce big and high for
analyzing this big data Hadoop is easily
one of the most famous Big Data tools
now let us move on to the next one that
is mongodb mongodb is a cross-platform
documentary oriented database it has the
ability to deal with large amount of
unstructured data processing of data
which is unstructured and processing of
data sets that change very frequently is
done using mongodb Talon D provides
software and services for data
integration data management and cloud
storage it specializes in Big Data
integration talente Open studio is a
free open source tool for processing
data easily on a big data environment
Cassandra is used widely for an
effective management of large amounts of
data it is similar to Hadoop in its
feature of fault tolerance where data is
automatically replicated to multiple
nodes Cassandra is preferred for
real-time processing spark is another
tool that is used for data processing
this data processing engine is developed
to process data way faster than Hadoop
mapreduce this is done in a way because
spark does all the process processing in
the main memory of the data nodes and
thus it prevents unnecessary input
output overheads with the disk whereas
map reduce is disk based and hence spark
proves to be faster than Hadoop
mapreduce storm is a free big data
computational system which is done in
real time it is one of the easiest tools
for Big Data analysis it can be used
with any programming language this
feature makes storm very simple to use
finally we will look into another big
data tool which is known as Kafka Kafka
is a distributed streaming platform
which was developed by LinkedIn and
later given to Apache software
Foundation it is used to provide
real-time analytics result and it is
also used for fault tolerant storage
these were few of the big data analytics
tools now let us move on to our last
topic for today that is big data
application domains here we will look
into the various sec factors where big
data analytics is actively used the
first sector is e-commerce nearly 45
percent of the world is online and they
create a lot of data every second big
data can be used smartly in the field of
e-commerce by predicting customer Trend
forecasting demands adjusting the price
and so on online retailers have the
opportunity to create better shopping
experience and generate higher sales if
big data analytics is used correctly
having big data doesn't automatically
lead to a better marketing strategy
meaningful insights need to be derived
from it in order to make right decisions
by analyzing big data we can have
personalized marketing campaigns which
can result in better and higher sales in
the field of Education depending on the
market requirements new courses are
developed the market requirement needs
to be analyzed correctly with respect to
the scope of a course and accordingly a
scope needs to be developed there is no
point in developing a course which has
no scope in the future hence to analyze
the market requirement and to develop
new courses we use big data analytics
here there are a number of uses of big
data analytics in the field of
healthcare and one of it is to predict a
patient's health issue that is with the
help of their previous medical history
big data analytics can determine How
likely they are to have a particular
health issue in the future the example
of Spotify that we saw previously showed
how big data analytics is used to
provide a personalized recommendation
list to all its users similarly in the
field of media and entertainment big
data analytics is used to understand the
demands of shows songs movies and so on
to deliver personalized recommendation
lists as we saw with Spotify big data
analytics is used in the field of
banking as we saw previously with a few
use cases big data analytics was used
for risk management in addition to risk
management it is also used to analyze a
customer's income and spend patterns and
to help the bank predict if a particular
customer is going to choose any of the
bank offers such as loans credit cards
schemes and so on this way the bank is
able to identify the right customer who
is interested in its offers it has
noticed that telecom companies have
begun to embrace big data to gain profit
big data analytics helps in analyzing
Network traffic and call data records it
can also improve its service quality and
improve its customer experience let us
now look into how big data analytics is
used by governments across the world in
the field of law enforcement big data
analytics can be applied to analyze all
the available data to understand crime
patterns intelligent Services can use
predictive analysis it takes to forecast
the crime which could be committed in
Durham the police department was able to
reduce the crime rate using big data
analytics with the help of data police
could identify whom to Target where to
go went to petrol and how to investigate
crimes big data analytics helped them to
discover patterns of crime emerging in
the area now that we have seen the
various sectors where big data analytics
is used we now have access to tools and
techniques that process data and extract
the information we need for instance
there are data processing tools for data
wrangling we have new and flexible
programming languages that are more
efficient and easier to use
with the creation of operating systems
that support multiple OS platforms it's
now easier to integrate systems and
process Big Data application designs and
extensive software libraries help
develop more robust scalable and
data-driven applications
data scientists use these Technologies
to build data models and run them in an
automated fashion to predict the outcome
efficiently
this is called machine learning which
helps provide insights into the
underlying data they can also use data
science technology to manipulate data
extract information from it and use it
to build tools applications and services
but technological skills and domain
expertise alone without the right
mathematical and statistical knowledge
might lead data scientists to find
incorrect patterns and convey the wrong
information
now that you have learned what data
science is it will be easier to
understand what a data scientist does
data scientists start with a question or
a business problem
then they use data acquisition to
collect data sets from The Real World
the process of data wrangling is
implemented with data tools and modern
technologies that include data cleansing
data manipulation data Discovery and
data pattern identification
the next step is to create and train
models for machine learning
they then design mathematical or
statistical models
after designing a data model it's
represented using data visualization
techniques
the next task is to prepare a data
report
after the report is prepared they
finally create data products and
services
let us now look at the various skills a
data scientist should have
data scientists should ask the right
questions for which they need domain
expertise the Curiosity to learn and
create Concepts and the ability to
communicate questions effectively to
domain experts
data scientists should think
analytically to understand the hidden
patterns in a data structure
they should Wrangle the data by removing
redundant and irrelevant data collected
from various sources
statistical thinking and the ability to
apply mathematical methods are important
traits for a data scientist
data should be visualized with graphics
and proper storytelling to summarize and
communicate the analytical results to
the audience
to get these skills they should follow a
distinct roadmap it's important they
adopt the required tools and techniques
like Python and its libraries they
should build projects using real world
data sets that include data.gov NYC open
data Gap minder and so on
they should also build a data-driven
applications for Digital Services and
data products
scientists work with different types of
data sets for various purposes now that
big data is generated every second
through different media the role of data
science has become more important
so you need to know what big data is and
how you are connected to it to figure
out a way to make it work for you
every time you record your heartbeat
through your phone's biometric sensors
post or tweet on The Social Network
create any blog or website switch on
your phone's GPS Network upload or view
an image video or audio in fact every
time you log into the internet you are
generating data about yourself your
preferences and your lifestyle
big data is a collection of these and a
lot more data that the world is
constantly creating in this age of the
internet of things or iot big data is a
reality and a need
big data is usually referenced by three
vs volume velocity and variety
volume refers to the enormous amount of
data generated from various sources
big data is also characterized by
velocity
huge amounts of data flow at a
tremendous speed from different devices
sensors and applications
to deal with it an efficient and timely
data processing is required
variety is the third V of Big Data
because big data can be categorized into
different formats like structured
semi-structured and unstructured
structured data is usually referenced to
as rdbms data which can be stored and
retrieved easily through sqls
semi-structured data are usually in the
form of files like XML Json documents
and nosql database
text files images videos or multimedia
content are examples of unstructured
data
in short big data is a very large
information database usually stored on
distributed systems or machines
popularly referred to as Hadoop clusters
but to be able to use this database we
have to find a way to extract the right
information and data patterns from it
that's where data science comes in data
science helps to build information
driven Enterprises
let's go on to see the applications of
data science in different sectors
social network platforms such as Google
Yahoo Facebook and so on collect a lot
of data every day which is why they have
some of the most advanced data centers
spread across the world
having data centers all over the world
and not just in the US help these
companies serve their International
customers better and faster without any
network latency
they also help them deal effectively
with the enormous amount of data so what
do all these different sectors do with
all this big data
their team of data scientists analyze
all the raw data with the help of modern
algorithms and data models to turn it
into information
they then use this information to build
Digital Services data products and
information driven Maps
now let's see how these products and
services work we'll first look at
LinkedIn
let's suppose that you are a data
scientist based in New York city so it's
quite likely that you would want to join
a group or build connections with people
related to data science in New York City
now what LinkedIn does with the help of
data science is that it looks at your
profile your posts and likes the city
you are from the people you are
connected to and the groups you belong
to then it matches all that information
with its own database to provide you
with information that is most relevant
to you this information could be in the
form of news updates that you might be
interested in Industry connections or
professional groups that you might want
to get in touch with or even job
postings related to your field and
designation these are all examples of
data services let's now look at
something that we use every day Google's
search engine
Google search engine has the most unique
search algorithm which allows machine
learning models to provide relevant
search recommendations even as the user
types in his or her query
this feature is called autocomplete it
is an excellent example of how powerful
machine learning can be
there are several factors that influence
this feature
the first one is query volume Google's
algorithms identify unique and
verifiable users that search for any
particular keyword on the web
based on that it builds a query volume
for instance Republican debate 2016
Ebola threat CDC or the center of
Disease Control and so on are some of
the most common user queries
another important factor is a
geographical location
the algorithms tag a query with the
locations from where it is generated
this makes a query volume location
specific it's a very important feature
because this allows Google to provide
relevant search recommendations to its
user based on his or her location
and then of course the algorithms
consider the actual keywords and phrases
that the user types in
it takes up those words and crawls the
web looking for similar instances
the algorithms also try to filter or
scrub out inappropriate content
for instance sexual violent or
terrorism-related content hate speeches
and legal cases are scrubbed out from
the search recommendations
but how does data science help you
today even the health care industry is
beginning to tap into the various
applications of data science to
understand this let's look at wearable
devices these devices have biometric
sensors and a built-in processor to
gather data from your body when you are
wearing them
they transmit this data to the big data
analytics platform via the iot Gateway
ideally the platform collects hundreds
of thousands of data points and the
collected data is ingested into the
system for further processing
the big data analytics platform applies
data models created by data scientists
and extracts the information that is
relevant to you
it sends the information to the
engagement dashboard where you can see
how many steps you want what your heart
rate is over a period of Time how good
your sleep was how much calories you've
earned and so on
knowing such details would help you to
set personal goals for a healthy
lifestyle and reduce overall health care
and insurance costs it would also help
your doctor record your vitals and
diagnose any issue
the finance sector can easily use data
science to help it function more
efficiently
suppose a person applies for a loan the
loan manager submits the application to
the Enterprise infrastructure for
processing
the analytics platform applies data
models and algorithms and creates an
engagement dashboard for the loan
manager
the dashboard would show the applicant's
credit report credit history amount if
approved and risks associated with him
or her
the loan manager can now easily take a
look at all the relevant information and
decide whether the loan can be approved
or not
governments across different countries
are gradually sharing large data sets
from various domains with the public
this kind of transparency makes the
government seem more trustworthy it
provides the country data that can be
used to prepare itself for different
types of issues like climate change and
Disease Control
it also helps encourage people to create
their own digital products and services
the US government hosts and maintains
data.gov a website that offers
information about the federal government
it provides access to over 195 000 data
sets across different sectors
the US government has kicked off a
number of strategic initiatives in the
field of data science that includes U.S
digital service and open data
we have seen how data science can be
applied across different sectors let's
now take a look at the various
challenges that a data scientist faces
in the real world while dealing with
data sets data quality the quality of
data is mostly not up to the set
standards you will usually come across
data that is inconsistent inaccurate and
complete not in the desirable format and
with anomalies
integration
data integration with several Enterprise
applications and systems is a complex
and painstaking task
unified platform data is distributed to
Hadoop distributed file system or hdfs
from various sources to ingest process
analyze and visualize huge data sets the
size of these Hadoop clusters can vary
from few nodes to thousand nodes the
challenge is to perform analytics on
these large data sets efficiently and
effectively
this is where python comes into play
with its powerful set of libraries
functions modules packages and
extensions
python can efficiently tackle each stage
of data analytics that includes data
acquisition python libraries such as
Scrappy comes handy here
data wrangling python data frames are
very efficient in handling large data
sets and makes data wrangling easier
with its powerful functions
explore
matplotlib libraries are very rich when
it comes to data exploration
model
scikit learns statistical and
mathematical functions to help to build
models for machine learning
visualization modern libraries such as
Voca creates very intuitive and
interactive visualization
its huge set of libraries and functions
make big data analytics seem easy and
hence solves a bigger problem
python applications and programs are
portable and helps them scale out on any
big data platform
python is an open source programming
language that lets you work quickly and
integrate systems more effectively now
that we have talked about how the python
libraries help the different stages of
data analytics let's take a closer look
at these libraries and how they support
different aspects of data science
numpy or numerical python is the
fundamental package for scientific
computing
scipy is the core of scientific
Computing libraries and provides many
user-friendly and efficiently designed
numerical routines
matplotlib is a python 2D plotting
Library which produces publication
quality figures in a variety of hard
copy formats and interactive
environments across platforms
scikit-learn is built on numpy scipy and
matplotlib for data mining and data
analysis
pandas is a library providing high
performance easy to use data structures
and data analysis tools for python
all these libraries modules and packages
are open source and hence using them is
convenient and easy
there are numerous factors which
positions python well and makes it the
tool for data science
python is easy to learn it's a general
purpose function and object-oriented
programming language
as python is an open source programming
language it is readily available easy to
install and get started it also has a
large presence of Open Source Community
for software development and support
Python and its tools enjoy
multi-platform support
applications developed with pycon
integrate easily with other Enterprise
systems and applications
there are a lot of tools platforms and
products in the market from different
vendors as they offer great support and
services
Python and its libraries create unique
combinations for data science because of
all these benefits it's usually popular
among academicians mathematicians
statisticians and technologists
python is supported by well-established
data platforms and processing Frameworks
that help it analyze data in a simple
and efficient way
Enterprise Big Data platform
Cloudera is the Pioneer in providing
enterprise-ready Hadoop Big Data
platform and supports python
hortonworks is another Hadoop Big Data
platform provider and supports python
mapreduce map bar is also committed to
Python and provides the Hadoop Big Data
platform
big data processing framework
mapreduce spark and Flink provides very
robust and unique data processing
framework and support python
Java Scala and python languages are used
for big data processing framework
but to access Big Data you have to use a
big data platform which is a combination
of the Hadoop infrastructure also known
as Hadoop distributed file system or
hdfs and an analytics platform
Hadoop is a framework that allows data
to be distributed across clusters of
computers for faster cheaper and
efficient computing
it's completely developed and coded in
Java one of the most popular analytics
platforms is Spark
it easily integrates with hdfs
it can also be implemented as a
standalone analytics platform and
integrated with multiple data sources
it helps data scientists perform their
work more efficiently
spark is built using Scala
since there is a disparity in the
programming language that data
scientists use and that of the Big Data
platform it impedes data access and Flow
as python is a data scientist's first
language of choice both Hadoop and Spark
provide python apis that allow easy
access to the Big Data platform
consequently a data scientist need not
learn Java or Scala or any other
platform-specific data languages and can
instead focus on performing data
analytics
there are several motivations for python
Big Data Solutions
big data is a continuously evolving
field which involves adding new data
processing Frameworks that can be
developed using any programming language
moreover new innovation and research is
driving the growth of Big Data Solutions
and platform providers
it would be difficult for data
scientists to focus on analytics if they
have to constantly upgrade themselves on
information or under the hood
architecture or implementation of the
platform therefore it's important to
keep the entire data science platform
and any language agnostic to simplify a
data scientist's job
consequently almost all major vendors
solution providers and data processing
framework developers are providing
python apis this allows a data scientist
to perform big data analytics using only
python rather than learning other
languages like Java or Scala to help
them work on the big data platform
let's look at an example and understand
how data is stored across hadoop's
distributed clusters
big data is generated from different
data sources a large file usually
greater than 100 megabytes gets routed
from a name node to data nodes
name nodes hold the metadata information
about the files stored on data nodes it
stores the address and information of a
block of file and the data node
associated with it
data nodes hold the actual data blocks
the file is split into multiple smaller
files usually of 64 megabytes or 128
megabyte size
it's then copied to multiple physical
servers the smaller files are also
called file blocks one file block gets
replicated to different servers the
default replication factor is three
which means a single file block gets
copied at least three times on different
servers or data nodes there is also a
secondary name node which keeps a backup
of all the metadata information stored
on the main or primary node this node
can be used if and when the main name
node fails
now that you have understood a little
about hdfs let's look at the second core
component of Hadoop mapreduce the
primary framework of the hdfs
architecture
a file is split into three blocks as
split 0 split 1 and split 2. when a
request comes in to retrieve the
information the mapper task is executed
on each data node that contains the file
blocks
the mapper generates an output
essentially in the form of key value
pairs that are sorted copied and merged
once the mapper task is complete the
reducer works on the data and stores the
output on hdfs this completes the
mapreduce process
let's discuss the mapreduce functions
mapper and reducer in detail
the mapper
Hadoop ensures that mappers run locally
on the nodes which hold a particular
portion of the data to avoid the network
traffic
multiple mappers run in parallel and
each mapper processes a portion of the
input data the input and output of the
mapper are in the form of key value
pairs note that it can either provide
zero or more key value pairs as output
the reducer
after the map phase all intermediate
values for an intermediate key are
combined into a list which is given to a
reducer all values associated with a
particular intermediate key are directed
to the same reducer this step is known
as Shuffle and sort
there may be a single reducer or
multiple reducers
note that the reducer also provides
outputs in the form of zero or more than
one final key value pairs
these values are then returned to hdfs
the reducer usually emits a single key
value pair for each input key
you have seen how mapreduce is critical
for hdfs to function a good thing is you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python with the help of Hadoop streaming
API
Hadoop streaming acts like a bridge
between your python code and the Java
based hdfs and lets you seamlessly
access Hadoop clusters and execute
mapreduce tasks
you have seen how mapreduce is critical
for hdfs to function thankfully you
don't have to learn Java or other Hadoop
Centric languages to write a mapreduce
program you can easily run such Hadoop
jobs with a code completely written in
Python
shown here are some user-friendly python
functions that are written for the
mapper class
suppose we have the list of numbers we
want to square
we have the square function defined as
shown on the screen
we can call the map function with a list
and a function which is to be executed
on each item in that list
the output of this process is as shown
on the screen
reducer can also be written in Python
here we would like to sum the squared
numbers of the previous map operation
this can be done using the sum operation
as shown on the screen
we can now call the reduce function with
the list of data which is to be
aggregated and aggregator function in
our case sum is used for this purpose
Big Data analysis requires a large
infrastructure Cloudera provides
enterprise-ready Hadoop Big Data
platform which supports python as well
to execute Hadoop jobs you have to first
install Cloudera
it's preferable to install Cloud era's
virtual machine on a Unix system as it
functions best on it
to set up the Cloudera Hadoop
environment visit the Cloudera link
shown here
select quick start download for CDH 5.5
and VMware from the drop down lists
click the download now button
once the VM image is downloaded please
use 7-Zip to extract the files to
download and install it visit the link
shown on screen
Cloudera VMware has some system
prerequisites
the 64-bit virtual machine requires a
64-bit host operating system or Os and
the virtualization product that can
support a 64-bit guest OS
to use a VMware VM you must use a player
compatible with workstation 8.x or
higher such as player 4.x or higher or
Fusion 4.x or higher
you can use older versions of
workstation to create a new VM using the
same virtual disk or vmdk file but some
features in VMware tools will be
unavailable
the amount of ram required will vary
depending on the runtime option you
choose
to launch the VMware Player you will
either need VMware Player for Windows
and Linux or VMware Fusion for Mac
so please visit the VMware link shown on
screen to download the relevant VMware
Player Now launch the VMware Player with
the Cloudera VM
the default username and password is
Cloudera
click the terminal icon as shown here it
will launch the Unix terminal for Hadoop
hdfs interaction
to verify that the Unix terminal is
functioning correctly type in PWD which
will show you the present working
directory you can also type in LS space
hyphen LRT to list all the current files
folders and directories these are some
simple unix commands which will come in
handy later while you are implementing
mapreduce tasks
you have seen how the Hadoop distributed
file system works along with mapreduce
the data is written on and read by disks
mapreduce jobs require a lot of disk
read and write operations which is also
known as disk IO or input and output
reading and writing to a disk is not
just expensive it can also be slow and
impact the entire process and operation
this is specifically true for iterative
processes Hadoop is built for write once
read many type of jobs which means it's
best suited for jobs that don't have to
be updated or accessed frequently but in
several cases particularly in analytics
and machine learning users need to write
and rewrite commands to access and
compute on the same data more than once
every time such a request is sent out
mapreduce requires that data is read and
or written onto disks directly note that
though the time to access or write on
disks is measured in milliseconds when
you are dealing with large file sizes
the time Factor gets compounded
significantly this makes the process
highly time consuming
in contrast Apache spark uses resilient
distributed data sets or rdds to carry
out such computations
rdds allowed data to be stored in memory
which means that every time users want
to access the same data a disk i o
operation is not required they can
easily access data stored in the cache
accessing the cache or Ram is much
faster than accessing disks for instance
if disk access is measured in
milliseconds in-memory data access is
measured in sub milliseconds this
radically reduces the overall time taken
for iterative operations on large data
sets in fact programs on spark run at
least 10 to 100 times faster than on
mapreduce that's why spark is gaining
popularity among most data scientists as
it is more time efficient when it comes
to running analytics and machine
learning computations
one of the main differences in terms of
Hardware requirements for mapreduce and
Spark is that while mapreduce requires a
lot of servers and CPUs spark
additionally requires a large and
efficient Ram
let's understand resilient distributed
data sets in detail as you have already
seen the main programming approach of
spark is rdd
rdds are fault tolerant collections of
objects spread across a cluster that you
can operate on in parallel they are
called fault tolerant because they can
automatically recover from machine
failure
you can create an rdd either by copying
the elements from an existing collection
or by referencing a data set stored
externally say on an hdfs
rdds support two types of operations
Transformations and actions
Transformations use an existing data set
to create a new one for example map
creates a new rdd containing the results
after passing the elements of the
original data set through a function
some other examples of Transformations
are filter and join
actions compute on the data set and
return the value to the driver program
for example reduce Aggregates all the
rdd elements using a specified function
and returns this value to the driver
program
some other examples of actions are count
collect and Save
it's important to note that if the
available memory is insufficient then
spark writes the data to disk
here are some of the advantages of using
spark it's almost 10 to 100 times faster
than Hadoop mapreduce
it has a simple data processing
framework it provides interactive apis
for python that allow faster application
development
it has multiple tools for complex
analytics operations these tools help
data scientists perform machine learning
and other analytics much more
efficiently and easily than most
existing tools
it can easily be integrated with the
existing Hadoop infrastructure
Pi spark is the python API used to
access the spark programming model and
perform data analysis
let's take a look at some transformation
functions and action methods which are
supported by pi spark for data analysis
these are some common transformation
functions
map returns rdd formed by passing data
elements from The Source data set
filter
returns rdd based on selected criteria
flat map
s items present in the data set and
returns a sequence
Reduce by key
returns key value pairs where values for
each key is aggregated by a given reduce
function
let's now look at some common action
functions
collect returns all elements of the data
set as an array
count Returns the number of elements
present in the data set
first Returns the first element in the
data set
take
Returns the number of elements as
specified by the number in the
parentheses
spark context or SC is the entry point
to spark for the spark application and
must be available at all times for data
processing
there are mainly four components in
spark tools
spark SQL it's mainly used for querying
the data stored on hdfs as a resilient
distributed data set or rdd in spark
through integrated apis in Python Java
and Scala
spark streaming it's very useful for
data streaming process and where data
can be read from various data sources
ml lib it's mainly used for machine
learning processes such as supervised
and unsupervised learning
graph x it can be used to process or
generate graphs with rdds
let's set up the Apache spark
environment and also learn how to
integrate spark with Jupiter notebook
first visit the Apache link and download
Apache spark to your system
now use 7-Zip software and extract the
files to your system's local directory
to set up the environment variables for
spark first set up the user variables
click new and then enter spark home in
the variable name and enter the spark
installation path as variable value
now click on the path and then click new
and enter the spark bin path from the
installed directory location
now let's set up the pi spark notebook
specific variables
this will integrate The Spark engine
with Jupiter notebook
type in pi spark it will launch a
jupyter notebook after a while
create a python notebook and type in SC
command to check the spark context
fusions of Hadoop
which have package the Apache Hadoop in
a cluster management solution which
allows everyone to easily deploy manage
monitor upgrade your clusters so here
are some window specific distributions
we have Cloudera which is the dominant
one in the market we have hot and works
and now you might be aware that clouder
and hortonworks have merged so it has
become a bigger entity you have map R
you have Microsoft is your IBM's
infosphere and Amazon web services so
these are some popularly known
vendor-specific distributions if you
would want to know more about the Hadoop
distributions you should basically look
into Google and you should check for
Hadoop different distributions Wiki page
so if I type Hadoop different
distributions and then I check for the
Wiki page that will take me to the
distributions and Commercial support
page and this basically says that the
sold products that can be called in
release of Apache Hadoop come from
apache.org so that's your open source
community and then you have various
vendor specific distributions which
basically are running in one or the
other way Apache Hadoop but they have
packaged it as a solution like a
installer so that you can easily set up
clusters on set of machines so have a
look at this page and read through about
different distributions of Hadoop coming
back let's look at our next question so
what are the different Hadoop
configuration files now whether you're
talking about Apache Hadoop Cloudera
hortonworks map r or no matter which
other distribution these config files
are the most important and existing in
every distribution of Hadoop so you have
Hadoop environment.sh wherein you will
have environment variables such as your
Java path what would be your process ID
path where will your logs get stored
what kind of metrics will be collected
and so on your core iPhone site file has
the hdfs path now this has many other
properties like enabling trash or
enabling High availability or discussing
or mentioning about your zookeeper but
this is one of the most important file
you have hdfs hyphen site file now this
file will have other information related
to your Hadoop cluster such as your
replication Factor where will name node
store its metadata on disk if a data
node is running where would data node
store its data if a secondary name node
is running where would that store a copy
of name nodes metadata and so on your
mapred hyphen site file is a file which
will have properties related to your map
reduce processing you also have Masters
and slaves now these might be deprecated
in a vendor-specific distribution and in
fact you would have a yarn hyphen site
file which is based on the yarn
processing framework which was
introduced in Hadoop version 2 and this
would have all your resource allocation
and resource manager and node manager
related properties again if you would
want to look at default properties for
any one of these for example let's say
hdfs hyphen site file I could just go to
Google and type in one of the properties
for example I would say DFS dot name
node dot name dot directory and as I
know this property belongs to hdfs
hyphen site file and if you search for
this it will take you to the first link
which says stfs default XML you can
click on this and this will show you all
the properties which can be given in
your stfs hyphen site file it also shows
you which version you are looking at and
you can always change the version here
so for example if I would want to look
at
2.6.5 I just need to change the version
and that should show me the properties
similarly you can just give a property
which belongs to say core hyphen site
file for example I would say FS dot
default fs and that's a property which
is in core hyphen sci-fi and somewhere
here you would see core minus default
dot XML and this will show you all the
properties so similarly you could search
for properties which are related to yarn
hyphen site file or mapred hyphen site
file so I could say yarn dot resource
manager and I could look at one of these
properties which will directly take me
to yarn default XML and I can see all
the properties which can be given in
yarn and similarly you could say map
reduce dot job dot reduces and I know
this property belongs to mapreduce
hyphen site file and this takes you to
the default XML so these are important
config files and no matter which
distribution of Hadoop you are working
on you should be knowing about these
config files whether you work as a
Hadoop admin or you work as a Hadoop
developer knowing these config
properties would be very important and
that would also showcase your internal
knowledge about the configs which drive
your Hadoop cluster let's look at the
next question so what are the three
modes in which Hadoop can run so you can
have Hadoop running in a standalone mode
now that's your default mode it would
basically use a local file system and a
single Java process so when you say
Standalone mode it is as you downloading
Hadoop related package on one single
machine but you would not have any
process running that would just be to
test Hadoop functionalities you could
have a pseudo distributed mode which
basically means it's a single node
Hadoop deployment now Hadoop as a
framework has many many services so it
has a lot of services and those Services
would be running irrespective of your
distribution each service would then
have multiple processes so your pseudo
distributed mode is a mode of cluster
where you would have all the important
processes belonging to one or multiple
Services running on a single node if you
would want to work on a pseudo
distributed mode and using a Cloudera
you can always go to Google and search
for cloudera's quick start VM you can
download it by just saying Cloud era
quick start VM and you can search for
this and that will allow you to download
a quick start VM follow the instructions
and you can have a single node Cloudera
cluster running on your virtual machines
for more information you can refer to
the YouTube tutorial where I have
explained about how to set up a quick
start VM coming back you could have
finally a production setup or a fully
distributed mode which basically means
that your Hadoop framework and its
components would be spread across
multiple machines so you would have
multiple services such as hdfs yarn
Flume scope Kafka hbase Hive Impala and
for these Services there would be one or
multiple processes distribute across
multiple nodes so this is normally what
is used in production environment so you
could say Standalone would be good for
testing pseudo distributed could be good
for testing and development and fully
distributed would be mainly for your
production setup now what are the
differences between regular file system
and hdf
So when you say regular file system you
could be talking about a Linux file
system or you could be talking about a
Windows based operating system so in
regular file system we would have data
maintained in a single system so the
single system is where you have all your
files and directories so it is having
low fault tolerance right so if the
machine crashes your data recovery would
be very difficult unless and until you
have a backup of that data that also
affects your processing so if the
machine crashes or if the machine fails
then your processing would be blocked
now the biggest challenge with regular
file system is the seek time the time
taken to read the data so you might have
one single machine with huge amount of
disks and huge amount of ram but then
the time taken to read that data when
all the data is stored in one machine
would be very high and that would be
with least fault tolerance if you talk
about sdfs your data is distributed so
is DFS stands for Hadoop distributed
file system so here your data is
distributed and maintained on multiple
systems so it is never one single
machine it is also supporting
reliability so whatever is stored in
stfs say a file being stored depending
on its size is split into blocks and
those blocks will be spread across
multiple nodes not only that every block
which is stored on a node will have its
replicas stored on other nodes
replication Factor depends but this
makes sdfs more reliable in cases of
your slave nodes or data nodes crashing
you will rarely have data loss because
of Auto replication feature now time
taken to read the data is comparatively
more as you might have situations where
your data is distributed across the
nodes and even if you are doing a
parallel read your data read might take
more time because it needs coordination
for multiple machines however if you are
working with huge data which is getting
stored it will still be beneficial in
comparison to reading from a single
machine so you should always think about
its reliability through Auto replication
feature its fault tolerance because of
your data getting stored across multiple
machines and its capability to scale so
when you talk about sdfs we are talking
about horizontal scalability or scaling
out when you talk about regular file
system you are talking about vertical
scalability which is scaling up now
let's look at some specific sdfs
questions what is this why is sdfs Fault
tolerant now as I just explained in
previous slides your sdfs is Fault
tolerant as it replicates data on
different data nodes so you have a
master node and you have multiple slave
nodes or data nodes where actually the
data is getting stored now we also have
a default block size of 128 MB that's
the minimum since Hadoop version 2. so
any file which is up to 128 MB would be
using one logical block and if the file
size is bigger than 128 MB then it will
be split into blocks and those blocks
will be stored across multiple machines
now since these blocks are stored across
multiple machines it makes it more fault
tolerant because even if your machines
fail you would still have a copy of your
block existing on some other machine now
there are two aspects here one we talk
about the first rule of replication
which basically means you will never
have two identical blocks sitting on the
same machine and the second rule of
replication is in terms of rack
awareness so if your machines are placed
in racks as we see in the right image
you will never have all the replicas
placed on the same rack even if they are
on different machines so it has to be
fault tolerant and it has to maintain
redundancy so at least one replica will
be placed on some other node on some
other rack that's how sdfs is Fault
tolerant now here let's understand the
architecture of sdfs now as I mentioned
earlier you would in a Hadoop cluster
the main service is your hdfs so for
your sdfs service you would have a name
node which is your master process
running on one of the machines and you
would have data nodes which are your
slave machines getting stored across
marketing or the processes running
across multiple machines each one of
these processes has an important role to
play when you talk about sdfs whatever
data is written to hdfs that data is
split into blocks depending on its size
and the blocks are randomly distributed
across nodes with auto replication
feature these blocks are also Auto
replicated across multiple machines with
the first condition that no two
identical blocks will sit on the same
machine now as soon as the cluster comes
up you
nodes which are part of the
and based on config files would start
sending their heartbeat to name node and
this would be every
seconds what does name node do with that
name node will store this information in
its Ram so name node starts building a
metadata in its RAM and that metadata
has information of what are the data
nodes which are available in the
beginning now when a data writing
activity starts and the blocks are
distributed across data nodes data nodes
every 10 seconds will also send a block
report to name node so name node is
again adding up this information in its
Ram or the metadata in Ram which earlier
had only data node information now name
node will also have information about
what are the files the files are split
in which blocks the blocks are stored on
which machines and what the file
permissions now while name node is
maintaining this metadata in Ram name
node is also maintaining metadata in
disk so that is what we see in the red
box which basically has information of
whatever information was written to hdfs
so to summarize your name node has
metadata in Ram and metadata in disk
your data nodes are the machines where
your blocks or data is actually getting
stored and then there is a auto
replication feature which is always
existing unless and until you have
disabled it and your read and write
activity is a parallel activity however
replication is a sequential activity now
this is what I mentioned about when you
talk about name node which is the master
process hosting metadata in disk and RAM
so when we talk about disk it basically
has a edit log which is your transaction
log and your FS image which is your file
system image right from the time the
cluster was started this metadata in
disk was existing and this gets appended
every time read write or any other
operations happen on stfs metadata in
Ram is dynamically built every time the
cluster comes up which basically means
that if your cluster is coming up name
node in the initial few seconds or few
minutes would be in a safe mode which
basically means it is busy registering
the information from data nodes so name
node is one of the most critical
processes if name node is down and if
all other processes are running you will
not be able to access the cluster name
nodes metadata in disk is very important
for name node to come up and maintain
the cluster name nodes metadata in Ram
is basically for all or satisfying all
your client requests now when we look at
data nodes as I mentioned data nodes
hold the actual data blocks and they are
sending these block reports every 10
seconds so the metadata in name nodes
Ram is constantly getting updated and
metadata in disk is also constantly
getting updated based on any kind of
write activity happening on the cluster
now data node which is storing the block
will also help in any kind of read
activity whenever a client requests so
whenever a client on an application or
an API would want to read the data it
would first talk to name node name node
would look into its metadata on
and confirm to the client which machines
could be reached to get
that's where your client would try to
read the data from sdfs which is
actually getting the data from data
nodes and that's how your read write
requests are satisfied now what are the
two types of metadata in name node
server holds as I mentioned earlier
metadata in disk very important to
remember edit log NFS image metadata in
Ram which is information about your data
nodes files files being split into
blocks blocks residing on data nodes and
file permissions so I will share a very
good link on this and you can always
look for more detailed information about
your metadata so you can search for sdfs
metadata directories explained now this
is from hortonworks however it talks
about the metadata in disk which name
node manages and details about this so
have a look at this link if you are more
interested in learning about metadata on
disk coming back let's look at the next
question what is the difference between
Federation and high availability now
these are the features which were
introduced in Hadoop version 2. both of
these features are about horizontal
scalability of name node subscribe to
version 2 the only possibility was that
you could have one single Master which
basically me means that your cluster
could become unavailable if name node
would crash so Hadoop version 2
introduced two new features Federation
and high availability however High
availability is a popular one so when
you talk about Federation it basically
means any number of name nodes so there
is no limitation to the number of name
nodes your name nodes are in a Federated
cluster which basically means name nodes
still belong to the same cluster but
they are not coordinating with each
other so whenever a write request comes
in one of the name node picks up that
request and it guides that request for
the blocks to be written on data nodes
but for this your name node does not
have to coordinate with other name node
to find out if the block ID which was
being assigned was the same one as
assigned by other name node so all of
them belong to a Federated cluster they
are linked via a cluster ID so whenever
an application or an API is trying to
talk to Cluster it is always going via
an cluster ID and one of the name node
would pick up the read activity or write
activity or processing activity so all
the name nodes are sharing a pool of
metadata in which each name node will
have its own dedicated pool and we can
remember that by a term called namespace
or name service so this also provides
High fault tolerance suppose your one
name node goes down it will not affect
or make your cluster unavailable you
will still have your cluster reachable
because there are other name nodes
running and they are available now when
it comes to heartbeats all your data
nodes are sending their heartbeats to
all the name nodes and all the name
nodes are aware of all the data nodes
when you talk about high availability
this is where you would only have two
name nodes so you would have an active
and you would have a standby now
normally in any environment you would
see a high availability setup with
zookeeper so zookeeper is a centralized
coordination service so when you talk
about your active and stand by name
notes election of a name node to be made
as active and taking care of a automatic
failover is done by your zooky button
High availability can be set up without
zookeeper but that would mean that a
admins intervention would be required to
make a name known as active from standby
or also to take care of failover now at
any point of time in high availability a
active name node would be taking care of
storing the edits about whatever updates
are happening on sdfs and it is also
writing these edits to a shared location
standby name node is the one which is
constantly looking for these latest
updates and applying to its metadata
which is actually a copy of whatever
your active name node has so in this way
your standby name node is always in sync
with the active name node and if for any
reason active name node fails your
standby name node will take over and
become the active remember zookeeper
plays a very important role
it's a centralized coordination service
one more thing to remember here is that
in your high availability secondary name
node will not be allowed so you would
have a active name node and then you
will have a standby name node which will
be configured on a separate machine and
both of these will be having access to a
shared location now that shared location
could be NFS or it could be a quorum of
Journal nodes so for more information
refer to the tutorial where I have
explained about sdfs high availability
and Federation now let's look at some
logical question here so if you have a
input file of 350 tmb which is obviously
bigger than 128 MB how many input splits
would be created by sdfs
what would be the size of each input
split so for this you need to remember
that by default the minimum block size
is 128 MB now that's customizable if
your environment has more number of
larger files written on an average then
obviously you have to go for a bigger
block size if your environment has a lot
of files being written but these files
are of smaller size you could be okay
with 128 MB remember in Hadoop every
entity that is your directory on sdfs
file on sdfs and a file having multiple
blocks each of these are considered as
objects and for each object hadoop's
name notes Ram 150 bytes is utilized so
if your block size is very small then
you would have more number of blocks
which would directly affect the name
nodes of M if you keep a block size very
high that will reduce the number of
blocks but remember that might affect in
processing because processing also
depends on split
split more number of splits more the
parallel processing so setting of block
size has to be done with consideration
about your parallelism requirement and
your name nodes Ram which is available
now coming to the question if you have a
file of 350 MB that would be split into
three blocks and here two blocks would
have 128 MB data and the third block
although the block size would still be
128 it would have only 94 MB of data so
this would be the split of this
particular file now let's understand
about rack awareness how does rack
awareness work or why do we even have
racks so organizations always would want
to place their nodes or machines in a
systematic way there can be different
approaches you could have a rack which
would have machines running on the
master processes and the intention would
be that this particular rack could have
higher bandwidth more cooling dedicated
power supply top of rack switch and so
on the second approach could be that you
could have one master process running on
one machine of every rack and then you
could have other slave processes running
now when you talk about your rack
awareness one thing to understand is
that if your machines are placed within
racks and we are aware that Hadoop
follows Auto replication the rule of
replication in a rack aware cluster
would be that you would never have all
the replicas placed on the same rack so
if we look at this if we have block a in
blue color you will never have all the
three blue boxes in the same rack even
if they are on different nodes because
that makes us that makes it less fault
tolerant so you would have at least one
copy of block which would be stored on a
different rack on a different note now
let's look at this so basically here we
are talking about replicas being placed
in such a way now somebody could ask a
question can I have my block and its
replicas spread across three racks and
yes you can do that but then in order to
make it more redundant you are
increasing your bandwidth requirement so
the better approach would be two blocks
on the same rack on different machines
and one copy on a different track now
let's proceed how can you restart name
node and all demons in Hadoop so if you
were working on an Apache Hadoop cluster
then you could be doing a start and stop
using Hadoop demon scripts so there are
these Hadoop demon scripts which would
be used to start and stop your Hadoop
and this is when you talk about your
Apache Hadoop so let's look at one
particular file which I would like to
show you more information here and this
talks about your different clusters so
let's look into this and so let's look
at the start and stop and here I have a
file let's look at this one and this
gives you highlights so if you talk
about Apache Hadoop this is how the
setup would be done so you would have it
download the Hadoop tar file you would
have to unturn it edit the config files
you would have to do formatting and then
start your cluster and here I have said
you using scripts so this is in case of
Apache Hadoop you could be using a start
all script that internally triggers
start DFS and start yarn and these
scripts start DFS internally would run
Hadoop demon multiple times based on
your configs to start your different
processes then your start yarn would run
yarn demand script to start your
processing related processes so this is
how it happens in Apache Hadoop now in
case of cloud era or hortonworks which
is basically a vendor specific
distribution you would have say multiple
Services which would have one or
multiple demons running across the
machines let's take an example here that
you would have machine 1 Machine 2 and
machine 3 with your processes spread
across however in case of cloud era and
hot remarks these are cluster Management
Solutions so you would never be involved
in running a script individually to
start and stop your processes in fact in
case of Cloudera you would have a
Cloudera SCM server running on one of
the machines and then Cloudera SCM
agents running on every machine if you
talk about hortonworks you would have
ambari server and ambari agent running
so your agents which are running on
every machine are responsible to monitor
the processes send also their heartbeat
to the master that is your server and
your server is the one or a service
which basically will give instructions
to the agents so in case of vendor
specific distribution your start and
stop of processes is automatically taken
care by these underlying services and
these Services internally are still
running these commands however only in
Apache Hadoop you have to manually
follow these to start and stop coming
back we can look into some command
related questions so which command will
help you find the status of blocks and
file system health so you can always go
for a file system check command now that
can show you the files for a particular
sdfs path it can show you the blocks and
it can also give you information on
status such as under replicated blocks
over replicated blocks misreplicated
blocks default replication and so on so
your fsck file system check utility does
not repair if there is any problem with
the blocks but it can give you
information of blocks related to the
files on which machines they have stored
if they are replicated as per the
replication factor or if there is any
problem with any particular replica now
what will happen if you store too many
small files in a cluster and this
relates to the block information which I
gave some time back so remember Hadoop
is coded in Java so here every directory
every file and file Creator block is
considered as an object and for every
object within your Hadoop cluster name
nodes Ram gets utilized so more number
of blocks you have more would be usage
of name nodes RAM and if you're storing
too many small files it would not affect
your disk it would directly affect your
name node slam that's why in production
clusters admin guys or infrastructure
specialist will take care that everyone
who is writing data to hdfs follows a
quota system so that you could be
controlled in the amount of data you
write plus the count of data and
individual writes on hdfs now how do you
copy data from local system onto sdfs so
you can use a put command or a copy from
local and then given your local path
which is your source and then your
destination which is your sdfs path
remember you can always do a copy from
local using a minus F option that's a
flag option and that also helps you in
writing the same file or a new file to
hdfs so with your minus F you have a
chance of overwriting or rewriting the
data which is existing on sdfs so copy
from local or minus put both of them do
the same thing and you can also pass an
argument when you are copying to control
your replication or other aspects of
your file now when do you use DFS admin
refresh nodes or RM admin refresh notes
so as the command says this is basically
to do with refreshing the node
information so your refresh notes is May
mainly used when say a commissioning or
decommissioning of notes is done so when
in node is added into the cluster or
when a node is removed from the cluster
you are actually informing Hadoop master
that this particular node would not be
used for storage and would not be used
for processing now in that case you
would be once you are done with the
process of commissioning or
decommissioning you would be giving
these commands that is refresh nodes and
RM admin refresh nodes so internally
when you talk about commissioning
decommissioning there are include and
exclude files which are updated and
these include and exclude files will
have entry of machines which are being
added to the cluster or machines which
are being removed from the cluster and
while this is being done the cluster is
still running so you do not have to
restart your master process however you
can just use this refresh commands to
take care of your commenting
decommissioning activities now is there
any way to change replication of files
on sdfs after they are already written
and the answer is of course yes so so if
you would want to set a replication
Factor at a cluster level and if you
have admin access then you could edit
your sdfs hyphen site file or you could
say Hadoop hyphen site file and that
would take care of replication Factor
being set at a cluster level however if
you would want to change the replication
after the data has been written you
could always use a set rep command so
set rep command is basically to change
the replication after the data is
written you could also write the data
with a different replication and for
that you could use a minus D DFS dot
replication and give your application
Factor when you are writing data to the
cluster
cluster so in Hadoop you can let your
data be replicated as per the property
set in the config file you could write
the data with a different replication
you could change the replication after
the data is written so all these options
are available now who takes care of
replication consistency in a Hadoop
cluster and what do you mean by under
over replicated blocks now as I
mentioned your fsck command can give you
information of over or under replicated
blocks now in a cluster it is always and
always name node which takes care of
replication consistency so for example
if you have set up a replication of
three and since we know the first rule
of replication which basically means
that you cannot have two replicas hiding
on the same node it would mean that if
your replication is 3 we would need at
least
data nodes available now say for example
you had a cluster with three nodes and
replication was set to three at one
point of time one of your name node
crashed and if that happens your blocks
would be under replicated that means
there was an application Factor set but
now your blocks are not replicated or
there are not enough replicas as per the
replication Factor set this is not a
problem your master process or name node
will wait for some time before it will
start the replication of data again so
if a data road is not responding or if a
disk has crashed and if name node does
not get information of a replica name
node will wait for some time and then it
will start re-replication of those
missing blocks from the available nodes
however while name node is doing it the
blocks are in under replicated situation
now when you talk about over replicated
this is a situation where name node
realizes that there are extra copies of
block now this might be the case that
you had three nodes running with the
replication of three one of the node
went down
down it for failure or some other issue
within few minutes name node
re-replicated the data and then the
failed node is back with its set of
blocks again name node is smart enough
to understand that this is a over
replication situation and it will delete
set of blocks from one of the nodes it
might be the node which has been
recently added it might be your old node
which has joined your cluster again or
any node that depends on the load on a
particular node now we discussed about
Hadoop we discussed about sdfs now we
will discuss about map reduce which is
the programming model and you can say
processing framework what is distributed
cache in mapreduce now we know that when
we talk about mapreduce the data which
has to be processed might be existing on
multiple node so when you would have
your mapreduce program running it would
basically read the data from the
underlying disks now this could be a
costly operation if every time the data
has to be read from disk so distributed
cache is a mechanism wherein data set or
data which is coming from the disk can
be cached and available for all worker
nodes now how will this benefit so when
a map reduce is running instead of every
time reading the data from disk it would
pick up the data from distributed cache
and this this will benefit your
mapreduce
so distributed Cache can be set in your
job conf where you can specify that a
file should be picked up from
distributed cache now let's understand
about these roles so what is a record
reader what is a combiner what is a
partitioner and what kind of roles do
they play in a map reduce processing
Paradigm or map reduce operation so
record reader communicates with the
input split and it basically converts
the data into key value Pairs and these
key value pairs are the ones which will
be worked upon by the mapper your
combiner is an optional face it's like
mini radius so combiner does not have
its own class
it relies on the reducer class basically
your combiner would receive the data
from your map tasks which would have
completed works on it based on whatever
reducer class mentions and then passes
its output to the reducer phase
partitioner is basically a phase which
decides how many reduce tasks would be
used aggregate or summarize your data so
partitioner is a phase which would
decide based on the number of keys based
on the number of map tasks your
partition would decide if one or
multiple reduced tasks
care of
process so either it could be
partitioner which decides on how many
reduce tasks would run or it could be
based on the properties which we have
set within the cluster which will take
care of the number of reduced tasks
which would be used always remember your
partitioner decides how outputs from
combiner are sent to reducer and to how
many reducers it controls the
partitioning of keys of your
intermediate map outputs so map phase
whatever output it generates is an
intermediate output and that has to be
taken by your partitioner or by a
combiner and then partitioner to be sent
to one or multiple reduce tasks this is
one of the common questions which you
might face why is mapreduce slower in
processing so we know mapreduce goes for
parallel processing we know we can have
multiple map tasks running on multiple
nodes at the same time we also know that
multiple reduce tasks could be running
now why does then mapreduce become a
slower approach first of all your map
reduce is a batch oriented operation now
mapreduce is very rigid and it strictly
uses mapping and reducing phases so no
matter what kind of processing you would
want to do you would have to still
provide the mapper function and the
reducer function to work on data not
only this whenever your map phase
completes the output of your map face
which is an intermittent output would be
written to X
and thereafter underlying disks and this
data would then be shuffled and sorted
and picked up for reducing phase so
every time your data being written to
hdfs and retrieved from sdfs makes
mapreduce a slower approach the question
is for a map release job is it possible
to change the number of mappers to be
created Now by default you cannot change
the number of map tasks because number
of map tasks depends on the input splits
however there are different ways in
which you can either set a property to
have more number of map tasks which can
be used or you can customize your code
or make it use a different format which
can then control the number of map
by default number of map tasks are equal
to the number of splits file you are
processing
so if you have a 1GB of file that is
split into eight blocks or 128 MB there
would be eight map tasks running on the
cluster these map tasks are basically
running your mapper function if you have
a hard coded properties in your mapred
hyphen site file to specify more number
of map tasks then you could control the
number of map tasks
let's also talk about some data types so
when you prepare for Hadoop when you
want to get into Big Data field you
should start learning about different
data form
now there are different data formats
such as Avro Park
you have a sequence file or binary
format and these are different formats
which are used now when you talk about
your data types in Hadoop the SE are
implementation of your writable and
writable comparable interfaces so for
every data type in Java you have a
equivalent in Hadoop so end in Java
would be in writable in Hadoop float
would be float writable long would be
long writeable double writeable Boolean
writable array writable map writable and
these are your different data types
or mapreduce program and these are
implementation of writable and writable
comparable interfaces what is
speculative execution
now imagine you have a cluster which has
huge number of nodes and your data is
spread across multiple slave machines or
multiple nodes now at a point of time
due to a disk degrade on network issues
or machine heating up or more load being
on a particular node there can be a
situation where your data node will
execute task in a slower manner now in
this case if speculative execution is
turned on there would be a shadow task
or a another
similar task running on some other node
for the same processing so whichever
task finishes first will be accepted and
the other task would be killed so
speculative execution current
code if you are working in a intensive
workload kind of environment where if a
particular node is slower you could
benefit from a unoccupied or a node
which has less load to take care of your
processing going further this is how we
can understand so node a which might be
having a slower task you would have a
scheduler which is maintaining or having
knowledge of what are the resources
available so if speculative execution
has a property is turned on then the
task which was running slow a copy of
that task or you can say a shadow task
would run on some other node and
whichever task completes first will be
considered this is what happens in your
speculative execution now how is
identity mapper different from chain
mapper now this is where we are getting
deeper into mapreduce Concepts so when
you talk about mapper identity mapper is
the default mapper which is chosen when
no mapper is specified in mapreduce
driver class so for every mapreduce
program you
have a math class which is taking care
of your mapping phase which basically
has a mapper function and which would
run one or multiple map tasks right your
programming your program would also have
a reduce class which would be running a
reducer function which takes care of
reduced tasks running on multiple nodes
now if a mapper is not specified within
your driver class so driver class is
something which has all information
about your flow what's your map class
what is your reduce class what's your
input format what's your output format
what are the job configurations and so
on so identity mapper is the default
mapper which is chosen when no mapper
class is mentioned in your driver class
it basically implements an identity
function which directly writes all its
key pairs into output and it was defined
in old map reduce API in this particular
package but when you talk about chaining
mappers or chain mapper this is
basically a class to run multiple
mappers in a single map task or
basically you could say multiple map
tasks would run as a part of your
processing the output of first mapper
would become as an input to Second
mapper and so on and this can be defined
in the under mentioned class or
what are the major configuration
parameters required in the mapreduce
program obviously we need to have the
input location we need to have the
output location so input location is
where the files will be picked up from
and this would preferably on sdfs
directory output location is the path
where your job output would be written
by your mapreduce program you also need
to specify input and output formats if
you don't specify the defaults are
considered then we need to also have the
classes which have your map and reduce
functions and if you intend to run the
code on a cluster you need to package
your class in a jar file export it to
your cluster and then this jar file
would have your mapper reducer and
Driver classes so these are important
configuration parameters which you need
to consider for a map reduce program now
what is the difference or what do you
mean by map side join and reduce side
join map side join is basically when the
join is performed at the mapping level
or at the mapping phase or is performed
by the mapper so each input data which
is being worked upon has to be divided
into same number of partitions
input to each map is in the form of a
structured partition and is in sorted
order so Maps I join you can understand
it in a simpler way that if you compare
it with rdbms Concepts where you had two
tables which were being joined it will
always be advisable to give your bigger
table as the left side table or the
first table for your join condition and
it would be your smaller table on the
left side and your bigger table on the
right side which basically means the
smaller table could be loaded in memory
and could be used for joining so map
side join is a similar kind of mechanism
where input data is divided into same
number of parties
when you talk about reduced side join
here the join is performed by the
reducer so it is easier to implement
than map side join as all the sorting
and shuffling will send the values or
send all the values having identical
keys to the same reducer so you don't
need to have your data set in a
structured form so look into your map
site drawing or reduce side join and
other joints just to understand how
mapreduce Works however I would suggest
not to focus more on this because
mapreduce is still being used for
processing but the amount of mapreduce
based processing has decreased overall
or across the industry now what is the
role of output committer class in a
mapreduce job so output committer as the
name says describes the commit of task
output for a mapreduce job so we would
have this as mentioned or Apache Hadoop
map reduce output committer you could
have a class which extends output
committer class
so mapreduce relies on this map reduce
relies on the output committer of the
job to set up the job initialization
cleaning up the job after the job
completion that means all the resources
which were being used by a particular
job setting up the task temporary output
checking whether a task needs a commit
committing the task output and
discarding the task so this is a very
important class and can be used within
your mapreduce job what is the process
of spilling in mapreduce what does that
mean so spilling is basically a process
of copying the data from memory buffer
to disk when obviously the buffer usage
reaches a certain threshold so if there
is not enough memory in your buffer in
your memory then the content which
offer or memory has to be flushed out so
by default a background thread starts
spilling the content from memory to disk
after 80 percent of buffer size is
filled now when is the buffer being used
so when your mapreduce processing is
happening the data from data is being
read from the disk loaded into the
buffer and then some processing happens
same thing also happens when you are
writing data to the cluster so you can
imagine for a hundred megabyte size
buffer the spilling will start after the
content of buffer reaches 80 megabytes
this is customizable how can you set the
mappers and reducers for a mapreduce job
so these are the properties
so number of mappers and reducers as I
mentioned earlier can be customized so
by default your number of map tasks
depends on the split and number of
reduced tasks depends on the
partitioning phase which decides number
of reduced tasks which would be used
depending on the keys however we can set
these properties either in the config
files or provide them on the command
line or also make them part of our code
and this can control the number of map
tasks or reduce tasks which would be run
for a particular job let's look at one
more interesting question what happens
when a node running a map task fails
before sending the output to the reducer
so there was a node which was running a
map task and we know that there could be
one or multiple map tasks running
running on multiple nodes and all the
map tasks have to be completed before
the further stages that such as combiner
or reducer come into existence so in a
case if in node crashes where a map task
was assigned to it the whole task will
have to be run again on some other note
so in Hadoop version 2 yarn framework
has a temporary demon called application
master so your application Master is
taking care of execution of your
application and if a particular task on
a particular node failed due to
unavailability of node it is the role of
application Master to have this task
scheduled on some other node now can we
write the output of mapreduce in
different formats of course we can so
Hadoop supports various input and output
formats so you can write the output of
mapreduce in different formats so you
could have the default format that is
text output format wherein records are
written as line of text you could have
sequence file which is basically to
write sequence files or your binary
format files where your output files
need to be fed into another mapreduce
jobs
go for a map file output format to write
output as map files you could go for a
sequence file as a binary output format
so that's again a variant of your
sequence file input format it basically
writes keys and values to a c
S6 so when we talk about binary format
we are talking about a non-human
readable format DB output format now
this is basically used when you would
want to write data to say relational
databases or say no SQL databases such
as hbase so this format also sends the
reduce output to a to a SQL table now
let's learn a little bit about yarn yarn
which stands for yet another resource
negotiator it's the processing framework
so what benefits did yarn bring in
Hadoop version 2 and how did it solve
the issues of mapreduce version one so
map reduce version 1 had major issues
when it comes to scalability or
availability because sorry in Hadoop
version 1 you had only one master
process for processing layer and that is
your job tracker so your job tracker was
listening to all the task trackers which
were running on multiple machines so
your job tracker was responsible for
resource tracking and job scheduling in
yarn you still have a processing Master
but that's called resource manager
instead of job tracker and now with
Hadoop version 2 you could even have
resource manager running in high
availability mode you have node managers
which would be running on multiple
machines and then you have a temporary
demon called application master
so in case of Hadoop version 2 your
resource manager or Master is only
handling the client connections and
taking care of tracking the resources
the jobs scheduling or basically taking
care of execution across
across nodes is controlled by
application Master till the application
completes
so in yarn you can have different kind
of resource allocations that could be
done and there is a concept of container
so container is basically a combination
of RAM and CPU cores yarn can run
different kind of workloads so it is not
just map reduce kind of workload which
can be run on Hadoop version 2 but you
would have graph processing massive
parallel processing you could have a
real-time processing and huge processing
applications could run on a cluster
based on yarn so when we talk about
scalability in case of your Hadoop
version 2 you can have a cluster size of
more than 10 000 nodes and can run more
than 100
000 concurrent tasks and this is because
for every application which is launched
you have this temporary demon called
application
so if I would have 10 applications
running I would have 10 app Masters
running taking care of execution of
these applications across multiple nodes
compatibility so Hadoop version 2 is
fully compatible with whatever was
developed as per Hadoop version 1 and
all your processing needs would be taken
care by yarn
so Dynamic allocation of cluster
resources taking care of different
workloads allocating resources across
multiple machines and using them for
execution all that is taken care by yarn
multi-tenancy which basically means you
could have multiple users or multiple
teams
you could have open source and
proprietary data access engines and all
of these could be basically hosted using
the same cluster now how does yarn
allocate resources to an application
with help of its architecture so
basically you have a client or an
application or an API which talks to
resource manager resource manager is as
I mentioned managing the resource
allocation in the Clusters when you talk
about resource manager you have its
internal two components one is your
scheduler and one is your applications
manager so when we say resource manager
being the master is tracking the
resources The Source manager is the one
which is negotiating the resources with
slave it is not actually resource
manager who is doing it but these
internal components so you have a
scheduler which allocates resources to
various running applications so
scheduler is not bothered about tracking
your resources or basically tracking
your applications so we can have
different kind of schedulers such as
fifo
first out you could have a fair
scheduler or you could have a capacity
scheduler and these schedulers basically
control how resources are allocated to
multiple applications when they are
running in parallel so there is a queue
mechanism so scheduler will schedule
resources based on requirements of
application but it is not monitoring or
tracking the status
your applications manager is the one
which is accepting the job submissions
it is monitoring and restarting the
application Masters so it's application
manager which is basic basically then
launching a application application
Master which is responsible for an
application so this is how it looks so
whenever a job submission happens we
already know that resource manager is
aware of the resources which are
available with every node manager so on
every node which has fixed amount of RAM
and CPU cores some portion of resources
that is your RAM and CPU cores are
allocated to node manager now resource
manager is already
of how much resources are available
across node so whenever a client request
comes in resource manager will make a
request to node manager it will
basically request node manager to hold
some resources for processing node
manager would basically approve or
disapprove this request
holding the sources and these resources
that is a combination of RAM and CPU
cores are nothing but containers we can
allocate containers of different sizes
within yarn hyphen site file so your
node manager based on a request from
resource manager guarantees the
container which would be available for
processing that's when your resource
manager starts a temporary demon called
application Master to take care of your
execution so your app Master which was
launched by resource manager or we can
say internally applications manager will
run in one of the containers because
application Master is also a piece of
code so it will run in one of the
containers and then other containers
will be utilized for execution this is
how yarn is basically taking care of
your allocation your application Master
is managing resource needs it is the one
which is interacting with scheduler and
if a particular node crashes it is the
responsibility of App Master to go back
to the master which is resource manager
and negotiate for more resources so your
app Master will never ever negotiate
Resources with node manager directly it
will always talk to resource manager and
the source manager is the one which
negotiates the resources container as I
said is a collection of resources like
your RAM CPU Network bandwidth and your
container is located based on the
availability of resources on a
particular node so which of the
following has occupied the place of a
job tracker of mapreduce so it is your
resource manager so resource manager is
the name of the master process in Adobe
version now if you would have to write
yarn commands to check the status of an
application so we could just say yarn
application minus status and then the
application ID and you could kill it
also from the command line remember your
yarn has a UI and you can even look at
your applications from the UI you can
even kill your applications from the UI
however knowing the command line
commands would be very useful can we
have more than one resource manager in a
yarn based cluster yes we can that is
what Hadoop version 2 allows us to have
so so you can have a high availability
yarn cluster where you have a active and
standby and the coordination is taking
care by your zookeeper at a particular
time there can only be one active
resource manager and if active resource
manager fails your standby resource
manager comes and becomes active however
zookeeper is playing a very important
role remember zookeeper is the one which
is coordinating the server State and it
is doing the election of active to
standby failover what are the different
schedulers available in yarn so you have
a fee for scheduler that is first in
first out and this is not a desirable
option because in this case a longer
running application might block all
other small running applications
capacity scheduler is basically a
scheduler where dedicated
have fixed amount of resources so you
can have multiple applications accessing
the cluster at the same time and they
would be using their own queues and the
resources allocated to them if you talk
about Fair scheduler you don't need to
have a fixed amount of sources you can
just have a percentage and you could
decide what kind of fairness is to be
followed which basically means that if
you were allocated 20 gigabytes of
memory however the cluster has 100
gigabytes and the other team was
assigned 80 gigabytes of memory then you
have 20 access to the cluster another
team has 80 percent however if the other
team does not come up or does not use
the cluster in a fair scheduler you can
go up to maximum of 100 percent of your
cluster to find out more information
about your schedulers you could either
look in Hadoop definitive guide or what
you could do is you could just go to
Google and you could type for example
yarn scheduler let's search for yarn
scheduler and then you can look in
Hadoop definitive guide and so this is
your Hadoop definitive guide and it
beautifully explains about your
different schedulers how do multiple
applications run and that could be in
your fifo kind of scheduling it could be
in capacity scheduler or it could be in
a fair scheduling so have a look at this
link it's a very good link you can also
search for yarn untangling and this is a
Blog of four or this is a series of four
blocks where it's beautifully explained
about your yarn how it works how the
resource allocation happens what is a
container and what runs within the
container so you can scroll down you can
be reading through this and you can then
also search for part two of it which
talks about allocation and so on so
coming back
we basically have these schedulers what
happens if a resource manager fails
while executing an application in a high
availability cluster so in a high
availability cluster we know that we
would have two resource managers one
being active one being standby and
zookeeper which is keeping a track of
the server States so if a RM fails in
case of high availability the standby
will be elected as active and then
basically your resource manager or the
standby would become the active one and
this one would instruct the application
Master to a bot in the beginning then
your resource manager recovers its
running state so there is something
called as RM State Store where all the
applications which are running their
status is stored so resource manager
recovers its running state by looking at
your state store by taking advantage of
container statuses and then continues to
take care of your process now in a
cluster of 10 data nodes each having 16
GB and 10 cores what would be total
processing capacity of the cluster take
a minute to think 10 data nodes 16 GB
Ram per node 10 cores so if you mention
the answer as 160 GB RAM and 100 cores
then you went wrong now think of a
cluster which has 10 data nodes each
having 16 GB RAM and 10 cores remember
on every node in a Hadoop cluster you
would have one or multiple processes
running those processes would need RAM
the machine itself which has a Linux
file system would have its own processes
so that would also be having some RAM
usage which basically means that that if
you talk about 10 data nodes you should
deduct at least 20 to 30 percent towards
the overheads towards the cloud database
Services towards the other processes
which are running and in that case I
could say that you could have 11 or 12
GB available on every machine for
processing and say six or seven cores
one multiply that by 10 and that's your
processing capacity remember the same
thing applies to the disk usage also so
if somebody asks you in a 10 data node
cluster where each machine has 20
terabytes of disks what is my total
storage capacity available for hdfs so
the answer would not be 200 you have to
consider the overheads and this is
basically which gives you your
processing capacity now let's look at
one more question so what happens if
requested memory or CPU cores Beyond or
goes beyond the size of container now as
I said you can have your configurations
which can say that in a particular data
node which has 100 GB Ram I could
allocate say 50 GB for the processing
like out of 100 cores I could say 50
cores for processing so if you have 100
GB RAM and 100 cores you could ideally
allocate 100 for processing but that's
not ideally possible so if you have 100
GB Ram you would go for 50 GB and if you
have 100 cores you would go for 50 cores
now within this RAM and CPU course you
you have the concept of containers right
so container is a combination of RAM and
CPU cores so you could have a minimum
size container and maximum size content
now at any point of time if your
application starts demanding more memory
or more CPU cores and this cannot fit
into a container location your
application will fail your application
will fail because you requested for a
memory or a combination of memory and
CPU cores which is more than the maximum
container size so look into this yarn
tangling website which I mentioned and
look for the second blog in those series
which with that we have come to an end
of this session if you have any queries
regarding the topic or if you require
the sources used in the sessions like
the PPT code Etc please feel free to
write Us in the comment section below
what are your thoughts on big data and
its future let us know in the comments
below I hope you understand the Big Data
please share it with your friends and
family thank you for watching and keep
learning
[Music]
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here