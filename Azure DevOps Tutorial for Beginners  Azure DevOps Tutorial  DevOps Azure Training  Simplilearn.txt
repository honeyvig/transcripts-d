imagine you're the owner of a small
software development firm and you want
to scale your business up
however a small team size the
unpredictability of demand and limited
resources are roadblocks for this
expansion
that's when you hear about cloud
computing but before investing money
into it you decide to draw up the
differences between on-premise and
cloud-based computing to make a better
decision
when it comes to scalability you pay
more for an on-premise setup and get
lesser options too
once you've scaled up it is difficult to
scale down
and often leads to heavy losses in terms
of infrastructure and maintenance costs
cloud computing on the other hand allows
you to pay only for how much you use
with much easier and faster provisions
for scaling up or down
next let's talk about server storage
on-premise systems need a lot of space
for their servers notwithstanding the
power and maintenance hassles that come
with them
on the other hand cloud computing
solutions are offered by cloud service
providers who manage and maintain the
servers saving you both money and space
then we have data security on-premise
systems offer less data security thanks
to a complicated combination of physical
and traditional it security measures
whereas cloud computing systems offer
much better security and lets you avoid
having to constantly monitor and manage
security protocols
in the event that a data loss does occur
the chance for data recovery with
on-premise setups are very small
in contrast cloud computing systems have
robust disaster recovery measures in
place to ensure faster and easier data
recovery
finally we have maintenance
on-premises systems also require
additional teams for hardware and
software maintenance loading up the
costs by a considerable degree
cloud computing systems on the other
hand are maintained by the cloud service
providers reducing your costs and
resource allocation substantially
so now thinking that cloud computing is
a better option you decide to take a
closer look at what exactly cloud
computing is
cloud computing refers to the delivery
of on-demand computing services over the
internet on a pay-as-you-go basis in
simpler words rather than managing files
and services on a local storage device
you'll be doing the same over the
internet in a cost efficient manner
cloud computing has two types of models
deployment model and service model
there are three types of deployment
models public private and hybrid cloud
imagine you're traveling to work you've
got three options to choose from
one you have buses which represent
public clouds
in this case the cloud infrastructure is
available to the public over the
internet
these are owned by cloud service
providers
two then you have the option of using
your own car this represents the private
cloud with the private cloud the cloud
infrastructure is exclusively operated
by a single organization this can be
managed by the organization or a third
party and finally you have the option to
hail a cab this represents the hybrid
cloud
a hybrid cloud is a combination of the
functionalities of both public and
private clouds
next let's have a look at the service
models
there are three major service models
available es pass and sas
compared to on-premise models where
you'll need to manage and maintain every
component including applications data
virtualization and middleware cloud
computing service models are hassle-free
is refers to infrastructure as a service
it is a cloud service model where users
get access to basic computing
infrastructure
they are commonly used by it
administrators
if your organization requires resources
like storage or virtual machines is is
the model for you you'll only have to
manage the data runtime middleware
applications and the os while the rest
is handled by the cloud providers
next we have pass
pass or platform as a service provides
cloud platforms and runtime environments
for developing testing and managing
applications
this service model enables users to
deploy applications without the need to
acquire manage and maintain the related
architecture
if your organization is in need of a
platform for creating software
applications pass is the model for you
pass only requires you to handle the
applications and the data the rest of
the components like runtime middleware
operating systems servers storage and
others are handled by the cloud service
providers
and finally we have sas
sas or software as a service involves
cloud services for hosting and managing
your software applications
software and hardware requirements are
satisfied by the vendors so you don't
have to manage any of those aspects of
the solution
if you'd rather not worry about the
hassles of owning any it equipment the
sas model would be the one to go with
with sas the cloud service provider
handles all components of the solution
required by the organization
time for a quiz now in which of the
following deployment models are you as
the business responsible for the
application data and operating system
one is
two
pass
three
sas
four is and pass
let us know your answer in the comments
section below
everybody likes stories it so let's get
started with the story in a city not so
far away as ceo had plans to expand his
company globally and called one of his
id personnel for an i.t opinion and this
guy has been in the company for a long
time and is very seasoned with the
company's infra and he nicely answered
the questions with what he foresaw and
he said i have a good news and a bad
news for us to go global and he starts
with the good news he said sir we're
well on our way to become one of the
world's largest shipping company and the
bad news is however our data centers
have almost run out of space and setting
up new ones around the world would be
too expensive and very time consuming
now the it personal let's call him mike
now he explains the situation from how
he saw it but the ceo had done some
homework about how he was going to do it
and he answered mike saying don't worry
about that mike i've come up with a
solution for a problem and it's called
microsoft azure well mike is an hard
working and honest id professional
working for that company but he did not
spend time on learning the latest
technologies and he asked this question
very honestly oh how does it solve a
problem and the ceo begins to explain
azure to mike and he starts with what is
cloud computing and then he goes on and
talks about azure and the service is
offered by azure and why azure is better
than the other cloud providers and what
are the great companies that uses azure
and how they got benefited out of it and
then he winds it all up with the use
cases of azure so he begins his
explanation saying microsoft azure is
known as the cloud service provider and
it works on the basis of cloud computing
now microsoft azure is formerly known as
windows azure and it's a microsoft's
public cloud computing platform it also
provides a range of cloud services
including some of them are compute
analytics storage and networking we can
always pick and choose from these
services to develop and scale our
applications or even plan on running
existing applications in the public
cloud microsoft azure is both a platform
as a service and infrastructure as a
service let's now fit their conversation
out and let's talk about what is cloud
computing azure services offered by
azure how is azure
leading when compared to other cloud
service providers and what are the
companies that are using azure let's
talk about that in simple terms cloud
computing is being able to access
compute services like servers storage
database networking software analytics
intelligence and lot more over the
internet which is the cloud with the
flexibility of the resources that we use
like any time i wanted results i can use
one and it becomes available immediately
and anytime if i want to retire and
resource i can simply retire as a
service and not pay for it and we also
typically pay only for these services
that we use and this helps greatly with
our operating costs to run our
infrastructure more efficiently and
scale our environment up or down
depending on the business needs and
changes and all the servers and storages
and databases and networking all that
are accessed through the network of
remote systems or remote computers
hosted in the internet typically in the
provider's data center which is azure in
this case now we don't use any physical
server or an on-premises server here
well we still use physical servers and
vms you know hosted on a hardware or a
physical server but they're all in the
provider's environment and none of them
sit on premises or in our data center we
only access them remotely it looks and
feels the same except for the fact that
they are in a remote location we access
them remotely do all the work remotely
and when we're done we can shut it down
and not pay for them so some of the use
cases
some of the use cases of cloud computing
are creating applications and services
the other use cases are storing or using
cloud for storage alone
if there is one thing that ever grows in
their organization is the storage every
new day there is a new storage
requirement and it's very dynamic it's
very hard to predict and if we go out
and buy a big storage capacity up front
until we use the storage capacity fully
the empty storages you know we're
wasting money on them so instead i can
go for a storage with scales dynamically
that's in the cloud put storage or put
data in the cloud and pay only for what
you're storing and for the next month if
you have deleted or flushed out some
files or data pay less for it so it's a
very dynamic storage in the cloud and a
lot of companies are getting benefited
from storing data in the cloud because
of its uh dynamic in nature and the cost
that comes along with it the cheap cost
that comes along with it and also they
give a lot of the providers like azure
they give a data replication for free
they promise an sla along with the data
we store in the cloud so there's an sla
attached to it and they also provide
data recoveries as well if in case
something goes wrong with the
physical disk where our data is stored
azure automatically makes our data
available from the redundant or other
places where it had stored our data
because of the sla they wanted to keep
the other use case for azure is hosting
websites and running blogs using the
compute service
be it storing music and letting your
users stream the music azure is a good
place to store music and stream the
music with the benefit of
cdn content delivery network which
allows us to stream a video or audio
files with great speed you know with
that with azure our audio or video
application works seamlessly because
they are provided to the client with
very low latency and that improves the
customer experience for our application
azure compute service is a good place
for delivering software on demand there
are a lot of software's embedded
softwares that we can buy using azure
and everything on a pay-as-you-go
service model so anytime we need a
software we can go out and immediately
buy the software for the next one or two
hour let's say and use them and then
return it back
we're not bound to any yearly licensing
cost by that azure computing services
has analytic available for us with which
we can analyze get a good visualization
of what's going on in a network be it
logs be the performance be the metrics
you know instead of looking at logs and
searching logs and trying to do manual
things over the heaps and heaps of logs
that we have saved azure analytics
services helps us to get a good visual
of what's going on in the network where
have we dropped where have we increased
or what's causing what's the major
driver what is the top 10 errors that we
get in the server in the application
stuff like that those can be easily
gathered from the azure analytic
services
now cloud is really a very cool term for
the internet a good analogy would be
looking back anytime we look at a
diagram when we do not know how things
are transferred we simply draw a cloud
right for example a mail gets sent from
a person in one country to a person in
the other country a lot of things
happening in between from the time you
hit the send button and the time the
other person hits the real button right
and we the the simple and the easiest
way of putting it in a picture is simply
draw a cloud and on the one end one
person will be sending the email and on
the other end the other person will be
reading the email so a cloud is a really
cool term for the internet now that's
some basics about cloud computing now
that we've understood about cloud
computing in general let's talk about
microsoft azure as a cloud service
now microsoft azure is a set of cloud
services to build
manage and deploy applications on a
network with the help of microsoft
azure's frameworks now microsoft azure
is a computing service created by
microsoft basically for building testing
deploying and managing applications and
services through a global network of
microsoft managed data centers
now microsoft azure provides sas which
is software as a service and pass which
is platform as a service and ias
infrastructure as a service and they
support many different programming
languages tools and framework and those
tools and framework include both
microsoft specific and third-party
software
now let me pick and talk about a
specific service for example management
azure automation provides a way for us
to automate the manual long running and
frequently repeated tasks that are
commonly performed tasks both in cloud
and enterprise environment it saves us a
lot of time and increases the
reliability and it kind of gives a good
administrative control and even
schedules the task automatically to be
performed
on a regular basis
to give you a quick history of microsoft
azure it was launched on first february
2010
and it was awarded or it was called an
industry leader for infrastructure and
platform as a service by a gartner now
gartner is the world's leading research
and advisory company this microsoft
azure supports a number of programming
languages like c sharp java and python
all these cool services we get to use
and pay only for how much we use for
example if we use for an r we only get
to pay for an r even the costliest
system available if you use them for an
r we only pay for that particular r and
then we're done no more billing on the
resource that we have used
a microsoft azure has spread itself more
than 50 regions around the world so it's
quite easy for us to pick a region and
start provisioning and running our
applications probably from day one
because the infrastructure and the tools
and technologies needed to run our
application are already available all
that we have to do is commit the code in
that particular region or build an
application and launch it in that
particular region and they become live
starting day one now because we have 50
regions around the world we can very
carefully design our environment to
provide low latency services to our
customers all right instead of in
traditional data center let's say you
know customers will have to or their
request will have to travel all the way
around the globe to reach a data center
which lives in the other side of the
planet and this adds more latency to it
and it is really not feasible to build a
data center
near
each customer location because of the
cost involved but with azure it's
possible azure already has data centers
around the world and all that we have to
do is just pick a data center build an
environment there they are available
starting day one number one and also the
cost is considerably saved because we
are using a public cloud instead of an
physical infrastructure to serve those
customers from a very local location and
the services that azure is offering is
ever increasing as of now as we speak we
have like 200 plus services offered and
they span through different domain or
different platform or different
technologies available within the azure
console portal now we're going to talk
about that later in this section so hold
your breath till we talk about it but
for now just know that we have like 200
plus services offered by azure
let's now talk about different services
and azure starting with artificial
intelligence plus machine learning where
we have a lot of tools and technologies
so the wide variety of services
available on azure includes artificial
intelligence plus machine learning plus
analytic services to get and or to give
us a good visual of how the data or how
the application is performing or the
type of the category of data stored and
to read from the logs and variety of
compute services different vms with
different size and different operating
systems different containers available
different type of databases available a
lot of developer tools that are
available for us
an identity service to
manage our users in the azure cloud and
those users can be integrated or
federated with let's say google
facebook you know linkedin so there are
some external federation services they
can be used to integrate with our
identity system iots iot services iot
tools and technologies available and
management tools to manage the users
creating identity is one and then
managing them on top of it is a totally
different thing and we have tools
technologies to manage the
users
cool services for data migration data
migration is now made simple tools and
technologies available for mobile
application development and i can plan
my own network in the cloud with the
networking services i can implement my
own security both azure provider and
third-party security services on azure
cloud that's now possible and a lot of
storage options available in the cloud
so these are just a glimpse of the big
list of services available in azure
cloud so that was a glimpse of what's
available in the cloud let's talk about
the services in a specific let's take
compute for example you know whenever
we're building a new application or
deploying existing ones the azure
compute service provides the
infrastructure we need to run and
maintain our application we can easily
tap in the capacity that azure cloud
service has and we can scale our compute
requirement on demand we can also
containerize our application we have the
option of choosing windows or linux via
machine and take the advantage of the
flexible options azure provides for us
to migrate our vms to azure and lot more
and these compute services also include
a full-fledged identity solution meaning
integration with active directory in the
cloud or in on-premises and lot more
let's look at some of the services that
this compute domain provides
some of the services the compute domain
provides are virtual machines and this
azure virtual machines
gives us the ability to develop and
manage a virtual computer environment or
a virtualized environment inside azure's
cloud environment that do in a virtual
private network now we will talk about
virtual private network at a later point
but as of now just i know that there are
a lot of services available in azure
compute servers that we can get
benefited from we can always choose from
a very wide range of compute options for
example you know we have an option to
choose the operating system we have the
option to choose whether the system
should be in on-premises or in the cloud
or do we want to maintain the
environment both in on-premises and in
the cloud we have the option of choosing
the operating system whether we want to
use our own operating system with some
software attached to it or do we want to
go and buy the operating system from the
cloud from azure marketplace and these
are just a few of the options available
for us when we want to buy the compute
environment and these compute
environments are easily scalable meaning
we can easily scale our vm instances
from one instance to thousands of
virtual machines in a matter of minutes
or simply put in a couple of button
clicks and all these services are
available on a pay for what we use model
meaning there is no upfront cost we use
the service and then pay for the
services that we have used there's no
literal long-term commitment when it
comes to using virtual machines in the
cloud and these most of the services are
built on a paper minute billing basis
all right and at no point because of the
pay-per-minute billing model at no point
we will be overpaying for any of the
services that's that's attractive isn't
it now let's talk about batch service
now batch service is always independent
regardless of whether you choose windows
or linux it's going to run fairly well
and with batch servers we can take
advantage of the environments unique
features and not only that in short the
batch service helps us to manage the
whole batch environment and also it
helps to schedule the jobs now this
azure batch service is actually runs on
a large scale parallel and high
performance computing because of that
batch jobs are highly efficient in azure
and when we run bad services this azure
batch creates a pool of computer nodes
and installs the needed applications
that we want to run and then it
schedules jobs to those individual nodes
in those pools as a customer there is no
need for us to install a cluster or
there is no need for us to install a
software that actually could use the
jobs or even to manage or even to scale
those infrastructure are the software
because everything is managed by azure
and this batch service is a platform as
a service there is no additional charge
for using this batch service except for
i mean the only charges that we'll be
paying is for the virtual machines that
the service uses and the storage that we
will be using of course and the
networking services that we would be
using for this batch service let's
summarize this batch service we have a
choice of operating system that we can
pick and use and it scales by itself now
the alternative for the batch would be
qs but in queues we'll have to
pre-provision and pay for the
infrastructure even if we're not using
it but with a batch
we only pay for what we use and this bad
service helps us to manage the
application manage the scheduling as a
whole as if they are just one thing as
next thing in compute domain let's talk
about this fabric service now this
fabric service is actually a distributed
system platform
that helps us to package
deploy and manage a scalable and a very
reliable microservice and containers and
what does it help this azure fabric
service helps us or it helps the
developers and administrators so they
can avoid the complex infrastructure
problems and they can focus only on
implementing workloads or taking care of
their development taking care of their
application instead of spending time on
infrastructure so what service fabric
service fabric it provides runtime
capabilities and life cycle management
two applications that are composed of
micro services no infrastructure
management at all and with service
fabric we can easily scale the
application to tens or hundreds or even
to thousands of machines here machines
represent containers as next thing in
compute domain let's talk about virtual
machine scale set now this virtual
machine scale set it lets us to create a
group of identical load balanced vms i
just want to mention it again it helps
us to manage a group identical and load
balanced vms the number of instances or
the number of vm instances in an in a
scale set can increase or decrease
in response to
the demand or in response to a schedule
that we define you know the resources
needed on a monday morning is not the
same as that would be required on a
saturday or a sunday morning all right
and even within the day the resources
that would be needed in the beginning of
the business hour is not the resources
that would be needed
at noon or you know after eight or nine
in the evening so the demands could
actually vary in the environment and the
skill set helps us to take care of the
varying demand or
take care of the different
infrastructure requirement at a
different schedule throughout the day
throughout the week throughout the month
or could be throughout the year as well
the skill set also allows us to provide
high availability to our applications
and it helps us to centrally manage
configure and update a large number of
vms as if they they are just one thing
now you might ask well virtual machines
are enough why would we need a virtual
machine scale set just like i said this
virtual machine skill set helps us with
a greater redundancy and improved
performance for our applications and
those applications can be accessed
through a load balancer that actually
distributes other requests to the
application instances so in a nutshell
this virtual machine scale set it helps
us to create a large number of identical
virtual machines number one and with
skill set we can increase or decrease
the virtual machines with virtual
machine scale set we can centrally
manage and configure and update a big
group of vms and it's a great use case
when it comes to big data or container
workloads as next thing in compute
domain let's talk about cloud services
now this azure cloud service is actually
a platform as a service and it's very
friendly in fact it is designed for
applications that support scalability or
an application that requires scalability
or reliability and and on top of it you
want them to be very inexpensive to
operate so azure cloud service provides
all these so where would this cloud
service run well it runs on a vm but
it's a platform as a service vms are
infrastructure as a service and when we
run applications on vm through cloud
service it becomes platform as a service
so here is how you got to be thinking
with infrastructure as a service like
vms we first create and configure the
environment and then we run applications
on top of it let's look at the
responsibility the responsibility for us
in vm is that we manage everything end
to end like uh you know deploying new
patches
picking the versions of the operating
system and making sure they are intact
and all that stuff it's all managed by
us but on the contrary with platform as
a service it's
i mean it's as if the environment is
already ready all that you have to do is
deploy your application in it and manage
the platform i mean manage the platform
not as an administrator because all the
administration is taken care by azure
like uh deploying new versions of the
operating system it's all handled by the
azure so we deploy the application and
we manage the application that's it
infrastructure management is handled by
azure so what does cloud service provide
this cloud service provides a platform
uh where we can uh
write the uh application code and we
don't have to worry about hardware
simply hand over the code and cloud
service takes care of it so no worry on
the hardware at all so responsibilities
like patching what do we do if something
crashes how do i update the
infrastructure how do i manage
the maintenance or the downtime in the
underlying infrastructure all that is
handled by azure it also provides a good
testing environment for us you know we
can simply run the code test it before
it's actually released to the production
i want to expand a bit on these testing
applications so this azure cloud service
it actually gives us an staging
environment for testing a new release
without it affecting the existing
release which actually reduces the
customer downtime so we can run the
application test it and anytime that's
ready for production all that's needed
for us to do to move it to production is
simply to swap the staging environment
into the production environment and the
old production environment will now
become the new staging environment where
we can add more to it and then swap it
back at a later point so it kind of
gives us unswappable environment for
testing our applications and not only
that it gives us health monitoring
alerts it helps us to monitor the health
and availability of our application and
there is a dashboard we can benefit from
when we use azure cloud services and
that shows the key statistics all in one
place and we can also set up real-time
alerts to one when a service
availability or a certain metrics that
we are concerned about degrades as next
thing in compute domain let's talk about
functions now functions are serverless
computing now many time if you heard
about azure being serverless a lot of
time they are referencing or the person
who's talking to you is referencing to
serverless computing or azure functions
which is a serverless computing service
hosted on microsoft azure the main
motive of
a function is to accelerate and simplify
application development functions helps
us to run code on demand without
we need to pre-provision or manage any
azure infrastructure so azure functions
are script or a piece of code that gets
run in response to an event that you
want to handle so in short we can just
write a code that you need for a problem
at hand without actually worrying about
the whole application or the
infrastructure that will be running
that code and the best of all the best
is when we use functions we only pay for
the time that our code runs
so what does functions provide or what
does azure functions provide azure
functions allow users to build
applications using serverless
simple functions with the programming
language of our choice so the current
programming languages that are supported
is c sharp f sharp node.js java and php
so here we really don't have to worry
about provisioning or maintaining
servers if a code requires more resource
yes azure functions handles or it
provides the additional resources needed
by the court and the best part is we
only pay for the amount of time the
functions are running not the resources
but the amount of time the function is
running as next thing and moving to the
new domain let's talk about the
container domain in azure now the
container domain or the container
service it allows us to quickly deploy a
production ready kubernetes or a docker
swamp cluster now what's a container a
container is a standard unit of software
that packages of code and all its
dependencies so the applications run
quickly and reliably from one computing
environment to another it could be a
testing
to staging to developing development
environment to staging to production or
from one production to another
production are on premises to cloud or
one cloud to another cloud vice versa
now imagine we had an option not to
worry about the vm and just focus on the
application well that's exactly
what containers helps us achieve so
these container instances enable us to
focus on applications and not worrying
about managing vms or not worrying about
the learning the new tools required to
manage the vms or even the deployment
and our applications that we create they
run in a container running in a
container is what helps us to achieve
all these not being able to manage or
not needing to manage the virtual
machines so these containers
they can be deployed into the cloud
using a single command if you're using
command line interface and a couple of
button clicks if we are using the azure
portal and these containers are kept
lightweight but they are equally secure
as virtual machines let's talk about
container services next thing the
container service or
sometimes called as azure kubernetes
service it helps us to manage the
containers container is one thing and a
service that's used to manage the
container is another thing now this
kubernetes service or acs it helps us to
manage the containers so let's expand on
this a bit so this azure container
service or acs it it actually provides a
way to simplify the creation
configuration and management of a
cluster of virtual machines that are
pre-configured to run containerized
applications on top of them and
deploying them deploying these
containers might take like 15 to 20
minutes or deploying the virtual
machines that run containers in it might
take 15 to 20 minutes and once they are
provisioned we can actually manage them
by using simple ssh tunnel into them and
this acs when it runs application it
runs applications from docker images
what does that mean a docker images
makes sure that the applications the
container runs are fully portable images
are portable and acs also helps us to
orchestrate the container environment
not only that it also helps us to ensure
that these applications that we run in
containers can be scaled to thousands or
even tens of thousands of containers so
in a nutshell managing an existing
application into a container and running
it using aks or acs is really easy or
that's what it is all about to make the
application management or migration easy
now managing the container based
architecture and we discussed that
containers could be tens or even tens of
thousands of containers so managing them
is made simple using this container
services and even training of model
using a large data set in a complex and
resource intensive environment this aks
helps us to simplify that environment
all right as next thing in container
domain let's talk about container
registry we spoke about registry a
little bit when we spoke about docker
images so container registry is a single
place where we can store our images
which are docker images when we use when
we use containers it's it's docker
images that we use for our image
purposes so these container images are a
central registry that can be used to
ease container development by easing the
storage and management of container
images so there we can store all kind of
images like
docker swamp are the images used in
docker swamp are in kubernetes
everything can be stored in container
registry in azure now anytime we store a
container image it provides us an option
for a geo replication what that means is
that we can efficiently manage a single
registry replicated across multiple
regions
now this jio replication it actually
enables us to manage global deployments
assuming we are having an environment
that requires a global deployment so it
helps us to manage global deployments as
one entity because we are
geo-replicating we would be updating we
would be editing one image and that
image gets replicated throughout
the global
replication centers we would have set up
and so just one editing would have
actually edited the global images and
those global images would have
provisioned at the global application so
one edit replication and then
provisioning of the applications
global wide and this replication also
helps us to
helps us network latency because you
know anytime an application needs to
deploy it does not have to rely on a
single source which which can be reached
only through high latency network
because we have global replications
around the world anytime the application
wants to check back it would check back
the application which is in a very
nearby location for the application
itself global replication means that we
are managing it as a single entity
that's being replicated across the
multiple regions in the globe as next
thing in a learning let's talk about
azure databases now these azure
databases are
rational in fact they have many flavors
in them we're going to look at different
flavors no sql nosql cache type of
database that azure offers so we're
going to learn one at a time or we're
going to learn one by one so this azure
sql database is a relational database in
fact it's a regional database as a
service it's managed by azure we don't
get to do a lot of management in it so
it's a regional database as a service uh
based on microsoft
sql
server database engine and this database
is a high performance database it's very
reliable and
it's very secure as well and this high
reliability high performance and for
this high security really don't have to
do anything it comes along with it and
it's managed by azure and there are two
things that i definitely need to mention
about azure sql database that is it's an
intelligent service number one it's
fully managed by azure and it also has
this one good thing which is it has
built-in intelligence that learns app
patterns and adapts to maximize
performance and reliability and data
protection of the application that's
something that's not found in many of
the other cloud providers that i'm aware
of so i thought i'll mention it so it
uses built-in intelligence to learn
about the user's database patterns and
helps improve performance and protection
and migration or importing data is very
easy when it comes to azure sql database
so it can be readily or immediately used
for analytic reporting and
intelligent applications in azure as
next thing let's talk about azure
cosmodb now azure cosmodb is a database
service that is for nosql type and
it's it's created to provide low latency
and
an application that scales dynamically
or that scales rapidly
now this azure cosmodb is an
a globally distributed service
and it's a multi-model
database this can be provisioned in a
click of a button that's all we got to
do if we need to provision and azure
cosmodb in the azure it helps with
scaling the database now we can
elastically and independently scale
throughput and storage across this
database and in any of the azure
geographic regions it provides a good
throughput it provides good latency it
provides good availability and
it provides or azure promises a
comprehensive sla that
no other database can offer that's the
best part about cosmodb so this cosmodb
was built with a global distribution in
mind and it's built
with the horizontal scale in mind and
all this we can use by only paying for
what we have used and remember the
difference between azure cosmodb and the
sql database is that azure cosmodb
supports nosql whereas sql doesn't all
right few other things about azure
cosmodb is it allows users to use key
value graph a column family and document
data it also gives users a number of ap
options like sql javascript mongodb and
few others that you might want to check
the document at the time of reading and
the best part here is that all that we
mentioned we get to use only by paying
for the amount of storage and throughput
that are required and the storage and
the throughput can be elastically scaled
based on the requirement of that r all
right let's talk about
redis cache discussion about azure
database won't be complete without we
talking about redis cache now redis
cache is a secure data cache it's also
called it's also sometimes called as
messaging broker that provides high
throughput and low latency access to
data for the applications now that is
cache is based on an a popular open
source caching product which is redis
sometimes called as that is cache now
what's the use case it's typically used
to cache to improve the performance and
scalability of a system that rely
heavily on backend data stores now
performance when we use zs cache is
improved by temporarily copying the
frequently accessed data to a fast
storage located very close to the
application now with redis cache this
fast storage is located in memory with
redis cache instead of being loaded from
the actual disk in the database itself
now this redis cache can also be used as
an in-memory data structure store not
only that it can be used as an
distributed non-relational database and
a message broker so there are variety of
use cases for this redis cache and by
using redis cache the application
performance is improved by taking
advantage of the low latency and the
high throughput performance that this
redis cache engine provides so to
summarize this redis cache when we use
redis cache data is stored in the memory
instead of the disk to ensure that there
is high throughput and low latency when
the application needs to read the data
it provides various levels of scaling
without any downtime or interference now
this redis cache is actually backed by
reduce server and it supports
a string hashes linked list and various
other data structures now let's talk
about security and identity services now
identity management in specific is a
process of authenticating first and then
authorizing using security principles
and not only that identity management
involves controlling information about
those principle identities you might ask
now what's a principle identity now
identity or principle identity are
services applications users groups and a
lot more the speciality about this
identity management is that it not only
helps authenticate and authorize
principles in cloud it also helps
authenticate and authorize principles or
resources on premises especially when
you run an hybrid cloud environment so
all these services and features that
this identity management helps us to get
additional level of validation like
identity management can provide
multi-factor authentication it can
provide access policies based on
condition a permit or deny based on
condition it can also monitor suspicious
activity and not only that it can also
report it it can also help generate
alerts for potential security issues and
in a way to mitigate it can send us an
alert so we can get involved and prevent
and a security accident from happening
so let's talk more about identity
management so some of the services under
security and identity management are
azure security center now this azure
security center provides security
management and threat protection across
the workloads in both cloud and in the
hybrid environment it helps control user
access and application control to stop
any malicious activity if present it
helps us to find and fix vulnerabilities
before they can be even exploited it
integrates very well with analytic
methods that helps us to identify or it
gives us the intelligent to identify or
detect attacks and prevent them before
it can actually happen and it also works
seamlessly with hybrid environment so
you don't have to have one policy for
on-premises and one policy for the cloud
it's now a unified service both for
on-premises and the cloud the next
service in security and identity would
be key vault now a key vault is a
service or a feature that helps
safeguard the cryptographic keys and any
other secrets used by the cloud
applications and the services in other
words this azure key vault is a tool for
securely storing and accessing the
secrets of the environment i mean the
secret keys now a secret is anything
that you really want to have a very
tight control access like the
certificates like the passwords stuff
like that now if i tell you what keyword
actually solves that would actually
explain what keyword is now key vault is
used in secrets management it's helped
in securely storing the tokens the
passwords the certificates it helps in
key management you know it really helps
in creating and controlling the
encryption keys that we would use to
encrypt data it helps in certificate
management talking about certification
management it helps us to easily
provision manage and deploy public and
private ssl tls certificates in azure
and lot more so in a nutshell this
keyword it provides users the ability to
provision new walls and keys in just a
matter of minutes all that in a single
command or all that in a couple of
button clicks it also helps users to
centrally manage their keys secrets and
policies
next in the list let's talk about azure
active directory now azure active
directory it helps us to create
intelligent driven access policies to
limit resource usage and manage user
identities but what does that mean now
this azure active directory is a cloud
based active directory and identity
management service now azure active
directory combines and it's actually a
combination of the core directory
services plus application access
management plus identity protection
and one good thing about this azure in
fact there are a lot of good things but
especially when you're running hybrid
environments you might wonder well how
this azure active directory is going to
behave now this azure active directory
is built to work on on-premises and
cloud environment as well not only that
it also works seamlessly with mobile
applications as well
so in a nutshell this azure active
directory it acts as a central point of
identity and access management for our
cloud environment it also provides good
security solutions that protect against
unauthorized access of our app and the
data
now that we've discussed about security
and identity let's talk about the
management tools that azure has to offer
azure provides built-in management and
account governance tools that helps
administrators and
developers that helps them to keep the
resources secure and very compliant and
again it helps both in on premises and
in the cloud environment and these
management tools help us to monitor the
infrastructure monitor the applications
it also helps in provisioning and
configuring resources it also helps in
updating apps it helps in analyzing
threats taking backup of the resources
build
disaster recoveries it also helps in
applying policies and conditions to
automate our environment we use azure
management tools and it's also used in
cost control methods so this azure
management plays a wide role across the
azure services
and in the management tools first comes
the azure advisor now this azure advisor
it acts as a guide to educate us about
azure best practices it throws
recommendations that we can select on
the basis of the category of servers and
it also provides the impact it can have
or the impact that would happen in our
environment if we follow the
recommendations given and
recommendations are
first one is the recommendations are
kind of templatized and it throws the
templatized recommendations not only
that it also provides customized
recommendations on the basis of the
configuration on the basis of our usage
patents and these recommendations are
not hard it's not like something that it
recommends and then just leaves us
hanging there these recommendations
provided are very easy to follow very
easy to implement and see results you
can think of as your advisor as an a
very personalized cloud consultant that
helps you to follow best practices to
optimize our deployments it kind of
analyzes our resources our
configurations our usage and then it
recommends a solution for us that really
helps in improving the cost
effectiveness improving the performance
improving high availability and
improving security in our azure
environment so with this azure advisor
we can get a proactive actionable and
personalized best practice
recommendations now you don't have to be
an expert just follow the azure advisor
and your environment is going to be good
it also helps in improve the performance
security high availability of our
environment and also it helps in
bringing down the overall azure spend
and the best part is it's a free service
that analyzes our azure usage and
provides recommendations how we can
optimize our azure resource to reduce
cost and reduce cost at the same time
boost the performance helps in
strengthening the security and improve
the overall reliability of our
environment
next in the list would be network
watcher now this network watcher helps
users identify and gain insights in the
overall network performance and the
health of the overall environment now
these azure watchers provides enough
tools to monitor to diagnose to view the
metrics and to enable or disable logs
which means regenerate and collect the
logs for resources in the azure virtual
network so with network watcher you can
monitor and diagnose issues in
networking without even logging into the
virtual machines with just the logs
which are real time we can actually come
to a conclusion what could be wrong in a
certain resource in a vm or in a
database you know by just looking at the
logs and not only that it's used for
analytic or to gain some intelligence of
what's happening in our network we can
gain a lot of insight to the current
network traffic pattern using the
security group flow logs
that this network watcher offers it also
helps in investigating vpn connectivity
issues using detailed logs now you might
or might not know that you know vpn
troubleshooting requires both parties or
it involves two parties another person
the network administrator on this side
and the network administrator on the
other side and they will have to check
logs in their end and we'll have to
check logs and hour and stuff like that
but with the network watcher it kind of
takes it to the next level the logs
itself we could easily identify which
side is having the issue and suggest an
appropriate fix
and the next in the list would be
microsoft azure portal now this
microsoft azure portal it provides a
single unified console to perform
various number of activities like
building not only building managing and
monitoring the web applications that we
build now this portal can be used to
organize our environment or the
appearance of the environment or the
visual of the environment based on our
work style and using azure portal users
can control who gets to manage or access
the resources all from the azure portal
and this azure portal gives a very good
visibility on the spends that happen on
each resource right and if we can
customize it we can also identify spends
based on team spends based on days
spends based on department stuff like
that so it kind of gives us a good
visual of
where the money is spending or where is
the bill consumed within the azure
environment
next in the list would be azure resource
manager now azure resource manager
enables us to manage
the usage of the application resources
now we use resource manager to deploy
monitor and manage solution resources as
a group as if it's one single entity now
the infrastructure of our application is
typically made of various components
which includes virtual machine storage
virtual network
web app database servers some other
third-party services that we might use
in our environment and they are by
nature separate services but with azure
resource manager we don't see them as
different components or different
entities instead we see them as related
services in a group that supports an
application now we kind of get the
relation between them instead of you
know letting them spread azure resource
manager identifies the relation between
them and helps us to visually see them
all as one or single entity not only
that azure resource manager helps or it
ensures that the resources that we
provision are deployed at a constant
rate along with the other application it
also helps users to visually see their
resources and how they are connected and
that helps in managing the resources lot
better
resource group also is used to control
who can access the resources within the
users organization kind of gives you the
fine grained control over who gets to
access and who does not get access and
the last one in the management tools
would be automation and this automation
gives us the ability to automate
configure and install upgrades across
hybrid environments
it provides a cloud-based automation and
configuration service not only that this
can be applied for non-azure
environments as well which is
on-premises so some of the automation we
could do is process automation update
management automation configuration
features automation stuff like that and
this azure automation provides complete
control during the deployment operation
and also during the decommissioning of
the workloads and resources
with automation we can actually automate
a time consuming or
mundane or any task that's error prone
because of human errors those things can
be automated so irrespective of how many
times you run it it's going to run the
same way and that really helps in
reducing the overall time and also the
overhead cost because a lot of the
things are automated which means it's
human error free which means the
application is not going to break and
keep running for a longer time with
automation we can actually build a good
inventory of operating system resources
and configuration items all in one place
with ease and this really helps in
tracking the changes and investigating
the issue let's say something happened
because we have automation because it's
logging the configuration changes it's
easy to track easy to identify easy to
identify what has changed lately that
has broken the environment go back and
fix it or kind of roll it back that
solves the problem
and that actually summarizes the azure
management tools or management services
now let's talk about the networking
tools or the networking services
available in azure there are variety of
services especially networking services
that azure offers and i'm sure it's
going to be an interesting one let's
begin our discussion with content
delivery network now the content
delivery network in short cdn it allows
us to perform secure and a very reliable
content delivery not only that it also
helps in accelerating the delivery time
or in other words reducing the delivery
time also called as low times it also
helps in saving bandwidth and increases
in responsiveness to the application
let's expand on this the content
delivery network is actually a
distributed network of servers that can
efficiently deliver web content to users
now cdns we're going to use the word cdn
here cdns store cached content on global
edge servers also called as
pops point of presence locations that
are very close to the end users so the
latency is minimized it's like taking a
copy of the data or taking a multiple
copy of the data and storing it in
different parts of the world and whoever
is requesting it the data gets delivered
to them from a server which is very
locally to them so this cdn offers
developers a global solution for rapidly
delivering high bandwidth content to
users by caching the content in a
strategically placed location which is
very near to them so these content
delivery networks it really helps in
handling that's one advantage you get
for content delivery network that's we
can handle spikes and heavy loads very
efficiently and we can also run analytic
against the locks that gets generated in
content delivery network which helps in
gaining good insight on the workflow and
what would be the future business need
for that application and this just like
a lot of other services this is on a pay
as you go type so you use the results
first and then you only pay for what you
have used
the next one in networking would be
express route now express route is
actually a circuit or a link that
provides an a direct private connection
to azure and because it's direct it
gives low latency link to azure it gives
good speed and reliability for the azure
data transfer it could be on premises to
azure so it gives very good speed it
gives increased reliability and low
latency for that connection let's expand
on this a bit
now this express route is an service
that actually provides and private
connection between microsoft data center
and infrastructure in our premises or in
a different co-location facility that we
might have now these express routes do
not go over the public internet and
because they don't go over the public
internet they offer a high security
reliability and speed and low latency
compared to the connections which are in
the internet because it's fast because
it's reliable because it has low latency
it can be used as an extension of our
existing data center you know users are
not going to feel the difference whether
they are accessing services from an
on-premises or in the cloud environment
because latency is minimized as much as
possible users are really not going to
see the difference
and because it's a private line and not
an public internet line it can be used
to build hybrid applications without
compromising a privacy or the
performance now these virtual private
cloud these express routes can be used
for taking backups if assume a backup
going through the internet that would be
a nightmare if you use express route for
backups that's going to be fast and
imagine recovering a data through the
internet from the cloud through the
internet to the on premises in a time of
disaster that would be the worst
nightmare so these express routes can be
used not only to backup but also to
recover the data because it provides
good speed low latency recovering the
data is going to be a lot sooner the
next product or service we're going to
discuss in networking is azure dns
now azure dns allows us to host domain
name in azure and these domain names
come with an exceptional performance and
availability now azure dns is used to
set up and manage dns zones and records
for our domain name in the cloud now
this azure dns is a service for dns just
like the name says and it provides name
resolution by using azure's
infrastructure and by using this domain
we can actually manage the dns r cells
through the azure portal with the same
credential imagine having a dns provider
which does not even belong in rit
imagine that environment you know we
would have a separate portal to manage
the dns environment now those are gone
and now we can actually manage the dns
in the very same azure portal where we
use the rest of the other services and
this azure dns very much integrates with
other dns service providers it uses a
global network of name servers to
provide fast response to dns queries
and these domains are having additional
availability compared to the other
domain service providers availability
promises these are going to have more
availability than the rest because
most of the servers are maintained by a
microsoft and it helps resolve sooner it
helps re-syncing let's say a server
fails it kind of helps re-syncing with
the rest of the servers so all the
microsoft's environment all the
microsoft's global network of name
servers kind of ensures that our domain
names are resolved properly not only
properly but also are available most of
the time all right next in the list in
networking services is virtual network
i'm sure this is going to be very
interesting and i'm sure you're going to
like it so this networking or virtual
networking in azure it actually allows
us to set up our own private cloud in
the public cloud it gives us an isolated
and highly secure environment for our
application let's expand on this so this
azure virtual network helps us to
provision azure virtual machines and it
helps us to securely communicate with
other on-premises and internet networks
it also helps in controlling the traffic
that flows through or flows in and out
of this virtual network to other virtual
networks and to the internet now this
azure a virtual network sometimes called
as v-net is actually a representation of
our own network in the cloud it's
actually a logical isolation of the
azure cloud dedicated to our
subscription all our environments are
provisioned in a v-net that is separate
from another customer's units that way
we have that logical separation there so
this virtual network can also be used to
provision a vpns in the cloud so we can
connect the cloud and the on-premises
infrastructure and lot more especially
in a environment where we have hybrid
environment surely we will be using
virtual network because that's going to
require a vpn for secure data transfer
in and out of the cloud and in and out
of the on-premises environment
all right so it kind of gives us an
boundary for all the resources so all
the traffic between the azure resources
they kind of logically stay in between
or logically stay within the azure
virtual network and here we can design
the network it's given over to us you
know you can pick the ip you can pick
the routing you can pick the subnet and
a lot of freedom is given or i would say
a lot of control on how the network is
designed it's not like something that's
already cooked and we only get to use it
no we can actually build the network
from the scratch we can pick the ip
address that we like we can pick you
know which subnet needs to communicate
with the other subnet stuff like that
and like i said if you are using hybrid
environment you definitely would be
requiring a virtual network because it
helps connect the on-premises and the
cloud in a secure fashion using vpn the
last product we're going to discuss in
networking is a load balancer this load
balancer actually provides application a
good availability and a good network
performance so how does it work it
actually works by load balancing the
traffic to and from the virtual machine
and the cloud resources not only that it
also load balances between cloud and
across premises virtual networks with
azure load balancer we can actually
scale our application and create high
availability for our services which
means our application will be available
most of the time if any of the server
goes dead the server does not get
traffic what happens if the server gets
traffic user is going to experience
downtime what happens if the server does
not get traffic user won't experience
any downtime the connection is shifted
to an healthy server so the user
experiences uptime all the time so this
load balancer supports inbound and
outbound scenarios and it provides low
latency it gives high throughput of the
data transfer and we can actually scale
up the flow of the tcp and udp
connections from hundreds to thousands
to even millions because we have a load
balancer now in between the user and the
application so how does it operate this
load balancer actually receives the
traffic and it load balances the traffic
to the backend pool of instances
connected to it according to the rule
and the health probe that we set that's
how it maintains high availability so
what does load balancer help it helps in
creation of high available scalable
application in the cloud in minutes it
can be used to automatically scale the
environment with the increasing
application traffic and one feature of
load balancer is to check the health of
the user's application instance and it
removes or it stops sending the request
to the unhealthy instance and kind of
shifts that connection to the healthy
instance that way a user or a connection
does not get stuck with an instance
that's not healthy that's all that you
need to know about the networking
services now let's talk about the
storage services or the storage domain
in azure now azure storage in general is
a microsoft manage service providing
cloud storage which basically is highly
available secure durable scalable and
redundant because it's all managed by
azure we don't get to manage a lot of it
and these azure storages are a group of
storage services they cater different
needs and the storage products include
azure blobs which is actually an object
storage it includes azure data lake it
includes azure files as you see it in it
includes azure queues it includes azure
tables and lot more but let's start our
discussion with azure store simple azure
store simple is an hybrid cloud storage
solution that actually lowers the cost
of storage to nearly 60 percent of how
much you'd be actually spending without
using it so azure simple storage or
store simple is an integrated storage
solution that manages the storage task
between on-premises and the cloud
storage what i really like about azure
is that it's built around a hybrid
environment in might there are a lot of
other cloud providers that are there
where running and hybrid environment is
a big challenge you know it has some
compatibility you won't be able to find
an hybrid or a on-premises and cloud
solution for your need stuff like that
but with azure especially when it comes
to storage a lot of the things that
we're gonna see it clearly is designed
with hybrid environment in mind all
alright so let's come back and talk
about store simple so store simple is
then very efficient cost effective and a
very easily manageable san storage area
networking solution in the cloud i
thought i'll throw in this information
the reason why it got store simple is
really because it uses store simple 8000
series devices which are used in azure
data center and this uh stores simple or
simple storage it comes along with
storage steering to manage the stored
data across the various storage media so
the current the very current data is
actually stored in on-premises on
solid-state drives and data that is used
less frequently is stored in hdds or
hard disk drives and the data that
requires archived or that needs to be
archived very old data let's say less
frequently used data candidate for
archive they are actually pushed to the
cloud so you see how this storage
steering automatically happens in store
simple and one another cool feature of
store simple is that it enables us to
create an on-demand and scheduled
backups of data and then store the data
locally or in the cloud and these
backups are actually taken in the form
of incremental snapshot which means that
they can be created and restored quickly
it's not a complete backup it's an
incremental backup and these cloud
snapshots they can be critically
important when there is a disaster and
when there is a disaster recovery
scenario because these snapshots can be
called in and they can be put on storage
systems and then they become the actual
data so recovering is faster if you have
proper scheduled backups or if you have
frequent backups and this storage simple
it really helps in easing our backup
mechanism which means it kind of eases
our disaster recovery steps or
procedures as well so this store simple
it can be used to automate data
management data migration data movement
data turing across the enterprise both
in cloud and on-premises it actually
improves the compliance and accelerates
the disaster recovery for our
environment and if there is one thing
that's increasing every new day in our
environment that would be storage and
this store simple addresses that need
and we really don't have to pre-plan or
think in deep or having a proper storage
because now we have a simple storage
available in the cloud and moreover it's
on a pay-as-you-go type so not much
pre-planning on storage is needed yes
there will be a need but not as much as
i would without the cloud or without the
simple storage and the next service
under storage that we would like to
discuss is the data lake store this data
lake store or storage it's a cost
effective solution for big data
analytics in specific so let's expand
this so this data lake storage is an
enterprise-wide repository for big data
analytic workload now that's the major
service that's dependent on this data
lake store and this data lake enables us
to capture data of any size of any type
and of any injection speed and it kind
of collects them in one single space or
in one single place for operational
efficiency i mean operational efficiency
and for analytic purpose hadoop in azure
is very dependent on this data lake
storage and this
data lake store is designed with
performance for analytics in mind so any
time you think of or anytime you're
using analytic in the cloud or anytime
you're using hadoop in the cloud in
azure we are definitely using or we will
be to the most part or the normal
procedure or the right storage to pick
would be data lake store in azure it's
designed with security in mind so
anytime we use azure storage we can be
rest assured that we are using storage
from within a data center which has or
which was built with security in mind so
this data store also uses azure blob
storage behind the scenes for global
scale durability and for performance
let's talk about blob storage now blob
storage provides large amount of storage
and scalability now this blob storage is
the object storage solution for azure
cloud let's expand a bit on blob storage
azure blob storage is microsoft offering
for object storage now this blob storage
is optimized for storing massive amount
of unstructured data which could be text
or binary data it's designed and it's
optimized for rapid reads if i explain
to you on what scenarios we would be
using blob storage that might help you
get a good understanding of what blob
storage is so it's help or its design as
of now it's been used in many idea
environments to serve images or
documents directly to the browser it
helps in storing files for distributed
access a lot of fetchers can fetch data
from azure blob storage and it currently
helping users stream video and audio
it's currently being used for writing
log files it's currently being used to
store data as backup and restore at a
later point in times of disaster
recovery it also is used as an archiving
storage in a lot of cloud id
environments it's widely used in storing
analytic data not only storing but also
running analytic query against the data
stored in it so that's a wide use case
for blob storage not only that in
addition to all that we mentioned it
also supports versioning so anytime
somebody updates and data a new version
gets created which means at any point i
can roll back as in when needed and it
provides a lot of flexibility on
optimizing the user's storage need it
also supports tiering of the data so
based on need when i actually explore i
would find a lot of options i can pick
from that uh you know suits to my unique
storage environment or unique storage
need and like i said it stores
unstructured data and this unstructured
data is available for customers through
rest based object storage environment
the next product and storage service
would be a queue storage now q storage
provides durable cues for large volume
cloud services it's a very simple and a
cost effective durable messaging queue
for large workloads let's expand this
queue storage for a moment now this
queue storage is a service for storing
large amount of messages that can be
accessed from anywhere in the world
through http and https calls a single
queue or a single cube message can be up
to like 24 kb in size and a single queue
can contain millions of such 24 kb in
size messages and how much can it hold
it can hold up to the total capacity of
the storage account itself so that's
kind of easy to translate how much would
it hold and this azure queue storage it
provides an messaging solution between
applications and components in the cloud
what does it help it helps in designing
an application for scale it helps in
decoupling the application so you know
it's not very dependent or sometimes
it's not at all dependent on the other
application because now we have a queue
in between which kind of translates or
which kind of connects or which kind of
decouples both the environment now we
have a queue in between both the
environment can scale up or scale down
independently the next in the storage
service would be file storage
let's talk about file storage now these
azure files provide secure simple and
managed cloud file shares now with file
share in the cloud it actually extends
the user servers on on-premises
performance and capacity and a lot of
familiar tools for the cloud file share
management can be used along with the
file storage that we're talking about so
let's expand a bit on file storage now
this azure files or azure file storage
offers a fully managed file shares in
the cloud that can be accessed via the
uh smb protocol server message block
protocol now this azure file shares can
be mounted concurrently by cloud or in
on-premises deployments a lot of
operating systems are compatible with it
windows are compatible linux is
compatible mac os is compatible in
addition to all this being able to run
on on-premises and on the cloud or being
able to access from on-premises and on
the cloud it can also offer cash for
caching the data and keeping it locally
so it's immediately available when
needed so that's some additional feature
i would say that's an advanced feature
that it offers compared to the other
file shares available in the market
let's talk about table storage let's
talk about table storage now table
storage is a no sequel key value pair
storage for quick deployments with the
large semi-structured data sets the
difference between one important thing
to note with table storage is that it
has a flexible data schema and also it's
highly available let's expand a bit on
table storage so anytime you want to
pick a schema less a nosql type table
storage is the one will end up picking
it provides an key pair attribute
storage with a schema-less design this
table storage is very fast and very cost
effective for many of the applications
and for the same amount of data
it's a lot cheaper when you compare it
with the traditional sql data or data
storage so some of the things that we
can store in the table storage are of
course they're going to be flexible data
sheets such as user data for web
application address books device
information and other types of metadata
for our service requirements and it can
have any number of tables up to the
capacity limit of the storage account
now this is not possible with sql this
is only possible with nosql especially
with table storage in azure explanation
of storage really concluded the length
and breadth of the explanation this ceo
was giving his id personal but this id
personal is not done with it yet he
still has a question even after this
lengthy discussion and his question was
well there are a lot of other cloud
providers available what made you
specifically choose azure i mean from
the kind of question that he asked we
can say that he is very curious and uh
he definitely had asked and very
thoughtful question so his ceo went on
and started to explain about the other
capabilities of azure or how it kind of
outruns the rest of the cloud providers
so he started or he again started his
discussion but from a different angle
now so he started to explain what are
the capabilities or how azure is better
than the competitors so he started with
explaining the platform as a service
capabilities and i'm going to tell you
what the ceo told his id person so this
platform as a service or in platform as
a service the infrastructure management
is completely taken care by
microsoft allowing users to focus
completely on the innovation no more
infrastructure management
responsibilities go and focus on
innovation that's that's a fancy way of
saying it when we buy platform as a
service that's what we get we can
contribute our time on innovation and
not just maintaining the infrastructure
and azure especially is a dotnet
friendly azure supports the.net
programming language and it has or it is
built or designed or it is optimized to
work with old and the new applications
deployed using dot net programming
framework so if your application is a
dark net most of the time you would end
up picking azure i mean if you try to
compare most of the time you would end
up picking azure as your cloud service
provider and the security offerings that
azure offers is it's designed based on
the security development uh lifecycle
which is an industry leading assurance
process when we buy services from azure
it assures that the environment is
designed based on security development
life cycle and like i mentioned many
times in the past and i would like to
mention it again azure has well thought
about the hybrid environments which a
lot of other cloud providers have failed
so it's very easy to set up and hybrid
environment to migrate the data or not
to migrate the data and still run a
hybrid environment they work seamlessly
with
azure because azure provides seamless
connection across on-premises data
centers and the public cloud it also has
a very gentle learning curve if you look
at the
documentation it's picture rich and the
documentations are neat and clear would
really it would encourage you to learn
more it would encourage you to think and
imagine and try easily get a grasp of
how services work so it has a very
gentle learning curve azure allows the
utilization of technologies that several
businesses have used for years so there
is a big history behind it it has a very
gentle learning curve the certifications
the documentations the stage by stage
certification levels it's all very
gentle learning curve which is generally
missing in other cloud service providers
now this would really impress the ctos
or are people working in finance and
budgeting if an organization is already
using microsoft software they can
definitely go and avail or be bold and
ask for a discount that can reduce the
overall azure spending in other words
overall a pricing of the azure so that's
what helped or they are the information
that helped the ceo pick azure as his
cloud service provider and then the ceo
goes on and talks about the different
companies that are currently using azure
and they are definitely using azure for
a reason like pixar boeing samsung
easyjet xerox bmw 3m they are major
multinational multi-billion companies
they rely
run operate their id in azure and this
ceo has a thought that his id person is
still not very convinced unless and
until he shows him a visual of how easy
things are in azure so he goes on and
explains about a practical application
of azure which is what exactly i'm going
to show you as well all right a quick
project on building an azure app using
or building a dotnet application
in azure web app and making it connect
to an sql database will solidify all the
knowledge that we have gained so far so
this is what we're gonna do i have an
azure account open as you see logged in
and everything is fresh here let me go
to resource group there's nothing in
there it's it's kind of fresh all right
i'm logged in and this is what we're
going to do
so we're going to create an application
like this which is nothing but an to-do
application
to do list application which is going to
run from the web app get information
from us and save it in the database
that's connected to it so you can
already see it's a two-tiered
application web
and db all right so let me go back to my
azure account the first thing is to
create an resource group let's give it
an a meaningful name let's call it azure
simply learn all right and it's going to
be a free trial and the location
pick one that's nearest to you or you
know wherever you want to launch your
application now for this use case i'm
going to pick central us and create
it's going to take a while to get
created there you go it's created it's
called azure simply learn now what do we
need we need an web app and and a
separate sql database let's first get
our web app running so go to app
services and then click on add it's not
the web app plus sql that we want we
want web app alone for this
example so let's create and web app
uh give it a quick name let's call it
azure
simply learn
the subscription is free trial and i'm
going to use my existing resource group
a resource group that we created some
time back it's going to run out of
windows and we're going to publish
the code all set we can create it all
right while this is running
let me create my
database
right sql database create a database
give it a name let's call it azure
simply learn db
put it in our existing resource group
that we created it's going to be a blank
database all right and it's going to
require some
settings like
the name of the server and the admin
login the password that goes along and
in which location this is going to be
created the server name is going to be
azure simply learn db that's the server
name and the admin login can be
what can be the admin login name let's
see
so let's call it simply learn that's my
admin login name
and let me pick a password
click on create so what have we done so
far we have created an web app and we
have created an
a database in
the resource group that we have created
so if i go to resource group it's going
to take some time before things show up
so if i go to my resource group i only
have one resource group as of now azure
simply learn and there i have a bunch of
resources
being created and not still being
created right in the meantime i have my
application right here that's running
out of
or that's in visual studio as of now
right so once
the infrastructure is set and ready
in the azure console uh we're gonna go
back to visual studio
feed these inputs
in the visual studio so the code knows
what the database is the the credentials
to log into the database and stuff like
that so we're gonna feed those
information in visual studio by that
we're actually feeding it into the
application and then we're going to run
it from there
deploying this application takes quite a
while
we really got to be patient right now we
have all the resources that we need for
the application to run here's my
database and here is my app service
there's one more thing we need to do
that is
create and firewall exception rule so
one more thing needed is to create and
firewall exception uh rule right so the
application is gonna run from my local
desktop and it's gonna connect to the uh
database right so let's add an exception
rule by simply adding the client id it's
going to pick my ip the ipf laptop i'm
using as of now and it's going to create
an exception to access the
database so that's done now we can go
back to our visual studio i already have
a couple of
apps running or a couple of
configurations pushed from
visual studio i'm going to clean that up
if you're doing it for the first time
you may not need to do this
all right so let's start from the
scratch
this is very similar to how you would be
doing
in your environment right so we're going
to
select an existing azure app service now
before that i have logged in as you can
see i have logged in
with my credentials so it's going to
pull a few things automatically from my
azure account so in this case i'm going
to use an existing azure app so select
existing and then click on publish
all right if you recall these are the
very same resources that we created a
while back
all right we have clicked on save and
it's
running kind of validating the code
and it's going to come up with an url
now initially the url is
not going to work because
we haven't mapped the application to the
database that would be the next thing
all right so the app has been
published and it's running from my web
app as of now
it's gonna throw an error
like you see
it's throwing an error that's because we
haven't mapped
the app and the db together so let's do
that
all right let's do that so let's go to
server explorer
this is where
we're going to see our
databases that we have created now let's
quickly verify that
go back to
the resource group
right appropriate resource group
which is right here
and here i have my
database azure simply learn
database
all right it has some issues connecting
to my database give me a quick moment
let's fix it
all right so
we'll have to map the database into this
application
all right so let's go to the solution
explorer click on
publish
and a page like this gets shown and from
here we can go to configure
here is our a web app all right with all
its credentials let's validate the
connection number one
all right and then click on next
this is my
db connection string right which the app
is going to use to connect to my db now
if you recall rdb was a
azure simply learn db and that's not
being shown here so let's fix that
right so let's fix that
click on configure
and here
let's put our db servers
url now before that let's change this to
sql server
all right and then in here
put the
db's
url so go back to azure
here is my
db or
server's name
put that here
right
the username to connect to the server
that's
right here
put that in
and the password
to connect to the server let's put that
in
all right it's trying to connect
to our azure
portal or the azure infrastructure and
here is my
database if you recall it's azure
sldb that's the name of the database
let's test the connection
connection is good click on ok
so now it's showing up correctly azure
simply learn db that's the name of the
database
that'll be created
now it's configured
all right let's modify the data
connections
right let's map it to the appropriate
database again
all right so our name of the database is
azure simply learn db
and then
it's going to be
sql
server that's the data source
the
username is simply learn
and the password is
what we have given in the beginning
all right let's validate
the connection it's good click ok
now we're all set
and ready to publish our application
again now the application knows how to
connect uh to the database we have
educated it with the uh
the correct connection strings the dns
name
the username and the password for the
application to connect to the database
so visual studio is building this
project and once it is up and running
will be prompted with an url
to connect and
anytime
we put or we give inputs to the url
that's going to
receive the input and save it in the
database
all right so here is my to-do list app
and i can start
creating to-do list for myself all right
so i have items already listed
i can create an entry and these entries
get stored in
the
in the database i can create another
entry
you know take the dock for a walk that's
gonna get stored i can create another
entry
uh book tickets for
a scientific
exhibition
and that's going to receive and put that
in the database and that concludes our
session so through this session we saw
how i can use azure services to create a
web app and
connect that to the db instance and how
those two services which are
decoupled by default which are separate
by default how i can you know use the
connection strings to make connection
between the app server and the database
and be able to create and working app i
strongly believe you enjoyed me walking
you through this session right from the
start software development comprise two
different departments the development
team that develops the plan designs and
builds the system from scratch and the
operation team for testing and
implementation of whatever is developed
the operations team gave the development
team feedback on any bugs that needed
fixing and any rework required
invariably the development team would be
idle awaiting feedback from the
operations team
this undoubtedly extended timelines and
delayed the entire software development
cycle
there would be instances where the
development team moves on to the next
project while the operations team
continues to provide feedback for the
previous code
this meant weeks or even months for the
project to be closed and final code to
be developed now what if the two
departments came together and worked in
collaboration with each other what if
the wall of confusion was broken
and this is called the devops approach
the devops symbol resembles an infinity
sign suggesting that it is a continuous
process of improving efficiency and
constant activity the devops approach
makes companies adapt faster to updates
and development changes the teams can
now deliver quickly and the deployments
are more consistent and smooth though
there may be communication challenges
devops manages a streamlined flow
between the teams and makes the software
development process successful the
devops culture is implemented in several
phases with the help of several tools
let's have a look at these phases
the first phase is the planning phase
where the development team puts down a
plan keeping in mind the application
objectives that are to be delivered to
the customer
once the plan is made the coding begins
the development team works on the same
code and different versions of the code
are stored into a repository with the
help of tools like get and merged when
required this process is called version
control the code is then made executable
with tools like maven and gradle in the
build stage
after the code is successfully built it
is then tested for any bugs or errors
the most popular tool for automation
testing is selenium
once the code has passed several manual
and automated tests we can say that it
is ready for deployment and is sent to
the operations team
the operations team now deploys the code
to the working environment the most
prominent tools used to automate these
phases are ansible docker and kubernetes
after the deployment the product is
continuously monitored and nagios is one
of the top tools used to automate this
phase
the feedback received after this phase
is sent back to the planning phase and
this is what forms the core of the
devops life cycle that is the
integration phase
jenkins is the tool that sends the code
for building and testing if the code
passes the test it is sent for
deployment and this is referred to as
continuous integration
there are many tech giants and
organizations that have opted for the
devops approach for example amazon
netflix walmart facebook and adobe
netflix introduced its online streaming
service in 2007.
in 2014 it was estimated that a downtime
for about an hour would cost netflix two
hundred thousand dollars
however now netflix can cope with such
issues
they opted for devops in the most
fantastic way
netflix developed a tool called the
simeon army that continuously created
bugs in the environment without
affecting the users
this chaos motivated the developers to
build a system that does not fall apart
when any such thing happens
so on this note here is a quiz for you
match the devops tool with the phase it
is used in
a
b
c
d
none of the above
today more and more companies lean
towards automation with the aim of
reducing its delivery time and the gap
between its development and operations
teams to attain all of these there's
just one gateway
devops
and if you're also looking forward to
doing the same and excel in devops check
out simply learn's postgraduate program
in devops design and collaboration with
caltech ctme the program can help you
master several in-demand devops skills
and tools like continuous integration
and delivery devops on cloud kubernetes
docker and much more
today we're going to go through a
complete end-to-end journey on what it
takes to set up a devops team uh we're
going to go through what we need to be
able to do to go to devops what the
arguments are and why you need to do
devops and they will actually go through
all the individual tools you need to be
able to successfully implement devops
within your organization in addition to
that we're also going to take time and
go through each of those tools so you
get a good understanding of a
step-by-step instructions on how to do
basic setup of each of those tools so
let's get started so what was devops
before so what was the process that we
took for doing delivery before devops
well it was a model called waterfall and
waterfall was a very traditional
approach to actually building out
solutions and the reason why it's called
waterfall is that you bring out all the
individual requirements and individual
sections of a project and they cascade
off each other so if we look at the
breakdown we have requirements design
and we have implementation we have
verification we have maintenance you'll
have user acceptance testing and this is
all based on the software development
lifecycle model or sdlc and it's been
around for quite some time and it's
still used by a lot of companies today
the challenge you had with the waterfall
model is that it really is a very long
drawn out model for actually building
and delivering solutions so it took a
very long time to actually um write code
and then deploy the code and it was very
difficult to actually identify problems
within the code and provide feedback to
the development team on what to fix and
this really was a very time consuming
we're talking about months sometimes
years for projects to actually go
through a waterfall model process so
along came a new method of being able to
do delivery and it's called agile and
the agile approach is a way of being
able to take the actual work that's done
in a waterfall model and compress it
down into small iterations and what we
would do is a fundamental change is that
you would actually take uh teams that
would disparate and as part of the
individual cascades within a waterfall
project you actually bring them together
so you have your requirements team in
person design developer and release
management team all together in one
group working on an iteration the great
thing about agile is that you took a
process that was weeks or months or even
years in length as it was with waterfall
and you reduce it down to two or four
week sprints depending on the cadence
for your team uh typically you have a
two week sprint and then the goal is is
that at the end of each sprint or
sometimes every other sprint you would
do a software release and so that
customers were getting the software much
faster the problem that we still ran
into though with um agile is
fundamentally similar to what we were
having with waterfall you have your
devops person working on code on their
system and you'll be working great on
their computer and then you have the
operations person who's migrating the
code from the developers environment the
test environment to the production
environment and you would run into
issues where the code simply doesn't
work and there's a lot of reasons why
that would happen uh the actual
developer environment would often be
very different or would have different
dependencies in it so the the hardware
the the software there may be additional
uh applications that were installed on
the operating system that simply hadn't
been transferred over to the operations
environment and so what you would have
is a disconnect between the developer
environment and the operations
environment making it difficult um to
actually roll out code so you'd run into
a program where when you rolled out code
you'd have to have a rollback plan in
case the code wouldn't work in
production and so each release became an
event where everybody got very stressed
about the actual event of releasing code
because you didn't know whether it was
going to work so devops really looks to
address and solve a lot of these
problems so the keyword that you'll
often hear with
devops is continuous integration and
what that means is essentially that as a
developer is working on their code the
code is constantly being tested against
not just the actual code itself with
unit testing but the environment with
which it's going to be released in and
the goal from a devops model is that the
breakdown of communication that happens
with waterfall and agile where dev
developers and operations teams aren't
working in the same environment is being
removed and you're able to provide a
continuous and contiguous environment
between the developer and the apple
actual operating model so the reality is
when the developer is working on their
code they're actually working in an
environment that is identical to the
production environment and so when the
actual operations person comes to
actually do releases for the code and
you can see some teams are doing as many
as 20 to even up to 50 releases to
production environments every single day
you're able to guarantee that the actual
code itself will work and releases go
from being a stressful event to a
byproduct of good testing and good setup
and structure for how you actually build
out your solutions so what we're seeing
here so the goal is that as a developer
and as an operations person that the
code is working continuously in both
environments you have continuous
integration and continuous delivery so
simply put what we're able to do is
we're able to eliminate the problem of
the operation environment not being in
sync with the development environment
and this is a
improvement on agile this is not to say
that waterfall or agile are wrong as
delivery models what it is is just a
maturity of the ability to deliver
solutions and devops is just another
rung in that maturity curve using tools
that are available to us now that five
ten years ago simply weren't available
so the goal is for you as a team to move
to a devops model where you can
implement continuous releases on your
software as long as you're using the
tools that are available and the good
news is those tools are open source
tools so let's go through some of the
benefits of why you'd want to go and use
devops so you know essentially what's in
it for you so let's over the next few
slides we're going to go through what is
devops we're going to go through the
benefits of devops so in the last few
slides you've actually seen you know
what is devops and the benefits of
devops along with the life cycle but
we're also going to start digging into
the tools that you have that are useful
for devops and we're going to focus in
on seven tools that can provide an
end-to-end infrastructure for delivering
devops solutions but there are
significantly more tools available on
the market but these are seven of the
most popular uh for each of their
categories so devops really is an
essential collaboration between the
development team and the operations team
these are teams that have in past been
somewhat at conflict with each other and
what you have now is an opportunity
where those teams can now work
continuously with each other the
expectation with devops is that it will
continue to mature indeed you're
actually even seeing some groups which
are now called devsecops where they're
integrating security as part of the
delivery between the development team
and the operations team the bottom line
is a devops engineer is highly in demand
the demand for devops engineer is
literally going through the roof with
salaries going up exponentially around
that so let's dig into some of the
benefits of devops it's not just a new
catchphrase it's actually got
significant value and how you can speed
up delivery of your software so the
benefits of devops are really broken up
into a number of key areas first of all
we have continuous delivery of software
which allows you to continuously release
new features with the security and
understanding that the software going
out is of high quality it allows the
teams that are working on the software
delivery within your organization to
more effectively collaborate with each
other so that you're all talking from
the same page and understanding of what
needs to be delivered the deployment
process itself moves from being an event
where there's a lot of stress and
there's a lot of contingency plans to
being a much easier deployment the
efficiency within the actual code that
you're writing the ability to scale up
using the different tools are available
allows you to be able to bring in and
scale up and reduce the teams you have
running the software as needed errors
can be fixed much earlier and more
quickly and can be caught before
anything gets pushed out to the
production environment and fundamentally
what we're looking for is improving the
security of the actual releases so the
actual concept of security is center to
all the work you're doing and then
finally what really allows you to
reduce the number of errors is that
there is much less manual intervention
you there is a greater reliance on
scripted environments that you can
actually test and validate for their
security reliability and uptime
efficiency so let's talk a little bit
about the life cycle of a devops so the
very first step that you'll take is to
actually build out a build and test
environment and this is a continuous
building test environment and this is
merged with the first step of your
source code once you move through that
and you're looking at continuous
integration which means that every time
somebody checks in their code they're
validating that the code actually can
run in the production environment once
you've actually then passed the end
continuous integration and the testing
that you have with your code you're
looking at continuous deployment if the
code works and is available to be
released into the production environment
let's go ahead and release it and once
you actually have release code then you
want to be able to validate that your
environment is working efficiently you
may release code that is a new feature
within your application and customers
may then gravitate immediately to that
new feature if they do you want to be
able to ensure that the code is working
and more importantly that the
infrastructure is there to support and
then finally you're looking at software
released as a continuous event and then
you go back to the beginning you start
working on more code you run it through
your build environment and continuous
integration deployment continuous
monitoring and keep that cycle moving so
let's dig into the tools that you as a
devops engineer would need to learn if
we break down the environment that we
have all the way from source code
management to software release and there
are a number of key tools that you want
to be able to use so for instance source
code management and git is an open
source tool that you would want to use
for managing your code the continuous
build and test environment will be
managed with mavin and selenium
integration with the environments that
you're working on is managed through
jenkins the actual deployment to your
production environment would be managed
with products such as ansible and docker
and then the monitoring of your network
would be used with tools like ngos the
thing that you have to remember with all
these tools is that they're open source
tools there is no licensing that you
have to uh purchase uh some of the tools
will have a pro level licensing that you
can choose to select but to get started
all of these are open source tools you
can actually start using for free right
now so let's dig into git so let's look
at the the challenge that get is able to
address so before git and you if you had
a team of developers that were working
on different pieces of code one of the
key problems you have is that there was
no collaboration between the team and
the the challenge you have is that with
version control it was difficult and
often required i'm having to check in to
a large environment or you had very you
know quasi version control environments
it was time consuming often there were
problems that what would happen if the
version control server would crash and
there wasn't a backup then you'd have to
kind of essentially go back to square
zero to see whether or not you actually
had to do the work again so let's look
at what git does to actually solve this
so first of all one of the things that
git does is it makes team collaboration
much easier the software itself is more
effectively documented and more and more
effectively maintained the actual code
that actually gets worked on by each
developer in a git environment is the
complete code so it makes it much easier
for backups and for sharing content
amongst each developer so the way in
which git is able to accomplish this is
that it is essentially a distributed
version controls solution
and what that means is that when you
have multiple developers yes you do have
a git remote hub that you connect and
are able to upload and download
different branches of the code that
you're working on but essentially as a
developer when you um actually have the
code you actually have all of the code
and
you're able to imagine directly from
your local pc or your local development
machine and without having to worry too
much about the network or the server
that you're going down so some of the
key things in which um git is really
good at it is you know a software
management tool is designed from the
ground up to manage code development it
does track the changes on that code
easily and effectively makes it easy for
you as a developer to track your code
the ability for multiple developers to
work together is much easier in
comparison to um other solutions and and
this is really the the key success point
with i get is that it allows for
non-linear code development so you can
actually have people working on
different areas of the code that may be
released at different times because of
what they're working on so if we
actually break down the architecture of
git it really falls into four key areas
of a working directory a local staging
area a local repository and a remote
repository so the process you would go
through is you would add files to the
stem staging area from your working
directory and this would be usually the
get add command and then from your
staging area when you're ready to
actually then commit those files to a
local repository you use the git commit
command and then from there you would
actually then push your local repository
out to the remote repository for a final
commit to the remote repository um and
this allows then the rest of the team to
pull down your latest final changes so
you can be working on your code locally
and you can be using your local local
staging area local repository and when
you're ready to commit your work you can
commit it and as soon as it's committed
the rest of the team can then pull down
the latest changes that you have worked
on and this allows for a complete and
holistic checkout and check-in process
and then sometimes you want to be able
to go through and then take the checked
out code that you cram that you're
pulling out and merge it with your local
code and the merge process always
ensures that
you are working on the latest and best
version of the code and everybody on the
team is being consistent with the
communication of the actual code that's
being delivered okay so what we're going
to do is we're going to validate that we
have get running on our
computer we're going to then create a
directory add a file to that directory
make some changes to that file and then
use the commit commands to be able to
check in the files and validate changes
that have been made so first thing we do
is we're going to see if we have git
installed and to do that you just use
the command git version and what your uh
sorry git dash dash version and that you
then give you the version number and now
we're going to create a new folder so mk
der green and that will create a new
folder and we'll move our cursor into
the green folder by changing the
directory and we can go back into
windows and we can validate that that
directory is actually there so here we
go let's have a look we should see it
there's the green folder if i double
click into that you'll see there's
nothing in the folder right now so what
we have to do is we actually have to add
the folder as part of a git project and
so we're going to use our get initiate
init and they'll actually initialize the
folder and make it a git repository and
so let's go ahead and create a file
that'll actually go inside of that
new initialized folder and you'll see
that there's a dot get folder that's
been added so the new text file we're
going to create is going to be called
class and it'll actually be class.txt
the extension is hidden by default in
the uh text file we're just going to
type in the text
welcome to simply learn and save that
now let's go ahead and check the status
um of that file and git so let's do git
status and you'll see that it says yes
there's a file there but it hasn't been
committed to the repository that's why
it's in red so what we have to do now is
commit the file to the repository and it
won't be able to track any changes that
we do to that file so let's go ahead and
commit the file so we do commit add
class.txt and they'll add that to the
repository and there we are it's added
and the final step is we want to do is
commit it let's do git so let's do git
commit dash m but and this will be our
first commit and this is the
description and so it actually shows us
committing the file it's the only item
in the folder so now we can go over to
the file that we just created and we can
make some changes to it so let's open up
the file so it says welcome to simply
learn uh let's put in some text
afterwards that says this is my demo and
we'll save that close that and now if we
go back to git we can do a comparing
contrast between the original file that
we committed and the new um updates that
we've just done and we can check the
difference by using git diff and what
you'll see is the
uh red text was the original text and
the green text is now the new text
that's been added and so that shows you
how you can actually go and create a um
a new get repository go through adding
files committing the files into the
repository and being able to see the
different version controls let's move on
to maven so why use mavin so so why use
maven well let's look at some of the
things that you would have done in the
past if you actually building out the
tasks so let's break down what you would
have done before using maven if you were
to create a game of football using java
you would actually have to go out and
for your actual game you would have to
collect all the different components
needed within the java environment to
make sure that everything would work
correctly and if you made the slightest
mistake you wouldn't get the right
output that you needed the actual
process of building and applying a
project really would take quite a lot of
time after mavin mavin allows you to be
able to take templates that are stored
locally and be able to use those to be
able to improve the efficiency of being
able to build out your environment by
removing dependencies within the
application it makes it a lot easier if
you be able to focus on just writing the
code for your game or your solution
rather than having to manage the
environment within which that solution
would be deployed now the focus within
mavin is that it is an automation tool
and it's used for projects that have a
short period of time so as you're
starting on your environment for
building out um solutions mavin is great
to get started with and then if you need
to have a longer term projects then you
can look at other alternative tools on
the market so four key areas that you
want to focus on and why mavin can help
you is that it does allow for efficient
um parallel builds to be run
concurrently it's really easy to use you
can get up and started with madden very
quickly uh you do have fast access to
new features and new configurations
quickly within your environment and the
build process that will allow you to be
able to visualize your code happens very
quickly the actual architecture of maven
is that the execution and commands are
managed through what's known as a pom
file the pong file itself is a project
object model and it's an xml file that
actually has the details for the project
and the configuration for the build
environment pump file itself will then
fetch dependencies from the local
repository and will apply any plugins
that you may have also included within
your mapping environment the goal is
that your software is built much more
controlled
manner so what we're going to do now is
we're going to show you how to go
through and run your very first project
using maven the first steps you can have
to do is make sure that you have the
latest jdk installed on your computer so
we're gonna go ahead and open up our web
browser and get this installed on your
computer so we're gonna type in jdk
download into our chrome web browser
and go to the oracle website which
allows us to download the jdk now
because we're running this on windows
we're going to download the windows
version of the jdk
and once you have that installed
so the next step is to actually then go
ahead and download the maven files so
it's going to take us to the maven
apache dot org website where you can
actually go ahead and download the
version of maven that you need for your
operating system we're going to go ahead
and download the zip file instead of
just an executable file and this will
give you more control over how you
install it onto your windows pc after
the file has been downloaded we want to
go ahead and extract the entire
file so you have the unzipped folder
running on your computer
so one of the things that we're gonna
have to do is go through and set a
number of system variables for the jdk
and for the path to your maven files and
to do that you want to go into your
control panel and select the system
security and system settings and then go
into advanced system settings this will
give you access to
the
edit system variable setting that you're
looking for
so we're going to go down to the path
variable and you'll see already that we
have pasted in the link to the jdk
right there underneath the variable
value if you've installed the jdk it's
likely that it's already installed this
path for you correctly
the good news is actually fairly easy to
install the path to maven and to the jdk
because all you have to do is copy the
path for where the folder is located on
your system and just paste it into the
variable value under the edit systems
variable folder and file system
and that should now get us all the files
and you can see that we have our
extracted folder already on our desktop
there
and the system path is now
set to where the maven folder is located
so we also have to go to system
protection and
set the system path to your maven folder
now we have to go ahead and do a user
variable that also part links to the
pathway maven files
so what we're doing right now is we're
just validating that all the files have
been extracted correctly they have which
is good we can now go ahead and link to
these moment folder
all right so now we're going to do is
we're going to move over to command
prompt this is where we're going to do
most of our work from here on out first
command we're going to do is uh mv
so we're going to go ahead and create a
new user variable and
we're going to type in we're going to
call this one m2 dash and then we'll put
in the path uh to the apache maven
folder
you should be able to use m2 dash home
for your projects but if it's not you
can always use the variable name maven
dash home and that will do exactly the
same as
m2 we're going to do both here
just so we have a backup
so we're going to open up command prompt
and we're going to see if maven is
installed we do that by writing mvn dash
dash version
and here we are yes so now we're going
to go ahead and create our first maven
project and the first part of creating
the maven project is actually the
directory in which we will store the
files that will be used to create for
our project
so we're just going to go ahead into the
folder and we're going to create a new
directory and we're going to call this
one and we're going to call it simply
learn
and we can just copy the path for that
so i'm going to write this the
correct path for us so we're going to
change directory cd and copy the file
location over alright we are now moved
ourselves over to the simplyland folder
and we'll be able to install the pom
file which is what all the instructions
from mavin are stored in uh into this
area so to do that we're actually going
to use a template that's already been
created by mavin and this is going to be
the mvn archetype template and so you
just do nvm archetype colon generate and
this will go ahead and create all the
files for us
so this can be quite a few files and
they're all being downloaded from the
mavin website uh so it can take some
time but as you see we've gone ahead and
all the files have downloaded and we
have everything up and running correctly
inside of our environment on command
line and command prompt so we'll have
some values that we do have to set the
first one is group id and we're going to
call this one
com.mav and then the next one is going
to be artifact id and this is going to
be
mav mav
project hit return and then version is
going to be one dot zero dash snapshot
which is put in the same value for this
version which will be 1.0 dash snapshot
and the package we're going to call this
one com.mav dot demo
and then we'll select yes so wife to
create the environment and it will build
the environment for us
and what we have now is we have this
screen which shows everything is up and
running for us and we have a successful
build and so let's go into the maven
folder and just see that we have our
simply them folder let's open up and see
what's in there so we double click into
that directory
yep and there we are starting to see
some of our files so it's um the same
project names there as mav dash project
and then we have our pom file that's in
there which is fantastic under the
source folder we'll actually see
the uh the demo and we have our app
under the demo app
we have our test file as well
so in the main folder we have a file
that's called the pom file and that
really is the most important file that
you need for your project you'll now go
and you'll be able to set all your
settings inside of that pom file but
that's it that's what we have done here
as we've gone through and we've gone
through all the setup that allows you to
make sure you have the right files
running on your computer the right files
to download from the maven website how
to install all the files the settings
you need to make on your windows
computer and then all the settings that
need to be done in the command line to
be able to run the application and then
from here you can actually start running
maven now let's look at selenium as we
can start looking at testing so before
selenium if you wanted to run testing
and particularly if you want to do
sequential testing it would take up a
lot of time you'd have to run one test
and the second test and then all the
other tests and the actual time elapsed
would be quite significant
with selenium you're actually able to do
parallel testing so instead of looking
at the amount of time it would take to
actually run a test and run the test
sequentially the length of time to
actually run your test is based on the
length of the longest running test of
which in this example the longest
running test is two minutes now one of
the things that you'll notice with
selenium is that its focus is on web
applications um and it is an open source
tool and it's really good at regression
and functional testing of web
applications if you're doing a mobile
application or if you're doing an iot
application there are other tools you
can use on the market as well but if you
are new to devops and you're setting up
your first devops environment selenium
is a great place to start and web
applications are mature and allow you to
be able to test out and validate the
concept of continuous testing in your
environment so four key takeaways for
selenium um it really is fast i mean for
fast execution it's highly accurate so
you can always validate that what you're
doing will work correctly and feedback
can be sent straight back to the
developer if their uh code doesn't pass
the test um it allows you to script in
multiple languages um again this is a
great way for you to test them the focus
is on web applications but you can test
in multiple languages used for web
applications and most importantly allows
for parallel test execution which speeds
up the whole test process significantly
directly to break down the architecture
of selenium and what you'll find is the
actual libraries that you can use for
selenium are run in c sharp java python
javascript php these are the common
languages used to build out web
applications the actual web drive itself
is an api driver um which is fantastic
because it makes it easier for you to
integrate selenium with applications
through api and you can run the solution
through different web browsers the most
popular being firefox chrome and edge so
to focus the selenium webdriver codes
are being used to build out the client
libraries that make it
to allow you to be able to build
solutions across multiple web
application environments the webdriver
application interface is used to
integrate with the application to make
it easy for testing the webdriver
service provides an immutable stateless
environment that makes it very easy for
you to be able to use protocols such as
json to wire up your test environment
the driver manages the communication
between the browsers and the wire
protocol that's used for json within the
web browser itself and then the actual
commands are then run through the web
browser for you to be able to get your
results okay so before we actually get
into the setup of selenium the first
things we want to do is we want to make
sure that we have the right version of
java installed and so i'm just using
command window here and i'm going to
type in java and actually what i can see
now is i do have java installed which is
fantastic otherwise you'd have to go and
download java from oracle's website the
next step is to validate that i have
eclipse installed and i'm just typing
eclipse and there we are i have the
clips app installed uh you can actually
download um eclipse from the eclipse
website the third step i want to check
is to validate that i have a browser
driver installed so i'm using google's
chrome and for my main browser but you
can actually go out and download the
chromium driver and that's the actual
core engine that powers chrome
interesting enough being used in the new
microsoft edgeweb browser so you can
actually go and download the latest
chromium driver and install that and
then the final stage is to go to the
selenium website and download the jar
files themselves so just do a google
search on selenium download and here we
have the
seleniumhq.org website and it takes
straight to the download page and we
want to download the latest version and
we've gone ahead and done that already
so we have everything running and now
the next step is for us to install all
of this great content okay so before we
actually get into the setup of selenium
the first things we want to do is we
want to make sure that we have the right
version of java installed and so i'm
just using command window here and i'm
going to type in java and actually what
i can see now is i do have java
installed which is fantastic otherwise
you'd have to go and download java from
oracle's website the next step is to
validate that i have eclipse installed
and i'm just typing eclipse and there we
are i have the eclipse app installed you
can actually download um eclipse um from
the eclipse website the third step i
want to check is to validate that i have
a browser driver installed so i'm using
google's chrome for my main browser but
you can actually go out and download the
chromium driver and that's the actual
core engine that powers chrome
interesting enough being used in the new
microsoft web browser so you can
actually go and download the latest
chromium driver and install that and
then the final stage is to go to the
selenium website and download the jar
files themselves so just do a google
search on selenium download
and here we have the
seleniumhq.org website and it takes
straight to the download page and we
want to download the latest version
and we've gone ahead and done that
already so we have everything running
and now the next step is for us to
install all of this great content so
we're just going to go ahead and open up
the eclipse environment and it always
takes a moment to launch clips
i'm going to go ahead and create a new
project
and we're going to call our project xyz
i think that should be fine
and this is just a test after all and
we're going to go ahead and create a new
class file by right-clicking and select
new
class
and we're going to go in here and we're
going to give the class file a name of
simply learn and
see the finish button being selected so
maybe we need to select some other
options here oh you know i think it
could be the package name yeah let me go
ahead and change the package name and
i'm going to change that to
qwe
qw app yep and the fetch button has now
been highlighted i can select that i can
go select that and click finish
and
uh we now have a test page that's up and
running and but what we have to do is to
validate that selenium is actually part
of the project so to do that we're going
to go to the source to the actual source
xyz file and right click on it and we
want to validate that all the files are
there correctly
and so we're going to see that we have
the libraries are there yep there's
selenium if it's not there you can set
add external files and find the selenium
jar file select open and that would load
that in there and then you hit apply and
close and that would then apply it to
your project and now we can actually
start writing the code part
so we're going to put this code in the
public static
void section
and the first
command is going to be a system.set
property and we want to make sure that
we're configuring the web driver for
chromium correctly so we're going to do
webdriver
dot chrome
driver
and then the argument is actually going
to be the full path full network path to
the chromium driver that you have on
your pc so let's see we have chromium
here we want to copy that path
and so that paste that in and then after
we've pasted in the path we have to make
sure that we also
put a link to the actual file that we
want to execute which in this case is
chromedriver.exe
and then we want to make sure that the
driver is pointing to
the new chrome driver
and you'll notice that there's a couple
of red colors on here which means that
we don't have all of the public classes
imported correctly so you can right
click command and say import the web
driver and just select that real quick
and that adds the import correctly at
the top of the page and same for chrome
driver
and that's just a very quick way and
when you're working in eclipse to import
additional
files
so before we actually start our test we
want to make sure that we've actually
cleared out any potential browser
cookies that would be within the
chromium driver so for that we're
actually going to go ahead and we're
going to write a script that allows us
to remove the cookies
so we want to do driver.manage and then
delete all cookies
and the next line we're going to do is
to actually have the web page that we're
going to connect to open up in full
screen mode and
for that to happen we're going to go and
actually use the window property which
which is what i was writing before by
mistake and so this will allow us to go
full screen and now we need to pull in a
web page so we're going to get the get
command and what we're going to do is
we're actually just going to pull in a
web page from amazon which i just happen
to have here so i'm just going to go to
amazon web page and so i'm just going to
copy the url and you know just need to
copy https www.amazon.ian.org.com
wherever you're located
and so you can see that i've pulled in
the amazon web page and then line 18
actually allows me to go and connect so
here we are we have the amazon web page
that i'm pulling in
and then what i also want to do is i
want to look for a specific id in one of
the elements and so i have the driver
which is called find an element and that
allows me to look for any element that
would be on the actual page so here we
have the html in the console screen and
i can actually then paste out an id from
the element that i want to use
so the find element is just the id for
each element and most of them have them
because they need it for that javascript
and css
so let's see there's quite a few ids so
and we can select any of those
so what we have here is we have two tab
search text box and then what that
actually does is that finds the id for
the text box used for searching and then
we're going to use the send key command
which allows us to actually pre-fill in
the search box and we're going to use
the test of puma shoes this is a fixed
string but you could actually put a
variable in here if you wanted to pull
the strings from a database or an xml
file and then the next
line on line 19 it shows how long
actually going to run the actual script
for the
search which is only 10 seconds and then
once the 10 seconds has elapsed we'll
actually quit the script which actually
closes the web browser and takes us back
into our screen here
and let's test that make sure it's
working and everything's looking good
the web browser opens which is what we
expect it goes to amazon fantastic um it
now actually goes to the search screen
and should fill in the puma shoes there
we go puma shoes and
waits 10 seconds and it closes and
that's exactly what we were expecting
now you can actually change the unit of
time that you would have for the actual
test so if you wanted to see what's
happening so longer than 10 seconds you
can actually change the actual time or
the actual metric and so in in this
instance we've got it says seconds and
units of time but you could do minutes
or hours or microseconds
and you can change the keyword as well
so it could be a jigsaws or anything you
wanted to check but if you're actually
building your own application you want
to be able to test your own data against
your own application that's where you
put in that specific data
so let's look at the center the heart of
your devops environment and that center
is with jenkins so before jenkins people
would developers would work on their
code and the code would then be checked
into a source repository you would then
check for any issues and then you would
then send that code over to
the operations team and there would be a
delay for actually delivering software
so the actual
challenges developers had is that if
they wanted to run any tests they had to
wait until their software was built and
there was a lot of problems in providing
feedback and particularly had large
teams working on software or who
actually had the error the actual
delivery of software was often
delayed because of these key problems so
after jenkins what we actually have is
the ability to be able to streamline
this whole process the build test and
deploy happens continuously and are able
to then notify developers and the actual
specific developer of any errors that
are detected being able to speed up the
delivery process much more efficiently
the focus of jenkins is that it
automates a continuous development
testing and deployment environment and
it's open source jenkins is easy to
install and configure it's been around
for many years now and it is very mature
there are many plug-ins that you can use
within jenkins to be able to have the
jenkins product work with your
environment if there isn't a plug-in you
can actually extend fire you can
actually write your own plug-ins if a
plugin doesn't exist so you can actually
continue and extend and invest in the
jenkins ecosystem and it can be easily
distributed across multiple machines so
let's break out the architecture of
jenkins so jenkins starts as a remote
source code repository and it then pulls
out the code every time there is a
commit from into the server master the
server the master will then have a slave
that will run on either windows linux or
mac os to be able to distribute this
load across those environments and after
it's actually gone through and run its
test it'll then send out a report the
goal of the report is to communicate
back to the team what has passed and
what has failed the actual check-in
process okay so we're going to go ahead
and show you how to install jenkins on
your local pc uh so the first thing you
want to do is check that you have java
installed so we're just going to go into
our system and see whether or not we
have the java jdk installed you can do
that by going into system properties
second advanced and environment settings
and
there we are we have java jdk installed
and you want to just double check on the
system variables to make sure that you
have the java unscore home installed
correctly as well and we have that
installed and under path also make sure
you have a link to the java jdk as well
so we have everything installed and what
we're going to do is we're going to open
up a command window and just do a double
check right there so open up command
window and we're just going to check
that we have java and sort so we do java
dash version and we have a version
number that comes back yeah if we didn't
have java installed we wouldn't get a
java version number come back we'd
actually have an error all right so
everything looks good so the next thing
you want to do is to go to the jenkins
website and download the files so we're
going to go to jenkins.io
and that's the official jenkins website
and if you want to download you just go
to jenkins.io
download or you can just uh type into
your search bar uh download jenkins and
that will give you a link straight to
the download page and one of the things
that you'll see is that there's lots of
different versions uh for jenkins
because jenkins runs on pretty much uh
most operating systems uh including
windows
linux and mac os um but we want to
select the windows version so we've
already gone ahead and downloaded a zip
file which contains all of the files
that we need for jenkins and so that's
the zip file and we've expanded it and
what we want to do now is double click
the installer file and we'll go ahead
and install that so we've already
installed this so um one of the things
that we'll get is a message to ask you
if you want to repair you'd actually get
a install message because it'll be a
fresh install for you and but once
you've done that it has all the files
and all the settings correctly installed
onto your windows computer and we do all
of our commands for jenkins through
a web page that's run locally on your pc
and so you're going to type in localhost
colon 8080 and when you do that they'll
actually take you to jenkins running
locally on your pc and here we have our
jenkins dashboard and we've already gone
in and created some sample jobs and you
can see those listed on the uh sorry on
the right hand side of the screen so
let's go through some of the things you
would want to set up here so first of
all let's go into the manage jenkins
screen and that will achieve so we'll go
to the manage jenkins tab on the left
hand side to do some standard
configuration settings so we're going to
the configuration systems and if you
want to if you're like remotely
connecting to a remote jenkins
environment you can actually install a
jdk remotely for on that machine if you
want to now we already have the jdk
environment already set up so yeah
that's good here so we don't need to do
that but you'll also see as you go
through the the list of applications
that you have gradle and maven docker
git and other tools that you would use
in a devops environment there are many
other tools you can install through
plugins as well let's have a look at one
more setting here with the security the
default security is jenkin's own user
database that probably is not allowed
within your company due to security
restrictions and so you can choose ldap
which allows you to connect with your
active directory configuration again
because there are so many plugins
for jenkins uh there are plugins for
saml authentication as well as oauth so
let's go ahead and start a new build new
build job and so we select a new item
and we're going to
select
demo one from that to create a new item
and we're going to use just a freestyle
project because i just want to demo how
you can actually create a build click
okay description uh gosh what could we
write here um we can put in a snippy
demo
and we're not going to use source code
for this one we're going to do that in
the next demo but uh let's what we want
to go down to is the job time and we're
going to so build and we can just do um
add for wind execute windows match
command and so let's just put in a very
simple command and so we're going to
type in echo open quotes hello world and
then we'll put in the timestamp so we'll
put in date first and then time as part
of our output close quotes and we're
going to save that also
save and that's our simple project so
what we want to do now is we want to
actually go and select the build now
command to actually run this project hit
build now and take a couple of minutes
for it to run and here we are this is
our project and we're going to now see
the different builds that have happened
and if you look on the build history
you'll see the latest history shows when
the build was executed and if you click
on that you can actually start getting
information such as the console output
where she provide information that we
were asking for so for instance uh we
have it's first saw we can see that the
build was successful and we see the
output includes uh the hello all and
date and time so we're gonna go ahead
and create one more project where we can
actually connect to a github and use the
get command tool to be able to build a
real project so we're gonna go ahead and
create a new item and we're going to
call this one jenkins it's freestyle
project and our description is demo
using git and so we're actually going to
connect to github and so to do that
we're going to go into source code
management tab and we're going to say
get and so we select that we can
actually now connect to
github and so to do that you go to your
github account and you want to go in and
select the clone or download and you get
the url link from there so copy that and
go back to jenkins and add that in the
repository url so you paste that in and
then for credentials we want to add in
our jenkins credentials and so just make
sure you have those selected correctly
and we've clicked add and then we just
pull those from the drop down and let's
not change any of the other settings and
then for the build we're actually going
to be building a java application so
it's a slightly different set of build
commands so we want to type in java c
for compile and then space and the file
name is simply.java and you want to put
java and space simply on a new line save
that it's again this is a really simple
application but it gives you the
understanding of what you need to do
here and so we saved it and let's now go
ahead and see if everything's working
we'll do the build now and see what's
happened and here is our first build we
select that we can go to the console
output for details and what we can
actually see if we scroll down is that
the finished at the finished output at
the very end says success which means
everything was built correctly and so
that's how you're able to connect
jenkins with a git repository and the
thing that's great with working with
jenkins is that it connects to so many
different systems and really does become
the central point of your devops tool
set so let's start looking at the
operation side of the devops environment
and let's focus in on docker so before
darker the problem you had as a
developer if you're working on a virtual
machine the actual virtual machine was a
your vm was a very complex piece of
software so first of all vms have not
been around for very long and we're
talking about less than 10 years and
the typical way of having a vm is that
it's very heavy on usage on your
computer you essentially are running
everything of the vm plus the tools to
run the vm on your computer with docker
what you're able to control is the
actual development environment you're
working in you don't have to be
encumbered by all of the additional
extraneous processing that a vm
environment has you're really stripped
down to just the bare minimum that you
need to have to run the environment
using a containerized solution known as
docker so docker is essentially an os
level virtualization software it is a
maturity of the solutions that were
started with virtual environments and
there are other solutions out there doc
has become the de facto container
solution that you can use for building
out and guaranteeing that what you the
developer is building is going to be the
same as what the operations team will
use so docker itself is highly scalable
and efficient and you can have many
docker environments running on a single
piece of hardware the boot up time is
incredibly short you can reuse the data
volumes very very easily it's not
complicated and the actual applications
are completely isolated this is like a
perfect sandbox environment so let's dig
through the architecture of darker so
there are really two parts to docker
that's the docker engine and there's the
docker client uh the docker engine is
comprised of the docker cli which
connects with the server daemon the
docker client will then issue the
commands to the docker demon and then
the docket demon will then interact with
their system to be able to provide the
tasks and tools needed for you to be
able to build out your solution the
actual images that docker uses are
instructions for creating docker
containers and the actual docket
container itself is a lightweight
package that contains all the
dependencies needed to run an
application now once you have created a
container that is of value to your team
you want to be able to store that
container so that other team members can
use it and this is where you would use a
docker registry to be able to host and
distribute multiple docker images so in
this video we're going to go ahead and
see how we can get docker
installed and running so we're going to
go ahead and open up our terminal window
and we want to validate whether or not
docker is installed uh if you haven't
installed docker you can actually go
ahead and use the following command and
that is to enter in sudo
apt install docker and that will go
ahead and run the command to install
docker now i already have docker
installed on my system so i'm not going
to do that right now so i'm just going
to delete that out but um what i am
going to do is check to see what docker
images i have installed on my system and
to do that i'm going to type in
sudo docker images and when i run this
this will actually show how many images
i have and right now i don't have any
images at all so when we go ahead and
install an image there are two ways you
can create an image for your dark
environment one is to get one from
docker hub and the other is to create
your own image and let's go ahead and
get one from docker hub and so the
command line that we'd want to write is
sudo darker pull
redis colon and after the colon we can
actually write the name of the actual
file that we're going to be connecting
to from docker so i'm going to go ahead
and go to docker hub and i've already
logged in with my user id and account so
i'm going to go to docker hub and
hub.docker.com
is the website address and i'm already
logged in with my simply learn account
and what you can actually see is
different repositories that i have
already created below and we clicked the
explore button
and i can see the the redis image and
these are all the different commands
that i can actually use all the
different variations that i can create
so i can i can do 5.0 i can do a 32-bit
version i can do um a 4.0 version or a
3.8 version or if i select latest that
will allow me to just pull the latest
version even if it's updated so i'm
going to go back to my command line and
do latest and what's going to happen now
is that we're actually going to pull the
files live over the internet and create
the docket image so it's going to take
just a few minutes of course if your
internet speed is faster then this will
happen a lot faster
yeah here we are we have everything
downloaded which is great and so let's
just go and check and see if the image
has been added to uh docker so we do
sudo docket images and now you can
actually see that we have the latest
redis um image and it's given the
default image id and there's the size of
the docker image and the tag is latest
so it's the latest version of redis
and i'm just checking to see if there's
any other images running and no there
isn't so let's go ahead and now create a
container
and so the command for that is sudo
docker run dash d dash p
0.0.0 i think it's dot 80 colon 80 space
redis colon latest and let's run that
got an error message okay now you know
that it's a real demo this is really
live so i'm just going to clear the
screen and um let's write that in again
and so sudo docker run
d dash p and it's
0.0.0.0 colon 8080 ready to start late
they have current latest and there we
are everything's up and running awesome
so let's just go ahead and check that we
actually have the image running in
docker so sudo docker piers and there
you see that we actually have the latest
redis version and it was created 16
seconds ago we have a new container id
for it and there's our port that we've
created
and let's see if we can view all the
running containers and sudo docker ps
dash a there we are and we created that
container 42 seconds ago one of things
you'll notice is um the name for the
docket is created automatically by
docker and here we have stupefied angle
bars is the name let's change that name
to something that's more meaningful for
the work that we're doing so that's
actually fairly easy too and it's going
to be our final task here so we're going
to type sudo darker and the command is
rename and we want to copy over the old
name and then we put in the old name and
then you do a space and then you put in
what you want the new name to be in
which case we're going to make it simply
learn we run that command that's good
now let's validate that everything has
to be changed so we're going to do sudo
docker ps dash a on just ps and there we
are everything has been renamed and that
gets you started working with docker now
let's move on to ansible so before
ansible the ability to be able to deploy
software across a network was pretty
hard and you know you couldn't guarantee
that your web servers were all
consistent and that the database
environments were consistent this was
okay if you had a small environment but
once you actually started getting into
more complex environments it became very
difficult to manage ansible is code that
allows you to be able to consistently
and reliably create and stand up the
environments that you need to be able to
store your code for production so
whether or not you require one server
environment or you need a thousand
server environments using tools like
ansible will allow you to be able to
consistently deploy out environments
that all look the same so essentially
ansible is a configuration management
tool and it allows you to be able to
deploy automatically to a large variety
of environments the actual tool itself
is a push-based configuration tool it is
an agent-less tool so there is no
communication coming back about the
health of the environment and we
actually have tools that we'll cover
later on in the video that cover that
answer does have a consistency of its
products performance so you know that if
you're using ansible that it's going to
work it's just a very consistent product
and it uses ssh for very secure
connections so it doesn't matter whether
or not you have one or a thousand
pieces of hardware you're looking to
configure you can actually be rest
assured that there'll be no security
infringement because of the use of ssh
now we're look into the architecture of
ansible it's a very simple architecture
there is the ansible management node
that has playbooks which contain the
instructions that you would have for
your environment and that playbook is
then pushed out to all the different
nodes that contain the hardware that you
wish to configure the actual module
collection is the configuration code
itself and the playbooks are the the
instructions of that configuration and
then the inventory is the document of
the different groups and hardware within
those groups that you would be pushing
out too so we're going to go ahead and
install ansible onto system here and
we're actually running our environment
in a linux environment right now um so
we have a master environment and an
extended node but we're actually going
to go into our terminal window and we're
going to do everything on the master
node so the first steps we want to do is
actually go ahead and install ansible
and we're going to do that by installing
sudo yum install ansible dash y
and this goes ahead and downloads all
the files that we need and starts to run
the installation
and let's go ahead and see if it's
installed correctly so we're typing
dash etc
ansible slash hosts and there we see we
actually have a default ansible hosts
already up and running which is
fantastic and we want to grab this ip
address which is ansible servers and
then the ip address with the password
and root access and the ip will be
specific to your local client so this is
my client system so if i type in my
config i'll actually see that the ip
address matches up and then you want to
make sure the ip address is correct for
your environment and so this is
essentially a node in the ansible
environment and this is the ip address
and the password for that node so you
can access effect um correctly so we'll
hit colon at the bottom of the screen
and take ourselves actually into ansible
and we're going to do now is create a
playbook to be able to run a sample
script so we're going to call it vi
sample dot yml and here we have a sample
playbook we have the name we have the
host we have the root user and we have
the task and in the task you actually
see that we're looking to create a
virtual web page with the content of
welcome so that we can actually see that
the web page is loading and all of this
is written with a very basic xml format
called yml which stands for yet another
markup language but it's very flexible
and it's very easy to learn so let's see
if we can go ahead and actually run this
playbook so you can actually see it in
action
and so we're going to type in ansible
playbook
sample.yml this will run the sample yml
script dash dash syntax dash check
let me tell you ansible dash playbook
sample.yml
so i had some issues there so i'm going
to try that again and
let's
go in and write the
command again so answer dash playbook
sample.yml
syntax dot dash check and then run the
actual playbook itself and
let's see if it works yes everything's
working and it'll take a moment for
everything to initialize and now what we
want to be able to do is go back to our
web browser and see whether or not we
were able to create that default web
page with the word welcome in it because
that was the command that we created in
our playbook and there we are there's
the word welcome exactly as we planned
and we can go back and we can review the
playbook that we created just to
validate that so let's go back into
screen here and let's go in and look at
the uh yaml page and there we go we see
that the word welcome is there exactly
as we expected now step into our final
area which is network monitoring using a
tool called nagios so before you had
nagios sometimes you didn't know whether
or not your network was working in fact
the worst case scenario was when your
customers caught up and said hey your
website's not working you know the
challenges you had is that you didn't
know
how the usage of the server was being
managed you didn't know whether you were
up to 80 usage um utilization on that
server or if you were at 50 it was
difficult to get that information easily
with nagios you're able to set up email
notifications and other notifications
that allow you to stay ahead of issues
that you would have within your
environment so with this example if
you're looking for memory utilization
you can receive a notification telling
you that your memory is almost full on a
particular server and that you should be
setting up a new server in this instance
use ansible to set up another server so
again it's an as with the other tools
we've been covering nagios is an open
source tool and it's specifically for
monitoring systems servers and networks
within your infrastructure very easy to
use it has comprehensive monitoring
tools that you can set and control it is
highly available and it is designed
specifically at problem remediation you
want to set it up so it is the eyes and
ears of your network without you having
to constantly monitor your network so
the way that niges works is that it's
just constantly checking out your
environment so the negative server is
constantly checking the status of your
systems and then is able to create
visualization tools of that data so you
can actually then make quick responses
or you can even send out alerts such as
an email alert or push notification
alone in this video we're going to take
you through the steps that you need to
follow if you want to set up nagios and
the dashboard nagios provides to be
helped to help monitor your network so
the first thing you want to do is go to
nagios.com and you can either download
the application or you can actually do
what we're going to do here is actually
just log into the actual application
running on the nagios website so for
that you want to go to nagiosna.com
and you can use the administration or
normal user access
ids that nagios provides and we've gone
ahead and we've actually pre-filled in
our id with net use admin and negatives
admin for the user id name and password
of course you wouldn't do that in all
real life um so that brings us into the
nagios dashboard and what you're going
to notice is that there are three
distinct areas that you should want to
focus on um you have the fixed sources
which is all your actual physical
hardware that you're monitoring you have
alert summary and abnormal behavior so
let's go check out the fixed sources and
so we have two default fixed sources
here and one in chicago one in tulsa um
and you can go ahead and set up
additional sources but what you'll see
is that this surprised a quick dashboard
of the activity in the last 30 minutes
the amount of usage on the disk you can
go ahead and um to the behavior tab and
you can modify how you're monitoring and
updating the actual source itself and
then the data from these reports
will be from these sources we'll
actually be then using reports queries
and alerts that you will use so we're
gonna go back to the board and here you
are there is the two fixed sources so
let's look at load summary when you
expand that out and the alert summary
provides a just a overview of the alerts
that are being managed the alerts that
you see here are the default alerts that
have been created as part of the test
environment um it's very easy to create
alerts and in this instance you can see
that the chicago source is being used to
manage the alert what you can do is when
you're actually setting up your own
sources you'll see that the alert will
then point specifically to the source
that it's um pulling the data from so in
the powerpoint in the presentation that
we just went through you actually see
how nagios and edgy is the center of
collecting all the information but it's
actually pulling the information from
the different networks and environments
you have and then it stores that data
locally to be able to create the
visualizations and the alerts and we
have here just a quick overview of an
alert here inside of the summary which
actually shows green which shows that
the alert is actually good now there are
three different types of colors there's
green yellow and red but as you can see
we have no critical alerts right now and
you'll see the same thing happening at
abnormal behavior as well where it
actually goes ahead and shows you the
different colors that are available the
thing that's great is that as soon as a
warning status is created then it allows
you to trigger an email notification
that gets sent out to you and has
information in the email which you can
then action so the alerts are very
important and the alerts again are
pulling the information from the fixed
sources so the abnormal behavior is just
gonna zero in and show any specific
abnormal behavior that would be
happening with your fixed sources and so
in this instance it's around bandwidth
and everything is green so everything's
looking good so we're going to the
source tab and we're going to look at
the bandwidth graph and this is a more
detailed visualization of the bandwidth
that's happening through the chicago
fixed source again you can create your
own fixed sources and you'll be able to
see the actual usage um such as the
bandwidth graph for that fixed source um
on this screen and at the bottom of the
screen we have just kind of the the most
popular information that um most network
engineers are looking to assess on how a
network is being used so we go ahead and
we can select reports and we can
actually come in and we can either
create our own custom report or you can
use one of the many saved reports are
already there if you're new to nagios
use the save reports because you'll be
able to get in and get the great
visualization tools that negios provides
very quickly and be able to then action
the information visually from what
you're looking at these reports are
absolutely fantastic and they're also
really good to send to other team
members that would have an interest in
understanding the network usage um and
the different areas that you're managing
uh now if you want to get really
specific on the type of data that you
want in the report you can actually go
in and create a query and the query can
be customized to really however you want
it to to work the points of view are
very specific insights into different
data points that interact and they're
very easy to create and a lot of them
have been created as you can see on the
screen already and as you can see on the
left hand side there is an edit button
that allows you to edit and modify those
points of view very quickly now we're
going to go ahead and create one right
now and we're just going to put one in
and put the limiter for an ip address
we're going to change the dead lifetime
we can either do hours days or weeks we
do 24 hours that's fine so we'll just go
ahead and we'll see what we have on the
screen and as as you can see there are
just a lot already created select one
we'll edit the ftp and we'll see what
the delay time for that is and it all
looks good so a query will show all of
the saved queries you've created but you
can actually go in and modify a query
very easy um you can go in and create a
query and really essentially ask it to
pull any kind of information that you
would be looking at analyzing that has
been captured by nagios and there's a
dictionary you can use that nagios has
on their website that allows you to get
better access and understanding the
querying constructs you can put together
the alerts in my opinion are the most
important part because these are the
actual actions that interact with the
data sources and will trigger alerts
that get sent to you so if we go back to
the presentation again you can see that
we're analyzing a lot of data but it's
the data that gets sent to you as an
alert that allows you to immediately
understand why you need to be able to go
in and check out your network and this
is why you're really using nagios
because you need to have those alerts
and so what we're going to do here is
we're going through the steps you use to
create an alert and we're just going to
do an alert here that is going to be on
analyzing bytes of traffic and the final
step is to actually go ahead and save
the alert and really put in what the
alert notification model uh method is
going to be and the default is to use an
email which is great but you can use a
nagios alert or as you know smp a simple
network protocol blood um or just use a
and a command email is definitely the
easiest one to use and so getting
started i would recommend you use email
so we're going to go ahead and we're
going to create one and do one on
workflow and we're going to set up the
way which we're going to manage this so
the threshold is going to be 34 bytes
with a critical threshold of 50 bytes
which is very small i expect an alert to
be triggered because of this and we're
just going to use the default emails to
send out and you'll see by default that
um the
new alert is now in pending and the
reason why we have impending is that
it's just there to
be reviewed by the system to make sure
it meets all the standards and i don't
think there'll be a problem and i'm just
looking at some of the other different
commands we have here so if we go back
and we can get alerts from other
negative networks we connect two
different networks right from this
configuration screen and same thing with
the smp receiver now we can use a simple
network management protocol to connect
to other receivers and the command tab
allows us to create scripts that can be
triggered from
alerts that get created so you may want
to have something that triggers a
rebuild of a network environment which
would allow you to authenticate through
other systems now of course there are
other things you can set up in the
negatives dashboards so you know things
like information on your profile you
know defaults here that such as language
and your username you have the same
thing for a system administrator where
you can actually go in and set defaults
for the assist admin there's ldap ad
integration for simple and for example
authentication go back to the main
dashboard if we scroll down you'll
actually see that we have an alert has
been approved that's the alert that
we've created before and it's now saying
critical and because more than 50 bytes
of information have been sent over our
test network in the uh fixed source of
chicago surprise price 50 bytes is very
easy to do so it's good to see that
the
system is working correctly and this is
how our nagios environment works and
from here the next step is for you to be
able to set up and configure nagios to
run with your infrastructure that's it
we've covered everything that you need
to start up your devops environment and
we encourage you to be able to go
through and play with the different
tools if you have any questions please
post them in the comments below we
always respond to questions that you
have there we have a team of people
looking at those questions today and if
you like this video we have a
significant number of other videos and
all you have to do is either go through
our video catalog or hit the subscribe
and like button and you'll be notified
when new videos drop have a great day
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos to nerd up and get certified
click here