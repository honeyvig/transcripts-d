are you prepared to unleash the oh
inspiring power of AWS and embark on an
exhilarating journey into the realm of
cloud computing get ready for an
extraordinary adventure with AWS Basics
the Ultimate key to unlock the door to
Limitless possibilities and shape the
future of your business like never
before picture a word where your
business solves to new heights of
efficiency effortlessly adapts to any
Challenge and fuels groundbreaking
Innovation welcome to the realm of AWS
where imagination becomes reality and
the cloud becomes your greatest Ally but
wait there's more prepare to be amazed
as I reveal a jaw dropping fact AWS Ryan
Supreme has the Undisputed champion of
the global public Cloud Market boasting
a staggering market share of over 30
percent and hold on to your seat because
by 2023 the global cloud computing
Market is projected to Skyrocket Beyond
a mind-blowing 800 billion dollars the
demand for AWS Services is soaring
higher than ever and the opportunity for
you you to seize this is truly Limitless
but AWS is not just a catalyst for
Success it's a gateway to an
extraordinary realm of boundless
potential and game-changing
Transformations from ambitious startups
making them Mark to multinational Giants
rewriting the rules of entire Industries
organizations to all sizes are turning
to AWS to propel their digital
transformation streamline operations and
craft Unforgettable customer experiences
now let's delve into the figures that
hold the key to your future success the
ones that will ignite your career like a
blazing firework AWS professionals are
in such high demand that they command
top dollar salaries with entry level
positions boasting jaw dropping ranges
of sixty thousand dollars to hundred
thousand dollars per year and as you
ascend the ranks gaining Mastery and
experience in AWS you will unlock even
more thrilling opportunities in this
rapidly expanding field so if you are
prepared to embark on a career path
pulsating with excitement Innovation and
Endless Possibilities look no further
than simply lunch Caltech postgraduate
program in class out Computing this
encompassing program will arm you with
the knowledge and skills needed to
navigate the vast cloud computing
landscape with unyielding confidence
delve into the depths of cloud computing
architecture concur deployment models
fortify security like a digital Fortress
and master migration strategies that
leave a lasting impact discover how to
harness Cloud computing's full suit of
services to construct applications that
are not just scalable but remarkable
works of art don't let this once in a
lifetime opportunity slip through your
fingers ignite your career and join the
ranks of time cloud computing
professionals check the link in the
description box to unveil the wonders of
the AWS Basics course immerse yourself
in O aspiring success stories from our
satisfied Learners who have harnessed
the boundless potential of AWS to
achieve the extraordinary the future
resides in the cloud and AWS is eagerly
waiting your arrival are you ready to
seize it and so to unimaginable Heights
unleash the magic of cloud computing and
its platforms like AWS Azure and gcp and
witness The Floodgate is open as your
website attracts a tidal wave of organic
traffic and enjoys unrivaled success in
the digital realm let's get started with
I will be permanently moving to Europe
with my family show sampling course has
helped me back a great job offered there
will be the 60 salary high we cannot
wait to start our new life in Finland we
are also excited hey I am
and I am currently living in Nigeria I
am working as Chief Information
Technology officer at Aries tires after
working for 23 years in IIT sector in
Nigeria I felt that my career had become
stagnant here I wanted to experience
what is like to live in Europe so I
started applying to various jobs there
as someone who is responsible for making
the decision I wanted to upgrade my
skills I also realized that to get a new
job I needed to have a professional
certification in cloud computing so last
year I started the postgraduate program
in cloud computing in association with
Caltech constant radio
it's not easy to get access to caltec
but simply didn't provided that with
this course I had the choice to study
while I continued with my job
either Technologies like ews and Azure
during the course luckily my new
employers were looking for someone who
knew these Technologies I have just
copulated the course but my new
employers were aware that I was taking
this course
and that is how I got this new job offer
with 60 percent salary hike in Finland I
will be leading the cloud computing unit
of my new company
I am really excited not just for myself
but for my family as well I have 30
dollars I think I will be able to help
them with their higher studies in Europe
as a father you always want to make
decision that have a positive impact on
your children as for me I am going to
live my dream now I think dreams come
true when you work hard to achieve them
meet Rob he runs an online shopping
portal the portal started with a modest
number of users but has recently been
seeing a surge in the number of visitors
on Black Friday and other holidays the
portal saw so many visitors that the
servers were unable to handle the
traffic and crashed is there a way to
improve performance without having to
invest in a new server wondered Rob
a way to upscale or downscale capacity
depending on the number of users
visiting the website at any given point
well there is Amazon web services one of
the leaders in the cloud computing
Market before we see how AWS can solve
Roth's problem let's have a look at how
AWS reached the position it is at now
AWS was first introduced in 2002 as a
means to provide tools and services to
developers to incorporate features of
amazon.com to their website in 2006 its
first Cloud Services offering was
introduced in 2016 AWS surpassed its 10
billion Revenue Target
and now AWS offers more than 100 cloud
services that span a wide range of
domains thanks to this the AWS cloud
service platform is now used by more
than 45 percent of the global market now
let's talk about what is AWS AWS or
Amazon web service is a secure cloud
computing platform that provides
computing power database networking
content storage and much more
the platform also works with a PSU go
pricing model which means you only pay
for how much of the service is offered
by AWS you use
some of the other advantages of AWS are
security AWS provides a secure and
durable platform that offers end-to-end
privacy and security
experience you can benefit from the
infrastructure management practices born
from Amazon's years of experience
flexible it allows users to select the
OS language database and other services
easy to use users can host applications
quickly and securely scalable depending
on user requirements applications can be
scaled up or down AWS provides a wide
range of services across various domains
what if Rob wanted to create an
application for his online portal AWS
provides compute services that can
support the app development process from
start to finish from developing
deploying running to scaling the
application up or down based on the
requirements the popular Services
include ec2 AWS Lambda Amazon light cell
and elastic Beanstalk for storing
website data Rob could use AWS storage
services that would enable him to store
access govern and analyze data to ensure
that costs are reduced agility is
improved and Innovation accelerated
popular services within this domain
include Amazon S3 EBS S3 Glacier and
elastic file storage
Rob can also store the user data in a
database with aw Services which he can
then optimize and manage popular
services in this domain include Amazon
RDS dynamodb and redshift if Rob's
businesses took off and he wanted to
separate his Cloud infrastructure or
scale up his work requests and much more
he would be able to do so with the
networking Services provided by AWS some
of the popular networking Services
include Amazon VPC Amazon Route 53 and
elastic load balancing other domains
that AWS provide services in are
analytics blockchain containers machine
learning internet of things and so on
and there you go that's AWS for you in a
nutshell today we're going to talk about
AWS in 10 minutes let's look at the
agenda here
let's talk about what AWS is
why it's such a big hit
we'll have a look at the overview of the
services we'll see how much it costs
how big it is and what the future of AWS
is
so what is AWS everyone is talking about
Amazon web services but let's have a
look at what it is
basically a global Cloud platform which
allows you to host and manage his
services on the internet
it's used by almost 80 percent of
Fortune 500 companies to host their
infrastructure and it has a lot of
services which it provides to its
customers there's infrastructure service
which means they provide bare servers as
a service so you don't need to manage
the backup and the power supply of the
service
they provide platform of the service
you can get Java Ruby PHP as a service
so that you don't have to manage the
binaries of these applications
you get software as a service wherein
you get email sending capabilities like
SES
you get queuing services like sqs and
it's a cloud storage platform wherein
you have a lot of storage options
including EBS and S3 so all in all AWS
is a hosting provider which gives you a
lot of services wherein you can run your
applications on the cloud
now let's take a look at why it's such a
big hit
so everyone is trying to use AWS
everyone is trying to put their
applications on the cloud
so what's the reason that AWS is the top
provider and the top choice for doing
anything on the cloud
one of the biggest reasons is the
billing so the billing is very clear you
get a per hour billing
every instance or every service has a
micro billing so be it instances on ec2
you get per hour billing rate which is
very transparent
even S3 buckets are charged on a per GB
basis although it is a storage service
but still there is micro billing
available there
the sign up process is easy you don't
need to sign any agreement nothing you
just go sign up with an email ID add a
credit card and you're good to go
you can go from 0 to 100 in just two
minutes
you can launch your servers big machines
without buying Hardware without
procuring any hardware you can just be
up and running in minutes
so their billing dashboard is also very
simple
they give you an integrated billing
dashboard which gives you reports
you can pull out reports every month you
can pull out reports based on Services
based on various parameters
[Music]
for the cloud provider to be a hit you
need it to be stable it has to be a
trusted thing so their services are
quite stable
in the last seven or eight years they
have seen some three or four major
outages but those have been only region
specific
so that means out of the 12 or 13
regions in which they operate the
outages have been region specific in a
particular country or continent and
those also have not been more than two
or three hours and have not affected all
of their customers
it's a trusted vendor so when we talk
about AWS it comes up as something which
is used by everyone in the industry
from small startups to Big Enterprises
everyone sees Amazon as a trusted
advisor
now let's have an overview of the most
commonly used services
so the first and the most commonly used
service is ec2 which is elastic compute
cloud
this is the service which gives you bear
servers so this service will give you a
machine which you can launch and you can
run your software on those
you can get small or big machines based
on your requirements
the second choice is VPC so Amazon will
not allow you full control of their
Cloud instead they give you chunks of
their Cloud which is VPC or the virtual
private cloud
so VPC lets you create networks in the
cloud and then run your servers in those
Networks
the next one is S3 which is simple
storage service so S3 gives you the
opportunity to upload and share files so
it is mostly a file storage and sharing
service
then you've got RDS which is relational
database service
so this RDS allows you to run and manage
databases on the cloud so they've got
almost all the major flavors of
databases right from SQL Server to
Oracle and MySQL postgray SQL
they have recently launched another one
which is called Aurora which claims to
be a very high performance database then
Route 53 is there for DNS so they've got
a managed DNS service wherein you can
point your DNS to Amazon and they take
care of the stuff
so it's a global DNS service it's a
scalable DNS service so it scales
according to demand
there's also elastic load balancer the
elb is a service which gives you the
opportunity to load balance incoming
traffic to multiple machines so this way
you can scale up your web applications
to any number of users
you've got Auto scaling which adds
capacity on the fly to elastic load
balancers so that your website or your
application is never down due to a load
how much does it cost this per our
billing is already mentioned for
everything if it's something like a
storage thing again there's a per hour
or per GB month storage so I think there
is region specific pricing so Virginia
is the cheapest region out of all of
them so the region-specific pricing is
because they have got some regions which
they have got good hold of and they are
the headquarters so Oregon and Virginia
are the cheapest actually
they also give you Services based on the
term so if you sign up for something for
a year it would be cheaper for you
rather than signing up for something on
an on-demand basis so they've got
reserved instances which are very cheap
as compared to the on-demand ones you
can get discounts from 20 to almost 60
percent if you sign up for a three year
term
they are spot resources examples of
these are spot instances so this is like
a bidding Market base where you can bid
for a price the only downside of this
kind of pricing is that your machine
might be terminated or your resources
might be terminated if someone bids
higher
people use these kind of things for
doing some ad hoc in search or some ad
hoc tasks which are really not critical
how big is it they have got 15 regions
across major countries of the world
they've got regions in the U.S Europe
Asia Pacific they've got a global
footprint so in today's world if you are
anywhere in the world you would have a
region within 1 000 miles of your
location they've got massive data
centers so each of the region has got
multiple availability zones so one
availability Zone can be thought of as a
big data center
the data centers have anywhere from
three hundred thousand to five hundred
thousand servers
what is the future of AWS
so they currently have 64 Services which
span across infrastructural service
software as a service platform as a
service
they are launching new services in all
demands every day
right now they are focusing on machine
learning so recently they've launched a
couple of services which Focus
exclusively on machine learning and they
are focusing on software as a service
product wherein they want to take
control of the service you want to
utilize they don't want you to do it
they want you to upload it to them and
every now and then they keep on reducing
the costs so you would hear it in the
blog that okay the price of ec2 machines
has been reduced
and this is because of their scale so
they scale up and they get the cost
benefit to the customer
that's all for today guys with just a
quick intro of the Amazon web services
Cloud talk about AWS here by the way I'm
Sam I work as a cloud architect and
Technical author with simply learn in
this section we're going to talk about
what is cloud computing and how things
were before AWS and we're going to talk
about what is AWS and the benefits of
AWS and the products and services that
Amazon offers as of now and then finally
we're going to end the session with a
project that is going to help us
understand how to create Jenkins build
server for continuous integration and in
this project we will learn about how to
deploy and host Jenkins and open source
automation software predominantly used
for CI CD otherwise continuous
integration and continuous development
so what is cloud computing cloud
computing is the on demand delivery of
compute resources through a cloud
platform accessed via the Internet and
it is is a it's also a pay as you go
service be it compute capacity or
database or storage in Cloud we access
them through the internet and that too
on a pay-as-you-go pricing model let's
talk about how things were before AWS
existed Expedia as we know as the people
Logistic Company or the company that
helps people move around the world with
ease has a certain problem before they
decided to migrate to AWS and migrating
to AWS was a solution to their problem
back then booking ticket was an
unpleasant experience compared to what
it is now it generally would take a long
time to book a ticket and sometimes it
would time out if there is a promotion
or if a huge sale announced and if the
environment is not scaled up to meet the
demand and sometimes it would time out
if there is a promotion or if a huge
sale announced and they could not scale
up the environment and keep up with the
speed of the promotion that's when they
got introduced to AWS and with the AWS
power to increase the capacity of the
environment on the Fly solved their
scaling issue instead of waiting on the
new hardware to arrive and wondering
what to do with the hardware when the
promotion is over with AWS we can
quickly scale up the environment when
there is a need and scale down the
environment during lull periods now that
being said let's talk about what is AWS
Amazon web services or in short AWS is a
secure cloud service platform offering
compute power database content delivery
and other functionality that you would
need to help your business scale and
grow AWS is a very secure Cloud platform
and such security Cannot Be Imagined in
on-premises without spending a fortune
almost any type of applications can be
created and deployed in the cloud which
includes the latest iot applications
machine learning artificial intelligence
and so on and the best part is we can
Avail the service from anywhere as long
as we have an active internet and all
this is availed on pay as you go
subscription now let's talk about the
benefits of AWS AWS Cloud platform is
very easy to use one needs no
complicated programming or compute
knowledge to access and use this
environment even with less programming
and compute knowledge one could become a
probe in accessing and maintaining the
cloud the products and services offered
are very flexible in nature and that
provides room for us to easily integrate
the service to other services in AWS and
services outside AWS cloud in general is
a very reliable service and AWS in
specific is very reliable cloud in
general is very reliable and when you
talk about AWS it's very true a
statistic says that about 95 percent of
the cloud computing in the whole world
is run on AWS due to AWS reliable nature
the services in AWS are scalable by
nature and most of the services provide
click button scaling services and AWS
are very cost effective and it becomes
very true when you start using more
services and AWS are very cost effective
and it becomes very true when you start
using more service services and AWS are
very cost effective and it becomes more
true when we start using more since when
we start using more service the per unit
price drops down even further Security
in the cloud is at its highest with
encryption and a multi-factor
authentications and being able to audit
the logs one can be sure that when one
hosts a service in AWS they are hosting
it in an environment which has been
built with Enterprise Security in mind
all right let's now talk about or talk
briefly about the different products and
services available in the cloud some of
the products and services available in
the cloud are compute storage database
migration Services networking and
content delivery developer tools and
services available for man managing the
AWS environment tools needed for machine
learning development analytic tools
needed for security and identity
compliance tools needed for mobile
application Services tools needed for
iot and gaming development are some of
the products that are available in AWS
all right let's first talk about compute
service now compute service it enables
us to develop lie run and scale our
applications and workloads on the cloud
you might agree with me that compute
service is the fundamental of any
applications and AWS provides many
options to use those compute services in
the cloud starting from ec2 which is a
VM in the cloud to Lambda which is
serverless Computing in the cloud to
compute service compute service also
includes container services and services
that help batch processing and deploying
applications and load balancing the
request when we run our applications on
multiple servers and services that help
batch processing and developing
application and load balancing the
request when we run our applications in
multiple servers we're going to talk
about the two compute products in
specific let's talk about ec2 ec2 is a
service with a resizable compute
capacity in the cloud that allows
business subscribers to run application
programs in the Computing environment
and the best part here is ec2 is
resizable anytime there is more demand I
can simply resize my computer to meet
the demand on the other hand Lambda is a
serverless compute service used to
execute back-end codes it really helps
us to focus on the core competencies
like application building the code Let's
Talk About Storage service for a while
AWS storage is known for its durability
and availability with S3 being the
object storage for the internet where
users or application can directly upload
and download the content from the
internet and EBS being a durable block
storage that gets attached to the ec2
instance and Glacier being the archival
solution in the cloud and storage
Gateway being the Gateway for storing
data locally and having a backup in the
cloud in case of disaster let's talk
about two important storage service
products in the cloud which is S3 now S3
is an web-based cloud storage service
it's designed for online backup And
archiving of data and the best part
about S3 is that it provides 11 9
durability which means once we stored
the data in S3 the chances of losing the
data is nearly zero EBS on the other
hand provides persistent and resizable
and migratable storage to the VMS or EC
tools that we can run in the cloud now
let's talk about the database offerings
in the cloud beginning with RDS for SQL
and dynamodb for nosql and elastic cache
for web caching services and Amazon
redshift for petabyte scale data
warehousing services AWS database
service provides all the database
service needs we might have let's talk
about two database services in specific
RDS RDS is an web service that is
designed to simplify the setup operation
and scaling of the relational database
in the cloud and dynamodb is an fast and
flexible nosql database service which is
well known for its low latency and
scalability performance Network and
content delivery service contributes to
the infrastructure as a service side of
the cloud it provides a secure Cloud
infrastructure and connects our physical
Network to our private virtual network
with low latency and high transfer
speeds and under networking and content
delivery services VPC is virtual private
Cloud networking in the cloud Route 53
is the DNS service in the cloud and
Direct Connect is a service that
connects our on-premises with AWS
environment through point-to-point
cables and cloudfront is content
delivery Network or caching service
throughout the world let's talk about
the VPC see for a while VPC it enables
us to launch AWS resources into a
virtual Network that we Define in the
cloud VPC resembles very similar to our
office network with the subnets
firewalls private zones public or DMZ
zones and lot more we can also create a
VPN connection between on-premises and
Cloud environment through this VPC and
Route 53 it is a scalable and highly
available domain name system web service
that helps to Route the end users to the
internet application now AWS is a domain
registrar as well and here we can buy
domain names for our organization and
use them to mask the IP addresses behind
it and by the way if you didn't know
already DNS is the one that translates
human readable names like
www.xyz.com into the numeric IP address
let's talk about developer tool services
now developer tool services in Amazon
provides services where developers can
store an application source code and
automatically build test and deploy
applications on AWS out of which AWS
codestar enables us to quickly develop
build and deploy applications in AWS and
code build is a fully managed build
service that compiles source code runs
test and produces software packages that
are ready to deploy and code Deploy on
the other hand is a service that
automatic that automates is a service
that automates software developments to
a variety of compute services including
Amazon ec2 Amazon Lambda and even
instances running on-premises and AWS
code pipeline is a continuous
integration and continuous delivery
service for fast and reliable
application and infrastructure updates
let's talk about code star code star is
a service designed for developers where
they can quickly deploy build and
develop application locations on AWS AWS
code star provides a unified user
interface enabling us to easily manage
our software development activities in
one place with AWS code star we can set
up our entire continuous delivery tool
chain in minutes allowing us to start
releasing codes faster and not only that
AWS code star makes it easy for you or
your whole team to work together
securely allowing you to easily manage
access and add owners contributors and
viewers to our project and not only that
AWS code star project comes with a
project management dashboard including
an integrated issue tracking
capabilities provided by Ajira software
with the AWS code star project dashboard
we can easily track progress across our
entire software development process from
our backlog of work items to teams
recent code deployments so on and so
forth AWS code built on the other hand
is an cloud service that enables an IT
developer to build and test code with
continuous scaling now here you pay only
for the build time that you have used
now code build is an fully managed
service that helps compiling a source
course and run tests and produce
software packages that are ready to
deploy AWS with AWS code built we there
is no need for us to provision manage
and scale our own build servers code
build scales continuously and process
and processes multiple bills
concurrently so our bills are not left
waiting in the queue because it's
processed concurrently let's talk about
security identity and compliance
services available in Amazon in general
the security identity and complaint
services in Amazon helps us to maintain
a secure environment in Amazon by
providing services for user
authentication for limiting the access
to certain set of users on the AWS
environment out of which IAM or identity
and access management it's a product
that enables us to manage access to AWS
services and resources that enables us
to manage access to AWS resources and
services in a secure fashion using IAM
we can create and manage AWS users and
groups and use permissions to allow and
deny their access to AWS resources on
the other hand KMS or Key Management
Service is a managed service that makes
it easy for us to create and control the
encryption Keys used to encrypt our data
and Cognito it lets us to add user sign
up sign in and access control to our web
and mobile apps quickly and easily and
Waf or web application firewall helps us
to protect our web applications from
common web exploits that could affect
application availability that could
compromise security or consume excessive
resources and Waf it gives us good troll
over which traffic to allow or block to
our applications by sort of defining
customizable web security rules we can
use Waf to create custom rules that
block common attack patterns such as SQL
injections or cross-site scripting and
we can also create rules that are
designed for our specific application
out of all the four identity and access
management it helps us to control access
to AWS services and resources for our
users with the identity and access
management I can restrict user access or
I can give a user complete access to the
environment if it's an admin I can
assign complete access and if it is a
user I can provide some privileged
access or less or degraded or no access
depending on the role that the user
plays in the organization and KMS is a
service that helps us to create and
control the encryption Keys used to
encrypt the data and also we can use
Hardware security model HSM to protect
the security security of our data in the
cloud also we can use Hardware security
model to protect the security of the key
itself and let's talk about the
management tools and services available
in Amazon management tools and services
it helps us to manage Monitor and
automate all the resources running in
the AWS infrastructure beginning with
the cloud watch a service which monitors
the environment by receiving logs and
creating a dashboard and sending alerts
as needed and cloud formation a service
that a service through which we can
deploy new environments using just
templates and auto scaling a product
that lets us to handle the dynamic
traffic with ease by dynamically scaling
up and scaling down the environment as
needed and cloudtrail is another product
or another monitoring tool that helps us
to monitor or collect the logs of all
the API calls to our environment and
provides a searchable tool based on
keywords and timestamps let's talk about
cloud watch a bit more detail now
cloudwatch is a monitoring service for
AWS with Cloud watch we can collect we
can track the metrics we can collect and
monitor log files we can also enable
Cloud watch to set alarms and
automatically react to changes in the
AWS environment cloudwatch can also
monitor AWS resources such as ec2
instances dynamodb tables Amazon RDS
instances as well as custom metrics
generated by your application and
services and any log files your
applications would generate now we can
use cloudwatch to get a system-wide
visibility into resource utilization
application performance and operational
Health we can use these insights to
react and keep your applications running
smoothly and cloud formation on the
other hand provides a common language
for us to describe and provision all the
infrastructure resources in our Cloud
environment now cloud formation allows
us to set up and model all our AWS
resources so that we can spend less time
in managing those resources and more
time on our application our cloud
formation is really an infrastructure as
a code service where I can build an
infrastructure based on the portable
template I might be carrying all right
in this demo session we'll talk about
how to install Jenkins on Amazon Linux
ec2 instance and before that let's talk
about what is Jenkins and what is the
purpose of it why it is used and all
that stuff now what is Jenkins now
Jenkins is an open source automation
server and it provides a lot of inbuilt
plugins for deploying and automating any
project in short you know it's and
continuous integration server and it is
used for many purposes used for tasks
that take care of continuous deployment
and delivery and it provides these
advantages like the developer now will
not have to focus on fixing errors
because errors are automatically handled
by Jenkins and because of that it leads
to faster development environment all
right so how do we go about doing it
let's get started the first thing is to
go to our console and launch an ec2
instance actually let me walk you
through the steps through the PPT and
then I'll show it in the console so let
me first walk you through the steps in
installing a Jenkins server in this lab
we're going to launch an VM or a virtual
application server to host our Jenkins
installation and then we are going to
deploy Jenkins on top of it and then
configure it as an build server and for
this we will need an AWS account to
begin with and then a bit of experience
in Jenkins and then some familiarity on
ec2 and how ec2 gets connected to VPC
stuff like that so when we're all done
this is how the architect is going to
look like you know users they get
connected to ec2 instance which is a
Jenkins server and then there are slaves
build slaves that can be monitored that
can be used in deployment by the Jenkins
server so let's see how it is done all
right so let's first launch a server Let
It Be and Amazon Linux instance I'm
going to pick T2 micro because of the
free eligibility and because this is a
lab that should be more than enough and
I'm going to keep this as default you
know I'm going to stick with the default
VPC and I'm going to keep all the
settings as defaults and then I'm not
going to add a storage I'm going to keep
the storage limited with the root volume
and then for Security Group because
we're going to access it through the
internet I would like Port 80 in fact
since this is a lab environment and
since I'll be tearing it down right
after the lap is over we're going to use
all IP or we're going to use anywhere
all traffic anywhere View and launch and
I already have a keypad that I have
created if you're not familiar with
creating keypad launching ec2 instance
you might need to spend some time
watching the AWS ec2 video that would
help so I already have a key pair that's
attached to this account in this region
so I'm using that keypair to launch the
instance
right then that's my Jenkins server
alright so once that is installed let's
log into it since this being an Linux
instance I'm going to use putty to log
in and then use the key pair that we
have downloaded to log in
and the key pair was
right here
all right now I'm connected to it as
first thing we will have to install the
Java environment I'm going to show some
commands towards end so if you're
planning to do Labs yourself you can use
those commands they will be very handy
in fact I can show it right now as we
speak
so these commands are to install Java
environment
and it has two versions as we speak and
I've chosen the latest version so I've
typed two and I've chosen the latest
version in installing Java environment
the second is to install Apache Maven
let me show you some commands that's
used to install Apache Maven and these
commands vary from you know Linux
version to Linux version so if you're
using a different Linux version just be
sure that you're using commands which
are respective to that particular
version right let's install Apache maven
looks like some of them are already
installed and it's showing error for
some but anyway it's it's considered
unsuccessful installation and then let's
install Jenkins
and the commands to install Jenkins is
right here so these are to install
Jenkins and I mean to download the
software and install it and then start
the Jenkins and make sure the Jenkins
stays running even after a reboot so
let's run these commands
all right now Jenkins should be ready or
the Jenkins server is installed in our
ec2 instance and it is ready for us to
access and the default Port is port 8080
so let me go to the URL or IP so let me
go to that and then try accessing it
using port 8080.
there you go so at the moment it's
locked it requires admin password to log
in and the admin password is present in
this particular folder and this
particular file in the Linux instance
so let me get that
okay one moment looks like the admin
password is not present just give me one
quick moment
okay it's just a permission issue
let me check the password in that folder
again with the elevated permissions
there you go that's my password
all right once we're logged in we can
install all the suggested plugins
now that's going to take some time to
run and install all the selected all the
default plugins needed to run a Jenkins
server
all right it's done
now as next thing it's going to ask for
the admin username and password you can
choose as an existing administrator or
you can also create an another admin
user in this case I'm going to create
another admin user
all right so the Jenkins server is
installed we can use these similar but
slightly different procedure on the
Jenkins nodes or the slave servers to
make them Jenkins slave and respond to
the bills that we push or receive the
bills or receive the jobs that we push
through this Jenkins server so we have
built this Jenkins server on Amazon ec2
a similar and slightly different
procedures can be followed to build
Jenkins build servers that can contact
Jenkins server to receive jobs and you
know start working
when we talk about networking in AWS
essentially we talk about how you can
isolate your resources in the cloud
that's something that is a key concern
or key requirement for all the
organizations that are moving their
workload to the cloud how do I do that
and you can do it using VPC the virtual
private Cloud we'll take a look at
virtual private Cloud as well then you
have something called elastic load
balancing where you can say I have many
applications and I want you to do a load
balancing for those applications and AWS
gives you out-of-the-box functionality a
managed service called elb elastic load
balancing which will do just that AWS
also has something called Route 53 this
is a fully managed highly available
Cloud DNS service and what it does is it
basically acts like a phone directory so
you put your IP address and and put your
website address and it converts that to
the IP address and then brings you to
your source which could be the S3 bucket
or your load balance or really anything
foreign
let's go to console and then take a look
at this thing right now I am in ec2 and
then I am at the load balancers if I
click on the load balancer I can have
two types of load balancer where I have
an application load balancer so think
about this if I am a retail website and
I have something like apparel I have
sports outdoors all of these sections
and I have a social section as well on
my website and I want load for all these
sections to be managed separately and
then what I can do I can come over here
and I can set up some Target groups and
then I can distribute my load for all
these sections of my website so
essentially I want to change the load of
my traffic and I want to divert that
traffic and I can do that all from here
and this again is available for HTTP and
https if you want to do a load balance
again a network Lan like a TCP or SSL
then I'm going to choose the classic
load balancer VPC or virtual private
Cloud again is a place where you can
isolate your resources in Cloud this is
one of the places where you can use a
VPN connection to create an encrypted
ipsec tunnel to talk between your data
center and your resources and clouds so
your VPC can talk to your resources in
your data center and you can do that by
using a customer Gateway at your end
your data center end then use a virtual
private Gateway at your Cloud end
there are firewalls that you can set up
which are called network access control
lists you have subnets which is a
logical grouping of your resources so
you would have resources who would talk
to the internet you would put that in
your public subnet by using internet
gateway attached to that or you can have
subnets that would not be talking to the
internet but would be talking internally
instead compute is one of the foundation
Services of AWS
in fact I was attending one of the
trainings at AWS and they told us that
73 percent of total revenue of AWS is
through ec2
that's a server in the cloud a compute
in the cloud so let's look at four
offerings among many that compute gives
us so the first one is light sale and
there are people who love to build their
servers have their databases put on the
firewall settings and then install a lot
of things but then on the other hand
there are people who simply want
something out of the box and they don't
want to provision a server to do
everything on end and they want it for a
monthly small price for all those use
cases we have something called light
sale which is one of the newest Services
launched by AWS and the plan starts at
about five dollars a month
and it involves the VM the storage the
data transfer you can access search it
from the console and you can have a
static IP or DNS management right from
the console and you don't need to worry
about it
we'll take a look at it in our demo
section then we have ec2 so this is
where you install any application that
you would want this is essentially the
compute in the cloud where they have
virtualized your services and then you
can access it anywhere from the world
using the internet they have multiple
pricing options on demand on reserve on
the spot so if you have your workload
which you know is going to run for a
very long time one year two years three
years you can go ahead pay up front
Reserve that thing and get 75 percent
off they have something called spot
option where all of the unused capacity
that AWS has for compute they put it on
auction and then you can put your price
over there and then you can probably get
close to 90 percent off and then it is
used for large workloads with huge huge
huge capacity
if you want to run microservices you
know that is a native support for Docker
in the cloud then ECS is the service
that does that you don't have to install
it you can operate and scale your own
cluster in the cloud and it's all a
managed Service open to you
now let's say you are focused only on
writing code you are a developer or you
want your team to focus only on writing
code you do not want to worry about the
infrastructure part of your application
then AWS Lambda is the service that is
the function as a service
so you don't have to worry about the
infrastructure at all AWS simply tells
you that you give me the code you tell
me how much memory you are going to need
and all I'm going to bill you for is the
time your function is going to take to
run in the cloud and I think it's an
amazing thing if you've heard about the
term serverless this is what serverless
is
[Music]
so we are going to take a look at the
demo of this service as well so let's go
back to console and see if we can take a
look at ec2 so here I am on the ec2
console and if you see here this is my
region and I am in Us East not in
Virginia region
now we are at ec2 dashboard and let's
look at how we can launch an instance
now before we start let's take a look at
the regions so I am in U.S East Northern
Virginia region and these are all the
regions that you can choose from and
this is where I'm going to launch my ec2
instance the first thing that I'm going
to do when I'm going to launch my
instance on the server in cloud is
choose an image which is known as Amazon
machine image Ami and this is nothing
but the software package so I'll go
ahead and I'll choose the default option
that I get over here and you can see
that it has the command line tools
python Ruby Pearl Java Docker PHP MySQL
so you could have anything that you want
and then I'll go ahead and I'll select
this
once I have selected this I'm going to
choose my instance type on hardware and
you can see that right now I have a T2
dot micro which simply is three-tier
eligible and the network performance is
kind of low to moderate but if I scroll
down you can see the network performance
and everything else goes way up and then
I can go and have a different purpose
compute memory fpga the graphics
anything that I want and this is the
variety of services you get with AWS
next step is to configure my install
details so I could have one or more
instances this is my networking so
essentially where do I want to put it do
I want to put it in my network or the
VPC that I have created or do you want
it in another VPC VPC is nothing but
your isolated Cloud your network inside
the cloud
apart from that I can put something
called Advanced details which is nothing
but boot up JavaScript so if you want
something to run when your ec2 instance
is provisioned when it starts this is
where you are going to put that so for
the purpose of the demo I'll keep it
where it is and move on to the next step
which is storage
so essentially your hard drive the root
volume is nothing it is the hard drive
where your OS is installed and then I
can add any more hard drives that I want
and then I can change the size of it if
I want it to be encrypted I can do that
as well so let's keep it simple and move
to the next step which is tags now tags
are used for identification purposes
let's say you have many teams there's a
Java team a dark net team and you want
to keep track of who is using all the
resources
so this is where you can add tags that
say name equals darknet team and this is
how you are going to identify who has
launched this thing
next up we have security groups these
are instance level firewalls so
firewalls for your server so typically
if you want to allow an SSH if you want
to allow an HTTP at https access this is
where you are going to go to add the
firewall rules and you can put the
source as anywhere which means anyone
from the internet can access this thing
now I do not want SSH access for
everybody so I just keep it HTTP and
https
the SSH access will be essentially for
my people that I want and that's about
it I go and I'll say review and launch
and we can review all of the settings
that we have here over here and we will
simply launch and now it's going to ask
for my keypad this is now something that
is required of me to SSH into this
particular instance and I'll simply say
that yes I have access to this thing and
it will launch the instance
that's about it it's as simple as that
to launch your ec2 instance in cloud and
if I come on the dashboard I can see
that my instance is being provisioned
and I have all the information I need
over here then if I want more
information I can click over here and I
can get all of the information that I
want over here
moreover
I have these status checks so there are
two types of status checks that check
the health of the instance so it will
make sure that everything all of the AWS
resources that are required to keep this
resource up and running are running fine
and it will also see if I have something
that is not required in my instance it
is also there to make it up and running
now that our instance is up and running
I can click on it and I can connect
using the console so I don't have to use
a command prompt I can click on the
console itself and I can do whatever I
want
on the other hand if I take the IP
address and if I go here there it is
out of the box I see my WordPress site
up and running I don't have to do
anything it is right there for me to use
and I can just customize it any which
way that I want let's see if I can have
the SSH window open here and I can just
use the login credentials that I have
there it is I can simply log into this
and start playing around with it I can
also see all the metrics I can see the
networking I can take a snapshot I can
do pretty much whatever I want this is
something that if you have your own
virtual private server in the cloud with
all the resources that you need you can
run your small applications on it you
can run your WordPress site on it you
can do so many things there are so many
use cases that you can use light sail
for it is one of the most popular
compute services that was recently
launched let's look at Lambda so we come
to compute and then we click on Lambda
this essentially functions as a service
you can see that I already have
something that is already there so let's
create a Lambda function we'll take
something that is provided out of the
box for us you just click on this and
then we click on an SNS message so this
is like the messaging service of AWS
simple notification services
and we can enable trigger the SNS topic
is incident response and let's click on
it now you can see I can type my
function name so I'll simply say
my Lambda function
and the language that I'm choosing Edge
node.js I can also have C sharp Java
python
Etc and this is my simple code and then
I have environment variables I can use I
will create a new role and then I will
name my new role
my Lambda
SNS
role
I'll keep everything as it is and that I
would simply say go next then I'll
create a function now this function
would be using some memory and this
function will take some amount of time
to execute that amount of time is what
AWS is going to charge me for
I do not get charged for anything else
so here my Lambda function is ready and
if I click on test
then I can simply say
there it is SNS
say and test
and it should give me something that
says hello from SNS
let's look at this function at the
detail that we got if you see here that
is the duration so it ran for 2.84
milliseconds and then the billing
duration or the build duration is 100
milliseconds the resources configure so
I said that I needed 128 MB of ram to
run this thing and the maximum memory
that I actually used that this function
actually used was 90 megabytes so that
is all I need to do I need to upload my
code and then I need to say how much
memory I'm going to use and it can be up
to 512 MB there's only one more
limitation that says that our function
should not run for more than five
minutes essentially a micro service
would not necessarily run for five
minutes if it's running for more than
five minutes then it's probably the best
idea to break it down into several more
functions we get billed by milliseconds
and the best part is in the first year
you get something like 1 billion
milliseconds available in the first year
go and test this thing this is one of
the most interesting things that you
will find as a developer as an
organization you want your people to
focus on the code you know how easily
you can push your code to production to
the customers and you don't want to
worry about infrastructure so this is
one service that is really worth
exploring let's take a closer look at
light sale the newest addition to
compute now this is one of those things
which will give you a complete package
it starts at less than five dollars a
month so let's create an instance and
right now I am in the Mumbai region and
I can change my region so let's put it
in to Virginia
we can pick up our instance image if I
want only OS operating system I can do
that if I want apps and Os I can do that
as well so I'll keep it to Wordpress
I want to make a WordPress website on my
server I can add any script that I want
to fire at the bootstrap I can also have
the SSH keypad that I will use to log in
from the console for light sale and then
depending on how many resources I need I
can choose my instance plan and it goes
all the way from five dollars a month to
eighty dollars a month if I'm running
some really heavy duty application on it
but because I'm only running my
WordPress website I simply need the
basic one
then I am very good with 512 MB RAM one
CPU and one terabyte of data transfer
then I want to name my instance and I'll
keep it the way it is and here I can
change the quantity as well if I click
on it then that's about it that's about
all I need to do to have my instance up
and running right now it is in a pending
State now this is something that is
really good for the virtual private
server I don't have to worry about ec2
instance I don't have to worry about
security patches I don't have to worry
about the firewalls it's all available
to me in one small box so all I do is
come here I click a few things and my
instance is up and ready
so if you are prepared to embark on a
career path pulsating with excitement
Innovation and Endless Possibilities
look no further than simply lunch
Caltech postgraduate program in cloud
computing this encompassing program will
arm you with the knowledge and skills
needed to navigate the vast cloud
computing landscape with unyielding
confidence delve into the depths of
cloud computing architecture concur
deployment models fortify security like
a digital Fortress and master migration
strategies that leave a lasting impact
discover how to harness Cloud
computing's full suit of services to
construct applications that are not just
scalable but remarkable work so far
don't let this once in a lifetime
opportunity slip through your fingers
ignite your career and join the ranks of
time cloud computing professionals check
the link in the description box to
unveil the wonders of the AWS Basics
course
let's talk about databases in AWS there
are a host of options to store your data
by using multiple database services in
AWS so quickly speaking you have RDS the
relational database service that
supports many engines then there's an
offering from AWS that is known as
Aurora it is a fully managed MySQL
compatible relational database service
then you have a nosql service offered
from AWS that is a fully managed Service
as well it is called
dynamodb we are going to take a look at
that then you have redshift that is a
petabyte level data warehousing service
again it is a fully managed service then
you have elastic cache that supports in
memory cache two very popular engines
such as memcached and redis then you
have a very cool service called database
migration service that basically allows
you to migrate your database from Oracle
to Aurora you can use the service and
then there is no downtime at all when
you use this service
so let's take a look at RDS
dynamodb elastic cache and redshift so
once you're in AWS console you can see
that this is the RDS and it is a
Management Service
which essentially means that batch
upgrade operating systems installation
are all taken care of by AWS then what
we are responsible for is when we store
our data we have to provide permissions
to set up the firewall settings and to
allow access for everything then of
course we are responsible for setting up
the databases but the engine is provided
by AWS
then if I click on this thing let's say
for example I want to fire my SQL
instance and if I click on this thing
I'll just show you how the high
availability as well as the backup is
taken care of by AWS
so I'll select my server my Hardware
that's based on my workload and then I
can do something called multi-az
deployment when I do this it basically
asks if you want to have a master save
configuration a high availability
configuration and when I do that then
yes I do get copies of my database in
the multiple availability zones within
the region and these are synchronized
replicated copies so in the event of
failure I will automatically be switched
onto the other availability Zone that
has the same copy of my database so
let's quickly add the dbe identifier and
the username and password so I'll just
call it MySQL for the sake of Simplicity
let me just copy and paste all these
okay it doesn't like that okay so it
doesn't like my SQL database and let us
copy it all
over
and then let's go to the next step
foreign
now when we go to advanced settings I'll
show you that they have something called
backup and then this backup is
automatically provisioned by AWS and you
can see here that there is a maintenance
window as well if I want I can give a
window then this is where you should
take my backup and the same thing for
the maintenance now these are all the
things that are taken care of by AWS and
in a traditional environment we have to
do all these things ourselves so this is
the beauty of using AWS that we don't
have to worry about the patches we don't
have to worry about the backup we don't
have to worry about the high
availability it's just configured and
all we have to do is set some security
rules some firewall rules and some
accessibility rules for the users
coming back to the nosql offering of AWS
so this is Amazon dynamodb again it is a
very fast and flexible no SQL database
so for all of your iot platforms for
your mobile for your gaming this is the
AWS service that you are going to use
because it is a schema-less database
essentially where you can have a keypad
value and you don't have to worry about
whether I'm going to store one value or
do I have to store values in all columns
as well it deals in something called
read capacity unit and write capacity
unit for your read requirement and For
Your Right requirement and that's how it
works you don't have to worry about the
provisioning of resources it scales
automatically
redshift is a petabyte scale data
warehousing solution and again it is a
managed service so you don't have to
worry about you know the OS level or the
patches the same thing goes with elastic
cache as well it supports the very
popular memcache and redis engines and
it is a fully managed service so you
don't have to worry about management or
scaling of all of the memory in these
engines
now let's look at storage services that
AWS offers
now storage makes up a very big part of
the services that AWS offers in the
cloud they are heavily used by multiple
organizations
individuals consumers around the world
so let's go and look at a few things so
EBS is essentially your block storage
your hard drives they are low latency
and all of your web servers and your
database servers your analytics engines
they all use EBS elastic block storage
let's say you have a requirement to have
shared file storage that is where you
are going to use elastic file storage
and again it provides High fault
tolerance with consistent latencies and
you pay only for what you use so you do
not have to pre-provision it just in the
case of EBS you need to pre-provision it
then you have S3 a simple storage
service and one of the most popular
Services of AWS launched way back in
2006 it is secure it has 11 9's
durability and it scales past tens of
trillions of objects and it has many
many use cases most importantly for
storage backup and Recovery tiered
archiver we're going to look at a demo
of that thing and it also acts like a
data link for big data analytics now if
we have something called code data that
we want to keep for our audit records or
some archival records AWS has something
called Glacier that is very very
economical archival service the rates
start as low as like 4 cents a gigabyte
which is very very cheap you also have
something called storage Gateway where
you apply this virtual device to your
data center and then you can store all
data through the storage Gateway in the
cloud this is one of the very popular
services in the hybrid environment where
you want to keep some data in the cloud
and some data in your data center
there are a host of data transfer
services which help you move your data
to cloud and it can go from a small
amount of data that you can transfer
over the internet to exabytes of data
that you can transfer through truck to a
truck to AWS storage services so you
have something called data connect where
you connect directly to AWS Regional
data transfer you bypass the internet
and then your network is connected then
you have something called snowball which
is like a box where you migrate
petabytes of data in batches to the
cloud then you have something called
snowmobile where you migrate exabytes of
data in batches to the cloud and they
will simply send a truck to your data
center you plug it in the truck will
roll into AWS Data Center and then they
will put data anywhere you want
so let's also take a look at Kinesis
fire hose as well
so you have these Kinesis streams and
you have ingestion process streaming
data your iot data your mobile data your
Twitter feed the click stream data these
are the things you can use Kinesis to
store and process all of that data
this is where you would come in AWS
Services AWS console to request snowball
these are all the things that AWS gives
you to transfer your data again your
data is secure it's encrypted uh it's
addressed in transit and you can access
it anywhere using the internet let's
look at S3 in AWS Management console
S3 is a place which is used for data
leaks for any sort of data that you
would want to upload in Cloud I can very
easily go and I can create a bucket so
let's create a bucket
bucket is where you are going to store
your data alright so I have created
Albert simply learn lab bucket and if I
click on it I can do so many things over
here I can upload any data that I want
over here I can send the permissions of
who all can access this data by default
in AWS it's a shared responsibility
model so I can upload the data and I can
choose who can access this data in S3
there are so many things that I can do
with this bucket that is my folder in
the cloud S3 that is simple storage
service I can enable versioning so I can
keep all the versions of my objects over
here I can set up logging I can set up a
static website hosting we will have a
demo for this static website hosting as
well but we don't have to worry about
this killing the load balancing Etc it
just scales automatically
I did mention that it's a great tool for
disaster recovery and what you can do is
you can set up your backups your
snapshots in S3 you can put all your
data in S3 you can simply enable the
cross region replication so what happens
is that let's say you are in Us East one
or Northern Virginia region and you want
to set the cross region replication to
Oregon or maybe somewhere in Europe you
can just come over here and you can
enable the cross region replications so
what happens is AWS will automatically
replicate your data from one region to
another of your choice you decide where
you want your data to be replicated AWS
will not do so on its own
there are so many types of things that
you can do you can tag everything you
can give permissions to all your people
there are so many types of permissions
that you can give there is an access
control list you can set up a bucket
policy it's also tightly integrated with
identity and access management so you
can very easily command who is able to
access your bucket apart from that there
are some really interesting things there
is something called life cycle now this
is a tiered storage so normally whenever
you get data typically that data is
going to be very hot so you would Define
the data or classify the data as a hot
data warm data or cold depending on how
frequently it is accessed then you would
normally have a tiered storage so S3 has
four classes that's a standard storage
and infrequent access storage reduced
redundancy store bridge where you would
put normally reproducible data and
archival storage that is Glacier so what
you normally do whenever you get some
data you would keep it in a standard and
you would know over a period of time you
would want to move that into an
infrequent storage like over here so I
will say that move this data into
infrequent storage which offers the same
security same durability but it's a lot
cheaper
okay and then after that I can also add
a data that I want to move into archive
which is Glacier after some time so we
can add it to Glacier after 60 days and
if I just click on next this is where I
will see it and then I can also delete
that object after whatever the amount of
time that I would want and I click on it
you see here that I have set up a
transition rule that says that for the
current version of the object anything
that I add to this S3 bucket I want to
transition all those objects
automatically to standard infrequent
access class
and then I will be charged lesser than
what I've been charged now and then I
also want to transition to Amazon
Glacier that is my archival service
after 60 days I do not want to delete
them ever so these are the things that
you can do with the S3 you also have a
lot of analytics tools which allow you
to see who is accessing your data what
data you have who is adding a lot of
data and these are all available to you
at a very economical rate
all these services are tightly
integrated with each other so you can
plug them in with an ec2 instance or IM
and you can get all the information at
your fingertips
so let's talk about static website
hosting on S3 and this is one of the
very very powerful features of S3 where
we can actually host a website and it's
static and some part dynamic as well so
you do not need a web server to host
your website I have this Bucket over
here AWS
agile.n I click on it and you can see
that I have a couple of HTML pages
error.html and
index.html and then my code my HTML CSS
files my image my video files are inside
the folder so I come over here I click
on the properties and you can see that
the bucket hosting is enabled so if I
click on it all I need is two documents
then whatever I want inside my website
if I simply click on this thing this is
what I get now these are the things that
I have inside my website my folder and
the beauty of it is that you do not need
any web servers you do not need any load
balancer you don't need any firewalls
you can simply give this URL and then
you can map it to your DNS the domain
name service and then anybody will type
your website name or your blog name and
then they are going to land on this page
and it's scalable it is load balanced
and you can scale it to any number of
people you can have millions of hits
coming in and this is one of the very
very very popular Services of S3
in the topic how to design cloud
services we will take a look at the AWS
well architected framework
and we'll take a deeper look at the four
pillars which make up the framework and
these are security
reliability
performance efficiency and cost
optimization
we'll then take a look at where you can
find more information about designing
cloud services for Amazon
and we will look at the AWS quick start
reference deployments
and where you can find out real life
examples about how real businesses are
using Amazon web services
so let's start by looking at the AWS
well architected framework
now this was introduced because building
Cloud infrastructure Services is vastly
different to what you're used to with
on-premise infrastructure so AWS created
this framework to help you understand
the pros and cons of decisions you make
while Building Systems on AWS
the AWS framework documents a set of
foundation questions that allow you to
understand if a specific architecture
aligns well with bitcloud best practices
and there's five principles it helps you
with the first of which is stop guessing
your capacity needs
test systems of production scale
lower the risk of architecture change
automate to make architecture
experimentation easier
and allow for evolutionary architectures
and we'll take a look at each of these
in Greater detail
so firstly let's take a look at stop
guessing your capacity needs now imagine
you're designing a brand new application
for your business
all the infrastructure decisions you
will make are made using estimates which
are basically guesses
if you overestimate you might end up
buying too much hardware and have a
fleet of expensive idle resources
conversely you might underestimate and
have to deal with the performance
implications of limited capacity
with cloud computing these problems go
away as AWS helps you eliminate the
guesswork in your infrastructure
capacity needs you can use as much or as
little capacity as you need and scale up
and down automatically depending on your
demand
the cloud allows you to test systems of
production scale traditionally in a
non-cloud environment it's usually very
difficult to fully Test new productsal
services in development as it's cost
prohibitive to have like-the-like
environments or you just don't have the
resources available
personally I've lost count how many
projects I've worked on when things have
gone wrong as soon as the product's Gone
live due to insufficient testing with
the cloud you can duplicate environments
on demand and when you've completed your
testing just shut them down and you only
have to pay for the time the test
environment was up and running so you
can test fully before you go live in
production
AWS allows you to lower the risk of
architecture change as you can automate
the creation of test environments
so you can emulate your production
configurations and Carry Out testing
against comparable environments you can
also remove testing bottlenecks where
various teams are lining up to use your
test environments as with AWS you can
cheaply spin up as many test
environments as you need and ensure that
all the production changes have been
sufficiently tested
you can automate to make architecture
experimentation easier
if you need to make a change to
production but not sure what the impact
will be well you can automate the
creation and replication of your systems
at low cost and low effort to test these
changes you can then order the impact
and then if necessary you can revert the
changes and try again
with AWS you can allow for evolutionary
architectures
in a traditional environment
architecture decisions are often
implemented as a static one-time event
that you have to live with during the
lifetime of a system but with the cloud
the capability exists to automate and
test on demand to lower the risk of
design changes so you can Implement new
Innovations and features easily
what this means is you aren't stuck with
last year's technology and you can
Implement new technology as soon as it
comes out
as mentioned earlier the well
architected framework is based on four
pillars security reliability performance
efficiency and cost optimization
so in the coming slides we're going to
take a further look at these four
pillars starting with security
so Amazon defines security as the
ability to protect information systems
and assets while delivering businesses
value through risk assessments and
mitigation strategies
what this means is you can apply
security at all layers so rather than
just running minimal security like
firewalls at the edge of your
infrastructure
you can use firewalls and other security
controls on all of your resources for
example security groups on ec2 instances
allow you to Define who and what can
access that specific resource rather
than just defining it at a global
infrastructure level
you can enable traceability so you can
log and order all changes to your
environment
you can automate responses to security
events so you can monitor and
automatically trigger alerts depending
on your needs
you can focus on sharing your system the
AWS shared responsibility model allows
you to focus on securing your
application data and operating systems
whilst AWS secures your infrastructure
and services
and you can automate security best
practices AWS allows you to create and
save a custom Baseline image of a
virtual server and then you can use that
image to automatically launch new
servers
this way you can ensure that all new
resources adhere to your security
standards
so I just briefly mentions the AWS
shared responsibility model now we take
a look at this in more depth in a later
lesson
but what it means is that the AWS shared
responsibility model allows AWS
customers to focus on using services to
accomplish their security and compliance
goals because AWS physically secures the
infrastructure that supports your cloud
services so if you look at this diagram
you can see it split into two
security measures that the cloud service
provider implements and operates is the
security of the cloud and that's a
responsibility of AWS
then there's a security measures that
the customer inputs and operates and
this is related to the security of
customer content and applications that
make use of the AWS services and this is
Security in the cloud and this is the
responsibility of you
so let's take a look at security in the
cloud and this is composed of four areas
data protection privilege management
infrastructure protection and detective
controls
let's start with data protection
before architecting any system practices
that ensure security should be in place
with data protection these are
categorizing data based on levels of
sensitivity
granting least privilege while still
allowing users to perform their work and
encryption to protect your sensitive
data
if you're storing customer credit card
data obviously this needs to be
categorized as sensitive and it needs to
be encrypted but log files from a daily
maintenance task
wouldn't need this level of security
now there's a number of tools AWS offers
to assist you with this
AWS makes it easy to encrypt data and
manage keys and key rotation using IAM
and Key Management Service
you can perform detailed logging using
cloudtrail
and AWS offers resilient storage systems
like Amazon S3 which has 11 nines of
durability
now other lines of durability means that
if you store 10 000 objects with Amazon
S3 on average you can expect to incur a
loss of a single object once every 10
million years so in other words you're
not going to lose many files and finally
AWS offers versioning and lifecycle
management again with S3 so you can
predict again accidental deletes or file
over rights
now a central part of any information
security program is privilege management
so that you can ensure that only
authorized and authenticated users are
able to access your resources
and that they can only access them in a
way that is acceptable
for example if a user needs access to a
resource don't just give them admin or
read write permissions by default
because they might actually only need
read-only access
Now to control your privilege management
you need to have a few things in place
the first of which is an access control
list and this is a document that needs
to be maintained which lists access
permissions attached to an object so if
you have some particular documents you
need to Define what access is allowed to
it
then you have role-based access controls
and this defines the permissions for a
particular end user's role or function
so an administrator would obviously need
administrator access to various
resources but you know a date a regular
end user might only need read-only
access so you need to Define which roles
have which permissions
then there's password management
and this includes complexity
requirements and change intervals so you
define how often people need to change
their passwords and how complex they
need to be
now Amazon helps you with all this with
the identity and access management or IM
service and this is the primary service
you use to control access to AWS
services and resources
we cover I am in huge detail in a later
lesson but the briefly it allows you to
apply granular policies which assign
permissions to users groups roles or
resources and you can also control the
password strength using this as well as
Federation which are existing directory
services such as Microsoft active
directory
we also need to protect your
infrastructure and to do this it's
recommended to have multiple layers of
defense and also multi-factor
authentication to meet best practices
with industry or regulatory obligations
now Amazon allows you to do this
by implementing stateful or stateless
packet inspection EVS in AWS native
Technologies or by using partner
products and services available through
the AWS Marketplace
Amazon virtual Cloud VPC enforces
boundary protection and monitoring
points of Ingress and egress and then
there's cloudtrail and cloudwatch which
provides comprehensive logging
monitoring and alerting
you should always have detective
controls in place so you can detect or
identify security breaches this needs to
happen so you can provide both a quality
support process and meet compliance
obligation
and by a quality support process I mean
one in which when you have a problem
you're committed to finding out the root
cause and actually resolving the issue
rather than just saying oh we think it
was that let's hope it doesn't happen
again
I mean I know I've worked for some
places like that and it's certainly not
a quality support process
AWS helps you with your detective
controls by providing these Services AWS
cloudtrail is a service that logs API
calls so you can find out the identity
of callers the time of calls Source IP
addresses and things like that so you
can find out who is doing what and when
Amazon cloudwatch is a monitoring
service for AWS resources so you can log
ec2 CPU usage or network activity and
you can use cloudwatch with many
different services like RDS EBS and more
you can also set up alarms so you can be
notified when certain things happen
AWS config is an inventory and
configuration history service that
provides information about the
configurations and changes in
infrastructure over time
with Amazon S3 you can see to access
auditing so you can find out who's been
accessing your S3 data and who when they
did it and when
an Amazon Glacier you can use the Vault
lock feature to preserve Mission
critical data with compliance controls
that are designed to be auditable for
long-term retention
the next pillar is reliability and
Amazon defines this as the ability of a
system to recover from infrastructure or
service failures dynamically acquire
Computing resources to meet demand and
mitigate disruptions such as
misconfigurations or transient network
issues
with reliability in the cloud there's a
number of things to look at firstly you
can test recovery procedures
Cloud infrastructure means you can test
how your system fails and you can
validate your recovery strategy
places I've worked at in the past
Disaster Recovery was a big event it was
only done annually it took months of
preparation it involved working a whole
weekend and then some and was also
incredibly disruptive to the business
imagine being able to duplicate your
production environment and test failure
and failover whenever you want
you can automatically recover from
failure with the cloud monitoring a
system for a key performance indicator
or kpi you can automatically trigger
automated recovery processes when a
threshold is breached
you can scale horizontally to increase
aggregate system availability
using multiple small resources instead
of one large resource will reduce your
single points of failure
and also you can stop guessing capacity
gone are the days of using lack of
memory or not enough CPU is a failure
for a resource with the cloud you can
scale as and when you need to so you
can't blame badly estimated capacity as
the reason for reliability issues
not reliability in the cloud is composed
of three areas these are foundations
change management and failure management
to achieve reliability a system must
have a well-planned foundation and
monitoring in place plus systems for
handling changes in demand or
requirements is also required in an
Ideal World your system would be
designed to detect failure and
automatically heal itself
before architecting any system you
should have foundations in place so that
your reliability is not impacted you
should have sufficient Network bandwidth
to your data center you should have
sufficient compute capacity
your staff should be trained in the
areas they need to be and you should
have support contracts with your vendors
with AWS Network bandwidth and compute
capacity is already taken care of
AWS also offers a range of support
contracts to help you with issues and a
range of training courses for your staff
change management is a critical part
being aware of how a change affects the
system allows you to plan proactive and
monitoring allows you to quickly
identify trends that could lead to
capacity issues or SLA breaches
with AWS you can monitor the behavior of
a system and automate the response to
kpis for example you can add additional
servers as a system gains more users so
you can control who has permissions to
make system changes and order the
history of these changes
unfortunately it's a given that failures
will occur so you need to have some
failure Management in place
it's good practice to understand why
things failed so you can prevent it from
happening again no one wants to work for
a company that when things go wrong they
just get it working again with no
understanding of why
a key to managing failure is frequent
and automated testing of systems to
failure and through recovery ideally you
do this on a regular schedule and also
after you've made significant system
changes
now Amazon allows you to do this with a
few products firstly AWS cloud formation
this allows you to launch temporary
copies of whole systems at very low cost
so you can use automated testing to
verify your recovery processes
you can use cloudwatch to set up
automation to react to monitoring data
that indicates a failure
and you can store all your data on
Amazon S3 buckets for future use
the next pillar is performance
efficiency and Amazon defines this as
the ability to use Computing resources
efficiently to meet system requirements
and to maintain that efficiency as
demand changes and Technologies evolve
so what does this mean well performance
efficiency in the cloud means you can
democratize Advanced Technologies
AWS provides products such as nosql or
Media transcoding or even machine
learning as a service so rival in your
it team having to learn how to host and
run these new technologies AWS does it
for you
you can go Global in minutes AWS allows
you to easily deploy your systems in
multiple regions around the world with
just a few clicks of your mouse so this
leads to lower latency for your
customers
you can use serverless architectures in
the cloud which remove the need for you
to run and maintain servers to carry out
traditional compute activities
for example storage Services can act as
static websites which remove the need
for web servers and you can also use
serverless Technologies such as Lambda
or elastic Beanstalk
and you can also experiment more often
with virtual and automotable resources
you can quickly carry a comparative
testing using different types of
instances storage or configuration
so performance efficiency in the cloud
is composed of four areas compute
storage database and something called
space time trade-off and let's take a
look at each of these
with the cloud finding the optimal
server configuration for a product will
vary based on application design usage
patterns and configuration settings
traditional it infrastructure requires
making estimates in advance which can
lead to incorrect server configurations
and lower performance efficiency
AWS is virtualized so you can quickly
change the ec2 server configuration to
increase its performance efficiency
it may be optimal to run serverless
Computing for example AWS Lambda allows
you to execute code without running an
instance an elastic Beanstalk allows you
to run web applications in a similar
manner
from an operational standpoint you
should have monitoring in place to
notify you of any degradation in
performance
the optimal storage solution for a
particular system will vary based on
whether you need block file or object
storage the type of throughput required
frequency of access and availability and
durability constraints
well architected systems use multiple
Storage Solutions and enable different
features to improve performance
in AWS storage is virtualized and is
available in a number of different types
this makes it easier to match your
storage methods more closely with your
needs and also offers storage options
that are not easily achievable with
on-premise infrastructure for example
Amazon S3 is designed for 11 9's
durability and provides a variety of
storage classes for your different data
categories like Glacier for archive data
EBS and EFS offer numerous options for
your ec2 instances based on the level of
performance you require from your
storage
the optimal database solution for a
particular system can vary based on
requirements for consistency
availability partition tolerance and
latency
many systems use different database
solutions for various subsystems and
enable different features to improve
performance
selecting the wrong database solution
and features for a system can lead to
much lower performance efficiency now
for database usage you need to be able
to monitor them test them and also you
need to have some database platform
knowledge but Amazon makes this much
easier for you as it provides a suite of
Amazon relational database Services
which are fully managed relational
databases
for example Amazon dynamodb is a fully
managed nosql database that provides
single digit millisecond latency at any
scale Amazon redshift is a managed
petabyte scale data warehouse that
allows you to change the number or type
of nodes as your performance or capacity
needs changed you can also make use of
Aurora or Amis where you install your
own databases from templates
so what is the space-time trade-off well
you can think of space as memory or
storage and you can think of time as the
processing time or the compute time to
complete a task
so when you're architecting Solutions
there's always a series of trade-offs
where it might be better to have more
memory or storage to reduce the
processing time or maybe you're happy to
sacrifice the processing time in order
to reduce your memory or storage
requirements
AWS gives you the option to maximize one
or the other
for example you could launch your
systems globally so you're increasing
space and memory so that they're closer
to the end users in order to reduce
latency all the time
but whatever resource you use you need
to have monitoring in place to notify
you of any degradation in performance
and the final pillar of the well
architected framework is cost
optimization this is defined as the
ability to avoid or eliminate unneeded
cost or sub-optimal resources
AWS cloud has a number of ways you can
provide cost optimization
firstly you can transparently attribute
expenditure as the cloud makes it easy
to identify the costs of a system and
attribute it costs of individual
business owners so those owners will
have an incentive to optimize their
resources and reduce costs
you can use managed services to remove
the operational burden of maintaining
servers for tasks such as sending email
or managing databases
you can trade Capital expense for
operating expense instead of investing
heavily in data centers and servers
before you know how you're going to use
them with AWS you pay only for the
Computing resources you consume when you
consume them
also AWS allows you to achieve higher
economies of scale because they can buy
way more servers and Hardware than you
could ever imagine so it's much cheaper
to use their Hardware than buy your own
you should also stop spending money on
data center operations AWS is the heavy
lifting of racking stacking and powering
servers so you can focus on your
customers and business projects rather
than on your it infrastructure
cost optimization in the cloud is
composed of four areas
match supply and demand cost effective
resources expenditure awareness and
optimizing over time
matching Supply to demand will deliver
the lowest costs for a system
but you'll need to be able to provide
sufficient extra capacity to cope with
demand and failures
in AWS you can automatically provision
resources to match demand Auto scaling
time-based and event-driven and
cue-based approaches allow you to add
and remove resources as needed
monitoring tools and regular
benchmarking can help you achieve much
greater utilization of your resources
using the appropriate instances and
resources for your system is key to cost
savings for example a reporting process
might take five hours to run on a
smaller server but a larger server that
is twice as expensive might better do it
in just one hour
both jobs give you the same outcome but
the smaller server will incur more costs
over time
but to be able to find out these things
you need to have monitoring of
expenditure in place and also regular
benchmarking
AWS helps you with this AWS trusted
advisor goes through your resources and
tells you where you can make savings
you can use on-demand instances so you
only pay for compute capacity by the
hour with no minimum commitments
required or you could use reserved
instances which allow you to reserve
capacity and offer Savings of up to 75
percent off on-demand pricing
and with spot instances you can bid on
unused ec2 capacity at significant
discounts you can also take advantage of
managed AWS services such as RDS or
dynamodb which can also lower your costs
AWS and is virtually unlimited on-demand
capacity requires a new way of thinking
about expenditures
so you need to have a different way of
budgeting and you need to understand
your business unit's usage
to do this you can use cost allocation
tags to categorize and track your AWS
costs for AWS resources such as ec2 or
Amazon S3
an AWS generates a cost allocation
Report with your usage and costs
aggregated by your tags
so you can set up tags for each of your
business categories and units
this increased visibility of costs makes
it easier to identify resources or
projects that are no longer generating
value and should be decommissioned
billing alerts can be set up to notify
you of predicted overspend and the AWS
simple monthly calculator allows you to
calculate data transfer costs and you
can set up SNS alerts so you can be
notified when certain resources or cost
breaches occur
optimizing over time is an important
strategy to reduce your costs associated
with your Cloud environment AWS is
always releasing new Production Services
as such you need to reassess your
existing setup to see if it's the most
cost effective for example rather than
running a database on an ec2 instance it
might be cheaper to run an Amazon RDS
database
or Lambda might be more cost effective
than ec2
but where did you find out more
information about these new products and
services from Amazon
well the first place to look for any new
architecture guide is the AWS
architecture Center
this is a resource that provides you
with the guidance and application
architecture best practices to build
highly scalable and reliable
applications in the cloud
here you'll find the AWS reference
architectures
the AWS reference architecture data
sheets provide architectural guidance to
help you build an application on the AWS
Cloud you'll find data sheets that
include a description of how each
service is used plus visual
representations of the application
architecture for example web application
hosting or large-scale processing and
huge data sets or even building elastic
web front ends for an e-commerce website
AWS white papers provide a comprehensive
list of technical AWS white papers that
cover all Amazon related topics such as
architecture security and economics
there are new white papers being
released all the time and are written by
some of the most knowledgeable AWS
people around
AWS quick start reference deployments
use AWS cloud formation templates to
rapidly deploy a fully functional
environment for a variety of enterprise
software applications
supplementary deployment guides describe
the architecture and implementation in
detail
in this course we'll deploy things
manually step by step so you can see how
it works but with cloud formation
templates you can do several hours work
with just the click of a button
examples of cloud formation templates
are SharePoint Rd Gateway or even SQL
server on Windows Server failover
clusters
and finally case studies AWS maintains a
large list of case studies and success
stories from their clients you can check
to see how some of the largest and most
successful companies on the planet use
AWS for their business you'll learn how
the AWS well architected framework is
used in planning and designing
we'll take a look at scaling and all the
different versions
we'll look at the importance of loose
coupling how you can build redundancy
into your Cloud infrastructure
how you can save money by cost
optimization
we'll look at Automation and how it can
make management of an I.T environment
easier and also we'll take a look at
security and how you can secure your
Cloud resources
so let's start with scalability
in the past you would have to estimate
the peak load of your systems and
purchase your Hardware accordingly so
perhaps you were setting up a new
website and you are estimating that
you're going to get a million people per
month hitting your website so you buy
the hardware accordingly but then you
find out that only 50 000 people are
hitting it or 10 million people are
hitting it so your Hardware is
inappropriate for the work
cloud computing provides virtually
unlimited on-demand capacity so you can
scale whenever you need to to do this
your designs need to be able to
seamlessly take advantage of these
resources and there are two ways to
scale vertically and horizontally
vertical scaling means increasing the
specifications of an individual resource
for example increasing the memory or the
number of CPUs on the server
with AWS this is easily achieved with a
restart of your virtual server so that
it resizes to a larger or smaller
instance type the advantage of this
approach is that it's very easy and
requires little for in the short term
however this type of scaling can
eventually hit a limit and sometimes be
cost inefficient especially if these
scale up to some very very large
instances
horizontal scaling means increasing the
number of resources rather than the
specifications of each individual
resource
for example adding additional web
servers to help spread the load of
traffic hitting your application
this is a great way to build
applications that Leverage The
elasticity of cloud computing however
not all architectures can distribute
their workload to multiple resources
stateless application is one that needs
no knowledge of previous interaction and
stores no session information
an example of this would be a web server
that provides the same web page to any
end user
estate this application can scale
horizontally as any request from any end
user can be serviced by any of the
available compute resources
you can add more resources as required
and then remove them when the capacity
is no longer available
stateless applications do not need to be
aware of their peers all that is
required is a way to distribute the end
user sessions to them
and there are two ways to distribute
load for multiple nodes the first of
these is the push model
a load balancer such as the AWS elastic
load balancer is a popular way to
distribute a workload across multiple
resources
an alternative but less recommended
approach is to implement a DNS round
robin using Amazon Route 53 where DNS
responses return an IP address from a
list of valid hosts in a round robin
version
this is an easy to set up option but it
can cause issues if DNS records are
cached
the alternative to the push model is the
pool model
tasks that need to be performed can be
stored as messages in a queue and
multiple compute resources can pull and
process the messages in a distributed
fashion
examples of this are big data processing
scenarios where many servers work to
analyze data and return results or media
file conversion processes where multiple
servers convert the files as they arrive
Amazon sqls or Amazon Kinesis are
services that can provide pool model
load balancing
most applications need to maintain some
kind of state information for example an
automated multi-step process will also
need to track previous activity to
decide what its next action should be
you can make components in your
architecture stateless by not storing
anything on the local file system and
instead storing user or session-based
information in a database like dynamodb
or MySQL or on shared storage like
Amazon S3 or EFS
additionally Amazon swf can be used to
centrally store execution history and
make your workload stateless
so the opposite stateless applications
are stateful applications as it's not
possible to turn some layers of your
architecture into stateless components
for example databases which are stateful
by definition or applications that were
designed to run on a single server
providing multiple users a consistent
view of the database or application is
much simpler when your components are
not distributed
Distributing the load is possible low
through session Affinity this means that
all transactions of a session are bound
to a specific compute resource however
this means that existing sessions would
not be able to utilize newly introduced
compute nodes and also if a node is shut
down or becomes unavailable users bound
to that session will be disconnected
for situations when you need to process
large amounts of data is always
beneficial to see if a distributed
processing approach can be used
imagine a task that requires a huge
amount of data processing if it's to run
on a single compute resource it would
max out the resources and take a long
time to complete
you can divide the task into smaller
fragments of work then each of the tasks
can be executed across a larger set of
compute resources
examples of this are the AWS elastic map
reduced service which allows you to run
Hadoop workloads across multiple ec2
instances or Amazon Kinesis allows you
to run multiple shards of data on ec2 or
Lambda resources for real-time
processing of streaming data
the concept of disposable resources is
completely new with cloud computing
as cloud computing completely changes
the mindset of an I.T infrastructure
environment
traditional environments involve
purchasing Hardware up front and
required manual configuration of the
software Network Etc
whereas with cloud computing all
infrastructure is temporary or
disposable
you can launch new instances when you
need them and get rid of them when you
are done
automate compute resource initiation is
a way to speed up the creation of new
environments AWS has plenty of features
to make new environment creation and
automated and repeatable process and
there's three options available which
we'll look at in the coming slides
bootstrapping golden images and a hybrid
approach
bootstrapping means you can take a newly
launched AWS resource with its default
configuration and then execute automated
scripts to install software or copy data
to bring that resource to a required
state so you could launch an ec2
instance and copy data on so that it
becomes a web server
scripts can be parameterized so that
different environments production or
development can be initiated easily with
AWS bootstrapping can be achieved with
your own scripts Chef or puppet Ops work
lifecycle events or cloud formation
golden images mean you take snapshots of
your ec2 instances or RDS instances or
even EBS volumes and they can be used to
launch new instances
these ec2 instances can also be
customized and saved as Amis Amazon
machine images and then you can launch
as many instances as you want from this
template
if you have an on-premise virtualized
environment you can also use AWS VM
import export to create your own AWS
Amis
the hybrid approach makes use of both
bootstrapping and golden images
you can launch new instances from a
golden image but then bootstrap to
configure other actions basically with
AWS you are only restricted by your own
imagination or coding skills
AWS assets are programmable so you can
apply all these techniques and
principles that we've just talked about
to entire environments and not just
individual resources so you can
completely rethink the way you work your
it infrastructure
automation is a big part of planning and
designing Cloud infrastructure AWS
allows you to reduce the level of manual
interaction in your environment using
automation you can react to a variety of
events without having to do anything
let's take a look at some examples AWS
elastic Beanstalk developers can upload
their code and the Beanstalk service
automatically handles Resource
provisioning Auto scaling load balancing
and monitoring ec2 Auto Recovery in some
situations you can use auto recovery to
automatically recover an ec2 instance if
it becomes impaired the recovered
instance is identical to the original
instance name IP address metadata Etc
Auto scaling you can automatically add
resources to cope with demand spikes and
decrease again during quiet times
cloudwatch alarms and events cloudwatch
can send SMS notifications to alert when
a particular event occurs for example
CPU usage is too high the notifications
can be used to perform follow-up actions
like run a Lambda function
Ops works like cycle events you can
continually update your instances
configuration to adapt to environment
changes for example if a new database
instance is added to your environment
then an event can automatically trigger
a chef reppi that points existing
applications to the new server
loose coupling is an important concept
ion should be designed so that they're
broken into smaller Loosely coupled
components
the desired outcome is that a failure in
one component should not cause other
components to fail
this is achieved for a variety of
measures firstly well-defined interfaces
by ensuring that all components only
interact with each other through
specific technology agnostic interfaces
for example restful apis will result in
being able to modify resources without
affecting other components
Amazon API Gateway is a fully managed
service for apis which you can do this
with
loose coupling needs services to be able
to interact with each other without
having any prior knowledge of their
existence for example imagine two
servers communicating on hard-coded IP
addresses then the addition or
modification of resources will result in
manual interaction to update the
configuration however Loosely coupled
infrastructure is an essential
ingredient in making the most of cloud
computing elasticity the easiest way to
achieve this is to use an elastic load
balancer service to provide a stable
endpoint for all your services this way
they don't have to worry about each
other the elastic load balancer does
that for them
rather than use synchronous integration
where server a completes an action and
passes it to server B which then passes
it to server C asynchronous integration
involves the use of an intermediate
storage area like an sqsq
this approach means that when a server a
completes its action it sends a
notification to sqs this way the compute
resources are decoupled and not directly
linked to each other this means you can
easily add or remove resources without
having to update the configuration of
the existing compute resources
designing applications to fail
gracefully allows other resources to
continue a service without causing a
complete outage
examples of this would be if your
primary website fails the Route 53 DNS
failover feature points your users to a
backup site hosted on a static website
on Amazon S3
the idea is to continue to provide a
service even if it isn't the full
experience
traditional it infrastructure involves
developing managing and operating
applications with a wide variety of
underlying technology components
AWS isn't just about ec2 instances there
are Suite of products to do everything
mentioned and a lot more besides to help
you lower your it costs and allow you to
move faster as an organization
for example manage services AWS provides
many services that are fully managed
databases machine learning analytics
search email
Etc this lessens the burden on your it
infrastructure Department
RDS databases mean you don't have to
worry about backups for tolerance for
your simple databases
S3 allows you to store as much data as
you need without having to concern
yourself with capacity management
serverless architectures you cannot
reduce operational complexity of your
applications through the use of
serverless architectures tools such as
the Internet of Things Lambda S3
Beanstalk all allow you to run your
applications without paying for any
server infrastructure
in a traditional it environment the type
of database in use is typically
restricted by constraints on licensing
support capabilities Hardware
availability and things like that
AWS provides you with multiple managed
database server options that remove
these constraints meaning that you can
choose exactly the right database for
your needs rather than making use of
what is available to you AWS offers
fully managed easy scalable services for
relational databases nosql data
warehouses and search functions that
provide fully managed easily scalable
services
that can create synchronously replicated
standby instances in different
availability zones
and they'll also automatically fail over
without the need for manual intervention
we'll cover databases in a whole lesson
later on in this course
now we'll take a look at redundancy
introducing redundancy will ensure your
systems are highly available so they can
withstand the failure of individual or
multiple components for example hard
disks servers or network links
introducing redundancy will remove
single points of failure from your
systems this is achieved for having
multiple resources for the same tasks in
either standby or active mode
standby redundancy is often used for
stateful components like databases the
standby resource becomes the primary
resource by failing over to it the
standby resource can be brought online
only when required to save cost or it
can already be running to speed up the
failover process and minimize the outage
active redundancy is where multiple
redundant compute resources share
requests and are able to absorb the loss
of one or more of the instances failing
examples of this would be multiple web
servers sitting behind a load balancer
automatic failure detection allows you
to react to an outage automatically
without the need for manual intervention
services like elastic load balancer and
Route 53 that you configure health
checks so you can automatically Route
traffic to healthy resources
for example setting up a health check to
test for a HTTP 200 response which means
okay would allow AWS to know if a node
is healthy or not services such as Ops
Works elastic load balancer and ec2 auto
recovery will let you replace unhealthy
Resources with brand new healthy ones
durable data storage is an important
part of planning your Cloud resources
replicating your data to other sites or
resources will protect its availability
and integrity
there's three ways you can replicate
your data synchronously asynchronously
or Quorum based
synchronous replication ensures that
your data has been durably stored on
both the primary and replication
locations any write operation will only
be acknowledged as complete when this
has taken place
this should only be used on low latency
network connections where absolutely
necessary to try to ensure maximum
performance
asynchronous replication decouples the
primary node from the replica so that a
right operation doesn't have to wait for
any acknowledgment this can introduce a
replication lag but does not impact
performance and from personal experience
I've seen the difference between
synchronous and asynchronous replication
changing performance as much as a
hundred percent
Quorum based replication is a mix of
both synchronous and asynchronous
replication replication to multiple
nodes is controlled by defining the
minimum number of nodes that must
participate in a successful write
operation so if you have a cluster of
four replicas you can say I need
confirmation from at least two of them
before the acknowledgment is sent back
to the end user
a traditional data center failover
scenario involves failing over all your
resources to a secondary distant Data
Center
because of the long distances between
the two data centers synchronous
replication is often impractical slow
and usually involves data loss and isn't
tested very often however it does
protect against low probability
scenarios such as a natural disaster so
if your main data center is in New York
and your secondary was in say Washington
if New York had a massive outage you
could fail over to Washington but it's
going to be slow and practical and take
some time
a more likely scenario is a shorter
interruption in your data center which
isn't predicted to be too long in this
situation your choice is to sit it out
and wait or initiate the complex
failover procedure
AWS data centers are configured to
provide multiple availability zones in a
region with low latency network
connectivity this means you can
replicate your data across data centers
in a synchronous Manner and as a result
failover becomes much simpler also this
concept is baked into many AWS services
such as RDS and S3
active redundancy is great for Disaster
Recovery situations or balancing traffic
but what if all the requests to your
resources were harmful
for example if a particular request
caused a bug and it resulted in multiple
instances crashing
a practice called Shuffle sharding means
only sending some of the requests to
some of your resources this way of one
Shard of resources is infected or down
the other Sharda resources will be up
and running
in the example shown here if request 2
is bad then at least one set of
resources is still protected
AWS economies of scale offer
organizations huge opportunities to make
cost savings by making further use of
AWS capabilities there's even more scope
to create cost optimized cloud
architectures
right sizing is one such approach in
stark contrast to a traditional it
infrastructure cloud computing means you
can select the most cost-effective
resource and configuration to fit your
requirements ec2 RDS redshift and
elasticsearch give you a wide variety of
instant types to choose from
through benchmarking you can determine
the optimal configuration for your
workload in some cases it might be
optimal to select many of the cheapest
instant type available in others it
might be more beneficial to select fewer
instances but of a larger instance type
AWS storage offers the same level of
right sizing opportunities Amazon S3
offers many different storage classes to
suit your needs EBS also comes in a
variety of different volume types all of
which will be covered in later lessons
AWS offers many elasticity options to
help you save money you can Auto scale
your ec2 workloads horizontally or
vertically you can turn off production
workloads when you don't need them and
where possible you can use Lambda
compute workloads as you never have to
pay for idle resources
ec2 on-demand instance pricing means you
only pay for what you use with no
long-term commitments however there are
two more ways to pay for ec2 instances
one of these is reserve capacity by
committing to a defined period of
between 12 to 36 months you can receive
significantly discounted hourly rates
compared to on-demand pricing
AWS trusted advisor or AWS ec2 usage
reports identify the resources that
would benefit from reserved capacity
there is technically no difference
between on-demand and reserved instances
the only difference is the way you paid
for it this concept also exists for
redshift RDS dynamodb and cloudfront
ec2 spot instances are ideal for
workloads that have flexible start and
end times as you are allowed to bid on
spare ec2 Computing capacity and spot
instances are often available at
significant discounts compared to
on-demand pricing
your ec2 spot instance is launched when
your bid exceeds the current spot market
price and it will continue to run until
evu terminate it or the stock market
price exceeds your bid when the latter
happens your instance is terminated and
you will not be charged for the partial
Outlet it was running
so there's three strategies to use for
spot instances there's a one we've just
discussed which is bidding you could bid
much higher than the spot market price
for as long as the instant runs this way
even if the market price spikes
occasionally you'll still make a saving
overall
the mix strategy AWS recommends
designing applications that use a
mixture of reserved on-demand and spot
instances this way you can combine
predictable capacity with opportunistic
access to cheaper additional compute
resources
and then finally there's spot blocks AWS
allows you to bid for fixed duration
spot instances these have a different
hourly pricing but allow you to define a
duration requirement if your bid is
accepted your instance will run until
you terminate it or the duration ends
now let's take a look at caching caching
data means storing previously calculated
data for future use so you don't have to
recalculate it again
there's two approaches application
caching you can design applications to
store and retrieve information from Fast
managed in-memory caches
this way an application can look for
results in the cache first and if the
data isn't there it can then calculate
it or retrieve the data and store it in
the cache for subsequent requests
this approach approves user response
times and reduces load on your resources
an example of this is Amazon elasticash
which is a service that provides an
in-memory cache in the cloud suitable
for use with web services
Edge caching uses Amazon cloudfront so
you can cache static content such as
images or video and dynamic content such
as live video around the world using
Edge locations this way users can be
served to content that is closest to
them and resulting in low latency
response times
the principle applies to both download
and upload with Edge caching
and finally security it's important to
have security built into your plans and
designs for AWS infrastructure
AWS allows you to take most of the
security tools and techniques that you
already know and improve them by
formalizing security control design into
your AWS platform this simplifies the
system for administrators and Auditors
alike now we're not going to cover too
much here because this is all covered in
the I am lesson but there's things like
defense in depth which allows you to add
multiple layers of protection to your
resources reduced privileged access to
only give people the access they require
security as a code lets you capture
security requirements in one script that
you can use to deploy new environments
and real-time auditing AWS has multiple
services to ensure you can test and
audit your environment in real time
AWS offers a wide and deep set of
functionality that allows you to
integrate your existing network security
data lifecycle management and Resource
Management capabilities with the cloud
AWS allows you to extend your existing
on-premise network configuration onto
your AWS virtual private clouds so
they've got AWS resources will operate
as if they're part of your existing
Network
you can do this using Amazon virtual
private Cloud VPC which is a
logistically isolated Network in the
Amazon Cloud that gives you complete
control over your virtual networking
environment you can choose your own IP
range subnets route tables Network
Etc then there's Direct Connect rather
than use internet-based connections
between your on-site resources and the
Amazon Cloud Direct Connect lets you
establish a dedicated network connection
between the two to provide lower costs
and a higher level of service
you can use AWS to reliably and cost
effectively backup and secure your data
AWS allows you to replicate your data
across geographical regions manage the
life cycle of the data or even
synchronously replicate your data to a
local AWS Data Center
AWS storage Gateway is a VM that you
install in your data center and
configure to be associated with your AWS
account
then you can either use Gateway cached
which lets you use Amazon S3 storage as
your primary data storage while
retaining frequently accessed data
locally in your storage Gateway or
Gateway stored
Gateway stored volumes let you store
your primary data locally and
asynchronously back that up to AWS
Amazon simple storage service S3 offers
a proactively unlimited storage that's
available for backing up And archiving
your critical data and Amazon Glacier is
extremely Low Cost Storage for
infrequently accessed data for which
recovery intervals of several hours are
suitable
AWS security and access control is
easily managed using I am or Microsoft
active directory services
AWS identity and access management is
the service that enables you to securely
control user access to all your AWS
services and resources and AWS directory
service is a managed service that allows
you to connect your AWS Resources with
an existing on-premise Microsoft active
directory or you can set up a new
Standalone directory in the AWS cloud
AWS security and access control is
easily managed using I am or Microsoft
active directory services
AWS identity and access management is
the service that enables you to securely
control user access to all your AWS
services and resources and AWS directory
service is a managed service that allows
you to connect your AWS Resources with
an existing on-premise Microsoft active
directory or you can set up a new
Standalone directory in the AWS cloud
and there's three flavors of active
directory there's Microsoft ad which is
a fully blown managed Microsoft active
directory service that supports up to 50
000 users the simple ad which is a
standalone manage directory that's
available in two sizes small and large
small list for up to 500 users and large
supports up to 5000 users
all this ad connector and this is a
directory Gateway that allows you to
proxy directory requests to your
on-premise Microsoft active directory
and this also comes in two sizes small
and large a small ad connector is
designed for small organizations of up
to 500 users and a large ad connector is
designed for larger organizations for up
to 5000 users
you can also use integrated resource and
deployment management Tools in your
hybrid environment AWS provides
monitoring and management tools with
robust apis so you can easily integrate
your AWS resources of on-site tools and
most major software vendors already
support AWS like Microsoft VMware and
BMC
tools such as AWS Ops Works allow you to
deploy and operate applications in the
AWS cloud or in your own Data Center
you can use templates or build your own
tasks to specify an application's
architecture
AWS code deploy automates code
deployments to instances in your
existing data center or in the AWS cloud
AWS code deploy simplifies the code
deployment process to your install so if
you are prepared to embark on a career
path pulsating with excitement
Innovation and Endless Possibilities
look no further than simply lens Caltech
postgraduate program in cloud computing
this encompassing program will arm you
with the knowledge and skills needed to
navigate the vast cloud computing
landscape with unyielding confidence
delve into the depths of cloud computing
architecture concur deployment models 45
security like a digital Fortress and
master migration strategies that leave a
lasting impact discover how to harness
Cloud computing's full suit of services
to construct applications that are not
just scalable but remarkable work so far
don't let this once in a lifetime
opportunity slip through your fingers
ignite your career and join the ranks of
time cloud computing professionals check
the link in the description box to
unveil the wonders of the AWS Basics
course the main purpose of configuration
management is to quickly spin up new
resources whenever required you may need
to quickly spin up resources to upgrade
to new instance types
to replace a failed instance or
underperforming instance
for Disaster Recovery
for auto scaling
you may also need to deploy
configuration changes to a fleet of
existing and running instances
logging into each virtual machine and
updating the same configuration files
can take a ridiculous amount of time
so the deployment of the instances must
be automated and repeatable to make the
process easy and convenient AWS supports
a number of Technologies for configuring
and deploying Amazon ec2 instances and
other AWS infrastructure
user data
Amazon machine images or Amis
configuration management tools user data
is basically a command sequence that is
executed at the first launch of the
instance
when you launch an instance in Amazon
ec2 you have the option of passing user
data to the instance that can be used to
perform common automated configuration
tasks and even run scripts after the
instant starts
you can pass two types of user data to
Amazon ec2
shell scripts and Cloud init directives
you can also pass this data into the
launch wizard as plain text file for
launching instances via the command line
tools or as base64 encoded text for API
calls
the Ami provides the information
required to launch an instance which is
a virtual server in the cloud
you can specify an Ami when you launch
an instance and you can launch as many
instances from the Ami as you need
you can customize the instance that you
launched from a public Ami and then save
that configuration as a custom Ami for
your own use
instances that you launch from your Ami
can use all the customizations that
you've made
AWS also offers a variety of its own
configuration management tools and
services which allow you to configure
new instances using reusable templates
and update configuration dynamically in
response to change in this lesson we'll
review some of these tools such as AWS
cloud formation and AWS Ops works and
discuss strategies for combining these
Technologies into a comprehensive
configuration and deployment strategy
AWS cloud formation enables customers to
create and modify collections of AWS
resources using predefined reusable
infrastructure templates
the cloud formation templates provide
ec2 bootstrapping features to help
automate the initial installation and
configuration of the OS and application
stack templates can also be versioned
enabling customers to apply versioning
logic to their AWS infrastructure the
same way they do with software
AWS opsworks is a fully managed
application stack Management Service
that includes provisioning and
configuration of Amazon ec2 instances
Auto scaling application deployment and
application Health monitoring
it also provides the ability to
integrate with additional AWS services
such as elastic load balancing and
Amazon RDS instances
your organization may have decided that
it wants a base set of software included
on all instances launched within its
cloud
this may include homegrown utilities
in-house tools for using AWS services
and advanced software for Enterprise
scale activities such as monitoring and
intrusion detection among other
possibilities
in this circumstance consider using the
base Ami approach with this approach you
can pre-configure all of the software
your organization requires on an Amazon
ec2 instance and then create an Ami from
that instance
the new Ami would then become the amius
to create all new instances within the
organization
organizationally you could enforce use
of the base Ami in a couple of ways
create a custom tool set that becomes
the Gateway tool set for creating AWS
resources and that forces creation of
instances from the pool of custom Amis
create processes that scan the running
Amazon ec2 instances in your account and
terminate any instances that are not
using the standard Amis
third-party configuration tools such as
chef and puppet are used by many AWS
customers to start new Amazon ec2
instances in an automated and repeatable
manner the flexibility of these
environments enables the implementation
of a complete configuration management
system
this screen illustrates one way in which
configuration software can be integrated
within an AWS architecture
let's assume that you want to use
configuration software to initialize a
set of Amazon ec2 instances as web
servers the scenario breaks down as
follows
the administrator sets up a
configuration server this usually is a
standalone Amazon ec2 instance that will
contain a set of templates which
describe all of the applications files
and configuration that a server needs to
initialize itself for example a web
server might instruct an instance to
install Apache httpd configure apache's
httpd config file install all necessary
programming language environments such
as PHP Ruby and create a directory and
file structure for web server content
a configuration server will likely
contain a number of recipes for
configuring dozens of different types of
machines including MySQL server Nat
server Windows IIs server and so on
the administrator will create
configuration templates publish them to
the configuration server and check them
into a source control system for version
management and change tracking
the administrator will use custom
scripting cloud formation opsworks or
similar products to describe and create
a full AWS Cloud deployment environment
these tools may also specify what custom
applications will be hosted in these
environments on which instances they
should be hosted and how they ought to
be configured on each instance
for each Amazon ec2 instance that is
deployed the administrator will instruct
the instance on how to configure itself
as a configuration client of the
configuration server the administrator
will also Supply the name of the
templates to be downloaded and executed
on that machine
when an Amazon ec2 instance is deployed
using a deployment technology such as a
script or a cloud formation it will
configure itself as a configuration
client download the appropriate template
and execute it
various customers use different
combinations of the Technologies
discussed in this topic to manage their
Amis
the solution adopted by your
organization will depend on several
factors including your production
workflow and your organizational
standards
none of these techniques are mutually
exclusive they can all be used in tandem
to craft a strategy that's right for
your company in general placing software
in Amis is best reserved for
foundational slow changing components
that are truly required as standards
throughout your organization
to create an Amazon EBS backed Linux Ami
start from an instance you've launched
from an existing Amazon EVS backed Linux
Ami after you've customized the instance
to suit your needs create and register a
new Ami which you can use to launch new
instances with these customizations
ensure data Integrity by stopping the
instance before you create an Ami then
create the image when you create an
Amazon EBS backed Ami it's automatically
registered for you
Amazon ec2 Powers down the instance
before creating the Ami to ensure that
everything on the instance is stopped
and in a consistent State during the
creation process
some file systems such as xfs can freeze
and unfreeze activity making it safe to
create the image without rebooting the
instance
when you need to automate the creation
of an Ami you can use the AWS ec2 create
image command as displayed on the screen
during the Ami creation process Amazon
ec2 creates snapshots of your instances
root volume and any other EBS volumes
attached to your instance
if any volumes attached to the instance
are encrypted the new Ami will only
launch successfully on instances that
support Amazon EBS encryption
depending on the size of the volumes it
can take several minutes for the Ami
creation process to complete sometimes
up to 24 hours
you may find it more efficient to create
snapshots of your volumes prior to
creating your Ami this way only small
incremental snapshots need to be created
when the Ami is created and the process
completes quickly though the total time
for snapshot creation Remains the Same
after the process completes you have a
new Ami and snapshot created from the
root volume of the instance
when you launch an instance using the
new Ami a new EBS volume for its root
volume is created using the snapshot
both the Ami and the snapshot will
accrue charges to your account until you
delete them
you can copy an Ami within or across an
AWS region using the AWS Management
console the command line or the Amazon
ec2 API all of which support the copy
image action both Amazon EBS backed Amis
and instance store-backed Amis can be
copied you can't copy an encrypted Ami
between accounts instead if the
underlying snapshot and encryption key
have been shared with you you can copy
the snapshot to another account while
re-encrypting it with a key of your own
and then register this privately owned
snapshot as a new Ami
copying a source Ami results in an
identical but distinct Target Ami with
its own unique identifier
in the case of an Amazon EBS backed Ami
each of its backing snapshots is by
default copied to an identical but
distinct Target snapshot the exception
is when you choose to encrypt the
snapshot
The Source Ami can be changed or
de-registered with no effect on the
target Ami the reverse is also true
AWS does not copy launch permissions
user-defined tags or Amazon S3 bucket
permissions from The Source Ami to the
new Ami
after the copy operation is complete you
can apply launch permissions
user-defined tags and Amazon S3 bucket
permissions to the new Ami
the Ami creation process is different
for instance store-backed Amis
first launch an instance from an Ami
that's similar to the Ami that you'd
like to create you can connect to your
instance and customize it
when the instance is set up the way you
want it you can bundle it it takes
several minutes for the bundling process
to complete
after the process completes you have a
bundle which consists of an image
manifest and files that contain a
template for the root volume
next you upload the bundle to your
Amazon S3 bucket and then register your
Ami
when you launch an instance using the
new Ami we create the root volume for
the instance using the bundle that you
uploaded to Amazon S3
the storage space used by the bundle in
Amazon S3 incurs charges to your account
until you delete it
you can convert an instant store-backed
Linux Ami that you own to an Amazon EBS
backed Linux Ami this involves creating
a bundle but the bundle is then
downloaded onto a new Amazon EBS volume
after that you need to snapshot the
volume and create your new Ami from the
snapshot
please note you cannot convert an
instant store-backed Windows Ami to an
Amazon EBS backed Windows Ami and you
cannot convert an Ami that you do not
own using a configuration server can
simplify many common administrative
tasks the most common example is
provisioning and de-provisioning user
access to Amazon ec2 instances say that
Joe an engineer has been given access to
multiple Amazon ec2 instances across the
fleet meaning Joe has a private key that
enables him to log in with his unique
user ID and perform administrative tasks
on these instances but what happens when
Joe leaves the company without a
configuration server deprovisioning
Joe's access to these machines can be a
manual nightmare using a configuration
server however a system administrator
can easily change Joe's status in the
system enroll this change out to all
instances in the fleet with a few simple
commands
one of the advantages of using a
configuration server such as Chef puppet
or ansible is that configuration is it
impotent resources that are created or
configured by a configuration server are
configured or created only once
if manual modifications are made to the
configuration of an instance the
configuration server detects them and
rolls them back this ensures that the
Integrity of the instance isn't
compromised by an accidental change
most configuration software supports a
single machine or Standalone model which
means that it is typically capable of
managing one to hundreds of servers from
a central controller machine hence the
server orchestration turn
in a typical environment you can have a
single administrator machine on which
you create and edit the configuration
templates store them in the source
control system for example git and use
the configuration management tools to
orchestrate your infrastructure
we'll now review some typical failures
that might occur during the launch of
the instances
automated scripts or cloud formation
templates failed to create instances in
different regions
here you need to check the Ami IDs keep
in mind that in different regions you
always have different Ami IDs so you
need to make sure that you set the
correct ones in your configuration
another issue that you might come across
is when the instance is marked ready but
the configuration is not complete keep
in mind that the configuration Readiness
is separate from instance readiness
you need to take additional measures to
check Readiness of your configuration
for example your configuration scripts
can send an alert when the instance is
fully configured another common issue is
when the windows images take a long time
to boot from the Ami
the Microsoft system preparation or
sysprep tool simplifies the process of
duplicating a customized installation of
windows but it can't extend the time it
takes for an image to boot for instance
Readiness note that if you are using
cloud formation templates to deploy
infrastructure cloud formation provides
an object called weight condition that
can be used by user data scripts to
signal to cloud formation when
configuration has completed successfully
the cloud offers you lots of
possibilities but it also brings a lot
of questions about how to manage its
power and flexibility
how do you update servers that have
already been deployed into a production
environment
how do you consistently deploy an
infrastructure to multiple regions in
disparate geographical locations
how do you roll back a deployment that
didn't execute according to plan in
other words how do you reclaim the
resources that were already created
how do you test and debug a deployment
before rolling it out to production
how do you manage dependencies not only
on systems and Technologies but on
entire subsystems
the most commonly used Technologies for
automated repeatable deployments include
custom scripts and applications in which
you can use AWS command line interface
commands or AWS API to automate
deployments in a variety of languages
AWS cloud formation is a building block
service that enables customers to
provision and manage almost any AWS
resource via a json-based
domain-specific language
AWS cloud formation focuses on providing
foundational capabilities for the full
breadth of AWS without prescribing a
particular model for development and
operations
customers Define templates and use them
to provision and manage AWS resources
operating systems and application code
AWS Ops Works focuses on providing
highly productive and reliable devops
experiences for it administrators and
ops-minded developers to achieve this
AWS Ops Works employs a configuration
management model based on Concepts such
as stacks and layers and provides
integrated experiences for Key
activities like deployment monitoring
Auto scaling and automation
compared to AWS Cloud Nation AWS
opsworks supports a narrower range of
application oriented AWS resource types
including Amazon ec2 instances Amazon
EBS volumes elastic ipts and Amazon
Cloud watch metrics
the cloud simplifies the migration of
systems by making it easier to create
parallel environments there is no need
to buy additional Hardware to stand up a
pilot version of your new environment
you can simply create the resources that
you need however you still need to
figure out how to ramp users off the
existing system and onto a new one a
task that is much more challenging than
a so-called Greenfield or initial
deployment
some common patterns used to bring new
systems online in a cloud environment
include blue or green deployment a
parallel or green system is deployed by
Auto scaling alongside an existing or
blue system
the traffic is gradually shifted onto
the green system we'll discuss this
model in depth a little later
red or black deployment this is a
variation on the blue or green method
used by Netflix in this model the
existing or the red system exists in
parallel to the new system
once the new system is fully initialized
and running the infrastructure is
considered to be in a red red state next
the traffic is cut off to the existing
system putting it in a black state
the system is now in red black mode
the old system is no longer receiving
requests and all new traffic is going to
the new system
the black system is kept up and running
while existing connections are completed
to ensure that the red system is
operating without incident
Engineers can bring the black system
online if required
dark launch used mainly for testing a
dark launch brings new features into a
system without enabling the new feature
in the user interface dark launches are
used to ensure that a new feature works
correctly in production and can handle
the load associated with a company's
existing user base
on this screen an example of a blue or
green deployment is displayed in this
model a parallel environment with its
own load balancer and auto scaling
configurations is brought up side by
side with the existing environment
a feature of Amazon Route 53 called
weighted routing can be used to begin
shifting users over from the existing or
blue environment to the new or green
environment Technologies such as
cloudwatch and cloudwatch logs can be
used to monitor the green environment
if problems are found in the new
environment weighted routing can be
deployed to shift users back to the
running blue servers
once the new green environment is fully
up and running without issues the blue
environment can gradually be shut down
due to the potential latency of DNS
records a full shutdown of the blue
environment can take anywhere between
one day and one week
as discussed earlier one of the
challenges faced while deploying in
cloud is implementing version upgrades
you can solve this problem with the help
of Auto scaling there are multiple
approaches
use the oldest launch configuration
termination policy to drain old
instances old instances will be replaced
with the newer ones launched using the
latest version of launch configuration
bring up two separate Auto scaling
groups and use Amazon Route 53 and
weighted routing to drive different
percentages of incoming traffic to the
two groups
you can start creating new instances and
use AWS Auto scaling attach instances to
bring new instances into group and use
the AWS Auto scaling detach instances
policy to remove existing ones
AWS cloud formation enables you to
create and provision AWS infrastructure
deployments predictably and repeatedly
it helps you leverage AWS products such
as Amazon elastic compute Cloud Amazon
elastic Block store
Amazon's simple notification service
elastic load balancing and auto scaling
to build a highly reliable highly
scalable cost-effective applications
without worrying about creating and
configuring the underlying AWS
infrastructure
AWS cloud formation enables you to use a
template file to create and delete a
collection of resources together as a
single unit or a stack
the two major Concepts in AWS cloud
formation are templates and stacks a
template is a specification of the AWS
resources to be provisioned
a stack is a collection of AWS resources
that has been created from a template
you may provision or create a stack
numerous times
when a stack is provisioned the AWS
resources specified by its template are
created any charges incurred from using
these services will start occurring as
they are created as part of the AWS
cloud formation snack
when a stack is deleted the resource is
associated with that stack are also
deleted the order of deletion is
determined by AWS cloud formation you do
not have to direct control over what
gets deleted when
first we'll be creating an Amazon S3
bucket for using the cloud formation
template
you need to decide in which region you
want to create the template for this
demonstration we'll create the template
in the mubai region
go to the cloud formation service under
the management tools section
click create new stack
on this page you get various options for
designing or selecting a template
we already have a Json code ready for
this template
so let's select the upload a template to
Amazon S3 option
click choose file to upload the file
from your system
select the file from your system to
upload it
s a simple Json file that creates an
Amazon S3 bucket
click next
provide a stack name
click next
we'll keep all the values as default and
click next
on this page you can review the template
URL and the stack name
click create
under the events tab you can observe the
progression of the events related to
stat creation as you can see the status
is being shown as in progress
click the refresh icon to view the
progression of the steps in the stat
creation process
click the refresh icon again to refresh
The View
as you can see now stack has been
created
refresh the page again
now you can see the status also displays
that the stack has been created which
means that the stack is ready to use the
resources
click the resources tab to view the
bucket that has been created
as you can see Amazon has provided a
physical ID for the bucket
let's now check out the bucket and see
it has been created correctly let's go
back to the console
click S3 under the storage and content
delivery section
as you can see our bucket is available
click the bucket name as you can see the
bucket is empty
click the properties button to view the
properties of the bucket
as you can see it has been created in
the mubai region
let's now go back to the Management
console and click cloud formation under
the management tools section
we'll now go and delete the stack once
the stack is deleted this bucket will be
automatically deleted
select the stack
click actions and select the delete
stack option
click the yes delete button to confirm
your action
under the events tab you can view all
the events related to the deletion
process
click the refresh icon to get the
updated view every time
as you can see the stack and the bucket
have been deleted
you have learned how to create an AWS
cloud formation template
security is one of the key concerns for
all those who would want to come onto
the cloud it could be an Enterprise it
could be a startup it could be a small
business and they all worry about that
data how secure is the data so let's
spend some time talking about Security
on AWS security at AWS is job zero that
is what AWS talks about that is
everybody's responsibility and it starts
right from the infrastructure we spoke
about regions we spoke about
availability zones so availability zones
are the physical data centers and they
are designed to be fault tolerant they
are designed to be highly available so
they are always in a group of two or
more so that way if one goes down for
any reason we will still have the
services running from the second data
center AWS has something called a shared
security responsibility model
where they say that whatever happens to
the underlying infrastructure they are
responsible for securing that
infrastructure such as the regions the
availability zones The Edge location
they are responsible for that underlying
infrastructure
the customers are responsible for
everything that they put on top of that
infrastructure
so the data the application the access
the firewall rules the customer is
responsible for configuring access to
anybody on that infrastructure
by default AWS provides all the tools
and they do not provide any access to
firewalls that we create
any user that we create or any object
that we upload to the cloud
by default it will be private so we have
to explicitly make it public for any
user we have to give them permission to
do something
otherwise they will not be able to do
anything
whenever we add any data then it is
addressed we have to tell them who can
access it AWS gives you a whole set of
services to secure your data to secure
your applications to secure your
resources such as servers in the cloud
we will also talk a little bit about the
compliance that you need for your
application to be PCI DSS compliant or
if your application deals with the
healthcare industry and must be HIPAA
compliant and you need all those
documents that say that your
infrastructure is HIPAA compliant
there's a place to go and it's a shared
service so you go and you request
something
and you get it
take a look at all that AWS has to offer
when it comes to security
it all starts from the identity and
access management
at the heart of it you would have who is
accessing your system
and then what they are allowed to do in
that system
so we are talking about authentication
and we are talking about authorization
so AWS gives you so many options to
create users you can create rules you
can manage the credentials on your own
or you can have something called
identity Federation so you don't have to
create a login for everybody if you
already have their credentials in your
active directory
or if you want to have them connect with
Facebook ID or Gmail ID or LinkedIn ID
or Twitter ID they should be able to do
that this is known as identity
Federation and it allows you to do just
that they also have something called
roles where the credentials are managed
by AWS and they are rotated internally
so you don't have to worry about
managing credentials or storing them in
the servers or passing them to the Epi
request
they also have something called security
tokens which are nothing but temporary
credentials and you can allow somebody
to access your application or system for
a few minutes maybe 60 minutes maybe up
to a few hours and then these tokens
would automatically expire
they also have something called
multi-factor Authentication
you can have all your users use this
Hardware or software device to
authenticate themselves
you can have the MFA set up on their
phones and you'll be sure that anybody
who is logging in should be in
possession of that phone in order to log
in
similarly for your application they have
something called AWS Waf
that is a web application firewall and
in simply to protect your application
you can configure it to the best of your
choices
and it will simply protect you from any
unwanted traffic that you don't want to
hit your application in similar fashion
they have something called AWS Shield
that is the service to mitigate all DDOS
attacks it comes in two versions a
standard version and an advanced version
and the standard version is free of cost
so it will block most known DDOS and
other attacks
and you can configure it to protect your
Hardware or your resources from any
known attacks
let's say you know you have a lot of
resources in the cloud and you want to
monitor them continuously against any
security threat there's a service called
Amazon inspector where you simply
install an agent and this agent will
keep running we'll keep producing
reports for you on a monthly basis so
the bill is per agent per month and it
is really very economical when you
compare it with other services which are
either paid or open source and not
managed and there's something called AWS
certificate manager
that is a service that simply manages
your SSL and TLS certificate so you can
bring your own certificate and upload it
or you can buy that certificate from AWS
and in both ways you can simply manage
those certificates in one location
AWS gives you that capacity so you don't
have this hassle of managing those
services
now this is a very interesting AWS
artifact that we spoke about
so let's say you need a compliance
certificate that says that the
infrastructure that you're running on is
PCI certified because you are running an
application that takes credit card
information or does some credit card
transactions
you could simply come here and then you
can request the artifact that you want
so you have something like fedramp
partner package you have ISO
certification and you can simply request
that certification and you are going to
get that certification over here
so these are the suite of services that
AWS has to offer for security
apart from that there are firewalls at
your instance level at your server level
firewalls at your subnet level there are
encryption Services you can ask AWS to
do their own encryption or you can run
your own encryption algorithms
there are several kinds of credentials
for accessing different services
and I like to think about it like this
there was once a time when people would
not go on the cloud because of security
and now people organizations Banks
government agencies they're moving
towards Cloud because their data their
Network
is much more secure than what they could
have on their own data center let's see
the steps to create an AWS account
the first step is to open the AWS web
page
then click create a free account to
display the sign in or create an AWS
account page
in this page click I am a new user
enter your email ID or mobile number in
the email or mobile number box and then
click sign in using secure server
the AWS servers verify if an account
already exists with the entered email ID
if yes it displays an error message else
displays the login credentials page
in the login credentials page enter the
name
re-enter the previously entered email ID
and the password for your account
then click create account to display the
contact information page
this page gives you the option to create
a company account or a personal AWS
account
for this demonstration we will create
personal AWS account
then enter your full name
and address in the displayed fields
next enter the captcha characters from
the displayed security image
an important step is to open the AWS
agreement and read it thoroughly then
select the AWS customer agreement
checkbox
selecting the check box indicates that
you have read the AWS agreement and you
agree to it then click create account
and continue to display payment
information page
in this page enter the credit card
details and then click verify card and
continue to display the identity
verification page
before displaying the identity
verification page the AWS servers verify
the entered credit or debit card details
by performing a reversible transaction
of a small amount
this page displays your entered mobile
number and is used to verify and confirm
your identity
as you click call me now the AWS system
calls you on the displayed number and
your computer screen displays a four
digit verification pin
enter the displayed four-digit pin in
your phone using the keypad
once the verification is successful the
page displays a message
identity verification complete simply
click continue to select your support
plan to display the support plan page
page displays and explains about the
three support plans basic developer and
business
for this demonstration we will select
the free or basic plan and then click
continue to display the welcome page of
the AWS services
congratulations you have successfully
created an AWS account somewhere far in
the country a scientist has a lab far in
the woods and at one day he stumbled on
a calculation that he had trouble
solving and he was very certain that his
computer can handle it and he asked his
computer to do the math or the
calculation for him and the computer
thought about it for a while then took
couple of hours in trying to process the
data and eventually at one point it gave
up and died due to low memory and this
old scientist is now clueless about how
he was going to finish the task and
complete his invention and let the whole
world know about it and that's when he
was introduced to this new friend AWS as
you see on the screen AWS was very handy
in replacing his old machines and it was
able to give him the service that he
wanted at that moment AWS has been
around for a while helping people with
their computer needs and now this new
scientist friend in need is welcomed by
the the new and trendy ID technology AWS
and sure enough his new AWS friend
welcomed and walked him through the new
way of computing and this new Happy
faced AWS friend talked to this
scientist about AWS ec2 in specific and
walked him through about all the new
Innovations in ID and in Cloud that he
had been missing all these days so his
new friend AWS started the conversation
about explaining how there is no need
for any hardware units no managing
Hardware or provisioning in AWS and
secondly it explained increasing and
decreasing the capacity as per the
demand is just a click away and the best
part here is that there is no upfront
commitment and we only pay for what we
have used just like we pay electricity
and water bills and all this comes with
complete control from our side other
words the whole key for this
infrastructure is with us and as if this
was not enough all this comes with
Enterprise grade security that's beyond
imagination and on-premises and if this
tool does not excite you then this
definitely would you can work from
anywhere in the world now this really
made the scientist to get excited
especially when he thought about working
from home working from home there is
nothing like it isn't it now this person
the scientist is not tied up to a
particular environment he can work from
home work on the Fly work from anywhere
still the data and everything else in
his ID environment is secure and safe
now let's move to the next level of
discussion let's talk about what is ec2
and some of the use cases of ec2 and
this use case we're going to talk about
are this architecture we're going to
talk about it notifies a group of users
about a new letter and the components
are resources this architecture would
need be first ec2 instance then SNS a
simple notification service and coupling
ec2 and S3 service together that's all
it would require to get an architecture
that notifies a group of users about a
new newsletter and now I think it would
be a good time to talk about what ec2 is
AWS offers plenty of services offered
under different domains like you know
compute storage database migration
Network management tools Media Services
security business productivity
application integration a machine
learning game development and lot more
coming up out of which ec2 falls under
the compute capacity so what is ec2 ec2
is a web service which aims to make life
easier for Developers for providing
secure and resizable compute capacity in
the cloud with ec2 it is very easy to
scale up or scale down our
infrastructure based on the demand and
not only that this ec2 service can be
integrated well with almost all the
services in Amazon and out of all this
the best part could be we only pay for
what we use all right let's talk about
this use case a use case here is that a
successful business owner has a bunch of
users and a successful product that's
running and now he has developed few
more products that he thinks will be
very successful that he thinks his
customers are going to like now how can
he advertise his product to his new and
prospective customers or the solution
for this question can be addressed by
AWS in AWS we can use services like
simple notification service SNS and ec2
for compute and S3 for storage we can in
a way integrate them all and achieve
this business use case and that's what
I've got this business owner very chewed
up and now he wants to use the service
and he wants to get benefited from the
service he wants to advertise or he
wants to notify his users every time the
company creates a newsletter alright so
let's talk about about what it would
take to get this environment up and
running or what it would take to connect
the environment and put the applications
on top of it firstly we would require an
AWS account and then for compute
capacity we would require an ec2
instance and here's how we go about
doing it the first thing is to create an
Ami which is Amazon Mission image that's
really the softwares and the application
packages we would need to run our
application and the second is to choose
the hardware in here it's the instance
type depending on the workload we would
be choosing the hardware and depending
on the intents of the workload we will
be choosing the size of the hardware and
finally we would configure the instances
and how many instance do I want you know
which subnet do I want them in and
what's going to be the in a stop or
terminate a behavior of the instance and
do I want to update any patches when the
instance starts running all those piece
of information go in here when we
configure the instance and then the
first three steps is really about the OS
volume and the Basic Hardware now it's
time to add additional storage to the
ec2 instance that would be step four
here we add additional storage to the
ec2 instance and then tags we use tags
or we would configure tags to easily
identify an ec2 instance at a later
point you know we give it some
meaningful names so we can identify like
you know which team it belongs to which
billing department it belongs to what's
the purpose behind launching this
instance stuff like that in an
environment where we run 700 to 800 or
even more instances identifying an
instance and trying to understand you
know who owns the resource for what
purpose we created it could be an
full-time work so tagging comes to a
rescue at that time after tagging as
step 6 we would configure the firewall
which is also called Security Group for
the ec2 instance and this is where we
would allow or deny connection from
external world to this particular ec2
instance well it works both ways from
outside and from inside out this
firewall blocks the connection based on
port number and IP address and finally
as step 7 we review all the
configurations that you have done and we
make sure that the configurations is
what we wanted and finally click on
submit that's going to launch an ec2
instance alright this was just an
overview of how to create an ec2
instance now let's talk about each and
every step in detail so to begin with
let's talk about how to create an Ami
well the Ami is just a template a
template that's used to create a new
instance or an a new computer or a new
VM or a new machine based on the user
requirement the things that go into an
Ami are the software the operating
system the additional applications that
get installed in it stuff like that the
a Ami will also contain software
information you know information about
operating system information about
access permission information about
volumes they all compact in the Ami
again the Ami is of two types one is
predefined Amis are called Amazon
provided Amis the other one would be
custom Amis the Amis that we create and
if you're looking at a particular Ami
that you don't want to create but still
want to get it from Amazon there is a
place or a portal called Ami Marketplace
there we get like thousands of Amis in
there available for us to shop and use
them on a pay as you go business model
and use them as pay as you go billing so
there you can search Ami that you're
looking for most probably you'll be able
to get it there now let's talk about
choosing the instance type the instance
type is basically the hardware
specification that's required for a
machine that we're trying to build and
the instant types is categorized into
five main families they are to begin
with it's computer optimized now compute
optimize gives us lots of compute power
a lot of processing power so if my
application is going to require a lot of
processing power I should be picking
compute optimized instance and the
second one is memory optimized now this
is very good for application that
require in-memory caching you know there
are some applications that performs well
with cash or through cash or the
application would create a lot of data
that it wants to keep in cash for
rereading or for processing you know for
lengthy processing stuff like that for
those type of application this memory
optimized instance that comes within
memory cache is a very good use case and
the third one is the instant that comes
with the GPU otherwise called GPU
optimized GPU stands for graphical
process unit and this is very good for
application that deals with gaming this
is very good for application that's
going to require large graphical
requirements and storage optimized is
the fourth option just like the name
says this is a very good use case for
storage servers and the fifth family
type is general purpose just like the
name says it's for general purpose if
you're not particular about the family
then you generally would end up picking
the general purpose because here the
services are sort of equally balanced
you know you'll find a balance between
the virtual CPU and the memory and the
storage and the network performance it's
sort of balanced all the components all
the features that needs to go on a
computer are sort of balanced in general
purpose now these instant types are
fixed and they cannot be altered because
it's Hardware based we buy Hardware we
do not have much control on the hardware
that's being used well we have options
but we do not have control on the
hardware and these instant types are
divided into five main families they are
computer Optimum minimized memory
optimized GPU enabled storage optimized
and general purpose then as third thing
we have to configure the instance now
here is where I have a lot of options
about purchasing you know what type of
purchasing do I want to do do I want to
go for a spot instance do I want to go
for a reserved instance do I want to go
for an on-demand instance these are
different billing options available and
that's available under configure
instance not only that here's where I'm
going to put the ec2 instance do I want
an public ipaders assigned to it do I
want an IAM role attached to it IM role
is authentication what kind of
authentication am I going to provide and
the shutout Behavior the Sharon
behaviors include do I want to stop the
instance when the user shuts down the
machine from the desktop or do I want to
Simply terminate this instance when the
user shuts down the instance from the
desktop so those things go in here just
like the name says configure instance a
lot of instance configuration options
comes in here that's the third step and
not only that under the advanced details
or Advanced tab under configure instance
I can bootstrap the instance with some
scripts now bootstrap is nothing but the
scripts that you want to be run in the
instance before it actually comes online
let's say you're provisioning the
instance for somebody else you know
instead of you launching the instance
and then logging in and running some
commands and then handing it over to the
other person you can create bootstrap
shell scripts and you know paste it in a
console option available under configure
instance and Amazon is going to take
those commands run it on the instance
before it hands over to the user that
initially requested for that instance
now it could be a different user or just
you it sort of automates you know
software installation procedures in the
instance that we will be launching and
not only that there are multiple payment
options available under configure
instance the user can pick an instance
under normal price and that instance
would apply normal rates applied to it
and there are also options like reserved
instance where the user can pay for an
instance upfront before a year or before
months you know for a span of year or a
span of months and that way they can pay
less per hour for using that instance
not only that you can also go for spot
instance like bidding for those
instances whoever bids more they get the
instance for that particular time well
these instances are a lot cheaper than
on-demand instances and through bidding
and buying you can keep the instance as
long as your bid price doesn't exceed
the price that Amazon is proposing and
as the fourth step we will have to add
storage to the instance that we are
about to launch and here we have bunch
of storage options I can go for a
permanent storage which is free or I can
go for an external elastic block storage
also called EBS which is paid and it's a
permanent storage or else I can
integrate my ec2 instance with S3 for
its storage needs and the best part
about storage is a free subscription
users they get to use 30 gigabit of SSD
storage or magnetic storage for the
whole year in this page where we are
ready to add storage we will have to
mention or provide the size in gigabit
and the volume type is it going to be an
provisioned volume is it going to be an
general purpose volume is it going to be
a magnetic volume stuff like that there
are volume types and we also need to
give inputs about where the disk will be
mounted and whether this volume needs to
be encrypted or not so all these options
are all these inputs are received from
us under the adding storage section and
then the fifth option would be adding
tags like we just discuss some time back
tags are very helpful to identify a
machine in an environment where we have
700 or 1000 VMS running and security
groups are the actual firewall that sits
in front of ec2 instance and it protects
that ec2 instance from unintended
inbound and outbound traffic now here is
where I can fine tune the access to my
ec2 instance based on port numbers and
based on IP address from which it can be
accessed and finally we get to review
the whole changes or the whole
configurations that we have made to find
out whether they are intact with the
requirement and then click on submit
that's going to launch an ec2 instance
but hold on we're not done yet when
we're about to launch or before the
Amazon console actually launches the ec2
instance it's going to give us an option
to create a keypad remember I said it's
key pair you know key pair is two things
one is public and private the private
key is downloaded by the use user and is
kept with the user and the public key is
used by Amazon to confirm the identity
of the user so just go ahead and
download the private key and keep it for
yourself and this private key gets
downloaded as a DOT pem file it's a
format of the file and it gets
downloaded as dot pem file and our next
step is to access the ec2 instance and
because the instance that we have
launched in this example let's assume
it's Linux instance and that's going to
require a tool called putty to be able
to access it and this putty tool is
really needed when we are trying to
access a Linux instance from Windows
instance most of the time Windows
instance will have put the install in
them but in some rare cases they do not
come with putty in those cases we can go
ahead and download putty and putty
generator and we can start using it to
access the Linux instance now you might
ask well I understand put you what's
putty generator now the file that we
have downloaded is in dot pem format but
unfortunately putty does not accept dot
pem format as input you know it has to
be converted into a different format
called PPK and puttygen is a tool that
helps us to convert the dot pem file
into PPK file so as a quick way to do it
is download generator open it up click
on conversion and insert the dot pem key
that we have downloaded and save the
private key and this when we save the
private key it gets saved as a DOT PPK
type private key and when that's done
the next very step is to open putty and
try to log in and the way to log in is
to open putty put that IP address here
and then click on auth you know this is
where we would input the file that we
have created so click on opt and then
click on browse and find the dot PPK
file that we have converted and stored
browse it and uploaded and then click
con open now that's going to open up a
login screen for us and the Amis comes
with a default username depending on the
Ami that we have picked the username
might differ in our case we have picked
an Ami for which the username is ec2
hyphen user and this is the default by
the way let's put the username ec2
hyphen user and hit enter and that's
going to open up the Linux instance
using CLI there are a few other things
that we can do with the terminal that
will explain it a little later alright
so we have successfully launched an ec2
instance and yeah give yourself a pat on
your back launching an instance was just
one part of the solution so let's
actually talk about how we can notify
our customers SNS or simple notification
service is a service or a product in
Amazon that helps us to notify customers
through email so navigate to SNS in your
account and create a topic and we're
going to use this topic for public
notification so let's make it public and
then add subscribers to it now these
subscribers these subscribers are the
people who you want to be notified about
the newsletter so we already have the
email database in there add them to the
subscribers list and then they will
start getting new newsletters as and
when we post them to the topic and as
Next Step create a bucket in S3 where
you can store content and in that bucket
create an event that triggers a
notification to simple notification
service so this is how it will be set up
and notification will be sent to our
subscribers anytime we put something in
our S3 bucket so S3 bucket is going to
create an event for the notification and
the notification is going to make sure
that it's delivered to your end Customer
because they're already subscribed to
the topic as subscribers and finally
let's connect the S3 e with ec2 so the
bucket and the AWS instance are in sync
so we put some content in the S3 bucket
our email system notifies our customers
and the customers can go online to a
website that's hosted in ec2 and because
S3 and ec2 are in sync the items that we
put in S3 will also show up in ec2 see
how this is all connected and it's
working together and once this is all
connected our subscribers will regularly
be informed anytime we put new content
in the S3 bucket and the same content
will be made available in ec2 instance
through the website so if you are
prepared to embark on a career path
pulsating with excitement Innovation and
Endless Possibilities look no further
than simply lens Caltech postgraduate
program in cloud computing this
encompassing program will arm you with
the knowledge and skills needed to
navigate the vast cloud computing
landscape with unyielding confidence
delve into the depths of cloud computing
architecture the deployment models
fortify security like a digital Fortress
and master migration strategies that
leave a lasting impact discover how to
harness Cloud computing's full suit of
services to construct applications that
are not just scalable but remarkable
works of art don't let this once in a
lifetime opportunity slip through your
fingers ignite your career and join the
ranks of time cloud computing
professionals check the link in the
description box to unveil the wonders of
the AWS Basics course so what are we
going to learn we're going to learn
about what is cloud storage in general
and then the types of storage available
in general in the cloud and how things
were before anybody adapted using S3
that's something we're going to learn
and then we're going to immediately dive
into what is S3 and then the benefits of
using S3 over other storage and then
we're gonna do couple of labs or console
walkthroughs about what is object and
how to add an object what is bucket how
to create a bucket stuff stuff like that
and then we're going to talk about how
Amazon S3 generally works now it comes
with a lot of features it comes with a
lot of Promise
um how does this all work how does
Amazon able to keep up with the promise
we're going to talk about that and then
we will talk about some features add-on
features that comes along with the
Amazon S3 so what is cloud storage in
general cloud storage is a service that
provides web services where we can store
our data and not only that the data can
be easily accessed and the data can be
easily backed up everything over the
internet in chart if you could store
your data if you could access the data
if you could backup your data everything
you do through the internet then that's
a good definition for cloud storage and
additional definitions are in cloud
storage we only pay for what we use you
know no commitment no pre-proitioning is
you know pay as you go type subscription
and the best part is we pay on a month
basis you know we don't rent a hardware
for the year or we don't give commitment
for the whole year it's pay as you go
and pay on a monthly basis and these
Cloud storages are very reliable meaning
once you put the data in it it's never
going to get lost and these Cloud
storages are scalable assuming I have a
requirement to store 100 times of what
my actual data size is and I want it now
it's available in the cloud and these
storages are secure as well because
we're talking about data data virginity
they need to be secure and Amazon
provides tools and Technologies through
which we can secure our data and these
are generally not found in the
on-premises storage system so let's talk
about the different types of storage in
general so S3 is cloud storage in AWS
and then we have elastic Block store now
elastic Block store is actually the SSD
hard drives that gets attached to our
ec2 instance you know it's like the C
drive it's like the D drive it's like
the e Drive that gets attached to our
instances now EFS is elastic file system
the underlying technology is kind of the
same but it differs from EBS in a way
that EBS can be accessed only by the
system that's attached to it meaning the
e- volumes and the d-volumes we spoke
about they can be accessed only if there
is an instance connected to it but these
EFS are actually shared file systems
elastic file system all right they are
shared systems they can be accessed by
multiple systems they can be accessed
from inside the Amazon environment it
can be accessed from on-premises
equipment as well a glacier is actually
the archiving solution in the cloud if
you want to dump a data and try to keep
them in the low cost as possible then
Glacier is the product we should be
using and then we have storage Gateway
if I want to safely move my data from my
local environment to the cloud
environment and also want to keep a copy
of the data locally so users locally can
access them and in a cloud users or
Internet users can access the data from
the cloud if that's your requirement
then storage Gateway is the one we would
be choosing and then we have snowball
snowball is really an data Import and
Export a system but it is actually an
Hardware that gets shipped to our
premises where we can copy a data into
it and then I ship it back to Amazon and
Amazon would copy the data into whatever
destination we give them in our account
and if the data is really huge then I
can call for a snowmobile which is
actually a data center on a truck where
Amazon would send me and truck loaded
with the data center as you see that has
compute capacity lots and lots of
storage capacity and electricity AC and
a lot more so they get comparked near
our data center and cables run into our
data center we can copy data into it
send it back to Amazon and they would
copy it to our account and whatever
storage that we advise them so if the
data is really really huge then I would
be calling snowmobile snowmobile is not
available in all the regions all right
let's take this example how things were
before S3 was introduced you know two
professionals are having a conversation
and one of them is finding it very
difficult to sort of manage all the data
in the organization well if the data is
small it can be easily managed but as
company grow and we are living in an era
where data is everything we want data to
backup every idea we want data to backup
every proof of concept that we provide
so data is everything so in this era
it's all about collecting data analyzing
them and saving you know not losing logs
you know saving them analyzing stuff
like that so coming back to our
discussion here one person finds it very
difficult to store and manage all the
data that they have so some of the data
that this person is having problem
storing is data stat application use to
running and then datas that gets sent to
the customers and data us that the
websites require the data that are
because of the email backups and a lot
more are the storages that an Enterprise
can have and this person is having
problem backing up all those data and
even if we think of increasing the local
storage capacity now that's going to
cost the fortune and few things that
make it sometimes impossible to increase
the storage capacity in-house is you
know we will have to go and pay heavy to
buy hardware and software to run these
storages and we need to hire a team of
experts for maintaining them the
hardware and the software and anytime if
there is a dynamic increase in the
storage capacity the on-premises is our
in-house Hardwares won't be able to
scale and data security data security is
very costly when it comes to building
our own storage environment in-house and
adding data security on top of it so the
other guy in the conversation was sort
of quietly listening everything the
manager was saying and then he slowly
introduced him to S3 because he knew
that all the problem that this manager
was worried about can be solved to S3 in
other words all the scalability all the
data security all the not being able to
provision hardware and software
components are all available with S3 so
that actually brings us to the
discussion about what S3 is S3 is simple
storage service it actually provides an
object storage service let me talk to
you about object and block storage
object storage is where you can store
things in the drive all right you can't
install anything in it and this object
storage can be accessed directly from
the internet whereas block storage is
something that needs to be attached to
an instance and we can't directly access
it but we can install software in it so
that's a high level difference between
object storage and block storage and S3
is an object storage what does that mean
we can store data from the internet we
can retrieve from the internet but we
can't install anything in S3 all right
so so S3 is an object based storage and
it's it's really built for storing and
recovering or retrieving any amount of
data or information from anywhere in the
internet few other things you need to
know about S3 is that this S3 is
accessible through the web interface the
storage one type of accessing or one way
of accessing S3 is by dragging dropping
content into it and another way of
retrieving data from S3 is go to a
browser click on download that's going
to let you download any content and the
data can be five terabytes in size now
we're talking talking about one file you
know you can have hundreds or thousands
of files like that one file can be as
big as five terabytes in size and S3 is
basically designed for developers where
they can push logs into S3 or drive logs
anytime they want instead of storing
locally in the server they can use S3 as
code repositories you know where they
can save the code and have the
applications read the code from there
and lot more if they want to safely
share the code with another person with
a lot of encryption and security added
on top of it that's possible as well so
there are few things about S3 and on top
of all this S3 provides 11 9 durability
and four nine availability meaning
durability is if I store the data will
it get lost Amazon is like no it's not
going to get lost you're gonna have the
data because we provide 11 9 durability
for the data and availability is if you
want the data now will you be able to
show it Amazon is like yes we will be
able to show it we have 99.99
availability and when you request the
data we will be able to show the data to
you all right so let's talk about the
benefits of S3 S3 is durable as we saw
it provides 11 9 durability S3 is low
cost out of all the storage options in
Amazon S3 is the cheapest ns3 is very
scalable like we were saying there
there's no required to pre-provision a
storage capacity you know if you need
more go ahead and use more if you need
even more go ahead and use even more and
once you're done some data needs to be
removed just remove the data so that
particular month you will be paying less
so it's very scalable in nature and it's
very available as well S3 is in regional
service you know it's not based on one
availability zone so one availability is
on going down within Amazon the whole
availability going down it's not going
to affect your ability to access S3
storage and S3 is secure lot of security
features like back policy a lot of
security features like encryption and
then MFA authentication are possible
with S3 that actually adds a very good
security layer on top of the data that
we have stored in S3 and not only that
this S3 is very flexible in terms of
cost flexible in terms of where I want
to store the data in terms of cost there
are a lot of pricing tiers within S3 uh
S3 itself is a cheap service now within
that we have a lot of pricing tiers
depending on the durability so I can
always choose to put data on a different
storage tier or storage option in S3
we're going to talk about it as you
stick along and in terms of flexibility
in region I can always choose any of the
region available in the console or in
the S3 console to put my data to there
is no restrictions on where I can or
cannot put the data in the cloud as long
as there are regions available for me to
move the data to and data transferring
with S3 is very simple all I have to do
is browse to the bucket upload the data
and the data gets uploaded and we can
also upload data using CLI commands are
very similar to Linux commands are very
similar to what we would use in the Run
command prompt in Windows and what we
would use in the Run command prompt in
the Powershell all right let's talk
about a basic building block of S3 which
is bucket and objects now what's a
bucket what's an object object is the
actual data and bucket is the folder
where the objects get stored let me give
you a new definition for object so
object is the actual data plus some
information that reference the data like
is it an JPEG file the name of the file
and at what time it was added to so
they're called metadata right so object
is actually data plus metadata and
bucket is actually a container that
receives the data and safely stores in
it and when we add a data in a bucket
Amazon S3 creates an unique version ID
and allocates it to the object so we can
easily identify it at a later Point let
me show you a quick lab on S3
foreign
console if you're wondering how I
reached here go to Services Under
storage and S3 is right here
and let's create a bucket called simply
learn
now the bucket names will have to be
unique
so I really doubt if simply learn will
be available let's let's check it anyway
all right it doesn't seem to be
available so
um
let me pick another name or let's call
it simply learn Dot
samuel.com
and I'm going to put this in
Mumbai or I can choose Oregon
let me choose Oregon
yes let me choose Oregon
and let me create a bucket
sure enough a bucket got created let me
upload an object into the bucket
and you know these objects can be as big
as
five terabytes we talked about it right
all right let me upload an object
all right so that's my object so you get
the relation right here's a bucket
right here's my bucket and within that
is my object now object is the actual
file plus what type of file it is and
then
and then the size of the file the date
in which it got added and the storage
class it is in at the moment
so if I have to access it
I can simply access it through the
internet
so let's talk about how does this S3
bucket work anyway all right so how does
it work a user creates a bucket they
will specify the region in which the
bucket should be deployed we had an
option we could have chosen to deploy in
all the regions Amazon provides S3
service you know beat not Virginia beat
Mumbai B Tokyo beat Sydney uh beat uh
Oregon and we chose Oregon to be the
destination region where we want to
create bucket and save our data there
and when we upload data into the bucket
we can specify three types of storage
classes in our case we pick the default
which was a S3 standard as we saw on the
object data it was on S3 standard so
that's the basic thing later once the
object gets added we can always add a
bucket policies you know policies Define
who access it and who should not access
it what can they access are we going to
allow users only to read or to read
write as well stuff like that so that's
bucket policy we're defining the life
cycle or the lifespan of the data in the
S3 bucket now over the time do you want
the data to automatically move to a
different storage tier and at any point
do you want to sort of expire the data
you know get the data flushed out of
your account automatically you know
those things can be configured in life
cycle policies and Version Control is
creating multiple versions if you're
going to use S3 for a code repository
creating multiple versions let's say if
you want to roll back to whatever you
had before a month how are you going to
do it if you kept updating the file and
never took a version of it so Version
Control helps us to keep different
versions and it helps us to roll back to
the older version anytime that is a need
so let's talk about the different
storage classes in S3 the different
storage classes in S3 begins with S3
standard now this is the default and
it's very suitable for use case is where
you need less or low latency for example
if you want to access the data of a
student's attendance you would retrieve
them very quickly as much as possible so
that's a good use case to store data in
S3 let's understand the different
storage classes in Amazon S3 now let's
take a school for example and the the
different data is present in a school
and the features of those data the
validity of the data all right so let's
take this example and there are
different storage options in S3 let's
take S3 standard for example and what
would be the actual candidate data that
can be stored in S3 standard let's talk
about that so S3 standard in general is
suitable for use cases where the latency
should be very low and in here the good
example are the good candidate file that
can be stored in S3 is a data that needs
to be frequently accessed and that needs
to be retrieved quickly something like
students attendance report or students
attendance sheet which we access daily
and then it needs to be retrieved
immediately as and when we need it the
other type of storage tier in S3 is
infrequent access or in frequent data
access just like the name says the use
case for that is less frequently
accessed data I mean the candidate data
that should go in infrequent access data
is a student's academic record you know
which we don't need to access on a daily
basis but if there is a requirement we
might want to go and look at it then
that's going to be quick so it's not a
data that we would access on a daily
basis but it's data that needs to show
up on your screen real quick and the
third option is Amazon Glacier now
Glacier is really an archival solution
in the cloud so for archives high
performance is not a requirement so
let's talk about some candidate data
that can go into archives something like
students admission fee and it's not
critical also now anytime if you want to
look at the data you can always wait to
retrieve the data so in other words you
know put it in the archive and
retrieving from the archival takes time
so students old record are a good
candidate to be put in the archives the
other options are one zone IA storage
class where the data is infrequently
accessed and the data is stored in a
single availability zone for example you
know by default Amazon stores data in
multiple availability zones and there is
a charge for that now it's not an option
but the charge includes storing data in
multiple availability zones but if your
data requirement is you want to keep the
charges in a low even further you can
choose a one's own IA storage class
where it stores data in one availability
Zone and the candidate data that can go
in a one zone in frequent access is
students report card and the other
option with Amazon S3 is standard
reduced redundancy storage it is
suitable for cases where the data is not
critical and the data can be reproduced
quickly for example you know take a copy
of the library book take a copy of the
PDF library book for example now we
would have a source PDF and we would
make copies of it and then we make it
available for the readers to read the
other option in S3 is reduced redundancy
storage here the use case is data that's
not critical and a data can be
reproduced quickly for example a books
in the library are not that critical and
we always have a copy of the book we're
talking about PDF so if the customer
facing or the student facing book gets
deleted I can always copy the same PDF
put it in the destination folder and
make it available for the users to read
that would be a very good use case for
reduced redundancy storage all right
let's summarize everything that we
learned about different storage options
in S3 so S3 standard it's for frequently
accessed data it's the default storage
if you don't mention anything the data
gets stored in S3 standard it can be
used for cloud applications you know
content distribution gaming applications
big data analytic Dynamic websites they
are a very good use case for S3 standard
frequently accessed data the other one
on the contrary is S3 standard
infrequently accessed data just like the
name says the use case is this is good
for data that will be less frequently
accessed and and then the use case are
it's good for backups it's good for
disaster recovery and it's good for
lifelong storage of data Glacier on the
other hand is very suitable for
archiving data which is in frequent only
accessed and the Vault lock feature is
the security feature of the glacier that
also provides a long-term data storage
in the cloud this is the lowest storage
tier within S3 the other options are one
zone in frequent access storage class
just like the name says it's
infrequently accessed and it is stored
in just one availability Zone and use
cases are any data that doesn't require
any high level of security can be stored
here in one zone the fifth storage tier
is reduced redundancy storage this is
good for data that's frequently accessed
it's good for data that is non-critical
and that can be reproduced if it gets
lost and reduce redundancy storage or
RRS is and highly available solution
designed for sharing or storing data
that can be reproduced quickly all right
let's compare and contrast couple of
other features that are available in S3
for example durability availability SSL
support burst bike latency and life
cycle management so in standard the
durability is 11 9 durability it's the
same for standard standard IA one zone
IA Glacier except for reduced redundancy
the durability is 11 9 and availability
of all the storage classes is all the
same except for one zone IA where the
availability zone is 99.5 percentage all
of these products support SSL connection
and the first byte latency of these
products are most of them provide access
with millisecond latency except for
Glacier it provides retrieval time of
couple of minutes to a maximum of hours
and all of them can be used for a life
cycle management you know moving data
from one storage tier to another storage
here that's possible with the all of
these storage options all right now that
we've understood the different types of
storage options available in S3 let's
talk about some of the features that are
available on S3 lifecycle management now
lifecycle management is a service that
helps us to define a set of rules that
can be applied to an object or to the
bucket itself lifecycle is actually
moving the data from one storage tier to
another storage tier and finally
expiring it and and completing the life
cycle of the object with lifecycle
management we can manage and store our
objects in a very cost effective way it
has two features basically transition
actions and expiration actions let's
talk about transition actions with
transition action we can choose to move
the objects or move the data from one
storage class to another storage class
with lifecycle management we can
configure S3 to move our data from one
storage class to another storage class
at a defined time interval or at a
defined schedule let's talk about
transition actions in more detail let's
say we have our data in S3 at the moment
and we haven't used the data for quite
some time and it's that's how it's going
to be for the rest of the time so that
data is a very good candidate to move to
the infrequent axis because S3 standard
is a bit costlier and as three
infrequent access is a bit cheaper than
S3 standards so the kind of usage sort
of fits very well for moving that data
into infrequent access so using
lifecycle transition or lifecycle
management I can move the data to S3
infrequent access after 30 days and
let's say that the data stayed in in
frequent access for 30 more days and
then now I realize that nobody is
looking into the data so I can find an
appropriate storage here for that
particular data again and I can move it
to that particular storage which is
Glacier so in this case after 30 days or
in a total of 60 days from the time the
data got created the data can be moved
to Glacier and what is this really help
us with the life cycle management help
us to automatically migrate our data
from one storage cost to another storage
cost and by that it really helps us to
save the storage cost lifecycle
management can also help us with object
expiration meaning deleting the object
or flushing it out after a certain
amount of time let's say that our
compliance requirement requires that we
keep the data for seven years and we
have like thousands and thousands of
data like that it would be humanly
impossible to check all those or keep
track of all the dates and you know when
they need to be deleted stuff like that
but with lifecycle management it is very
much possible I can simply create a data
and set up lifecycle management for the
data to expire after seven years and
exactly after seven years the data is
going to expire meaning it's going to be
deleted automatically from the account
all right let me show you a lab on
lifecycle management and let me explain
to you how it's actually done so I'm
into the bucket that we have created and
here's a data into the bucket assume we
have thousands and thousands of data in
the bucket and that requires to be put
in a different storage tier over time
and that requires to be expired after
seven years let's say so the way I would
create lifecycle management is go to
management from inside the bucket and
click on life cycle and then add and
lifecycle rule just give it a name name
like expire so all the objects that's
present in this bucket meaning the
current version of it set a transition
for it so the transition is at the
moment they are in S3 so here I would
like to put them in infrequent access
after 30 days and then after it's been
an infrequent access for 30 days I would
like to move it to Glacier all right
plus 30 days so how do you read it so
for the first 30 days it's going to be
in Glacier so how do we actually read it
after I put the data in S3 the data is
going to get moved to standard IA and
then it's going to stay in standard IA
and after 60 days from the data creation
it's going to get moved to Glacier so on
the 31st day it's going to move to
standard IE on the 61st day it's going
to move to Glacier let's say if I want
to sort of delete the data if I want the
data to get deleted automatically after
seven years you know being in a glacier
how do I go about doing it let me open
up a quick calculator
365 into 7 that's
2555 days right after that the data is
gonna get deleted pretty much I have
created a life cycle so after on the
31st day it's going to get moved to
infrequent access and on the 61st day
Glacier and after seven years is over
any data that I put in the bucket it's
going to get deleted all right let's
talk about bucket policies bucket
policies are some permission files that
we can attach to an a bucket that allows
or denies access to the bucket based on
what's mentioned in the policy so bucket
policy is really an IM policy where you
can allow and deny permission to an S3
resource with bucket policy we can also
Define security rules that apply to more
than one file in a bucket now in this
case you know we can create an user or
let's say there's already a user called
simply learn we can allow or deny that
user connection to the s 3 bucket or
connecting to the S3 bucket using bucket
policy and bucket policies are written
in Json script and let's see how that's
done all right there are tools available
for us to help us create bucket policies
what you're looking at is a tool
available online that helps us to create
a bucket policy so let's use this tool
to create a bucket policy I'm going to
create a deny statement I'm going to
deny all actions to the S3 bucket and
what is the Arn to which we want to
attach the Arn of the resources actually
the name of the bucket but it really
expects us to give that key in a
different format so the Arn is available
right here copy bucket Arn so this is
actually going to deny everybody now we
wanted to deny a user just one user now
look at that now we have a policy that
has been created that's denying access
to the bucket and it's denying a user
called simply learn pretty much done so
I can use the policy and go over to
bucket policies so once I save it only
the user calls simply learn won't have
access to the bucket and the rest of
them will have access to it so once I
save it it gets added and only the user
simply learn will not have access to the
bucket because we're denying them
purposefully
the other features of S3 include data
protection we can protect our data in S3
with the now one of which is bucket
policy I can also use encryptions to
protect my data I can also use IM policy
to protect my data so Amazon S3 provides
a durable storage not only that it also
gives us unprotected and scalable
infrastructure needed for any of our
object storage requirements so here the
data is protected by two means one is
data encryption and the other one is
data versioning data encryption is
encrypting the data so others won't get
access or even if they get access to the
file they won't be able to access the
data without the encryption key and
versioning is making multiple copies of
the data so let's talk about them in
detail what's data encryption now data
encryption refers to protecting the data
while it is being transmitted and
protecting the data while it is at rest
now data encryption can happen in two
ways one is client encryption encryption
at rest and server side encryption
encryption that's in motion client-side
encryption refers to when client sends
the data they encrypt the data and send
it across to Amazon and server side
encryption is when the data is being
transferred they get encrypted and stay
encrypted throughout the transfer
versioning is another security feature
let I mean it helps us so our
unintentional edits are not actually
corrupting the data for example let's
say you edited the data and now you
realize that the data is incorrect and
you want to roll back now how do you
roll back without versioning it's not
possible in other words only with
versioning it's possible so versioning
it can be utilized to preserve recovery
and restore any early versions of every
object that we stored in our Amazon S3
bucket unintentional erases or overrides
of the object can be easily regained if
we have versioning enabled and it's
possible only if we have one file with
the same name and anytime we update the
file it keeps the file name but creates
a different version ID take this bucket
and data for example in a photo.pmg is a
file that was initially stored it
attached a version ID to it and then we
edited it let's say we added some
Watermark we you know added some graphic
to it and that's now the new version of
it when we store it we store it with the
same file name it accepts the same file
name but creates a new version ID and
attaches it anytime we want to roll back
we can always go to the console look at
the old versions pick the version ID and
roll back to the old version ID all
right let's take this example now I'm in
a bucket that we've been using for a
while and let me upload an object now
before we actually upload an object this
bucket needs to be versioning enabled so
let me watch enable this Bucket from
this point onwards this bucket is going
to accept versioning so let me upload an
object
photo.jpg let me upload it all right it
successfully got uploaded good enough
it's uploaded now let me upload another
object with the same file name now look
at that it was uploaded at 7 40 35 am
let me upload another object with the
same file name that I have it stored
right here that got up to uploaded but
with the same name
all right so that's the other photo now
what if if I want to switch back the way
I would switch back is simply switch
back to the older version Look at that
this is the latest version and this is
the old version that I have I can simply
switch back to the old version that was
created at such and such time and I can
open it
that's going to open the old file so in
short it creates different version of
the data that I create as long as it's
with the same name and at any point I
can go and roll back to the original
data this is a very good use case if you
want to use S3 for storing our codes
let's talk about other feature like
cross region replication now cross
region replication is an very cool
feature if you want to automatically
keep a copy of the data in a totally
different region for you know data
durability for any additional data
durability or if you want to serve data
to your customers who live in another
country or who are accessing your data
from another country if you want to
serve the data with low latency cross
region replication is a very cool
feature that you can use and get
benefited from and let's see how that's
done so before we actually do a lab on
Cross region replication let's put
together a proper definition for it
cross region application is a feature
that provides automatic copying of every
object uploaded to our bucket or your
bucket Source bucket and it
automatically copies the data to the
destination bucket which is in a
different AWS region as you see here in
the picture I put data only in region
one it's going to copy the data to
region 2. and for us to use cross region
replication versioning must be turned on
so it creates versions and copies the
versions as well if tomorrow the
original region goes down let's say the
other region will be active and it has
the complete data that was present in
the original region or at any point if
we want to Simply you know cut the
region replication and use the other
bucket as in Standalone bucket it can be
used as well because it already has all
the data that was present in the master
or all the data that was present in the
original replication bucket let's see
how that's done so there are two things
needed one is versioning and another one
is role when we transfer data from a one
bucket to another bucket we need proper
permissions to do so and these roles
they give us proper permissions to
transfer data from one bucket to another
bucket let's see how that's done right
so here's my bucket a bucket in US organ
let's create another bucket
and
Mumbai
call it dot Mumbai
.com
and put it in
put it in Mumbai
or created bucket in Mumbai they go we
have one in Oregon we have one in Mumbai
we're planning to replicate or create
replication between these two right
create application between these two
buckets so go to the first bucket go to
management and start a replication add a
rule so all content in this bucket is
going to get replicated this is my
source bucket it's quite simple select
the destination bucket now my
destination bucket is going to be simply
learn.samual.mumbai.com all right it
says well you have versioning enabled in
your Source bucket but not on your
destination bucket do you want to enable
it now without which we won't be able to
proceed further so let's go ahead and
enable versioning through the console
that shows up and then like I said it's
going to require permission to put data
onto the other bucket now I can create
different roles these are different
roles that are used for different other
services I can also choose to create a
role that specifically gives permission
only to move the data from one bucket to
another bucket three months it's done so
if I go over to my source bucket and if
I add some data to it let's say
index.html assuming I'm adding some
files to it in theory they should move
to the other region automatically there
you go I'm in the Mumbai region and the
data is they got moved to the other
region automatically let's talk about
the other feature called transfer
acceleration now it's a very handy and a
helpful tool or a service to use if we
are transferring data which is very long
distance from us meaning from the client
to the SC bucket let's say from my local
machine which is in India if I transfer
the data over to Oregon let's say it's a
long distance if it is going to go to
the Internet it's going to go through
high latency connections and my transfer
my get delayed if it is an 1 gigabit
file it's okay but if we're talking
about anything that's uh you know five
terabyte size now if you're talking
about anything that's a five terabyte in
size then uh it's not going to be a
pleasant experience so in those cases I
can use transfer accelerator uh with
which in a secure way but in a fast way
or a fastest way transfers my data from
the laptop or from client to the S3
bucket and it makes use of a service
called cloudfront to transfer or to
enable the data acceleration so the way
it would do it is instead of copying the
data directly to the location instead of
copying the data directly to the
destination bucket it copies the data
locally into a cloudfront location which
is available very local to whatever
place we are in and from there it copies
the data directly to an S3 bucket not
going through the internet that helps
eliminate a lot of latency that could
get added when transferring the data so
let's see how that's done
right here I'm in the S3 bucket and
under properties I can find transfer
accelerator and if I enable transfer
accelerator so I'm in another bucket let
me go to properties and let me go to
transfer accelerator and enable transfer
accelerator so now if I put data into
this bucket they're gonna get copied to
the local cloudfront location and from
there they're gonna get copied to the S3
Bucket from the cloud front now if we
need to compare you know how the speed
is going to be compared to directly
putting the data to the internet and
using cloudfront there is a tool
available that actually runs for a while
and then comes with a report that tells
me how much will be the improved speed
if I use transfer accelerator and it
shows for all the regions available in
Amazon so from the source to the
destination if you want to put uh you
know what's the normal and what's the
accelerated speed when you transfer the
data those results we will get it in the
screen so at the moment this tool is
going through it testing like uploading
some file through the internet and
uploading some file using cloudfront and
it has come up with the calculation that
if I'm uploading file to San Francisco
compared to uploading through the
internet and through cloudfront it's 13
times faster so similar way it's going
to calculate for all the regions
available and it's going to give me a
result one day at offers in a growing
company there was this question going on
between two it personal it was about the
new role that his colleague Jessica has
taken Benjamin the guy standing here
wants to know about it and Jessica's job
is new different and dynamic and
interesting too she scale and manage
servers and operating systems and apply
security patches onto them and monitor
all of these at the same time to ensure
the best quality for application is
given to the users and Benjamin was
awestruck with the amount of work
Jessica is doing and with the time it
would take to complete all of them but
Jessica being a very Dynamic person very
suitable for the job said it was easy
for her to handle all of it and even
more but that easiness on the job did
not last longer as the Environ grew more
and more it being a startup company
Jessica was getting drained and was not
really happy about all her job as she
used to be Jessica's manual way of
scaling and environment did not last
long and she was also finding out that
she missed to scale down some of the
resources and it's costing her a lot she
needs to pay for the service that she
was not at all using she sort of felt
that she was at the end of the road and
there was no way out from this manual
task she was very desperate and that's
when Benjamin suggested something and
that brought back the peace and joy
Jessica initially had Benjamin suggested
about a service called Lambda that can
ease the work that Jessica is doing at
the moment and Lambda as happy as it
looks like it's a solution that solves
the manual repetitive work and lot more
and Lambda introduces itself as the
problem solver and started to explain
the following things the very same thing
that we're going to learn about in this
series so in this section we're going to
learn about the features of AWS Lambda
and we're going to talk about what
Lambda is and then we're going to talk
about where Lambda is being used in the
it or in the cloud environment as we
speak and then we're going to talk about
how Lambda works and some use cases of
Lambda and we're going to be
particularly discussing about the use
case about automatically backing up the
data that's put in the cloud storage
let's talk about Lambda in detail Lambda
automatically runs our code without
requiring us to provision or manage
servers just write the code and upload
it to Lambda and Lambda will take care
of it that means that we don't require
any server to run or to manage and all
you need to do is write the code and
uploaded to Lambda and Lambda will take
care of it which also means that we can
stop worrying about provisioning and
managing servers the only thing Lambda
expects from you is a code that's
working AWS Lambda also automatically
scales our application by running code
in response to each trigger our code
runs in parallel and processes each
triggers individually scaling precisely
with the size of the workload scaling
here is done automatically based on the
size of the workload Lambda can scale
the application running the code in
response to each and every trigger that
it receives billing in Lambda is meter
on the seconds we only pay for the
amount of time that our code is running
which means that we're not charged for
any of the servers the only payment
required is for the amount of time the
code is computed with AWS Lambda we are
charged for every 100 milliseconds we
are actually charged for 100
milliseconds our code executes and the
number of times our code is triggered
and we don't pay anything when the code
is not running let's talk about what is
AWS Lambda now Lambda is one of the
servers that falls under the compute
section or the compute domain of
services that AWS provides along with
ec2 EBS elastic load balancer Lambda is
also a service that comes under the
bigger umbrella of compute services in
AWS and Lambda allows us to execute code
for any type of application with AWS
Lambda we can run code for virtually any
type of application or backend Services
all we need to do is supply our code in
one of the languages that AWS Lambda
supports as we speak the languages that
are supported by AWS Lambda are node.js
Java c-sharp go and Python and Lambda
can be used to run code in response to
certain events from other services and
based on the event it can run functions
and those functions can be of node.js
Java c-sharp Etc now let's talk about
where is Lambda used there are huge
number of use cases for Lambda and there
are many ways AWS Lambda is used
specifically in the business world let's
talk about some of them one use case is
AWS Lambda is used to process images
when it is uploaded in an S3 bucket
let's say the object gets uploaded in an
S3 bucket in a format that we don't
expect it to be which means the file
needs to be formatted so it gets
uploaded in a raw format and AWS Lambda
is triggered anytime a new object is
added to the bucket and the images are
processed and converted into thumbnails
based on the devices the other end
device that would be reading the data it
could be and PC it could be an apple
machine it could be an Android phone it
could be an Apple phone it could be a
tablet What Not So based on the devices
different formats Lambda can get
triggered and convert a video or convert
a picture into the different format that
it requires another use case for Lambda
is to analyze the social media data
let's say let's say we're collecting the
hashtag trending data and the data is
received and it's added into the Kinesis
stream to feed into the Amazon
environment and Lambda action get
triggered and it receives the data and
then the data is stored into the
database which can be used by businesses
for later analysis and some of the
companies that have gotten tremendous
benefit using Lambda or Thomson routers
benchlings nordstorm Coca-Cola robot are
some of the companies to name at the
moment that have received tremendous
amount of benefit by using Lambda let's
look at how Lambda Works in other words
let's look at how the complicated
function behind the scenes work in a
simple and in a seamless way so here
clients send data now clients send data
to Lambda and clients could be anyone
who's sending requests to AWS Lambda it
could be an application or other Amazon
web services that's sending data to the
Lambda and Lambda receives the request
and depending on the size of the data or
depending on the amount or volume of the
data it runs on the defined number of
containers if it is a single request or
if it is less request it runs on a
single container so the requests are
given to the container to handle and the
container which contains the code the
user has provided to satisfy the query
would run and if you're sort of new to
Containers then let me pause here for a
while and explain to you what container
is now container image is a lightweight
Standalone executable package of a piece
of software that includes everything
needs to run it like the codes the
runtimes the system tools the system
libraries and any other settings needed
and edges at the moment available both
on Linux and windows based application
and containerized software will always
run the same regardless of the
environment it's running on and
containers isolate software from its
surrounding for example there could be
difference between a development and
staging environment so that's sort of
isolated and this helps in reducing the
conflict between the teams running
different software on the same
infrastructure all right now that we
know containers needed to understand
Lambda so if there are few requests it
sends to a single container but as and
when the request grows it actually
creates multiple containers shares the
multiple requests to the different
containers there and depending on the
volume depending on the size depending
on the number of sessions the more
number of containers are provisioned so
to handle those requests and when the
requests reduce the number of containers
reduce as well and that helps in same
because we're not running any resources
and paying for it when we're not using
them and in fact we're not at all paying
for the resources we're only charged for
the amount of time a function is running
inside these containers now consider a
situation where you have to set up a
temporary storage and as a system to
backup the data as soon as the data is
uploaded which is a near real-time
backup now a near real-time manual
backups are nearly impossible and
they're not that efficient too and near
real-time manual backups that's what the
business demands but that's not near
real-time backup that to a manual one
that's not at all efficient even if we
come up with a solution of manually
backing up close to near real Time
That's not going to be efficient looking
at the amount of data that will be put
in and looking at the random times the
data will be put into this Source bucket
and there is no way we can do a manual
backup and they keep it as real as
possible but if that's still your
requirement we can use AWS Lambda and
set things up so AWS Lambda manually
handles the backup in other words for an
impossible or a difficult situation like
that AWS Lambda comes for the rescue and
this is what we do create a 2s3 bucket
one would be the source bucket where the
data will be uploaded and the other one
is and destination bucket where the data
will be backed up from the source bucket
and for these buckets to talk to
themselves it's going to require an IM
role and then for the automatic copy
it's going to require an Lambda function
to copy the files from The Source bucket
to the destination bucket and what
triggers the Lambda function the Lambda
function is triggered every time there's
a change in the metadata for the bucket
and this data is then uploaded into the
destination bucket and after setting all
this up we can literally tested by
putting a data in the source bucket
that's going to automatically replicate
or that's going to automatically copy
the data from The Source bucket to the
destination bucket all right let's see
how we can replicate data between two
bucket well we have a feature to cross
region replicate in S3 that's a feature
that comes along with S3 what if you
want to replicate between two different
buckets in two different accounts in
those cases we can use Lambda to
replicate the data between the buckets
so you put one data or you put data in
the source bucket and that data gets
replicated to the destination bucket and
let's see how that's done the procedure
here would be to have two buckets to
begin with and then create an IM role
that lets you access to pull data from
The Source bucket and put data on the
destination bucket and then create
Lambda files or Lambda event or triggers
to actually look for event in the source
bucket and anytime a new data gets added
Lambda gets triggered copies the data
from The Source bucket and moves the
data to the destination bucket and it
uses the IAM role and policy for the
needed permissions and in just a couple
of clicks we have set up a temporary
backup system that can run seamlessly
without any manual intervention and that
can be as near real time as possible
alright that's the concept and let's see
how it is done real time through this
lab so to begin with we need two buckets
so I have here a source bucket and a
destination bucket and the source bucket
as of now do not have any data in it so
as the destination bucket all right so
that's one down the second would be to
create an IM role right so let me create
an IM role and the role is going to have
this policy in it a policy that's
allowing get object on the source bucket
and a policy that's allowing put object
on the destination bucket and here I'm
going to use or I'm going to paste my
source and destination buckets Arn all
right Guru services just go to S3
Source bucket
all right click on the source bucket and
copy the bucket Arn
I said that would be the source bucket
Arn
right on on the destination bucket copy
the destination bucket Arn
and this is going to be my destination
bucket Arn so with this I'm going to
create a policy
all right
go to IM
and create a policy
now I I have already created a policy
with the same information all right
destination bucket Arn and a policy is
available with the name S3 bucket copy
Lambda attach the policy to the role
right go to rules create a role Lambda
is the service that's going to use it in
here attach the policy that we have
created
right give it the name
and then create a role now I have a role
created as well
all right copy Lambda and that's having
the policy that we have created sometime
back now create a Lambda function right
so go to Lambda
create a function give it a name like S3
bucket copy all right choose a role that
we want to use
all right that's the rule that we want
to use copy one two
create a function
all right
and then here we're going to use node.js
code right I have a sample code
this can be used as template in here
replace the source bucket with the name
of the source bucket and the destination
bucket with the name of the destination
bucket this is a node.js code that gets
run when an event get triggers now
what's an event anytime there is a new
object placed in the S3 bucket it
creates an event and the evil triggers
Lambda and Lambda checks this Source S3
bucket picks the data puts it in the
destination bucket all right paste it
here
paste it here and click on save all
right now before you click on Save just
ensure that you have the appropriate
rules defined that's all you got to do
click on save all right now I I already
have created
a Lambda function
all right
which is the same thing same code and
the rule is attached to it
now it's running it's active now let's
put this to test go to S3
pick the source bucket
put some data in it
all right and in theory those data
should be present in the destination
bucket as well
there you go it's all done by Lambda so
if you are prepared to embark on a
career path pulsating with excitement
Innovation and Endless Possibilities
look no further than simply lunch
Caltech postgraduate program in cloud
computing this encompassing program will
arm you with the knowledge and skills
needed to navigate the vast cloud
computing landscape with unyielding
confidence delve into the depths of
cloud computing architecture concur
deployment models 45 security like a
digital Fortress and master migration
strategies that leave a lasting impact
discover how to harness Cloud
computing's full suit of services to
construct applications that are not just
scalable but remarkable work so far
don't let this once in a lifetime
opportunity slip through your fingers
ignite your career and join the ranks of
time cloud computing professionals check
the link in the description box to
unveil the wonders of the AWS Basics
course this is the fourth lesson of the
AWS Solutions architect course
migrating to the cloud doesn't mean that
resources become completely separated
from the local infrastructure
in fact running applications in the
cloud will be completely transparent to
your end users
AWS offers a number of services to fully
and seamlessly integrate your local
Resources with the cloud
one such service is the Amazon virtual
private cloud
this lesson talks about creating virtual
networks that closely resemble the ones
that operate in your own data centers
but with the added benefit of being able
to take full advantage of AWS
so let's get started
[Music]
in this lesson you'll learn all about
virtual private clouds and understand
their concept
you'll know the difference between
public private and elastic IP addresses
you'll learn about what a public and
private Southerner is
and you'll understand what an internet
gateway is and how it's used
you'll learn what root tables are and
when they are used
you'll understand what and that Gateway
is
we'll take a look at security groups and
their importance
and we'll take a look at Network ACLS
and how they're used in Amazon VPC
we'll also review the Amazon VPC best
practices
and also the costs associated with
running a VPC in the Amazon Cloud
welcome to the Amazon virtual private
cloud and subnet section
in this section we're going to have an
overview of what Amazon VPC is and how
you use it and we're also going to have
a demonstration of how to create your
own custom virtual private cloud
we're going to look at IP addresses and
the use of elastic IP addresses in AWS
and finally we'll take a look at subnets
and there'll be a demonstration about
how to create your own subnets in an
Amazon VPC
and here are similar terms that are used
in vpcs
the subnets root tables elastic IP
addresses internet gateways Nat gateways
Network ACLS and security groups and in
the next sections we're going to take a
look at each of these and build our own
custom VPC that we'll use throughout
this course
Amazon defines a VPC as a virtual
private Cloud that enables you to launch
AWS resources into a virtual Network
that you've defined this virtual Network
closely resembles a traditional Network
that you'd operate in your own data
center but with the benefits of using
the scalable infrastructure of AWS
a VPC is your own virtual Network in the
Amazon Cloud which is used as a network
layer for your ec2 resources and this is
a diagram of the default VPC now there's
a lot going on here so don't worry about
that what we're going to do is break
down each of the individual items in
this default VPC over the coming lesson
but what you need to know is that a VPC
is a critical part of the exam and you
need to know all the concepts and how it
differs from your own Networks
throughout this lesson we're going to
create our own VPC from scratch which
you'll need to replicate at the end of
this so you can do well in the exam
each VPC that you create is logically
isolated from other virtual networks in
the AWS Cloud it's fully customizable
you can select the IP address range
create subnet configure root tables
setup Network gateways Define security
settings using security groups and
network access control lists
so each Amazon account comes with a
default VPC that's pre-configured for
you to start using straight away so you
can launch your ec2 instances without
having to think about anything we
mentioned in the opening section A VPC
can span multiple availability zones in
a region
and here's a very basic diagram of a VPC
it isn't this simple in reality
and as we saw in the first section
here's the default Amazon VPC which
looks kind of complicated
but what we need to know at this stage
is that cidr block for the default VPC
is always a 16 subnet mask so in this
example it's
172.31.0.0 16. what that means is this
VPC will provide up to 65
536 private IP addresses
so in the coming sections we'll take a
look at all of these different items
that you can see on this default VPC but
why wouldn't you just use the default
VPC
well the default VPC is great for
launching new instances when you're
testing AWS
but creating a custom VPC allows you to
make things more secure and you can
customize your virtual Network as you
can Define your owner IP address range
you can create your own subnets that are
both private and public and you can
tighten down your security settings
by default instances that you launch
into a VPC can't communicate with your
own network
so you can connect your vpcs to your
existing data center using something
called Hardware VPN access so that you
can effectively extend your data center
into the cloud and create a hybrid
environment
now to do this you need a virtual
private Gateway and this is the VPN
concentrator on the Amazon side of the
VPN connection then on your side in your
data center you need a customer Gateway
which is either a physical device or a
software application that sits on your
side of the VPN connection
so when you create a VPN connection a
VPN internal comes up when traffic is
generated from your side of the
connection
VPC peering is an important concept to
understand
appearing connection could be made
between your own bpcs or with a VPC in
another AWS account as long as it's in
the same region
so what that means is if you have
instances in vpca they wouldn't be able
to communicate with instances in vpcb or
C and thus you set up a peering
connection
pairing is a one-to-one relationship a
VPC can have multiple peering
connections to other vpcs but and this
is important transitive peering is not
supported
in other words
vpca can connect to B and C in this
diagram but C wouldn't be able to
communicate with B unless they were
directly paired
also vpcs with overlapping cidrs cannot
be paired so in this diagram you can see
they all have different IP ranges which
is fine but if they have the same IPO
ranges they wouldn't be able to be
paired
and finally for this section if you
delete the default VPC you have to
contact AWS support to get it back again
so be careful with it and only delete it
if you have good reason to do so and
know what you're doing
this is a demonstration of how to create
a custom VPC
so here we are back at the Amazon web
services Management console and this
time we're going to go down to the
bottom left where the networking section
is I'm going to click on VPC
and the VPC dashboard will load up now
there's a couple of ways you can create
a custom VPC there's something called
the VPC wizard
which will build vpcs on your behalf
from a selection of different
configurations for example a VPC with a
single public subnet or a VPC with
public and private subnets now this is
great because you click a button type in
a few details and it does the work for
you however you're not going to learn
much or pass the exam if this is how you
do it so we'll cancel that and we'll go
to your vpcs
and we'll click on create a VPC
and we're presented with the create a
VPC window so let's give our VPC a name
I'm going to call that simply learn
underscore VPC
and this is the kind of naming
convention I'll be using throughout this
course
next we need to give it the cidr block
or the classes into domain routing block
so we're going to give it a very simple
one
10.0.0.0 and then we need to give it the
subnet mask so you're not allowed to go
larger than 15 so if I try to put 15 in
it says no not going to happen
for a reference subnet mask of 15 would
give you around 131 000 IP addresses
and
subnet 16 will give you 65
536 which is probably more than enough
for what we're going to do
next you get to choose the tenancy and
there's two options default and
dedicated if you select dedicated then
your ec2 instances will reside on
Hardware that's dedicated to you so your
performance is going to be great but
your cost is going to be significantly
higher so I'm going to stick with
default
and we just click on yes create
it'll take a couple of seconds
and then in our VPC dashboard we can see
our simply learn VPC has been created
now if we go down to the bottom here to
see the information about our new VPC we
can see it has a root table associated
with it
which is our default root table
so there it is and we can see that it's
only allowing local traffic at the
moment
we go back to the VPC again
we can see it's been given a default
Network ACL
and we'll click on that and have a look
and you can see this is very similar to
what we looked at in the lesson
so it's allowing all traffic from all
sources inbound and outbound
now if we go to the subnet section
and just widen the VPC area here
you can see there's no subnets
associated with the VPC we just created
so that means we won't be able to launch
any instances into our VPC
and to prove it I'll just show you we'll
go to the ec2 section
so this is a glimpse into your future
this is what we'll be looking at in the
next lesson and we'll just quickly try
and launch an instance we'll select any
instance it doesn't matter
any size not important so here the
network section if I try and select
simply learn VPC it's saying no subnets
found this is not going to work
so we basically need to create some
subnets in our VPC
and that is what we're going to look at
in the next lesson
now private IP addresses are IP
addresses that are not reachable over
the internet and they're used for
communication between instances in the
same network
when you launch a new instance is given
a private IP address and an internal DNS
hostname that resolves to the private IP
address of the instance but if you want
to connect to this from the Internet
it's not going to work
so then you'd need a public IP address
which is reachable from the internet you
can use public IP addresses for
communication between your instances and
the internet
each instance that receives a public IP
address is also given an external DNS
hostname
public IP addresses are associated with
your instances from the Amazon Pool of
public IP addresses
when you stop or terminate your instance
the public IP address is released and a
new one is associated when the instance
starts
so if you want your instance to retain
this public IP address
you need to use something called an
elastic IP address
an elastic IP address is a static or
persistent public IP address that's
allocated to your account and can be
Associated to and from your instances as
required an elastic IP address remains
in your account until you choose to
release it there is a charge associated
with an elastic IP address if it's in
your account but not actually allocated
to an instance
this is a demonstration of how to create
an elastic IP address
so we're back at the Amazon web services
Management console we're going to head
back down to the networking VPC section
and we'll get to the VPC dashboard on
the left hand side we'll click on
elastic IPS
now you'll see a list of any elastic IPS
that you have associated in your account
and remember any of the elastic IP
address that you're using that isn't
allocated to something you'll be charged
for so I have one available and that is
allocated to an instance currently so we
want to allocate a new address
and it reminds you that there's a charge
if you're not using it I'm saying yes
allocate
and it takes a couple of seconds
and there's our new elastic IP address
now we'll be using this IP address to
associate with the NAT Gateway when we
build that
AWS defines a subnet as a range of IP
addresses in your VPC
he can launch AWS resources into a
subnet that you select you can use a
public subnet for resources that must be
connected to the internet and a private
subnet for resources that won't be
connected to the internet
the netmask for the default Subnet in
your VPC is always 20 which provides up
to 4096 addresses per subnet and a few
of them are reserved for AWS use
the VPC can span multiple availability
zones but the subnet is always mapped to
a single availability Zone this is
important to know so here's our basic
diagram which we're now going to start
adding to so we can see the virtual
private cloud and you can see the
availability zones and now inside each
availability Zone we've rated a subnet
now you won't be able to launch any
instances unless there are subnets in
your VPC so it's good to spread them
across availability zones for redundancy
and failover purposes
there's two different types of subnet
public and private
you use a public subnet for resources
that must be connected to the internet
for example web servers
a public subnet is made public because
the main root table sends the subnets
traffic that is destined for the
internet to the internet gateway and
we'll touch on internet gateways next
private subnets are for resources that
don't need an internet connection or
that you want to protect from the
internet for example database instances
so in this demonstration we're going to
create some subnets a public and a
private subnet and we're going to put
them in our custom VPC in different
availability zones
so we'll head to networking and VPC
wait for the VPC dashboard to load up
we'll click on subnets
we'll go to create subnet
and I'm going to give the subnet a name
so it's good to give them meaningful
names
so I'm going to call this first one for
the public subnet
10.0.1.0 and I'm going to put this one
in
the US East
one
B availability Zone
and I'm going to call that simply learn
public
so it's quite a long name I understand
but at least it makes it clear for what
what's going on in this example so we
need to choose a VPC so we obviously
want to put it in our simply learned VPC
and I said I wanted to put it in U.S
east 1B I'm using the North Virginia
region by the way
so we click on that then we need to give
it the cidr block
now as I mentioned earlier when I typed
in the name
that's the range I want to use and then
we need to give it the subnet mask and
we're going to go with
24. which should give us 251 addresses
in this range which obviously is going
to be more than enough if I try and put
a different value in that's unacceptable
to Amazon it's going to say
it's going to give me an error and tell
me not to do that
let's go back to 24 and click and cut
and paste this by the way just because I
need to type something very similar for
the next one
click create
it takes a few seconds
okay so there's our new subnet and I
just widen this
you can see so that's
the IP range that's the availability
Zone it's for simple learn and it's
public
so now we want to create the private
just gonna put the name in I'm going to
give the private the IP address block
that
I'm going to put this one in Us East 1C
and it's going to be the private
subnet
obviously I want it to be in the same
VPC
credibility zone of Us East 1C
and we're going to give it
10.0.2.0
24.
and we'll click yes create
and again it takes a few seconds
okay
so let me sort by name
so there we go we can see now we've got
our private subnet and our public subnet
okay let me just type in simply there we
are so now you can see them both there
and you can see they're both in the same
VPC
simply learn VPC
now if we go down to the bottom you can
see the root table associated with these
vpcs
and you can see that they can
communicate with each other internally
but there's no internet access
so that's what we need to do next in the
next lesson you're going to learn about
internet gateways and how we can make
these subnets have internet access
welcome to the networking section
in this section we're going to take a
look at internet gateways root tables
and NAC devices and we'll have a
demonstration on how to create each of
these AWS VPC items
so to allow your VPC the ability to
connect to the internet you need to
attach an internet gateway
and you can only attach one internet
gateway per VPC
so attaching the internet gateway is the
first stage in permitting internet
access to instances in your VPC now
here's our diagram again and now we've
added the internet gateway which is
providing the connection to the internet
to your VPC but before you can configure
internet correctly there's a couple more
steps
for an ec2 instance to be internet
connected you have to adhere to the
following rules
firstly you have to attach an internet
gateway to your VPC which we just
discussed
then you need to ensure that your
instances have public IP addresses or
elastic IP addresses so they're able to
connect to the internet
then you need to ensure that your
subnets root table points to the
internet gateway
and you need to ensure that your network
access control and Security Group rules
allow relevant traffic to flow to and
from your instance so you need to allow
the rules to let in the traffic you want
for example HTTP traffic after the
demonstration for this section we're
going to look at how root tables Access
Control lists and security groups are
used
in this demonstration we're going to
create an internet gateway and attach it
to our custom VPC
so let's go to networking VPC
bring up the VPC dashboard
and on the left hand side we click on
internet gateways
so here's a couple of Internet gateways
I have already
but I need to create a new one so create
internet gateway
I'll give it a name which is going to be
simply learn
internet gateway igw and I'm going to
click create so this is an internet
gateway which will connect a VPC to the
internet because at the moment our
custom VPC has no internet access
so there it is created
simpler than igw
but this state is detached because it's
not attached to anything so let me try
and attach it to a VPC
and it gives me an option of all the
vpcs that have no
internet gateway attached to them
currently
so I only have one which is simpler than
bpc
yes attach
now you can see our VPC has internet
attached and you can see that down here
so let's click on that and it will take
us to our VPC
but before any instances in our VPC can
access the internet we need to ensure
that our subnet root table points to the
internet gateway
and we don't want to change the main
root table we want to create a custom
root table and that's what you're going
to learn about next
a root table determines where Network
traffic is directed
it does this by defining a set of rules
every subnet has to be associated with a
root table and a subnet can only be
associated with one root table
however multiple subnets can be
associated with the same root table
every VPC has a default root table and
it's good practice to leave this in its
original state and create a new root
table to customize the network traffic
routes associated with your VPC
so here's our example and we've added
two root tables the main root table and
the custom root table
the new root table or the custom root
table will tell the internet gateway to
direct internet traffic to the public
subnet
but the private subnet is still
Associated to the default route table
the main route table which does not
allow internet traffic to it
all traffic inside the private subnet is
just remaining local
in this demonstration we're going to
create a custom root table associated
with our internet gateway and Associate
our public subnet with it
so let's go to networking and VPC
the dashboard will load and we're going
to go to Route tables
now our VPC only has its main route
table at the moment the default one it
was given at the end time it was created
so we want to create a new root table
and we want to give it a name so we're
going to call it simply learn
I'm going to call it root table rtb for
sure
and then we get to pick which VPC we
want to put it in so obviously we want
to use simply learn VPC
so we click create
which will take a couple of seconds and
there you are here's our new root table
so what we need to do now is change its
root so that it points to the internet
gateway so if we go down here to root
at a minute you can see it's just like
our main root table it just has local
access
so we want to click on edit
and we want to add another root
so the destination is
the internet
which is all the zeros and our Target
and we click on this it gives us the
option of our internet gateway which we
want to do so now we have internet
access to this subnet sorry to this root
table
and we click on Save
for successful so now we can see that as
well as local access we have internet
access
now at the moment if we click on subnet
associations
you do not have any subnet associations
so basically both our subnets the public
and private subnets are associated with
the main root table which doesn't have
internet access so we want to change
this so we'll click on edit
and we want our public subnet to be
associated with this root table
let's click on Save
so it's just saving that
so now we can see that our public subnet
is associated with this route table
and this route table is associated with
the internet gateway so now anything we
launch into the public subnet will have
internet access
but what if we wanted our instances in
the private subnet to have internet
access
well there's a way of doing that with a
Nat device and that's what we're going
to look at in the next lecture
you can use a Nat device to enable
instances in a private subnet to connect
to the Internet or other AWS services
but prevent the internet from initiating
connections with the instances in the
private subnet
so we talked earlier about public and
private subnets to protect your assets
from be directly connected to the
internet
for example your web server would sit in
the public Subnet in your database in
the private subnet which has no internet
connectivity
however your private sudden that
database instance might still need
internet access or the ability to
connect to other AWS resources if so you
can use a network address translation
device or a Nat device to do this
and that device forwards traffic from
your private subnet to the internet or
other AWS services and then sends the
response back to the instances
when traffic goes to the internet The
Source IP address of your instance is
replaced with the NAT device address and
when the internet traffic comes back
again then that device translates the
address to your instance's private IP
address
so here's our diagram which is getting
ever more complicated and if you look in
the public subnet you can see we've now
added a Nat device and you have to put
Nat devices in the public subnet so that
they get internet connectivity
AWS provides two kinds of nat devices
and that Gateway and in that instance
AWS recommends in that Gateway that it's
a managed service that provides better
availability and bandwidth than that
instances each Nat Gateway is created in
a specific availability Zone and is
implemented with redundancy in that zone
and that instance is launched from a Nat
Ami an Amazon machine image and runs as
an instance in your VPC so it's
something else you have to look after
whereas in that Gateway being a fully
managed service means once it's
installed you can pretty much forget
about it
and that Gateway must be launched into a
public subnet because it needs internet
connectivity it also needs an elastic IP
address which you can select at the time
of launch
once created you need to update the root
table associated with your private
subnet the point internet bound traffic
to the NAT Gateway this way the
instances in your private subnets can
communicate with the internet
so if you remember back to the diagram
when we had the custom root table which
was pointed to the internet gateway
now we're pointing our main root table
to the NAT Gateway so that the private
subnet also gets internet access but in
a more secure manner
welcome to the create and that Gateway
demonstration where we're going to
create a Nat Gateway so that the
instances in our private subnet can get
internet access
so we'll start by going to networking
and VPC
and the first thing we're going to do is
take a look at our subnets and you'll
see why shortly so here are our simply
learned subnets so this is the private
subnet that we want to give internet
access but if you remember from the
section
that gateways need to be placed in
public subnets so I'm just going to copy
the name of this subnet ID
for the public subnet and you'll see why
in a moment
so then we go to Nat gateways on the
left hand side
and we want to create a new Nat Gateway
so we have to put a subnet in there so
we want to choose our public subnet as
you can see
it truncates a lot of the subnet names
on this option so it's a bit confusing
so we know that we want to put it in our
simply learn VPC
in the public subnet but you can see
it's truncated so it's actually this one
at the bottom but what I'm going to do
is just placed in
the subnet ID which I copied earlier
so there's no confusion
then we need to give it an elastic IP
address now if you remember from the
earlier demonstration we created one so
that select that but if you hadn't
allocated one you could click on the
create new EIP button
so we'll do that
okay so it's telling me my Nat Gateway
has been created and in order to use you
on that Gateway ensure that you edit
your root table to include a route with
a target of and then on that Gateway ID
so it's given us the option to click on
our edit root tables so we'll go
straight there now here's our here's our
root tables
now
here's the custom root table that we
created earlier and this is the default
the main route over which was created
when we launched our when we created our
VPC so we should probably give this a
name so that we know what it is so let
me just call this simply learn rtb
nine
so now we know that's our main root
table
so if you take a look at the main root
table
and the subnet associations
you can see that our private subnet is
associated with this table
so what we need to do is put a root in
here that points to the NAT Gateway so
if we click on roots and edit
and we want to add another root and we
want to say that all traffic
can
either go to the simply the internet
gateway which we don't want to do we
want to point it to our nap instance
which is this Nat ID here
and we click save
so now any instances launched in our
private subnet we'll be able to get
internet access via around that Gateway
welcome to the using security groups and
network ACL section
in this section we're going to take a
look at security groups and network ACLS
and we're going to have a demonstration
on how you create both of these items in
the Amazon web services console
a security group acts as a virtual
firewall that controls the traffic for
one or more instances
you add rules to each security group
that allow traffic to or from its
Associated instances
basically a security group controls the
inbound and outbound traffic for one or
more ec2 instances
security groups can be found on both the
ec2 and VPC dashboards in the AWS web
Management console we're going to cover
them here in this section and you'll see
them crop up again in the ec2 lesson
and here is our diagram and you can see
we've now added security groups to it
and you can see that ec2 instances are
sitting inside the security groups and
the security groups will control what
traffic Flows In and Out
so let's take a look at some examples
and we'll start with a security group
for a web server now obviously a web
server needs HTTP and https traffic has
a minimum to be able to access it
so here is an example of the security
group table and you can see we're
allowing HTTP and https
the ports that are associated with those
two and the sources and we're allowing
it from the internet we're basically
allowing all traffic to those ports
and that means any other traffic that
comes in on different ports would be
unable to reach the security group and
the instances inside it
let's take a look at an example for a
database server Security Group
now imagine you have a SQL Server
database
then you would need to open up the SQL
Server Port so that people can access it
and which is Port 1433 by default so
we've added that to the table and we've
allowed the source to come from the
internet now because it's a Windows
machine you might want RDP access so you
can log on and do some Administration
so we've also added RDP access to the
security group now you could leave it
open to the internet but that would mean
anyone could try and hack their way into
your box so in this example we've added
a source IP address of
10.00.0.0 so only IP arranges from that
address can RDP to the instance
now there's a few rules associated with
security groups by default security
groups allow all outbound traffic so if
you want to tighten that down you can do
so in a similar way to you can Define
the inbound traffic
Security Group rules are always
permissive you can't create rules that
deny access so you're allowing access
rather than denying it
security groups are stateful so if you
send a request from your instance the
response traffic for that request is
allowed to flow in regardless of the
inbound Security Group rules
and you can modify the rules of a
security group at any time and the rules
are applied immediately
welcome to the create Security Group
demonstration where we're going to
create two security groups one the host
DB servers and one the host web servers
now if you remember from the best
practices section it said it was always
a good idea to tier your applications
into security groups and that's exactly
what we're going to do so if we go to
networking and VPC to bring up the VPC
dashboard
on the left hand side under security we
click on security groups
now you can also get to security groups
from the ec2 dashboard as well
so here's a list of my existing security
groups but we want to create a new
security group and we're going to call
it
simply learn
web server SG Security Group and we'll
give the group name as the same and our
description is going to be simply learn
web servers
security groups
okay and then we need to select our VPC
now it defaults to the default VPC but
obviously we want to put it in our
simply learn VPC so we click yes create
takes a couple of seconds
and there it is there's our new Security
Group
now if we go down to the rules the
inbound rules
you can see there are none so by default
a new security group has no inbound
rules
but what about outbound rules
if you remember from the lesson
a new Security Group by default allows
all traffic to be outbound
and there you are all traffic
has destination of everywhere so all
traffic is allowed we're going to add
some rules so let's click on inbound
rules click on edit
now this is going to be a web server so
if we click on the drop down
we need to give it http
so you can either choose custom TCP Rule
and type in your own port ranges or you
can just use the ones they have for you
so http
this pre-populates the port range and
then here you can add the source
now if I click on it it's to give me the
option of saying allow
access from different security groups
so you could create a security group and
say I only accept traffic from a
different Security Group which is a nice
way of securing things down you could
also put in here just your IP address so
that only you could do HTTP requests to
the instance but because it's a web
server
we want people to be able to see our
website otherwise it's not going to be
much use so we're going to say all
traffic so all source
traffic can access our instance on Port
http 80.
I want to add another rule because we
also want to do https
which is
hiding from it
there we are
and again we want to do
the same
and also because this is going to be a
Linux instance we want to be able to
connect to the Linux instance to do some
work and configuration so we need to
give it SSH access
again it would be good practice to tie
it down to your specific IP or an IP
range but we're just going to do all for
now then we click on Save
and there we are there we have our
ranges
so now we want to create our security
group for our DB servers it says click
create Security Group
and then we'll go through it and give it
a similar name
simply learn
TB servers s key
and the description is going to be
simpler than PB servers Security Group
and our VPC is obviously going to be
simpler than VPC
so let's click yes create wait a few
seconds
and here's our new security group as you
can see it has no inbound Rules by
default and outbound rules allow all
traffic
so this is going to be a SQL Server
database server and so we need to allow
SQL Server traffic into the instance
so we need to give
it Microsoft SQL Port access Now the
default port for Microsoft SQL Server is
1433
now in reality I'd probably change the
port the C4 server was running on to
make it more secure
but we'll go over this for now and then
the source so we could choose the IPA
ranges again but what we want to do is
place the DB server in the private
subnet and allow the traffic to come
from the web server
so the web server will accept traffic
and the web server will then go to the
database to get the information it needs
to display on its web on the website or
if people are entering information into
the website we want the information to
be stored in our DB server so basically
we want to say that this the DB servers
can only accept SQL Server traffic from
the web server Security Group
so we can select the simply then web
server Security Group as the source
traffic
but Microsoft SQL Server data
so we'll select that now our SQL Server
is obviously going to be a Windows
instance so from time to time we might
not we might need to log in and
configure it so we want to give RDP
access
now again you would probably put a
specific IP range in there we're just
going to do all traffic for now then we
click save
and there we are so now we have two
security groups
DB servers and web servers
a network ACL is a network access
control list
and it's an optional layer of security
for your VPC that acts as a firewall for
controlling traffic in and out of one or
more of your subnets
you might set up Network ACLS with rules
similar to your security groups in order
to add an additional layer of security
to your VPC
here is our Network diagram and we've
added Network ACLS to the mix now you
can see they sit somewhere between the
root tables and the subnets
this diagram makes it a little bit
clearer and you can see that a network
ACL sits in between a root table and a
subnet
and also you can see an example of the
default Network ACL
which is configured to allow all traffic
to flow in and out of the subnets to
which It's associated
each Network ACL includes a rule whose
rule number is an Asterix
this rule ensures that if a packet
doesn't match any of the other numbered
rules it's denied you can't modify or
remove this rule
so if you take a look at this table you
can see on the inbound some traffic
would come in and it would look for the
first rule which is 100 and that's
saying I'm allowing all traffic from all
sources so that's fine the traffic comes
in if that rule 100 wasn't there it
would go to the Asterix Rule and the
Asterix rule is saying traffic from all
sources is denied
let's take a look at the network ACL
rules
each Subnet in your VPC must be
associated with an ACL if you don't
assign it to a custom ACL it will
automatically be Associated to your
default ACL
a subnet can only be associated with one
ACL however an ACL can be associated
with multiple subnets
an ACL contains a list of numbered rules
which are evaluated in order starting
with the lowest as soon as a rule
matches traffic it's applied regardless
of any higher numbered rules that may
contradict it
AWS recommends incrementing your rules
by a factor of 100 so there's plenty of
room to implement new rules at a later
date
unlike security groups ACLS are
stateless responses to allowed inbound
traffic are subject to the rules for
outbound traffic
welcome to the network ACL demonstration
where we're just going to have an
overview of ocls where they are in the
dashboard
now you don't need to know a huge amount
about them for the exam you just need to
know how they work and where they are so
let's go to networking and VPC
and on when the dashboard loads on the
left hand side under security there's
Network ACLS so let's click on that now
you can see some ACLS that are in my my
AWS account so we want the one that's
associated with our simply learned VPC
so if we extend this VPC
column
that's our Network ACL there simply then
VPC now let's give it a name because
it's not very clear to see otherwise
also I'm kind of an obsessive tagger so
let's call it simply learn ACL and click
on the tick so there we go so now it's
much easier to see
if we click on inbound rules so this is
exactly what we showed you in the lesson
the rule is 100 so that's the first rule
that's going to get evaluated and it's
saying allow all traffic from all
sources
and the outbound rules are the same
so if you wanted to tighten down the new
rule you could say edit we would give it
a new rule number say which would be 200
so you should always increment them in
100 so that means if you had 99 more
rules you needed to put in place you'd
have space to put them in in between
these two
and then you could do whatever you
wanted you could say you know we are
allowing HTTP access from
all traffic
and we're allowing or you could say
actually you know what we're going to
deny it so this is the way of
blacklisting traffic into your VPC
now I'm not going to save that because
we don't need it
but this is where Network ACL sit and
this is where you would make any changes
it's also worth having a look at the
subnet associations with your ACL
so we have two subnets in our simply
learn VPC so we would expect to see both
of them associated with this network ACL
because it's the default
and there they are there's both our
public and our private subnets are
associated and you can also see up here
on the on the dashboard it says default
so this is telling us this is our
default ACL
if you did want to create a new network
ACL you would click create network ACL
you'd give it a name it's just a new ACL
and then you would associate it with
your VPC so we would say simply learn
VPC
takes a few seconds
and there we are there we have our new
one you can see this one says default no
because it obviously isn't the default
ACL for our simply learn VPC
and it has no subnets associated with it
so let's just delete that because we
don't need it but there you are there's
a very brief overview of network ACLs
welcome to the Amazon VPC best practices
and costs where we're going to take a
look at the best practices and the costs
associated with the Amazon virtual
private cloud
always use public and private subnets
you should use private subnets to secure
resources that don't need to be
available to the internet such as
database services
to provide secure internet access to the
instances that reside in your private
subnets you should provide a Nat device
when using Nat devices you should use a
Nat Gateway over Nat instances because
there are managed service and require
less Administration effort
you should choose your cidr blocks
carefully Amazon VPC can contain from 16
to 65
536 IP addresses so you should choose
your cidr block according to how many
instances you think you'll need
you should also create separate Amazon
vpcs for development staging test and
production or create one Amazon VPC with
separate subnets with a subnet each for
production development staging and test
you should understand the Amazon VPC
limits there are various limitations on
the VPC components for example you're
allowed five vpcs per region
200 subnets per VPC
200 route tables per VPC
500 security groups per VPC
50 in and outbound rules per VPC
however some of these rules can be
increased by raising a ticket with AWS
support
you should use security groups and
network ACLS to secure the traffic
coming in and out of your VPC Amazon
advises to use security groups for
whitelisting traffic and network ACLS
for blacklisting traffic
recommends tiering your security groups
you should create different security
groups for different tiers of your
infrastructure architecture inside VPC
if you have web tiers and DB tiers you
should create different security groups
for each of them
creating toy security groups will
increase the infrastructure security
inside the Amazon VPC
so if you launch all your web servers in
the web server security group that means
it'll automatically all have HTTP and
https open conversely the database
Security Group will have SQL Server
ports already open
you should also standardize your
Security Group naming conventions
following a security group naming
convention allows Amazon VPC operation
and management for large-scale
deployments to become much easier
always span your Amazon VPC across
multiple subnets in multiple
availability zones inside a region this
helps in architecting high availability
inside your VPC
if you choose to create a hardware VPN
connection to your VPC using virtual
private Gateway you are charged for each
VPN connection hour that your VPN
connection is provisioned and available
each partial VPN connection hour
consumed is billed as a full hour
you'll also incur standard AWS data
transfer charges for all data
transferred via the VPN connection
if you choose to create a Nat Gateway in
your VPC you are charged for each Nat
Gateway hour that your net Gateway is
provisioned and available data
processing charges apply for each
gigabyte processed through in that
Gateway each partial Nat Gateway hour
consumed is billed as a full hour
this is the practice assignment for
designing a custom VPC where you'll
create a custom VPC using the concepts
learned in this lesson
using the concepts learned in this
lesson recreate the custom VPC as shown
in the demonstrations
the VPC name should be simply learned
VPC the cidr block should be
10.0.0.016 there should be two subnets
one public with a range of
10.0.1.0 and one private with a range of
10.0.2.0 and they should be placed in
separate availability zones
there should be one internet gateway and
one that Gateway and also one custom
root table for the public subnet
also create two security groups simply
learn web server Security Group and
simply learn DB server Security Group in
this section we're going to learn about
why we need a solution called cloud
formation or why we need a product
called cloud formation and we're going
to talk in detail about what is cloud
formation and then how cloud formation
works and the concepts involved in cloud
formation like I know templates and
stacks there are some Concepts involved
in cloud formation we're going to talk
about that and how do we maintain access
control over cloud formation we're going
to talk about that as well in this video
and then we're going to go through a
demo or a lab where we create an lamp
stack environment from a template
through cloud formation and then we're
going to talk about some use cases one
of which include being able to create
and redeployable template using cloud
formation that's a very good use case
we're going to talk about that in this
video so let's talk about why cloud
formation so when the number of products
and services grow more and managing them
and maintaining them becomes an tedious
work and cloud formation is sort of
eases that environment now cloud
formation is really an infrastructure as
the code and it sort of solves all the
problem that we discussed sometime back
and with cloud formation we can actually
create a perfect clone of the server
configuration at any point in time and
we can also manage the configuration
changes and configuration changes with
cloud formation we can manage the
configuration changes across the
environment and not only that we can
easily embed in the ad hoc changes to
our existing environment so one might
ask why use cloud formation let's look
at the current problem that we have
creating and managing multiple AWS
resource in the portal is a big task
especially when you need to replace some
Services it becomes an bigger task now
cloud formation eases that word let me
explain it in detail now at the moment
without the cloud formation or
environments that don't use cloud
formation have this problem that is lot
of time is being spent on building the
infrastructure and less focus and less
time at the moment is being given on
developing that application that runs on
that environment that's because majority
of the time is consumed by building the
environment and if we need to redeploy
the same environment again and again the
same happens in a cycle again in a new
and with almond we start from the
scratch build environment and then put
application on top of it now that can be
avoided using cloud formation now
without cloud formation it still is a
problem because the the resource or the
bandwidth that is needed for the
application development is not provided
because that's taken by developing the
infrastructure and to solve that problem
we can use cloud formation now that
leads to a discussion about what is
cloud formation so let's talk about or
let's try understanding what cloud
formation is cloud formation is a
service that provides provides users a
simple way to create and manage
collection of AWS Resources by
provisioning and updating them in a very
orderly and in a predictable way if I
need to expand the explanation of cloud
formation cloud formation is an server
that really helps us to model and set up
our Amazon web services or resources so
we can spend less time on managing the
resources and show more focus on the
application that runs on top of it we
can simply create a template for an ec2
instance or for an RDS instance and
upload it in the cloud formation portal
and cloud formation will provision and
configure those resources based on the
template that we Define we really don't
have to sort out the complexities in
dependency between those applications
once the template is vetted and
validated cloud formation takes the
responsibility of handling the
dependencies between the application in
an orderly and a predictable way it
creates those services and makes it
available for us once the template is
fully run so in short the cloud
formation it allows us to create and
model our infrastructure and
applications without we having to
perform the manual action for example
the well-known company called Expedia is
able to easily manage and run its entire
front and back-end resources they run on
AWS in cloudfront what does that mean
they are spending less time on managing
infrastructure and More Time On The Core
Business and application that runs on
top of it and we're still talking about
what cloud formation is with cloud
formation it enables us to manage our
complete infrastructure or AWS resource
in a text file now we're going to see
that in a moment you know it's either a
Json or an yml file and that file is
called as the template to begin with the
resources the template provision is
called the stack all right so the
abstract of the code is called the
template and there is a Associated
Provisions is called the stack and this
stack can be I mean it's going to run
resources obviously and those resources
can be updated as well it's not like
once you create a stack gets done if you
need a change you need to go create
another resource that's not needed this
stack is an updatable stack let's say if
I'm including two more servers if I'm
sort of extending my environment it's
branching out to another application now
I'm including another functionality in
my environment all those can be emboded
in the update and can be updated so like
I said the stack is an updatable one now
let's talk about what the template can
do or some features and functionality of
the template Now using a template we can
almost include all the application and
resources that a user might require
let's say you know after the ec2
instance is provision running an
application on top of it and you want to
templatize that and include that in the
template that's possible as well and
these templates are portable ones
meaning I can use the template in one
region and I can use the same template
in another region and that's going to
build a very similar environment in
another region I can also share that
with the customer and when they run it
it's going to run an environment from
the portable template in their account
in their environment of course all the
security is taken care I'm just saying
that if we need to build a resource if
we need to build an architecture if you
need to build an environment which looks
very similar to the other environment
that is possible and and these templates
are usable by the use of some sections
in the template like the parameter
section the mapping section the
condition section within the template is
what makes the templates reusable and we
will talk about parameters mapping and
conditions down the line we're still
talking about AWS how AWS cloud
formation works and we're going to talk
about how a template becomes an
infrastructure right so to begin with
one request a template and the template
will be in Json or ym ml format so one
would develop a template based on the
requirement based on number of ec2
instances based on whether they want a
load balancer based on what they want
and a Windows Server based on whether
they want an database server in that
environment and then based on the
applications that will be running on top
of it that can be templatized in the
code itself so one creates a code and
then the code is saved locally or they
can be put in an S3 Bucket from which
the cloud formation will call the
template and provision the resource
right so we use cloud formation to
create a stack or create the resource
defined in the template and cloud
formation analyzes the template first
validates template and then it
identifies the dependencies between them
and then it starts provisioning the
resource one after the other for example
if you need five servers in a new VPC
obviously it's going to create the VPC
first and servers second and stack all
right let's understand the two major
components in cloud formation by now you
might have guessed it the two major
components are the template and the
stack together the aid or they
complement cloud formation and that's
how the resource of provision that's how
the results are managed and that eases
the job of the administrator who's
managing the environment so let's talk
about template first a template in cloud
formation is an text file or a formatted
text file in Json or yaml language and
that the code or the template or the
text file that actually describes how
your infrastructure looks like or will
look like once provision and for us to
create the template or for us to you
know modify an existing template we can
use a tool available called cloud
formation designer from the console or
any text editing tool that's available
but I would recommend the cloud
formation designer though that has a
rich Graphics in it and these templates
are built of or they consist of nine
main objects the first of which is the
format version and this really
identifies the capabilities of the
template based on the version number and
then the second object in a template
would be description and this helps us
to include any arbitrary comments about
the template for what reason are we
building this template what's the
purpose of it stuff like that and then
we have the object called metadata in a
template and that includes a details
about the template details of the
resources in the template and then we
have the optional parameters section
this really helps us to customize our
template now parameters enable us to
input custom values to our template each
time we create or update and stack and
the mapping object it helps us to match
a key to a corresponding set of named
values for example if you want to set a
value based on region then we get to
create a mapping that uses region name
as the key and then the value that we
want to specify for each specific
regions and then we have conditions
object this is again an optional object
with this we can include statements that
Define when a resource is created or
when a property is defined for example
we can compare whether a value is equal
to another value and based on the result
of that condition we can conditionally
create additional resources and then the
object called transform it's it's an
optional object as well and this
transform section specifies one or more
transforms that we can use in our cloud
formation template for example errorless
cloud formation transforms helps
simplify template authoring by
condensing the expression of the AWS
infrastructure as a code or the cloud
formation template and it enables reuse
of template components for example we
can condense multiple line resources
declaring it into a single line in our
template now we will talk about all
these examples in the upcoming slides
and then we have the required section
called resource that declares the AWS
resource that you want to include in the
stack for example if you want to include
an ec2 instance if you want to include
an S3 bucket we would be using the
resource object in our stack and then we
have object called output this output
object it's an optional object in the
template and it helps us to sort of
declare the outputs that we can import
into other Stacks or we can actually
show it on the cloud formation console
for example if you want the output of
the or the name of the S3 bucket that
got created in response to a cloud
formation template now we can have that
outputted into an output section in the
cloud formation console itself so sort
of easy for us to identify the resources
that the template provisioned so with
all of those objects put together this
is how a full-blown cloud formation
template with all the objects included
is going to look like now this includes
optional and mandatory object let's talk
about those objects in detail in the
upcoming section all right let's now
discuss each and every object of this
template structure and let's begin our
discussion with the format version The
Format version it really defines the
capabilities of the template the latest
version format version as of now is the
one that was updated on 2010. now the
value of the template format version it
has to be an string and if we really
don't specify the format version cloud
formation automatically assumes the
latest version talking about description
descriptions are really any comments
that help ourselves that we want to
embed in the template itself that can be
a description and this description in a
template it has to be an string the
length of the string can be between 0 to
124 bytes in length the metadatars are
really used in the template to provide
any additional information using the
Json and yaml objects and metadata holds
some implementation details of a
specific resource and parameters are
really a way to customize the template
each time we create or update our stack
these parameters are the ones that helps
us to give some custom value to the
template during runtime in the example
shown below the parameter named instance
type parameter it lets us to specify the
ec2 instance type for the stacks to use
when creating or updating the stack by
default as we see it's picking T2 micro
and this is the value that will get
picked if we don't provision another
value to it and mapping mapping really
enables us to map a key to a
corresponding named value that we
specify in a conditional parameter and
also we can retrieve the map or to try
the values in the map using the function
find in map in strict function let's
take this example let's say based on a
region if you want to set values in a
template we can create a mapping that
uses a key and holds the value that you
want to specify for each region and the
object conditions can be used when we
want to reuse the templates by creating
resources in a totally different context
for example in a template during stack
creation all the conditions that we
mentioned in the template are evaluated
and any resources that are associated
and that come up to be true are created
and any condition that fails or invalid
are kind of ignored automatically and
the check moves forward to the next
condition and conditional is an optional
object and it includes statement that
Define when a resource is created or
when a property is actually defined for
example you can create whether for
example you can compare whether a value
is equal to another value and based on
the result of that condition we can
conditionally create resources if you
have multiple conditions then they are
generally separated with commas
sometimes conditions are very helpful
when we want to create templates and we
want to reuse those templates which was
created for one environment to another
environment for example if you want to
use a template that was created in a
test environment in a Dev environment
conditions would be helpful for example
in a production environment we might
want to include instances with certain
capabilities and in a test environment
we might want to include instances with
some reduced capability in them for
saving money and condition helps us to
sort of create resources based on the
environment it will be used in we can
also use intrinsic functions to Define
conditions like equals we can also use
our conditions we can use not functions
intrinsic functions in a condition and
when put together the syntax is going to
look like the one that you see on the
screen now the optional object called
transform it specifies one or more
transforms that the cloud formation uses
to process our template now these
transform sections are built on a simple
declarative language of AWS cloud
formation with a powerful macro system
that helps us to reuse the templates or
the template components and they really
help us to condense or simplify our
codes by enabling reuse of the template
components for example condense a
multiple line resource declaration into
a single line in our template the
resource section it really declares the
AWS resource that you want to include in
the stack such as ec2 instance such as
S3 bucket and some of the resource
fields are the The Logical ID that it
shows type and the properties let's talk
about them in brief The Logical ID is
unique within the template and The
Logical ID name it actually reference
the resource in other parts of the
template in this case it's my bucket for
example I know if you want to map an
elastic Block store volume or if we want
to map an name of the bucket in another
place in the template not then we would
use the logical ID and again in this
case it is my bucket a logical name has
the physical ID with which we actually
assign the resource for example in this
case my S3 bucket was named as my bucket
to begin with and the resource type in
the template it identifies the type of
the resource is an S3 bucket and the
properties are the additional optional
information that we can specify to our
resource for example if it is an ec2
instance let's say we would like to know
which Ami used then property section
really defines the Ami that was used or
Ami as the property of the instance so
this section describes the output value
that needs to be imported to other
Stacks or the output value that needs to
be shown in the console for the user to
easily navigate to the resource that are
being created for example for a S3
bucket name we can declare an output and
once the bucket is created we can
actually declare the output and sort of
import the name of the bucket in a
different place in the console so it's
it's easy to view instead of you know
the user getting confused and going to
S3 and searching for the bucket name
this can be made available in the cloud
formation console itself and the output
Fields it can include some of these
followings like The Logical ID it's
actually the identifier of the current
output and it's very unique within the
template and it can also include a
description this description is really a
string type that describes the output
value and it can also include value and
value is the property that was returned
by the AWS cloud formation describe
Stacks command all right let's talk
about the template resource attribute in
this section we're going to talk about
attributes that you can add to a
resource to control additional behavior
and relationships so let's talk there
are many resource attributes available
out of which we have I mean if we need
to name them we have a creation policy
and then we have deletion policy it
depends on metadata attribute update
policy attribute let's talk about each
of them separately when we associate the
create policy attribute with the
resource it actually delays the resource
configuration actions before proceeding
with the stack creation I mean the
attribute stack creation is delayed till
the time the cloud formation receives
some number of success signals back I
mean this is a very good use case when
we do auto scaling and this creation
policy can be used only with a limited
number of services like Auto scaling and
ec2 instances
some of the use cases would be if you
want to install and software application
on an ec2 instance you might want the
applications to be running before it
proceeds further so in this case we
would use the creation policy attribute
to the instance and then send the
success signal after the applications
are installed and configured then cloud
formation goes to the next step here in
the syntax that we're looking at the
auto scaling creation policy and the
minimum successful instance percentage
it's actually a number of signals that
it will have to reply back before it can
move forward using deletion policy it
preserves backing up of resources when
the stack is getting deleted and by
default if you don't have a delete
policy the data gets deleted just like
that but if we use an deletion policy we
would be able to preserve the data by
taking a snapshot of it or just letting
the resource and moving on with deleting
the other resources in the stack and we
specify the delete policy attribute for
each resource specifically that we want
to control and if there are no deletion
policy mentioned for a particular
resource the cloud formation simply
deletes the resources as that's its
default behavior and if you want to keep
the results when the stack is deleted we
use we we mention V10 in the delete
policy and that simply retains the
resource from getting deleted so in this
example we're actually retaining an
dynamodb table rest of the resources in
the stack will get deleted and using the
depends on attribute we actually create
an order in which the resource gets
deleted for example if I map one
resource to another resource and say
that this resource depends on the other
resource the other resource gets created
first Then followed by this instance or
this resource so with the depends on
attribute we can specify that the
creation of specific resource is
actually followed by and another when we
add the depends on attribute to our
Source the resources created only after
the creation of the resource specified
by the depends on attribute let's take
this example in this example we're
saying that the resource X depends on y
we've said that X depends on y so in
this case when cloud formation is
executing it's going to deploy y first
because it has some resources that
depends on it so it's going to create y
first and then it's going to create the
resource called X on the same context
let's say in the template we say that an
ec2 is going to depend on S3 and when
the cloud formation is executing it's
going to create an S3 bucket first and
then the ec2 instant that's probably
because the codes needed for the ec2
instance are stored in the S3 bucket and
and what we're looking at is the syntax
of how that depends on template resource
attribute will look like this is how it
actually will look like when we put it
in the cloud formation the instance
depends on the buckets the other
template resource attribute is metadata
metadatars are the one that really helps
us to associate our Resources with the
the structured data and by including
this attribute in our template or by
including this attribute to a resource
we can specify the data in Json or in
yaml language for example in this
section of the template we're creating a
VPC and the metadata explains our
metadata defines that the VPC is part of
the region U.S east one and the update
policy template resource attribute and
cloud formation we can manage and
replace the updates of the instance in
an auto scaling room for example you're
looking at this sample where it says
that the update policy will it replace
the instance in an auto scaling group or
not so depending on what the value is
for the will replace section whether
it's true or false depending on that at
the update policy will replace the
instance or will replace the auto
scaling group itself when it's doing
enrolling deployment now that we've
discussed about templates let's talk
about AWS Stacks as well now we know
that AWS Stacks are the actual resources
that gets generated or that gets created
because we ran a template right so stack
is a collection of AWS results that we
can manage in a single unit in other
words you know we can create a stack
update a stack or delete a stack and
that would actually create update delete
the collection of resources within the
stack and all the resources in the stack
are really defined by the cloud
formation template and a stack for
instance can include all the resources
required to run a web application such
as a web server a database server and
some networking rules that come along
with it all that gets embedded in a
stack and let's say if we want to delete
all the resources that were created by
template that is in a separate stack we
can simply delete the stack and that
would sort of delete all the resources
that is in the stack so it sort of helps
us to manage all the resources in a
single unit or as a single unit so to
summarize stack is a collection of AWS
resource and it can be managed as a
single unit and the cloud formations
template defines a stack in which the
resource can be created deleted or
updated in a very predictable way and a
stack can have all the resources like
web server database server VPC the
network and the security configurations
in a VPC all that is required to run a
web or a database application now stack
could be nested as well with cloud
formation we can actually Nest a stack
with another stack and that's going to
create an hierarchy of the stacks for
example I mean this is a very good
picture that explains how the stacks are
nested and hierarchical way let's say
stack p is the root stack and that could
should be apparent stack for the stack
called R and the stack all Cube and the
same way the the stack call R is a
parent stack for the stack s and that
the stack s is the Varan stack for D and
Q you get that right so Stacks can
always be nested it's possible plot
formation allows us to create a
Microsoft Windows stack based on Amazon
ec2 Windows Amis and provide us with the
ability to install software and use a
remote desktop to access our stack and
update the configuration of our stack
and these Stacks can be run on Windows
Server instances there are a number of
pre-configured templates available and
it's directly available to use from the
AWS website itself to name a few we have
templates available for SharePoint
running on and a Windows Server 2008 R2
we also have templates that create a
single server installation of active
directory running on a Windows server
and in fact we have templates available
for elastic Beanstalk and some sample
applications that runs from an Windows
Server 2008 R2 let's talk about stack
sets stack sets actually has taken the
attack level implementation to the next
level using AWS cloud formation template
we can define a stack set that lets us
to create stack in AWS account across
the globe by using just a single
template and after we create the stack
or after the stack set is defined
creating updating or deleting the stack
in the Target account and the regions
can also be specified by us so the stack
said it really extends the functionality
of the stacks by allowing us to create
update and delete stack across multiple
accounts regions in just a single
operation and using an administrator
account we can Define and manage an AWS
cloud formation template and use the
template as the basis for provisioning
Stacks into you know some selected
Target accounts across a specified
region let's talk about cloud formation
access control with IAM I can specify
the list of users or list of privileged
users who would have access to the cloud
formation template who can create update
and delete Stacks so that's how IM helps
me with having or putting a layer of
control over the cloud formation service
and we can also use service roles
service roles are the one that allows
AWS cloud formation to make a call to a
resource in a stack on the user's behalf
and I can also have stack policies and
these stack policies are applied to the
users who already have access to the
cloud formation service and these
policies are the ones that help the
users who attempt to update the stack
all right let's do a quick lab on
creating and lamp stack on an ec2
instance with a local mySQL database for
storage now we can do the other way as
well I can pick an ec2 instance and
install the servers one top of the other
and that can be done as well but that's
going to take a lot of time now we're
talking about automating it we're
talking about you know provisioning and
infrastructure using a code and platform
machine helps us to do that so this is
what we're going to do we're going to
create or we have in cloud formation
template already and we're going to use
that template the provision web instance
and that is going to have look at that
that's going to have MySQL installed in
it that's going to have PHP that's going
to have httpd service installed in it
and running and it's also going to have
an a PHP website running from it so
that's going to receive information from
the website and that's going to store
that information in the database that's
very local to it
this is the template that it defines
that environment
all right this is a very simple
provisioning so let's provision this
environment let's create a stack
we're going to give a stack name
and a database password
right and
which region is it this is in California
is that
all right
now let me do one thing let me switch to
the Mumbai region or let me switch to
North Virginia region
Al from here actually let me switch to
Mumbai region
all right just to be sure it's the same
template that we're provisioning
right that's the same template that we
are provisioning
all right let's give this a name so it
can be great this could be simply learn
simply learn lamp stack and what could
be the DB password
and DB root password
and what could be the DB user
I'm going to keep the rest as defaults
and I'm going to launch
all right so there's an issue with the
password
[Music]
password should only have Alpha numeric
characters get that
so let's go back and create a new
password
right it also needs to have eight
characters
so what can be your password
look at that the template is actually
creating the resources
and here in the screen we can see the
template that we used to deploy the
environment
if you remember we have we discussed
about instant types in parameters right
the default instance type is T2 small
but these are also allowed
to run
all right parameter section is the one
that was used to receive information
from us like the DB name
the DB password the DB root password DB
user instance type key name and the SSH
locations
all right now this resource section is
is showing us the resource that it
actually uh created
right looks like it's rolling back why
would that be give me a moment
[Music]
all right this is one live example
anytime it could not provision or any if
there is any Interruption that it cannot
proceed it would simply roll back and
delete all the resources that it had
provisioned and that's what it did
now let me fix this
oh as a fix I have switched uh the regen
because I had some issue with the VPC
that's going to take time for me to
analyze and fix it so here we're going
to understand about plot formation so I
thought I'll run the cloud formation
from another VPC that I know is working
fine that's in North Virginia
all right let's provision this lamp
server which is running out of Linux
which has Apache installed on it which
has PHP and MySQL already installed in
it so one server gonna act as web and DB
server
all right so I'm going to create a stack
and from here I'm going to pick a sample
stack now if we look at if you look at
it in the designer
it's going to provision one lamp server
and a security attack Security Group
attached to it there you go right so
let's go ahead and create a stack
all right
parameters are used to receive
input from us what could be the DB
password
and the DB user
and
let's select the key name
right I'm going to keep the rest as
defaults and I'm going to create
a stack
all right so our stack
that we run from the template
at the moment it's getting created it's
in progress
from here I can watch the events as to
what is getting created at the moment
like I said in a lot of these would have
already sorted out their dependencies
you know which one needs to get created
first which one needs to get created
second all that is sorted out by cloud
formation itself
and parameters are the values that we
have inputted and resource
tab is section where we can find what
resources are being provisioned at the
moment so to begin with it created a
security group at the moment it's
creating an instance and output uh it's
empty as of now we're going to come back
and see it now output is where we can
get outputs of a finished job let's say
here I wanted an URL
of the server that got created so
instead of me uh you know going to a
couple of other places within the
console and find the output of the URL
or the the output of the resource that
was created I can simply get it from
here well this needs to be defined in
the templates First Once We Define it in
the template it's going to be made
available
like here
right so I can open it up
and uh so there you go so it has it
already has
a database locally installed and it's
connected over if I launch an
application on top of it that will get
connected to the database which is very
local and my single
server with web and database in it is
ready for production
now while we're talking about a lamp
server a single lamp server let's talk
about
uh creating an environment like this
which has elastic load balancer a VPC uh
server running in an auto Skilling group
and putting some restrictions that
nobody will be able to access the ECU
instance directly but they should be
able to access the ec2 instance or the
data only through the elastic load
balancer this is a bit complicated than
the environment that we've already
created let's see how this is done all
right in this lab we're going to create
elastic load balance to Auto scaling
Group which is going to create a load
balancer and an auto scaling group and
that auto scaling group receives traffic
only from the load balancer so here's
the template
and here's the architecture that we're
going to provision
let me call it all right and the key
pair it's going to use is uh simply loan
keypair now this is where the problem
was now we were selecting few subnets
so now if I provision it
now this is going to take its short time
to provision the resources
so through parameters we can see the
values that we have inputted
and the events look good no roll back no
no delete and the resource section shows
the resource that it is creating so
previously created a Target group now
it's creating the application load
balancer itself
all right so it created and
load balancer and then it create a
security group it creates a launch
configuration it at the moment it's
creating a web server group
and the events look good with all green
and yellow green ones are completed
yellow ones are in progress we don't
have any red delete or roll back errors
so it looks good all right it took a
while and
the resources have been created some of
them show that they are in progress but
the status of the stack is complete
all resources have been created and we
do have an output to access the URL
there you go
[Music]
and remember there was another feature
of the cloud formation that was it
should restrict access I mean direct
access to ec2 instance let's see if that
has been done
if I try to access using the load
balancer it lets me access but if I try
to access it using
the ec2 instance IP or the DNS name my
connection won't go through because it's
blocked through the security group but
direct access through the load balancer
is allowed and that's what we wanted to
achieve through that template all right
so what are the key takeaways we so if
you are prepared to embark on a career
path pulsating with excitement
Innovation and Endless Possibilities
look no further than simply lens Caltech
postgraduate program in cloud computing
this encompassing program will arm you
with the knowledge and skills needed to
navigate the vast cloud computing
landscape with unyielding confidence
delve into the depths of cloud computing
architecture concur deployment models
fortify security like a digital portrait
and master migration strategies that
leave a lasting impact discover how to
harness Cloud computing's full suit of
services to construct applications that
are not just scalable but remarkable
works of art don't let this once in a
lifetime opportunity slip through your
fingers ignite your career and join the
ranks of time cloud computing
professionals check the link in the
description description box to unveil
the wonders of the AWS Basics course
working for simply law here in this
section we're going to talk about what
is Security in AWS and then we're going
to slowly or gradually move on to the
other topics like types of security
available in AWS out of all the services
y I am is the most preferred one and
then we're going to talk about what is
IM in general how it works the building
blocks or components in IM and the
features it the the attractive features
that it provides that makes IM stand out
from the rest of the services available
we're going to talk about that and then
we're going to end today's session with
a demo about how IAM gets well
interacted with other services in Amazon
and help create a secure environment in
the cloud let's talk about AWS security
Now Cloud security is the highest
priority in AWS and when we host our
environment in the cloud we can be rest
assured that we are hosting our
environment in a data center or in a
network architecture that's really built
to meet the requirement of the most
security sensitive organization and this
high level of security is available to
us on a pay-as-you-go type meaning there
is really no upfront cost at that we
need to pay and the cost for using the
service is a lot lot cheaper than what
it is in an on-premises environment so
AWS Cloud provides an secure virtual
platform where we can deploy our
application now compared to the
on-premises with AWS the security level
is very high and the cost involved in
using or attaching that type of security
to our application is very low compared
to on-premises there's really no upfront
cost and the cost is very lower whatever
cost that we pay is very lower than what
it would be in on-premises and that all
this contributes to the secure
environment we can get through IM in AWS
Cloud there are really many types of
security services available in AWS to
name a few or to name the familiar ones
IM stands first followed by a key
management system KMS and then we have
Cognito that's another service and web
access firewall vaf is another security
service in AWS now let's start our
discussion by talking about IM and
that's what this whole series is about
now I am or identity and access
management it really enables us to
manage access to AWS services and AWS
resources in a very secure manner with
IM we can create groups we can actually
allow those users or groups to access
some servers or we can sort of deny them
to access the service whichever we want
that's all possible through identity and
access management and this feature comes
with no additional charge now let's talk
about how things were before AWS before
AWS or before I am in general it was not
that safe in a corporate to share the
password over the phone or through the
email that was the practice that was
existed at that time now we all can
remember the days when we need to switch
to a different account or we had just
one admin password commonly stored in a
location or one person would reset it
and maintain it and anytime or we need
to log in we call the person ask for the
admin password over the phone and then
we try logging in now that was not
secure at all back then no somebody
could walk by and sort of eavesdrop and
get the password now they walk away with
the password with them so they're all
possible when we share the password over
the phone and through the internet or
email now with AWS a lot of options are
available that I'm not sharing password
over the unsecure medium now a slack is
a 30 party product available with AWS
it's not an AWS product but slack is a
third-party application that's hosted on
AWS and it's really a communication tool
that helps people to share documents so
now we're not sharing the password over
the phone rather through the
applications and no eavesdrop person can
really catch the password and you know
try it in their system to access our
environment that's not possible so you
see the difference now back then sharing
password was only through phone and
email and you would write it on a paper
and you give it to somebody but now
technology is providing Provisions
enough for us to share the password in a
secure way we're still talking about I
am I am is an web servers for securely
controlling access to the AWS resource
now it really helps us to authenticate
or sort of limit access to a certain set
of users accessing the AWS account or
certain set of users accessing a certain
set of resources in AWS account now you
see this picture we have an IM
administrator who's trying to allocate
permissions to different group of people
so we have group one on the top and
group two in the middle and group three
towards the end the administrator using
IM or IAM empowers the administrator
allow access to certain group to certain
resources that's what IAM is all about
now let's talk about the building blocks
or let's talk about the elements that
make and complete I am so the elements
are categorized into six different
elements we have the principle in IM the
authentication the request the actual
request coming in the authorization
allowing denying access actions what are
the scope of actions can be taken and on
what resource is it easy to visit rdas
is it S3 bucket so what resource these
actions are applied to so they are the
basic elements of an IM workflow let's
talk about about principle for example
an action on AWS resource can only be
performed by a principal an individual
user they can be a principal a role in
AWS IM it can be a principle as well in
other words a principle is an entity
that can take an action on an AWS
resource you know a principle can be an
user a principle can be a role a
principle can be an application that's
trying to access the AWS environment
secondly authentication now
authentication is a process of
confirming the identity of the principle
trying to access the AWS environment now
how do we get authenticated or how does
a user get authenticated by providing
credentials or the required keys to
validate that this is what he or she is
as per what's in the record so all
principle must be authenticated and we
get authenticated using the username and
password to log into the console if it
is CLI or we get authenticated using the
access key and the secret access key the
other component that makes up IM is
request when a principal tries to use
the AWS Management console what the
principle is doing or what the user is
doing is sending a request to the AWS
environment and the request will have
the principal actually wants to perform
is it an put request or is it a get
request or is it a delete request stuff
like that and it's also going to carry
information about on which resource the
action needs to be done is it done ec21
is it on S3 bucket simplylearn.com stuff
like that so it has a specific action
that it wants to perform and the
resource on which it wants to perform
and also some information about from
where the request is originating from is
it within an AWS environment or is it
with another cloud service provider or
is it with on-premises stuff like that
so these information will be present in
the request authorization now this IM
uses information from the request
context to check for matching policies
and it determines it takes a decision
whether to allow or to deny the request
now here it has a logic by default all
the resources are denied and the IM
authorizes your request only if every
part of the request is allowed by a
matching policy so there is an
evaluation logic and that says by
default all requests are denied and if
that is an explicit allow then it
overrides the default deny and if you
are explicitly denying a service then
all the allow statements gets overridden
and all the other statements gets
overwritten and this explicit deny
stands by default only the AWS account
root user has access to all the
resources in that account so if you're
not signed in as a root user then we
must be specifically granted permission
through a policy in other words we need
to be authorized through the policy now
let's talk about action now after
authentic indicating and authorizing the
request AWS approves the action that we
are about to take Now using action we
can view the content we can create a
Content or create a bucket or create an
ec2 instance I can edit content or even
I can delete a resource that's all based
on the action that I'm allowed to do now
that was about actions now where is the
action performed the action is performed
on the resources a set of actions can be
performed on a resource and anything
that's not mentioned in the action or
anything that's not tagged with the
action and the resource they don't get
to complete for example let's say if you
attempt to delete an IM role and you
also request to access the ec2 instance
for that role then the request gets
denied now let's talk about some of the
other components of Im so the other
components are IM are the basic building
block is in user and then many users
together they form a group and then
policies are the engines that that
allows or denies a connection you know
based on policy one gets to access or
gets no access to a resource and then
roles roles are another component roles
are temporary credentials I would say
that can get assumed to an instance as
and when needed right let's talk about
this component called user in an IM with
IM we can securely manage access to AWS
services and we can create an IM user
anytime there is a new employee joining
our cartridge so it's a one-to-one
mapping in a one user or one employee
get a username directly attached to them
and a username or a user is an account
specific thing you know it's very local
to that AWS account that we are using so
for in detail and IM user is an entity
that we create in an AWS environment to
represent a person or a service that
interacts with AWS environment and this
user is going to have some credentials
attached to them and by default they do
not have a password and they do not have
any access key or secret access key
attached to them in fact literally no
credentials of any kind now we must
create an user and we must create the
type of credential that gets attached to
the user do they want to access a CLI So
based on that we would be adding
credentials to them a user by default is
not authorized to perform any action in
AWS and the advantage of having
one-to-one user is that we can assign
permissions individually to that user
for example we might want to assign
administrative permissions to few users
who can administer our AWS environment
and users are entity within an AWS
account users don't have to pay
separately you know they start using the
servers and that get bills under the
account they are attached to so separate
users doesn't mean that separate payment
for those visitors they are just users
in an account and you know the whole
account gets to pay for the resources
that they provision again the IM user
does not always represent a person an IM
user is really just an identity with
Associated credential and permissions
attached to it it could be an actual
person who is a user and it could be an
application also a who's a user next
we'll talk about groups talking about
groups or understanding group is very
easy a collection of user forms an IM
group and we can use IM group to specify
permission for multiple users so that
any permission that's applied to the
group is also applied to the users who
are at ad to that group now there are a
few things you might want to know about
group a group can have many users and a
user they can belong to many groups but
groups can't be nested meaning group can
only have users you know a group cannot
have a group inside a group that's not
possible and there's nothing called a
default group that sort of automatically
includes all users in the AWS account if
you want to have a group then we will
have to create a group now let's take
this diagram for example now this is a
diagram of an small company now the
company owner creates an admin group for
users to create and manage other users
as the company expands every day and the
admin group creates a developer group
and a test group each of these group
consists of users users who are people
users who are application that interact
with AWS environment so the different
types of users we have are gym is a user
Brad is an user and Dev app one is a
user and and so on so each users have an
individual set of security credential
like I said it's one to one and the uh
10 users that we see on screen are
shared between or they belong to
multiple groups in the account and
managing group is quite easy we we set
permission to the group and that
permission gets applied to all the users
automatically in the group and after we
applied the policy if I add another user
to the group the new user will
automatically inherit all the policies
all the permissions that's already
mentioned in that group so
administrative button is sort of taken
away administrative button about user a
privilege or user permission management
is sort of taken out with IM all right
the next thing we will want to discuss
is policy a policy sets permission and
controls the access to AWS resource and
the policies this permissions and
controls are stored in Json format a
policy is a very clear document it
defines who has access to a resource and
what are all the actions they can
perform so in other words policies and
entity in AWS that when a policy gets
attached to an identity or a resource
you know it really defines the
permission question what they can and
cannot do and AWS it evaluates these
policies when a principal such as a user
makes an request and the permission
statement and the policy it really
defines whether the user is allowed to
access a service or is he denied to
access a service all right let's take
this example now we have a task the task
is to give Paul who is a developer
access to Amazon S3 environment and what
you're looking at is a snippet of what a
policy would look like so these are some
questions we will have to answer or
these are some question the policy tries
to answer and once it found an answer
then it becomes in complete policy so
who wants access to the service it's
Paul who wants access to the service and
what action do Paul want now Paul wants
to download or Paul wants to upload
objects in S3 in Json language it's
otherwise called Paul wants to get
information Paul wants to put
information otherwise Paul wants to get
object Paul wants to put object in S3
and which resource does he want now in
this case every bucket the star symbol
represents all buckets in S3 so which
results it's all bucket in S3 so we can
start reading already from the data
present here so who wants access Paul
wants access what action does he want he
wants to get he wants to put and what's
the resource that he's trying to access
or you want to give him access it's all
the bucket in AWS and when does he want
these are additional statement that goes
in a policy so since when does he want
that access he wants access till March
2nd 2019 so all this who all this action
all this resource you know till the time
all this gets allowed so this is the
statement that really defines now does
this all refer to an allow statement or
does this all refer to a deny statement
if we had deny here it's the direct
opposite of what we discussed but we
have allow that means Paul is allowed to
get an input access in an S3 bucket and
all the buckets in S3 till March 2nd
2019 he's allowed to access the
resources so if we put together put all
the information that we have gathered or
all the answers that we have so far if
we put together in a Json format this is
how it's gonna look like so the effect
is allow or deny here it's allowing and
the principle is any now who can access
it it's any an action it's any bucket in
S3 any action in S3 put get request and
the resource is any bucket in S3 so
that's how a basic policy would look
like or that's how a basic policy is
formed by answering those basic
questions so let's talk about the types
of policies so we have two broad
categories manage policy and inline
policy now manage policy it's the
default policy that we attach to
multiple entries be users beat groups
and beat roles in our this account on
the other hand inline policy are the
policies that we create and manage them
and these policies are embedded directly
into a single user group or a role so if
I need to brief it a little bit more
then a managed policies are Standalone
identity based policies that we can
attach to multiple users groups and
roles in our AWS account we can use two
types of managed policies in other words
manage policies they themselves have two
categories one is AWS managed policies
which is created and managed by AWS and
the other one is customer managed
policies just like the name says it's
managed by the customer now we can
create and edit an IM policy in visual
editor or simply create a Json policy
document directly and upload it in the
console and start using them so that's
manage policy on the other hand inline
policies are policies that we create and
manage manage and that are emboded
directly into a single user group or
role let's talk about this next
component called roll now this role is
very similar to everything that we
discussed so far it's a set of
permission that defines what actions are
allowed and denied by an entity in an
AWS console the role is very similar to
an user I said very similar I didn't say
it's the same as the user but I said
it's very similar to the user and the
difference is that user is hard coded
user permissions are hard-coded but the
role permissions are accessed by any
entity in a beta user or a beaten AWS
service and moreover the role
permissions are temporary on the other
hand user permissions are permanent and
the role permissions are temporary so in
detail a role is very similar to a user
in that it's an AWS entity with
permission policies that determines what
the identity can and cannot do in AWS
now instead of it being uniquely
associated with one person a role is
intended to be assumable by anyone who
needs it rules generally do not have a
long-term credential you know password
or access Keys associated with it you
know it does not have long-term
credentials it only has temporary
credentials created dynamically and
provided to the user as in when the user
assumes a role and wants to access any
service roles can be assumed by users
roles can be assumed by applications
roles can be assumed by services that
normally we don't have access to AWS
resources for example you might want to
Grant a user in your AWS account access
to your results that they do not
normally have or Grant users in one AWS
account access to resource in another
AWS account now this is something that
we don't have by default and this is
something we may not need all the time
or you might want to allow a mobile app
to use AWS resource but you do not want
to save the key or save the credential
or save the passwords in the mobile app
or sometimes you might want to give
access to resources who already have
identities defined outside of AWS like a
user who's already and Google or
Facebook authenticator if you want to
give them some service or if you want to
let them to access some of the resources
in your account we can use roles for
that purpose or you also might want to
Grant access to your account to a third
party a consultant or an auditor so they
can for some time get some temporary
access to audit our AWS resources
remember they're not permanent users
such as Templar users they want some
temporary access to our environmental
the audit is over so for those cases
roles are a very good use case so let's
take this example let's see where role
sets and let's see how roles give
permission to other services so here is
a scenario where where an easy to
instance wants access to an S3 bucket by
default though both of them were created
by the same username or the same admin
they do not have access by default so by
default they don't have access and I
also do not want to give permanent
access you know one-on-one hard-coded
permanent access instead I would like to
create and share access so that's
possible through role so coming back to
the discussion uh the scenario here is
that an ac2 instance wants to access
data in an S3 bucket so the first thing
is to create a role or a permission we
saw how that's done right using policies
roles use policies policy is the actual
working engine for any permission
related actions right so create a role
with an appropriate policy that gives
access to S3's bucket and then launch an
ec2 instance with the role attached to
it we said rules are assumed by any
entity right it will be user it could be
an application could be a service it
could be another resource in AWS account
so here it's another resource in AWS
account right so we create a role and we
attach it to an ec2 instance so now the
ec2 instance can assume the role now the
same role can be attached to another ec2
instance the same role can be attached
to another user the same role can be
attached to another database service we
get the idea right roles can be assumed
by anyone so when the ec2 instance wants
to communicate with S3 it contacts the
role and then the role gives this ec2
instance some temporary username and
password that expires after one hour
let's say so it's a temporary username
and password now with that temporary
username and password the ec2 instance
can access the S3 bucket and access the
file with the credential that it already
has and that credential is a temporary
one and the credential will expire after
one hour or how much ever time the admin
sets it to be and then the next time
when is when an easy do instance wants
to contact S3 it will have to contact
the rule and the role will propose a new
access key and secret access key in
other words a new credential with that
new credential the second time the ec2
instance will be able to contact the S3
bucket and get the information so the
password is not a permanent or a
one-time or a hard-coded password it
keeps rotating it's a token it's a
secure token that an ec2 instance or any
application that uses the role gets to
access the resources on the other end so
let's talk about the features of IM the
main feature of IM is that I can create
separate username and password for
individual users or resources and I can
delegate access now those days where we
had just one username and password for
admin and we need to call the other
person to get the username and password
over the phone if it gets compromised
you know the whole account gets
compromised stuff like that those issues
are gone I can create separate username
and password for all thousand users in
my account no more username and password
sharing I can manage access to all those
thousand accounts in a very simple and
an easy way so IM provides share access
for all the Thousand users in my account
it provides share access to the AWS
environment and the permissions that can
be given through IM are very granular
now we saw how restrictions can be put
on get requests you know we can allow a
user to download information and not
update an information so that's possible
through the IM policies that we looked
at some time back and with the use of
roles you know we're authenticating an
ec2 instance answer of the application
running on top of it so if there are
like 10 or 20 applications running on
top of an ec2 instance instead of
creating a separate username and
password for all those 20 instances I
can create a role and assign that role
to the ec2 instance and all the
applications running on the ec2 instance
get a secure access to the other
resources use the role credential that's
provided to that ec2 instance now
besides username and password IM also
supports multi-factor authentication
where we provide a credential the
username and password plus and OTP from
our phone or an randomly generated
number that gets displayed in our phone
before Amazon console or before Amazon
CLI would authenticate and lets us
access some of the service in the
account so it's called multi-factor
authentication it's username and
password plus something else from what
you have IM also provides identity
Federation if a user is already
authenticated with a corporate ad or a
Facebook or Google you know IM can be
made to trust that authentication method
and then allow access based on that
authentication now without Federation
what would happen is the users will have
to remember two passwords you know one
for on-premises and one for cloud
anytime they work on on premises they'll
have to use one set of password and
anytime they switch to the cloud which
is going to be very frequent in a every
day so anytime they switch to the cloud
they will have to again type in the
cloud username and password now with
identity Federation we can sort of
Federate or we can connect both the
authentication systems both on-premises
and Im so users can just use one
username and password for both
on-premises and Cloud environments and
all that we saw so far it's free to use
there is no additional charge for IM
security there is no additional charge
for creating additional users groups or
policies it's a free service comes along
with any account IMS and PCI DSS
compliant product you know IM comes with
password policy where we can reset the
password remotely or rotate the
passwords remotely and we can set rules
like how a user should pick and password
it should not be the one that was used
in the past any of the past three times
stuff like that so you get the idea
right how the features of I am help us
to build and secure authentication and
authorization system in or for our AWS
account all right it's time to do a
quick lab let's let's put all the
knowledge that we have earned so far
together and apply it and try a lab and
solve the problem so here's a problem
statement we need to create an S3 bucket
for a company where users can read or
write the data only if they are
multi-factor authenticated so that could
be like 20 users in an account but a
user is allowed to read or write a data
in an S3 bucket only if the user is
multi-factor authenticated you know this
is a very good use case if we have a
sensitive data in an S3 bucket and you
only want privileged or MFA
authenticated users to do changes to
those buckets and for those privileged
users you would enable multi-factor
authentication so let's see how that's
done before that let me talk to you a
bit more about multi-factor
authentication a multi-factor
authentication is that additional level
of security process it's provided by AWS
here the user's identity is confirmed
only after the user proposes or the user
passes two level of verification now one
being the username and password and
another being an OTP that gets generated
in this case it's the mobile phone it
could also be an RSA token key that
generates the one-time password that's
possible as well but in this case it's
the mobile phone now that's going to be
very similar to how we login to our
Gmail account sometimes when we log into
a Gmail account and I mean if you have
the proper setting enable it's going to
send an 110 password to mobile phone and
only after we put that information it's
going to let us log in let's see how all
this is done in an AWS account so the
first thing is to log in to the AWS
account create a user and attach the
user to the virtual MFA device now we're
going to use Google Authenticator here
and we're going to attach the user to
Google Authenticator and make the user
use the Google Authenticator one-time
password every time he logs in to the
account now this is an addition to the
username and password that he or she
already has right so the first thing is
to log into the account connect or sync
the virtual Appliance with the AWS
username and password now when we sync
there will be an one-time password that
gets shown in the phone type in the
password in the AWS console and then the
phone and the username comes to sync and
from that point onwards the user anytime
they log in they will have to propose
their username and password which is the
first step of security and then once
they typed in the username and password
type in the MFA code that gets generated
on the phone that would be the second
step of security or the last step of
security once that is done the account
is going to let them log in and access
the AWS resources all right so in order
to test IM MFA S3 together this is what
we're gonna do so we're gonna create a
bucket and we're gonna have two users
and we're gonna allow or deny access to
the S3 bucket to those two users based
on whether they are MFA authenticated or
not and on purpose we're going to assign
MFA to one user and no MFA to the other
user like you might have guessed by now
the user with MFA will have access to
the bucket and the user with no MFA
attached will not have access to the
bucket both of them are going to have
full S3 services but MFA like I said is
one layer above the other permissions
MFA stands at all time so irrespective
of whether the underlying policy says
but they have full access the S3 bucket
they're not going to because MFA stands
on top of it let's see how that's done
to begin with firstly we need a bucket
so let's create a bucket
so I'm in my AWS console so let me go to
S3
and in here create a bucket call that
bucket as simply learn MFA bucket and
let me put it in North Virginia and
create
all right so that's done
now I'm going to put a separate folder
in here
I'm going to create another folder in
here and the name of the folder is tax
documents now this is where I'm planning
to keep my tax files
and I only want my privileged users to
be able to upload and download
information from the tax document so on
the bucket side it's partially done or
on the S3 side it's partially done so
let's go back to IM and here we're going
to create two users one a junior
employee and another one and a senior
employee the senior employee is the one
who's going to have MFA access and more
access to the S3 bucket and the junior
employee is the one who's not going to
have access to that particular bucket so
let's get started let's create an user
under iam
click on users and let's create a user
called as user
Junior
right so he's a junior user
and for the password
set a password
the user does not need to create a new
password at sign in because it's a test
environment and the user who creates the
username and the user are the same
person so let's keep it simple username
password Management console access
and this user is going to have full S3
bucket access all right this is really
the policy
we should be familiar with this jsr
document by now it says effect is
allowed and what action any action in S3
and what resource anybody who's attached
to it review
and create a user
so this user is having S3 full access
now let's create another user call them
as senior employee
all right attach
a policy to the user
you know from the surface level both of
them have the same permissions both of
them have S3 full access
all right so both of them have S3 full
access as we see
there's one thing different we're gonna
do to this senior employee now we're
going to attach MFA to this employee so
the way we would do it is under the
summary section of that particular user
click on security credential and see
here is a place we can attach an MFA to
this user so let's attach MFA to this
user for MFA I can use both virtual MFA
or Hardware MFA this being in lab
environment we're going to use Virtual
MFA now that is an application there are
a lot of applications available but
there is one application that I'm using
at the moment called Google
Authenticator
so once we have installed the
application and once it is running
so I can go to Virtual MFA device and
once it is running I can scan the code
using the Google Authenticator in my
mobile phone once you turn on the
application and once you do scan the
barcode that is showing right here now
that's going to give us codes that we
can use and we will have to put in those
codes here twice we'll have to wait for
some time and it will generate a second
set of key that we can put so my first
set of key is four zero two three five
one right I'll have to wait for some
time and then it would throw me a second
set of key it's like validating twice
right here so let me put in my second
set of key
so this user is now MFA authenticated
user so anytime this user logs in it's
going to ask for MFA anytime this user
proposes something it's going to attach
the MFA key along with it all right so
we're done with the IM what have you
done with IM we have created two users
both of them have s34 access but one of
them have MFA access and the other one
does not have MFA access and based on
whether the user is having MFA or not
you know we're identifying privileged
users by this MFA right so based on
whether the user has MFA or not are we
going to allow deny access in the S3
bucket
so we already have a bucket we already
have a folder and this is a very
privileged folder path here I would like
to restrict access based on whether the
user is having MFA or not
so let me create a bucket policy here
now here's a policy that I have written
the policy says that you know under tax
document folder give access only if the
user is MFA authenticated right if the
user is not MFA authenticated simply
deny the access to that user so as you
might guess right now the user 1 will
not have access or the junior employee
you will not have access because it's a
privilege and you know Junior employee
might tend to delete them unknowingly
and only privileged users get access to
the tax document because it's
confidential because it's privileged
because if it gets deleted we won't be
able to get it back for those reasons
right so apply the bucket policy
now try logging in as the different
users and see how the permissions are
applied on both the users all right so
let's make a note of the IM URL
and then the two usernames and password
let's login
so let me log in as the privileged user
sure enough I'm being asked for an MFA
code let me put my MFA code
so let's directly navigate to the S3
bucket
right here's my privileged path let me
try uploading some content
claim forms
sure enough I'm able to upload content
into it will I be able to delete
yes I'm able to delete as well now let's
see what's the situation for the other
user
so let's log in as the other user
navigate to the SC bucket
if you noticed when logging in it did
not ask for an MFA code because the user
is not set up for MFA
so navigate to the folder which we want
to test
now try adding some files
as you see it's failing to add content
to this bucket it shows error because
the bucket policy is requiring MFA and
this user is not MFA and we did that in
IEM or under iam
but still as you see you know they will
be able to view the content in the
bucket they can do all that we have
specifically restricted access to upload
and download content from the bucket and
that's exactly this user is now denied
with here we wrap up with AWS Basics if
you like the video hit the like button
consider subscribing to our Channel and
hit the Bell icon to never miss updates
from simple learn till then keep
watching and keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here