foreign
like a vast River every second this
river is creating a huge amount of data
like tiny droplets and we are trying to
catch and understand all of them well
that's what big data is all about big
data offers a wealth of valuable
insights unables real-time and
Predictive Analytics and significantly
impact interests such as Healthcare
smart cities social media and security
its potential and applications
constantly expand as technology
progresses and more data becomes
accessible for analysis big data is a
powerful tool that allows us to make
sense of the overwhelming amount of
information in our digital world it's
like having a secret code book that
unlocks the Hidden Treasures of
knowledge empowering us to make smarter
decision and explore new boundaries Big
Data presents various career
opportunities including roles such as
data analyst data scientists data
engineer bi analyst DB administrator and
architecture this position demand a
diverse skill set it including data
analysis machine learning programming
and database management according to
Glassdoor the average annual salary for
a big data engineer in the United States
is approximately 120 000 in India they
are a salary for this role is 8 lakh per
annum on that note hey Learners simply
learn offers you a postgraduate program
in data engineering developed in
collaboration with Purdue University and
IBM which provides an excellent
opportunity for professional growth with
a curriculum aligned with AWS and Azure
certification this Hands-On learning
experience equips participate with
essential data engineering skills don't
miss out on the opportunity to enhance
your data skills through our Cutting
Edge Big Data engineer course offered by
simply learn do check out the course
Link in the description for more details
before we begin let's take a minute to
hear from our Learners who have
experienced huge success in their career
the learning from Simply learn has been
really amazing and extremely helpful
whoever is watching this video I
recommend you go out to Simply learns
website and check out their courses
they have brilliant instructors who are
industry-wide solution Architects and
directors working at the biggest firm
right now
hi my name is Aditya and I'm working as
a consultant in atos in the field of AI
machine learning and RPA I did my
engineering from Maharaja surajmal
Institute of Technology I majored in
electronics and communication
engineering I passed out in 2013. and on
campus I got placed into a company
called nucleus software so I started
working in software engineering team the
team was involved into application
development using Oracle forms SQL and
database languages I wanted to explore
better Technologies which were there in
the Market at that time I got to know
that machine learning and Big Data they
revolve around SQL so if you are a SQL
person you are very much comfortable
with learning new machine learning
techniques and database Technologies
like big data Hadoop so that's when I
started looking out for the training
institutes and then my relative referred
me once I decided to go for simply learn
I had one course in my mind which was
Big Data only but when I approach simply
learn they offered me a bundle of
courses in one master program which
included not only big data but also
machine learning and python Basics are
Basics data science with python and some
Java fundamentals as well so that was
really good to have what I liked most
about the training was the context of
the training as well as the instructor
to student engagement
after the training I got an opportunity
to learn more about these Technologies
and once I was done learning I attended
a lot of interviews so they were in some
big mncs and the interviewers were real
life big data Architects and data
science experts and they were really
happy to see the knowledge which I have
so simply learn definitely set up a base
for me on that and once I started giving
interviews eventually I landed up in
imnc in Chennai where I'm working as a
data science expert and a consultant
simply learns Big Data master program
helped me to get a new job with a very
good salary hike a very big thanks to
Simply learn for making me who I am I
feel really fortunate to be associated
with these Technologies now I can lead
different teams and drive different
business transformation programs using
my skills sector I really wanted to
upskill because in the market and in the
IIT industry upscaling is really
important
so without any further delay let's begin
by understanding the basics of big data
we value smartphones but have you ever
wondered how much data it generates in
the form of texts phone calls emails
photos videos searches and music
approximately 40 exabytes of data gets
generated every month by a single
smartphone user now imagine this number
multiplied by 5 billion smartphone users
that's a lot for our mind to even
process isn't it in fact this amount of
data is quite a lot for traditional
Computing systems to handle and this
massive amount of data is what we term
as Big Data let's have a look at the
data generated per minute on the
internet
2.1 million Snaps are shared on Snapchat
3.8 million search queries are made on
Google one million people log onto
Facebook 4.5 million videos are watched
on YouTube
188 million emails are sent that's a lot
of data so how do you classify any data
as Big Data this is possible with the
concept of five V's volume velocity
variety veracity and value let us
understand this with an example from the
healthcare industry Hospitals and
Clinics across the world generate
massive volumes of data
2314 exabytes of data are collected
annually in the form of patient records
and test results all this data is
generated at a very high speed which
attributes to the velocity of Big Data
variety refers to the various data types
such as structured semi-structured and
unstructured data examples include Excel
records log files and x-ray images
accuracy and trustworthiness of the
generated data is termed as veracity
analyzing all this data will benefit the
medical sector by enabling faster
disease detection better treatment and
reduced cost this is known as the value
of big data but how do we store and
process this big data to do this job we
have various Frameworks such as
Cassandra Hadoop and Spark let us take
Hadoop as an example and see how Hadoop
stores and processes Big Data
Hadoop uses a distributed file system
known as Hadoop distributed file system
to store Big Data if you have a huge
file your file will be broken down into
smaller chunks and stored in various
machines not only that when you break
the file you also make copies of it
which goes into different nodes this way
you store your big data in a distributed
way and make sure that even if one
machine fails your data is safe on
another
mapreduce technique is used to process
Big Data a lengthy task a is broken into
smaller tasks
b c and d now instead of one machine
three machines take up each task and
complete it in a parallel fashion and
assemble the results at the end thanks
to this the processing becomes easy and
fast this is known as parallel
processing
now that we have stored and processed
our big data we can analyze this data
for numerous applications in games like
Halo 3 and Call of Duty designers
analyze user data to understand at which
stage most of the users pause restart or
quit playing this Insight can help them
rework on the storyline of the game and
improve the user experience which in
turn reduces the customer churn rate
similarly Big Data also helped with
disaster management during Hurricane
Sandy in 2012. it was used to gain a
better understanding of the storm's
effect on the east coast of the U.S and
necessary measures were taken it could
predict the Hurricane's landfall five
days in advance which wasn't possible
earlier these are some of the clear
indications of how valuable big data can
be once it is accurately processed and
analyzed big data is a term for data
sets that cannot be handled by
traditional computers or tools due to
their value volume velocity and veracity
it is defined as massive amount of data
which cannot be stored processed and
analyzed using various traditional
methods do you know how much data is
being generated every day every second
even as I talk right now there are
millions of data sources which generate
data at a very rapid rate these data
sources are present across the the world
as you know social media sites generate
a lot of data let's take an example of
Facebook Facebook generates over 500
plus terabytes of data every day this
data is mainly generated in terms of
your photographs videos messages
Etc Big Data also contains data of
different formats like structured data
semi-structured data and unstructured
data data like your Excel sheets all
fall under structured data this data has
a definite format your emails fall under
semi-structured and your pictures and
videos all fall under unstructured data
all this data together make up for Big
Data it is very tough to store process
and analyze big data using rdbms if you
have looked into our previous videos you
would know that Hadoop is the solution
to this Hadoop is a framework that
stores and processes Big Data it stores
big data using the distributed storage
system and it processes big data using
the parallel processing method hence
storing and processing big data is no
more a problem using Hadoop big data in
its raw form is of no use to us we must
try to derive meaningful information
from it in order to benefit from this
big data do you know Amazon uses big
data to monitor its items that are in
its fulfillment centers across the globe
how do you think Amazon does this well
it is done by analyzing big data which
is known as big data analytics what is
big data analytics in simple terms big
data analytics is defined as the process
which is used to extract meaningful
information from Big Data this
information could be your hidden
patterns are known correlations market
trends and so on by using big data
analytics there are many advantages it
can be used for better decision making
to prevent fraudulent activities and
many other us we will look into four of
them step by step I will first start off
with big data analytics which is used
for risk management BDO which is a
Philippine banking company uses big data
analytics for risk management risk
management is an important aspect for
any organization especially in the field
of banking risk management analysis
comprises a series of measures which are
employed to prevent any sort of
unauthorized activities identifying
fraudulent activities with a main
concern for BDO it was difficult for the
bank to identify the fraud still from a
long list of suspects BDO adopted big
data analytics which helped the bank to
narrow down the entire list of suspects
thus the organization was able to
identify the fraudster in a very short
time this is how big data analytics is
used in the field of banking for risk
management let us now see how big data
analytics is used for product
development and Innovations with an exam
people all of you are aware of
Rolls-Royce cars right do you also know
that they manufacture jet engines which
are used across the world what is more
interesting is that they use big data
analytics for developing and innovating
this engine a new product is always
developed by trial and error method big
data analytics is used here to analyze
if an engine design is good or bad it is
also used to analyze if there can be
more scope for improvement based on the
previous models and on the future
demands this way big data analytics is
used in designing a product which is of
higher quality using big data analytics
the company can save a lot of time if
the team is struggling to arrive at the
right conclusion big data can be used to
zero in on the right data which have to
be studied and thus the time spent on
the product development is less big data
analytics helps in quicker and better
decision making in organ organizations
the process of selecting a course of
action from various other Alternatives
is known as decision making lot of
organizations take important decisions
based on the data that they have data
driven business decisions can make or
break a company hence it is important to
analyze all the possibilities thoroughly
and quickly before making important
decisions let us now try to understand
this with a use case Starbucks uses big
data analytics for making important
decisions they decide the location of
their new outlet using big data
analytics choosing the right location is
an important factor for any organization
the wrong location will not be able to
attract the required amount of customers
positioning of a new outlet a few miles
here or there can always make a huge
difference for an outlet especially a
one like Starbucks various factors are
involved in choosing the right location
for a new outlet for example parking and
accusing has to be taken into
consideration it would be inconvenient
for people to go to a store which has no
parking facility similarly the other
factors that have to be considered are
the visibility of the location the
accessibility the economic factors the
population of that particular location
and also we would have to look into the
competition in the vicinity all these
factors have to be thoroughly analyzed
before making a decision as to where the
new outlet must be started without
analyzing these factors it would be
impossible for us to make a wise
decision using big data analytics we can
consider all these factors and analyze
them quickly and thoroughly thus
Starbucks makes use of big data
analytics to understand if their new
location would be fruitful or not
finally we will look into how big data
analytics is used to improve customer
experience using an example Delta and
American Airline uses big data analytics
to improve its customer experiences with
the increase in global air travel it is
necessary that an airline does
everything they can in order to provide
good service and experience to its
customers Delta Airlines induce its
customer experience by making use of big
data analytics this is done by
monitoring tweets which will give them
an idea as to how their customers
Journey was if the airline comes across
a negative tweet and if it is found to
be the airline's fault the airline goes
ahead and upgrades that particular
customers ticket when this happens the
customer is able to trust the Airlines
and without a doubt the customer will
choose Delta for their next journey by
doing so the customer is happy and the
airlines will be able to build a good
brand recognition thus we see here that
by using analysis Delta Airlines was
able to improve its customer experience
moving on to our next topic that is life
cycle of big data analytics here we will
look into the various stages as to how
data is analyzed from scratch the first
stage is the business case evaluation
stage here the motive behind the
analysis is identified we need to
understand why we are analyzing so that
we know how to do it and what are the
different parameters that have to be
looked into once this is done it is
clear for us and it becomes much easier
for us to proceed with the rest after
which we will look into the various data
sources from where we can gather all the
data which will be required for analysis
once we get the required data we will
have to see if the data that we received
is fit for analysis or not not all the
data that we receive will have
meaningful information some of it will
surely just be corrupt data to remove
this corrupt data we will pass this
entire data through a filtering stage in
this this stage all the corrupt data
will be removed now we have the data
minus the corrupt data do you think our
data is now fit for analysis well it is
not we still have to figure out which
data will be compatible with the tool
that we will be using for analysis if we
find data which is incompatible we first
extract it and then transform it to a
compatible form depending on the tool
that we use in the next stage all the
data with the same Fields will be
integrated this is known as the data
aggregation stage the next stage which
is the analysis stage is a very
important stage in the life cycle of big
data analytics right here in this step
the entire process of evaluating your
data using various analytical and
statistical tools to discover meaningful
information is done like we have
discussed before the entire process of
deriving meaningful information from
data term which is known as analysis is
done here in this stage the result of
the data analysis stage is then
graphically communicated using tools
like Tableau power bi click view this
analysis result will then be made
available to different business
stakeholders for various decision making
this was the entire life cycle of big
data analytics we just saw how data is
analyzed from scratch now we will move
on to a very important topic that is the
different types of big data analytics
well we have four different types of big
data analytics as you see here these are
the types and below this are the
questions that each type tries to answer
we have descriptive analytics which asks
the question what has happened then we
have diagnostic analytics which asks why
did it happen Predictive Analytics
asking what will happen and prescriptive
analytics which question by asking what
is the solution we will look into all
these four one by one with an use case
for each we will first start off with
descriptive analytics as mentioned
earlier descriptive analytics asks the
question what has happened it can be
defined as the type that summarizes past
data into a form that can be understood
by humans in this type we will look into
the past data and arrive at various
conclusions for example an organization
can review its performance using
descriptive analytics that is it
analyzes its past data such as Revenue
over the years and arrives at a
conclusion with the profit by looking at
this graph we can understand if the
company is running at a profit or not
thus descriptive analytics helps us
understand this easily we can simply say
that descriptive analytics is used for
creating various reports for companies
and also for tabulating various social
media metrics like Facebook likes tweets
Etc now that we have seen what is
descriptive analytics let us look into
and use case of descriptive analytics
the Dow Chemical Company analyzed all
its past data using descriptive
analytics and by doing so they were able
to identify the underutilized space in
their facility descriptive analytics
helped them in this space consolidation
on the whole the company was able to
save nearly 4 million dollars annually
so we now see here that descriptive
analytics not only helps us derive
meaningful information from the past
data but it can also help companies in
cost reduction if it is used wisely let
us now move on to our next type that is
diagnostic analytics diagnostic
analytics asks the question why a
particular problem has occurred as you
can see it will always ask the question
why did it happen it will look into the
root cause of a problem and try to
understand why it has occurred
diagnostic analytics makes use of
various techniques such as data mining
data Discovery and drill down companies
benefit from this type of analytics
because it helps them look into the root
cause of a problem by doing so the next
time the same problem will not arise as
the company already knows why it has
happened and they will arrive at a
particular solution for it einet saws bi
query tool is an example of diagnostic
analytics we can use Query tool for
Diagnostic analytics now that you know
why diagnostic analytics is required and
what diagnostic analytics is I will run
you through an example which shows where
diagnostic analytics can be used all of
us shop on e-commerce sites right have
you ever added items to your cart but
ended up not buying it yes all of us
might have done that at some point an
organization tries to understand why its
customers don't end up buying their
product X although it has been added to
their cuts and this understanding is
done with the help of diagnostic
analytics an e-commerce site wonders why
they have made few online sales although
they have had a very good marketing
strategy there could have been various
factors as to why this has happened
factors like the shipping fee which was
too high or the page that didn't load
correctly not enough payment options
available and so on all these factors
are analyzed using diagnostic analytics
and the company comes to a conclusion as
to why this has happened thus we see
here that the root cost is identified so
that in future the same problem doesn't
occur again let us now move on to the
third type that is Predictive Analytics
as the name suggests Predictive
Analytics makes predictions of the
Future IT analyzes the current and
historical facts to make predictions
about future it always asks the question
what will happen next Predictive
Analytics is used with the help of
artificial intelligence machine learning
data mining to analyze the data it can
be used for predicting customer Trends
market trends customer Behavior Etc it
solely works on probability it always
tries to understand what can happen next
with the help of all the past and
current information a company like
PayPal which has 260 plus million
accounts always has the need to ensure
that their online fraudulent and
unauthorized activities are brought down
to nil fear of constant fraudulent
activities have always been a major
concern for PayPal when a fraudulent
activity occurs people lose trust in the
company and this brings in a very bad
name for the brand it is inevitable that
fraudulent activities will happen in a
company like PayPal which is one of the
largest online payment processes in the
world but PayPal uses analytics wisely
here to prevent such fraud activities
and to minimize them it uses Predictive
Analytics to do so the organization is
able to analyze past data which includes
a customer's historical payment data a
customer's Behavior Trend and then it
builds an algorithm which works on
predicting what is likely to happen next
with respect to their transaction with
the use of big data and algorithms the
system can gauge which of the
transactions are valid and which could
be potentially a fraudulent activity by
doing so PayPal is always ready with
precautions that they have to take to
protect all their clients against
fraudulent transactions we will now move
on to our last type that is prescriptive
analytics prescriptive analytics as the
name suggests always prescribes a
solution to a particular problem the
problem can be something which is
happening currently hence it can be
termed as the type that always asks a
question what is the solution
prescriptive and analytics is related to
both predictive and descriptive
analytics as we saw earlier descriptive
analytics always asks the question what
has happened and Predictive Analytics
helps you understand what can happen
next with the help of artificial
intelligence and machine learning
prescriptive analytics helps you arrive
at the best solution for a particular
problem various business rules
algorithms and computational modeling
procedures are used in prescriptive
analytics let us now have a look at how
and bear prescriptive analytics is used
with an example here we will understand
how prescriptive analytics is used by an
airline for its profit do you know that
when you book a flight ticket the price
of it depends on various factors both
internal and external factors apart from
taxes seat selection there are other
factors like oil prices customer demand
which are all taken into consideration
before the flights fare is displayed
prices change due to Avail ability and
demand holiday seasons are a time when
the rates are much higher than the
normal days Seasons like Christmas and
school vacations also beacons the rates
will be much higher than weekdays
another factor which determines a
flight's fair is your destination
depending on the place where you are
traveling to the flight Fair will be
adjusted accordingly this is because
there are quite a few places where the
air traffic is less and in such places
the flights fare will also be less so
prescriptive analytics analyzes all
these factors that are discussed and it
builds an algorithm which will
automatically adjust a flight's fare by
doing so the airline is able to maximize
its profit these were the four types of
analytics now let us understand how we
achieve these with the use of Big Data
tools our next topic will be the various
tools used in big data analytics these
are few of the tools that I will be
talking about today we have Hadoop
mongodb talindi Kafka Cassandra spark
and storm we will look into each one of
them one by one we will first start off
with Hadoop when you speak of Big Data
the first framework that comes into
everyone's mind is Hadoop isn't it as I
mentioned earlier Apache Hadoop is used
to store and process big data in a
distributed and parallel fashion it
allows us to process data very fast
Hadoop uses map reduce big and high for
analyzing this big data Hadoop is easily
one of the most famous Big Data tools
now let us move on to the next one that
is mongodb mongodb is a cross-platform
document oriented database it has the
ability to deal with large amount of
unstructured data processing of data
which is unstructured and processing of
data sets that change very frequently is
done using mongodb Talon D provides
software and services for data
integration data management and cloud
storage it specializes in Big Data
integration talente Open studio is a
free open source tool for processing
data easily on a big data environment
Cassandra is used widely for an
effective management of large amounts of
data it is similar to Hadoop in its
feature of fault tolerance where data is
automatically replicated to multiple
nodes Cassandra is preferred for
real-time processing spark is another
tool that is used for data processing
this data processing engine is developed
to process data way faster than Hadoop
map reduce this is done in a way because
spark does all the processing in the
main memory of the data nodes and thus
it prevents unnecessary input output
overheads with the disk whereas map
reduce is disk based and hence spark
proves to be faster than Hadoop
mapreduce storm is a free Big Data
computation system which is done in real
time it is one of the easiest tools for
Big Data analysis it can be used with
any programming language this feature
makes storm very simple to use finally
we will look into another big data tool
which is known as Kafka Kafka is a
distributed streaming platform which was
developed by LinkedIn and later given to
Apache software Foundation it is used to
provide real-time analytics result and
it is also used for fault tolerant
storage these were few of the big data
analytics tools now let us move on to
our last topic for today that is big
data application domains here we will
look into the various sectors where big
data analytics is actively used the
first sector is e-commerce nearly 45
percent of the world is online and they
create a lot of data every second big
data can be used smartly in the field of
e-commerce by predicting customer Trend
forecasting demands adjusting the price
and so on online retailers have the
opportunity to create better shopping
experience and generate higher sales if
big data analytics is used correctly
having big data doesn't automatically
lead to a better marketing strategy
meaningful insights need to be derived
from it in order to make right decisions
by analyzing big data we can have
personalized marketing campaigns which
can result in better and higher sales in
the field of Education depending on the
market requirements new courses are
developed the market requirement needs
to be analyzed correctly with respect to
the scope of a course and accordingly a
scope needs to be developed there is no
point in developing a course which has
no scope in the future hence to analyze
the market requirement and to develop
new courses we use big data analytics
here there are a number of uses of big
data and analytics in the field of
Health Care and one of it is to predict
a patient's health issue that is with
the help of their previous medical
history big data analytics can determine
How likely they are to have a particular
health issue in the future the example
of Spotify that we saw previously showed
how big data analytics is used to
provide a personalized recommendation
list to all its users similarly in the
field of media and entertainment big
data analytics is used to understand the
demands of shows songs movies and so on
to deliver personalized recommendation
lists as we saw with Spotify big data
analytics is used in the field of
banking as we saw previously with a few
use cases big data analytics was used
for risk management in addition to risk
management it is also used to analyze a
customer's income and spend patterns and
to help the bank predict if a particular
customer is going to choose any of the
bank offers such as loans credit cards
schemes and so on this way the bank is
able to identify the right customer who
is interested in its offers it has
noticed that telecom companies have
begun to embrace big data to gain profit
big data analytics helps in analyzing
Network traffic and call data records it
can also improve its service quality and
improve its customer experience let us
now look into how big data analytics is
used by governments across the world in
the field of law enforcement big data
analytics can be applied to analyze all
the available data to understand crime
patterns intelligent Services can use
Predictive Analytics to forecast the
crime which could be committed in Durham
the police department was able to reduce
the crime rate using big data analytics
with the help of data police could
identify whom to Target where to go when
to petrol and how to investigate frames
big data analytics help them to discover
patterns of crime emerging in the area
before we move on to the applications
let's have a quick look at the big data
market revenue forecast worldwide from
2011 to 2027. so here's a graph in which
the y-axis represents the revenue in
billion US Dollars and the x-axis
represents the years as it is seen
clearly from the graph big data has
grown until 2019 and statistics predict
that this growth will continue even in
the future this growth is made possible
as numerous companies use big data in
various domains to boost their revenue
we will look into few of such
applications the first Big Data
application we will look into is weather
forecast imagine there is a sudden storm
and you're not even prepared that would
be a terrifying situation isn't it
dealing with any calamities such as
hurricane storms floods would be very
inconvenient if we are caught off guard
the solution is to have a tool that
predicts the weather of the coming days
well in advance this tool needs to be
accurate and to make such a tool big
data is used so how does Big Data help
here well it allows us to gather all the
information required to predict the
weather information such as the climate
change details wind direction
precipitation previous weather reports
and so on after all this data is
collected it becomes easier for us to
spot a trend and identify what's going
to happen next by analyzing all of this
big data a weather prediction engine
works on this analysis it predicts the
weather of every region across the world
for any given time by using such a tool
we can be well prepared to face any
climate change or any natural Calamity
let's take an example of a landslide and
try to understand how big data is used
to tackle such a situation predicting a
landslide is very difficult with just
the basic warning signs lack of this
prediction can cause a huge damage to
life and property this challenge was
studied by the University of Melbourne
and they developed a tool which is
capable of predicting a landslide this
tool predicts the boundary where a
landslide is likely to occur two weeks
before this magical tool works on both
big data and Applied Mathematics and
accurate prediction like this which is
made two weeks before can save lives and
help in relocating people in that
particular region it also gives us an
insight into the magnitude of the
upcoming destruction this is how big
data is used in weather forecasts and in
predicting any natural calamities across
the world let us now move on to our next
application that is big data application
in the field of media and entertainment
the media and the entertainment industry
is a massive one leveraging big data
here can produce sky-high results and
boost the revenue for any company let us
see the different ways in which big data
is used in this industry have you ever
noticed that you come across relevant
advertisements in your social media
sites and in your mailboxes well this is
done by analyzing all your data such as
your previous browsing history and your
purchase data Publishers then display
what you like in the form of ads which
will in turn catch your interest in
looking into it next up is customer
sentiment analysis customers are very
important for a company the happier the
customer the greater the company's
Revenue Big Data helps in gathering all
the emotions of a customer through their
posts messages conversations Etc these
emotions are then analyzed to arrive at
a conclusion regarding the customer
satisfaction if the customer is unhappy
the company strives to do better the
next time and provides their customers a
better experience while purchasing an
item from an e-commerce site or while
watching videos on an entertainment site
you might have noticed a segment which
says most recommended list for you this
list is a personalized list which is
made Available to You by analyzing all
the data such as your previous watch
History your subscriptions your likes
and so on recommendation engine is a
tool that filters and analyzes all this
data and provides you with a list that
you would most likely be interested in
by doing so the site is able to retain
and engage its customer for a longer
time next is customer churn analysis in
simple words customer churn happens when
a customer stops a subscription with a
service predicting and preventing this
is of Paramount importance to any
organization by analyzing the behavioral
patterns of previously tuned customers
an organization can identify which of
their current customers are likely to
churn by analyzing all of this data the
organization can then Implement
effective programs for customer
retention let us now look into an use
case of Starbucks big data is
effectively used by the Starbucks app 17
million users use this app and you can
imagine how much data they generate data
in the form of their coffee buying
habits the store they visit and to the
time they purchase all of this data is
fed into the app so when a customer
enters a new Starbucks location the
system analyzes all their data and we
are provided with their preferred order
this app also suggests new products to
the customer in addition to this they
also provide personalized offer and
discounts on special occasions moving on
to our next sector which is Healthcare
it is one of the most important sectors
big data is widely used here to save
lives with all the available Big Data
medical researchers are done very
effectively they are performed
accurately by analyzing all the previous
medical histories and new treatments and
medicines are discovered cure can be
found out even for few of the incurable
diseases there are cases is when one
medication need not be effective for
every patient hence Personal Care is
very important Personal Care is provided
to each patient depending on their past
medical history and individual's medical
history along with their body parameters
are analyzed and personal attention is
given to each of them as we all know
Medical Treatments are not very pocket
friendly every time a medical treatment
is taken the amount increases this can
be reduced if readmissions are brought
down analyzing all the data precisely
will deliver a long-term efficient
result which will in turn prevent a
patient's readmission frequently with
globalization came an increase in the
ease for infectious diseases to spread
widely based on geography and
demographics Big Data helps in
predicting where an outbreak of epidemic
viruses are most likely to occur and
American Healthcare Company United
Health care uses big data to detect any
online medical fraud activities such as
payment of unauthorized benefits
intentional misrepresentation of data
and so on the Healthcare company runs
disease management programs the success
rates of these programs are predicted
using big data depending on how patients
respond to it the next sector we will
look into is logistics Logistics looks
into the process of transportation and
storage of goods the movement of a
product from its supplier to a consumer
is very important big data is used to
make this process faster and efficient
the most important factor in logistics
is the time taken for the products to
reach their destination to achieve
minimum time sensors within the vehicle
analyze the fastest route this analysis
is based on various data such as the
weather traffic the list of orders and
so on by doing so so the fastest route
is obtained and the delivery time is
reduced capacity planning is another
factor which needs to be taken into
consideration details regarding the
workforce and the number of vehicles are
analyzed thoroughly and each vehicle is
allocated a different route this is done
as there is no need for many trucks to
travel in the same direction which will
be pointless depending on the analysis
of the available Workforce and resources
this decision is taken big data
analytics also Finds Its use in managing
warehouses efficiently this analysis
along with tracking sensors provide
information regarding the underutilized
space which results in efficient
resource allocation and eventually
reduces the cost customer satisfaction
is important in logistics just like it
is in any other sector customer
reactions are analyzed from the
available data which will eventually
create create an instant feedback loop a
happy customer will always help the
company gain more customers let us now
look into a use case of UPS as you know
UPS is one of the biggest shipping
company in the world they have a huge
customer database and they work on data
every minute UPS uses big data to gather
different kinds of data regarding the
weather the traffic jams the geography
the locations and so on after collecting
all this data they analyze it to
discover the best and the fastest route
to the destination in addition to this
they also use big data to change the
routes in real time this is how
efficiently UPS leverages Big Data next
up we have a very interesting sector
that is the travel and tourism sector
the global tourism Market is expected to
grow in the near future big data is used
in various ways in this sector let us
look into a few of them hotels can
increase their revenue by adjusting the
room tariffs depending on the peak
Seasons such as holiday seasons festive
seasons and so on the tourism industry
uses all of this data to anticipate the
demand and maximize their revenue big
data is also used by Resorts and hotels
to analyze various details regarding
their competitors this analysis result
helps them to incorporate all the good
facilities their competitors are
providing and by doing so the hotel is
able to flourish further a customer
always comes back if they are offered
good packages which are more than just
the basic ones looking into a customer's
past travel history likes and
preferences hotels can provide its
customers with personalized experiences
which will interest them highly
investing in an area which could be the
Hub of Tourism is very wise few
countries use big data to examine the
tourism activities in their country and
this in turn helps them discover new and
fruitful investment opportunities let us
look into one of the best online
Homestay networks Airbnb and see how big
data is used by them Airbnb undoubtedly
provides its customers with the best
accommodation across the world big data
is used by it to analyze the different
kinds of available properties depending
on the customer's preferences the
pricing the keywords previous customers
ratings and experiences Airbnb filters
out the best result Big Data Works its
magic yet again now we will move on to
our final sector which is the government
and law enforcement sector maintaining
Law and Order is of utmost importance to
any government it is a huge task by
itself Big Data plays an active role
here and in addition to this it also
helps governments bring in new policies
and schemes for the welfare of its
citizens the police department is able
to predict criminal activities fear
before it happens by analyzing big data
information such as the previous crime
records in a particular region the
safety aspect in that region and so on
by analyzing these factors they are able
to predict any activity which breaks the
law and order of the region governments
are able to tackle unemployment to a
great extent by using big data by
analyzing the number of students
graduating every year to the number of
relevant job openings the government can
have an idea of the unemployment rate in
the country and then take necessary
measures to tackle it our next factor is
poverty in large countries it is
difficult to analyze which area requires
attention and development big data
analytics makes it easier for
governments to discover such areas
poverty gradually decreases once these
areas begin to develop governments have
to always be on the lookout for better
development a public survey voices the
opinion of a country's citizens
analyzing all the data collected from
such service can help governments build
better policies and services which will
benefit its citizens let us now move on
to our use case did you know that the
New York Police Department uses big data
analytics to protect its citizens the
department prevents and identifies
Crimes by analyzing a huge amount of
data which includes fingerprints certain
emails and records from previous police
investigations and so on after analyzing
all of this data meaningful insights are
drawn from it which will help the police
in taking the required preventive
measures against crimes now when we talk
about evolution of big data we have
known that data has evolved in last five
years like never before now in fact
before going to Big Data or before
understanding these Solutions and the
need and why there is a rush towards Big
Data technology solution I would like to
ask a question take a couple of minutes
and think why are organizations
interested in
why is there a sudden rush in Industry
where everyone would want to ramp up
their current infrastructure or would
want to be working on Technologies which
allow them to use the spectator think
about it what is happening and why are
organizations interested in this and if
you think on this you will start
thinking about what organizations have
been doing in past what organizations
have not done and why are organizations
interested in
now before we learn on big data we can
always look into internet and check for
use cases where organizations have
failed to use Legacy systems or
relational databases to work on their
data requirements now over in recent or
over past five years or in recent decade
what has happened is organizations have
started understanding the value of data
and they have decided not to ignore any
data as being uneconomical now we can
talk about different platforms through
which data is generated take an example
of social media like Twitter Facebook
Instagram WhatsApp YouTube you have
e-commerce and various portals say eBay
Amazon Flipkart alibaba.com and then you
have various Tech Giants such as Google
Oracle sap Amazon Microsoft and so
so lots of data is getting generated
every day in every business sector the
point here is that organizations have
slowly started realizing that they would
be interested in working on all the data
now the question which I asked was why
are the organizations interested in big
data and some of you might have already
answered or thought about that
organizations are interested in doing
precise analysis or they want to work on
different formats of data such as
structured unstructured semi-structured
data organizations are interested in
gaining insights or finding the hidden
treasure in the so-called big data and
this is the main reason where
organizations are interested in data now
there are various use cases there are
various use cases we can compare that
organizations from past 50 or more than
50 years have been handling huge amount
of data they have been working on huge
volume of data but the question here is
have they worked on all the data or have
they worked on some portion of it what
have they used to store this data and if
they have used something to store this
data what is happening what is what is
changing now when we talk about the
businesses we cannot avoid talking about
the dynamism involved now any
organization would want to have a
solution which allows them to store data
and store huge amount of data capture it
process it analyze it and also look into
the data to give more value to the data
organizations have then been looking for
Solutions now let's look at some facts
that can convince you or that would
convince you that data is exploding and
needs your attention right 55 billion
messages and 4.5 billion photos are sent
each day on WhatsApp 300 hours of video
are uploaded every minute on YouTube did
you guys know that YouTube is the second
largest search engine after Google every
minute users send 31.25 million messages
and watch 2.77 million videos on
Facebook Walmart handles more than one
million customer transactions every hour
Google 40 000 search queries are
performed on Google per second that is
3.46 million searches a day in fact you
could also say that a lot of times
people when they are loading up the
Google page is basically just to check
their internet connection however that
is also generating data IDC reports that
by 2025 real-time data will be more than
a quarter of all the data and by 2025
the volume of Digital Data will increase
to 163 Zeta bytes that is we are not
even talking about gigabytes or
terabytes anymore we are talking about
petabytes exabytes and zetabytes
zetamites means 10 to the power 21 bytes
bytes so this is how data has evolved
now you can talk about different
companies which would want to use their
data to take business decisions they
would want to collect the data store it
and analyze it and that's how they would
be interested in drawing insights for
the business now this is just a simple
example about Facebook and what it does
to work on the data now before we go to
Facebook you could always check in
Google by just typing in companies using
big data and if we say companies using
big data we should be able to find a
list of different companies which are
using big data for different use cases
there are various sources from where you
can find we could also search for
solution that is Hadoop which we'll
discuss later but you could always say
companies using Hadoop and that should
take you to the Wiki page which will
basically help you know what are the
different companies which are using this
so-called solution called Hadoop okay
now coming back to what we were
discussing about so organizations are
interested in Big Data as we discussed
in gaining insights they would want to
use the data to find hidden information
which probably they ignored earlier now
take an example of rdbms what is p
biggest drawback in using an rdbms now
you might think that rdbms is known for
stability and consistency and
organizations would be interested in
storing their data in Oracle or db2 or
MySQL or Microsoft SQL server and they
have been doing that for many years now
so what has changed now now when we talk
about rdbms the first question which I
would ask is do we have access to 100 of
data being online in rdbms the answer is
no we would only have 10 or 20 or 30
percent of data online and rest of the
data would be archived which means that
if an organization is interested in
working on all the data they would have
to move the data from the archived
storage to the processing layer and that
would involve bandwidth consumption now
this is one of the biggest drawbacks of
rdbms you do not have access to 100 of
data online in many of the cases
organizations started realizing that the
data which they were ignoring as being
uneconomical had hidden value which they
had never exploited I had read a
presentation somewhere which said
torture the data and it will confess to
anything now that's the value of data
which organizations have realized in
recent past take an example of Facebook
now this shows what Facebook does with
its big data and we'll come to what is
Big Data but let's understand the use
case now Facebook collects huge volumes
of user data whether that is SMS whether
that is likes whether that is
advertisements whether that is features
which people are liking or photographs
or even user profiles Now by collecting
this data and providing a portal which
people can use to connect Facebook is
also accumulating huge volume of data
and that's way Beyond petabytes they
would also be interested in analyzing
this data and one of the reasons would
be they would want to personalize The
Experience take an example of
personalized news feed depending on a
user Behavior depending on what a user
likes what a user would want to know
about they can recommend a personalized
news feed to every particular user
that's just one example of what Facebook
does with its data take an example of
photo tag suggestions now when you log
into Facebook account you could also get
suggestions on different friends whom
you would like to connect to or you
would want to tag so that they could be
known by others some more examples which
show how Facebook uses its data are as
follows so the flashback collection of
photos and posts that receive the most
comments and likes okay there was
something called as I voted that was
used for 2016 elections with reminders
and directions to tell users their time
and place of polling also something
called as safety checks in incidents
such as earthquake hurricane or mass
shooting Facebook gives you safety
checks now these are some examples where
Facebook is using big data and that
brings us to the question what is Big
Data this was just an example where we
discussed about one company which is
making use of that data which has been
accumulated and it's not only for
companies which are social media
oriented like Facebook where data is
important take an example of IBM take an
example of JP Morgan change they take an
example of G or any other organization
which is collecting huge amount of data
they would all want to gather insights
they would want to analyze the data they
would want to be more precise in
building their services or Solutions
which can take care of their customers
so what is Big Data big data is based is
basically a term is used to describe the
data that is too large and complex to
store in traditional databases and as I
gave an example it's not just about
storing the data it is also about what
you can do with the data it also means
that if there is a lot of dynamism
involved can you change the underlying
storage and handle any kind of data that
comes in now before we get into that
let's just understand what is Big Data
so big data is basically a term which
has been given to categorize the data if
it has different characteristics
organizations would want to have the big
data stored processed and then analyzed
to get whatever useful information they
can get from the state now there are
five V's of Big Data volume velocity
variety value velocity although these
are five V's but then there are other
V's which also categorize the data as
Big Data such as volatility validity
viscosity virality of data okay so these
are five V's of big data and if the data
has one or all of these characteristics
then it can be considered as big
including the other ways which I just
mentioned so volume basically means
incredible amount of data huge volumes
of data data generated every second now
that could be used for batch processing
that could be used for real-time stream
processing okay you might have data
being generated from different kind of
devices like your cell phones your
social media websites online
transactions variable devices servers
and these days with iot we are also
talking about data of getting generated
via internet of things that is you could
have different devices which could be
communicating to each other
getting data from Radars or leaders or
even camera sensors so there is a huge
volume of data which is getting
generated and if we are talking about
data which has huge volume which is
getting generated constantly or has been
accumulated over a period of time we
would say that is big data velocity now
this is one more important aspect of Big
Data speed at which the data is getting
generated think about stock markets
think about social media websites think
about online surveys or marketing
campaigns or airline industry so if the
data is getting generated with a lot of
speed where it becomes difficult to
capture collect process cure mine or
analyze the data then we are certainly
talking about Big Data the next aspect
of big data is variety now this is where
we talk about structured data
semi-structured data or unstructured
data and here I would like to ask a
question what is the difference when do
you call the data is structured
semi-structured or unstructured now
let's look at an example before we
theoretically discuss about this I
always would like to use some examples
let's look at a log file and let's see
what is it so if I look at this log file
and if I would say what kind of data is
this which is highlighted one the answer
would be it is structured data it has
specific delimiters such as space it has
data which is separated by space and and
if I had a hundred or thousand or
million rows which had similar kind of
data I could certainly store that in a
table I could have a predefined schema
to store this data so I would call the
one which is highlighted as structured
but if I look at this portion where I
would look at a combination of this kind
of data where some data has a pattern
and some data doesn't now this is an
example of semi-structured data so if I
would have a predefined structure to
store this data probably the pattern of
data would break the structure and if I
look at all the data then I would
certainly call it unstructured data
because there is no clear schema which
can Define this data now this is what I
mean by variety of data that is
structured data which basically has a
schema or has a format which could be
easily understood you have
semi-structured which could be like in
XML or Json or even your Excel sheets
where you can have some data which is
structured and the other is unstructured
and when we talk about unstructured we
are talking about absence of schema it
does not have a format it does not have
a schema and it is hard to analyze lies
which brings its own challenges the next
aspect is value now value refers to the
ability to turn your data useful for
business you would have a lot of data
which is being collected as we mentioned
in previous slides right there would be
a lot of data wrangling or data
pre-processing or cleaning up of data
happening and then finally you would
want to draw value from that data but
from all the data collected what
percentage of data gives us value and if
all my data can give me value then why
wouldn't I use it this is an aspect of
big data right veracity now this means
the quality of data billions of dollars
are lost every year by organizations
because the data which was going was not
of good quality or probably they
collected a lot of data and then it was
erroneous take an example of autonomous
driving projects which are happening in
Europe or us where there are car fleets
which are on the road collecting data
via radar sensors and Camera sensors and
when this data has to be processed to
train algorithms it has realized that
sometimes the data which was collected
was missing in some values might be was
not appropriate or had a lot of errors
and all this process of collecting the
data becomes a repetitive task because
the quality of data was not good this is
just one example we can take example
from healthcare industry or stock
markets or financial institutions and so
on so extracting loads of data is not
useful if the data is messy or poor in
quality and that basically means that
velocity is a very important V of big
data now apart from veracity volume
variety velocity and value we have the
other v such as viscosity how dense the
data is validity is the data still valid
volatility is my data volatile or
virality is the data viral now all of
these different V's categorize the data
as big data now here we would like to
talk on a big data case study
example of Google which obviously is one
of the companies which is churning and
working on huge amount of data now it's
actually said that if you compare one
grain of sand with one byte of data then
Google is processing or Google is
handling Whole World's sand every week
that is the kind of data which Google is
now in early 2000 and since then when
the number of Internet users started
Google also faced a lot of
storing increasing user data and using
the traditional servers to manage that
now that was a challenge which Google
started facing could they use
traditional data server to store the
data well yes they could Right storage
devices have been getting cheaper day by
day but then how much time does it take
to retrieve that data what is the seek
time what is the time taken to read
a data thousands of search queries were
raised per second No Doubt
billions and billions of queries are
raised by
every query read 100 MBS of data and
consumed tens of billions of CPU Cycles
based on these queries so the
requirement was that they wanted to have
a large
distributed highly fault tolerant file
system large to store to capture process
huge amount of data distributed because
they could not rely just on one server
even if that had multiple disks stacked
up that was not an efficient Choice what
would happen if this particular machine
failed what would happen if the whole
server was down so they needed a
distributed storage and a distributed
computing environment they needed
something which can be highly fault
tolerant right so this was the
requirement which Google had and the
solution which came out as a result was
GFS Google file system now let's look at
how GFS works so normally in any
particular Linux system or Linux server
you would have a file system you would
have set of processes you would have set
of files and directories which could
store the data GFS was different so to
facilitate GFS which could store huge
amount of data there was and
architecture an architecture which had
one master and multiple chunk servers or
you can say slave servers or slave
machines Master machine was to contain
metadata was to contain data about data
we say metadata we are talking about
information about data and then you have
the chunk servers or the slave machines
which could be storing data in a
distributed fashion now any client or an
API or an application which would want
to read the data but first Contact the
master server it would contact the
machine where the master process was
running and client would place a request
of reading the data or showing an
interest of reading the data internally
what it is doing is it is requesting for
metadata your API or an application
would want to know from where it can
read the data Master server which has
metadata whether that is in Ram or disk
we can discuss that later but then
Master server would have the metadata
and it would know which are the chunk
servers or the slave
where the data was stored in a
distributed fashion Master would respond
back with the metadata information to
the client and then client could use
that information to read
two base slave machines where actually
the data was stored now this is what the
process or set of processes work
together
to make GFS So when you say chunk server
we would basically have the files
getting divided into fixed size chunks
now how would they get divided so there
would be some kind of Chunk size or a
block size which would determine that if
the file is bigger than the pre-decided
chunk size then it would be split into
smaller chunks and be distributed across
the chunk servers or the slave machines
if the file was smaller then it would
still use one chunk or a block to get
stored on the underlying slave machines
so these chunk servers or slave machines
are the ones which actually store the
data on local disks as your Linux files
client which is interacting with Master
for metadata and then interacting with
chunk servers or read write operations
would be the one which would be
externally connecting to the cluster so
this is how it would look so you have a
master which would obviously be
receiving some kind of heartbeats from
the chunks wants to know their status
and receive information in the form of
packets which would let the master know
which machines were available for
storage which machines already had data
and master would build up the metadata
within itself the files would be broken
down into chunks for example we can look
at file one it is broken down into chunk
one and
File 2 has one chunk which is one
portion of it and then you have file two
residing on some other chunk server
which also lets us know that there is
some kind of Auto replication for this
file system right and the data which is
getting stored in the chunk could have a
data of 64 MB now that chunk size could
be changed based on the data size but
Google file system had the basic size of
the chunk as 64 MB junk
applicated on multiple servers the
default replication was three and that
could again be increased or decreased as
per requirement this would also mean
that if a particular slave machine or a
chunk server would die or would get
killed or would crash there would never
be any data loss because a replica of
data residing on the failed machine
would still be available on some other
slave server chunk server or slave
machine now this helped Google to store
and process huge volumes of data in a
distributed Manner and does have a fault
tolerant distributed scalable storage
which could allow them to store huge
amount of data now that was just one
example which actually led to the
solution which today we call
now when we talk about Big Data here I
would like to ask you some questions
that if we were talking about the rdbms
case take an example of uh something
like NASA which was working on a a
project called seti search of
extraterrestrial intelligence now this
was a project where they were looking
for a solution to take care of their
problem the problem was that they would
roughly send some waves in space capture
those waves back and then analyze this
data to find if there was any
extraterrestrial object in
they had two options for it they could
either have a huge server built which
could take care of storing the data and
processing it or they could go for
volunteer computing now volunteer
Computing basically means that you could
have a lot of people volunteering and be
part of this project and what they would
in turn do is they would be donating
their RAM and storage from their
machines when they
went there how would that happen
basically download
patch on their machine which would run
as a screensaver and if the user is not
using his machine some portion of data
could be transferred to these machines
intermittent storage and processing
using lab now this sounds very
interesting and this sounds very easy
however it would have its own challenges
right think about security think about
integrity but those those problems are
not bigger as much as is the requirement
of bandwidth and this is the same thing
which happens in rdbms if you would have
to move data from archived solution to
the processing layer that would consume
huge amount bandwidth Big Data brings
its own challenges huge amount of data
is getting generated every day now the
biggest challenge is storing this huge
volume of data and especially when this
data is getting generated with a lot of
variety when it can have different kind
of formats where it could be viral it
could be having a lot of value and
nobody has looked into the veracity of
data but the primary problem would be
handling this huge volume of data
variety of the data would bring in
challenges of storing it in Legacy
systems if processing of the data was
required now here again I would suggest
you need to think what is the difference
between reading a data and your data so
reading might just mean bringing in the
data from disk and doing some i o
operations and processing would mean
reading the data probably doing some
Transformations on it extracting some
useful information from it and then
storing it in the same format or
probably in a different format so
processing this massive volume of data
is the Second Challenge organizations
don't just store their big data they
would eventually want to use it to
process it to gather some insights now
processing and extracting insights from
Big Data would take huge amount of time
unless and until there was an efficient
solution to handle and process the
spectator securing the data that's again
a concern for organizations
encryption of big data is difficult to
perform if you would think about
different compression mechanisms then
that would also mean decompressing of
data which would also mean that you
could take a hit on the CPU Cycles or on
disk usage providing user authentication
for every team member now that could
also be danger hello Learners simply and
brings you a postgraduate program in
data engineering developed in
partnership with Purdue University and
IBM to learn more about this course you
can find the course Link in the
description box below
so that led to Hadoop Association so big
data data brings its own challenges be
data
here we have a solution
now what is Hadoop it's a open source
framework for storing data and running
applications on clusters of commodity
Hardware
Hadoop is an open source framework and
before we discuss on two main components
of Hadoop it would be good to look into
the link which I was suggesting earlier
that is companies using Hadoop and any
person who would be interested in
learning Big Data should start somewhere
here where you could list down different
companies what kind of setup they have
why are they having Hadoop what kind of
processing they are doing and how are
they using these so-called Hadoop
clusters to process and in fact store
capture and process huge amount of data
another link which I would suggest is
looking at different distributions of
Hadoop any person who is interested in
learning in Big Data should know about
different distributions of Hadoop now in
Linux we have different distributions
like Ubuntu Centos Red Hat Suzy dbn in
the same way you have different
distributions of Hadoop which we can
look on the Wiki page and this is the
link which talks about products that
include Apache Hadoop or derivative
works and Commercial support basically
means that Apache Hadoop the sole
products that can be called a release of
Apache Hadoop come from apache.org
that's an open source community and then
you have various vendor-specific
distributions like Amazon web services
you have cloud data you have hot and
works you have IBM's big inside you have
mapper all these are different
distributions of
so basically all of these vendor
specific distributions are depending or
using on core Apache Hadoop in brief we
can say that these are the vendors which
take up the Apache Hadoop package it
within a cluster management solution so
that users who intend to use Apache
Hadoop would not have difficulties of
setting up a cluster setting up a
framework they could just use a vendor
specific distribution with its cluster
installation Solutions cluster
Management Solutions and easily plan
deploy install and
math cluster let's rewind to the days
before the world turn digital back then
minuscule amounts of data were generated
at a relatively sluggish Pace all the
data was mostly documents and in the
form of rows and columns storing or
processing this data wasn't much trouble
as a single storage unit and processor
combination would do the job but as
years passed by the internet took the
World by storm giving rise to tons of
data generated in a multitude of forms
and formats every microsecond
semi-structured and unstructured data
was available now in the form of emails
images audio and video to name a few all
this data became collectively known as
Big Data although fascinating it became
nearly impossible to handle this big
data and a storage unit processor
combination was obviously not enough
so what was the solution
multiple storage units and processors
were undoubtedly the need of the hour
this concept was incorporated in the
framework of Hadoop that could store and
process vast amounts of any data
efficiently using a cluster of commodity
Hardware Hadoop consisted of three
components that were specifically
designed to work on big data in order to
capitalize on data the first step is
storing it the first component of Hadoop
is its storage unit the Hadoop
distributed file system or hdfs
storing massive data on one computer is
unfeasible hence data is distributed
amongst many computers and stored in
blocks so if you have 600 megabytes of
data to be stored hdfs splits the data
into multiple blocks of data that are
then stored on several data nodes in the
cluster 128 megabytes is the default
size of each block hence 600 megabytes
will be split into four blocks a b c and
d of 128 megabytes each and the
remaining 88 megabytes in the last block
e so now you might be wondering what if
one data node crashes do we lose that
specific piece of data well no that's
the beauty of hdfs hdfs makes copies of
the data and stores it across multiple
systems for example when block a is
created it is replicated with a
replication factor of 3 and stored on
different data nodes this is termed the
replication method by doing so data is
not lost at any cost even if one data
node crashes making hdfs fault tolerant
after storing the data successfully it
needs to be processed this is where the
second component of Hadoop mapreduce
comes into play in the traditional data
processing method entire data would be
processed on a single machine having a
single processor this consumed time and
was inefficient especially when
processing large volumes of a variety of
data to overcome this mapreduce splits
data into parts and processes each of
them separately on different data nodes
the individual results are then
aggregated to give the final output
let's try to count the number of
occurrences of words taking this example
first the input is split into five
separate parts based on full stops the
next step is the mapper phase where the
occurrence of each word is counted and
allocated a number after that depending
on the words similar words are shuffled
sorted and grouped following which in
the reducer phase all the grouped words
are given account
finally the output is displayed by
aggregating the results all this is done
by writing a simple program similarly
mapreduce processes each part of Big
Data individually and then sums the
result at the end this improves load
balancing and saves a considerable
amount of time
now that we have our mapreduce job ready
it is time for us to run it on the
Hadoop cluster this is done with the
help of a set of resources such as RAM
Network bandwidth and CPU multiple jobs
are run on Hadoop simultaneously and
each of them needs some resources to
complete the task successfully to
efficiently manage these resources we
have the third component of Hadoop which
is yarn yet another resource negotiator
or yarn consists of a resource manager
node Manager application master and
containers the resource manager assigns
resources node managers handle the nodes
and monitor the resource usage in the
node the containers hold a collection of
physical resources
suppose we want to process the mapreduce
job we had created first the application
Master requests the container from the
node manager once the node manager gets
the resources it sends them to the
resource manager this way yarn processes
job requests and manages cluster
resources in Hadoop in addition to these
components Hadoop also has various Big
Data tools and Frameworks dedicated to
managing processing and analyzing data
the Hadoop ecosystem comprises several
other components like Hive Pig Apache
spark Flume and scoop to name a few the
Hadoop ecosystem works together on big
data management before we dive into the
technical side of Hadoop going to take a
little detour to try to give you a
visual understanding and relate it to
maybe a more life setup and we're going
to go to the farm in this case so we
have in a farm far away I almost wish
they'd put far far away it does remind
me a little bit of a Star Wars theme so
we're going to look at fruit at a farm
we have Jack who harvestses grapes and
then sells it in the nearby town after
harvesting he stores his produce in a
storage shed or a storage room in this
case we found out though is there was a
high demand for other fruits so he
started harvesting apples and oranges as
well hopefully it has a couple fills
with these different fruit trees growing
and he said up there and you can see
that he's working hard to harvest all
these different fruits but he has a
problem here because there's only one of
him so he can't really do more work so
what he needs to do then is hire two
more people to work with him with this
harvesting is done simultaneously so
instead of him trying to harvest all
this different fruit he now has two more
people in there who are putting their
food away and harvesting a forum now the
storage room becomes a bottleneck to to
store and access all the fruits in a
single storage area so they can't fit
all the fruit in one place so Jack
decides to distribute the storage area
and give each one of them a separate
storage and you can look at this
computer terms we have our people that
are the processors we have our fruit
that's a data and you can see it's
storing it in the different storage
rooms so you can see me popping up there
getting my um hello I want fruit basket
of three grapes two apples and three
oranges I'm getting ready for a
breakfast with family a little large
family my family's not that large to
complete the order on time all of them
work parallely with their own storage
space so here we have a process of
retrieving or querying the data and you
can see from the one storage space he
pulls out three grapes she pulls out two
apples and then another storage room he
pulls out three oranges and we complete
a nice fruit basket and this solution
helps them to complete the order on time
without any hassle all of them are happy
they're prepared for an increase in
demand in the future so they now have
this growth system where you can just
keep hiring on new people they can
continue to grow and develop a very
large farm so how does this story relate
to Big Data and I hinted at that a
little bit earlier the limited data only
one processor one storage unit was
needed I remember back in the 90s they
would just upgrade the computer instead
of having a small computer you would
then spend money for a huge Mainframe
with all the flashing lights on it then
the Craig computers were really massive
nowadays A lot of the computers it sits
on our desktop are powerful as the
mainframes they had back then so it's
pretty amazing how time has changed but
used to be able to do everything on one
computer and you had structured data and
a database you stored your structured
data in so most of the time you were
querying databases SQL queries just
think of it as a giant spreadsheet with
rows and columns where everything has a
very specific size and fits neatly in
that rows and columns and back in the
90s this was a nice setup you just
upgraded your computer you would get
yourself a nice big sun computer or
Mainframe if you had a lot of data and a
lot of stuff going on and it was very
easy to do soon though the data
generation increased leading to high
volume of data along with different data
formats and so you can imagine in
today's world this year we will generate
more data than all the previous years
summed together we will generate more
data just this year than all the
previous years sum together and that's
the way it's been going for some time
and you can see we have a variety of
data we have our structured data which
is what we're you think about a database
with rows and columns and easy to look
at nice spreadsheet we have our
semi-structured data they have emails as
an example here that would be one
example your XML your HTML web pages and
we have unstructured data if you ever
look through your folder on photos I
have photos are taken on my phone with
high quality I've got photos from a long
time ago I got web photos low quality so
just in my pictures alone none of them
are the same you know there certainly
are groups of them that are but overall
there are a lot of variety in size and
setup so a single processor was not
enough to process such high volume of
different kinds of data as it was very
time consuming you can imagine that if
you are Twitter with millions of Twitter
feeds you're not going to be able to do
a query across one server there's just
no way that's going to happen unless
people don't mind waiting a year to get
the history of their tweets or look
something up hence we start doing
multiple processors so they're used to
process high volume of data and this
saved time so we're moving forward we've
got multiple processors the single
storage unit became the bottleneck due
to which network overhead was generated
so now you have your network coming in
and each one of these servers has to
wait before it can grab the data from
the single stored unit maybe you have a
SQL Server there with a nice setup or a
file system going the solution was to
use distributed storage for each
processor this enabled easy access to
store origin access to data so this
makes a lot of sense you have multiple
workers multiple storage units just like
we had our storage room and the
different fruit coming in your variety
you can see that nice parallel to
working on a farm now we're dealing with
a lot of data it's all about the data
now this method worked and there were no
network overhead generated you're not
getting a bottleneck somewhere where
people are just waiting for data being
pulled or being processed this is known
as parallel processing with distributed
storage so parallel processing
distributed storage and you can see here
the parallel processing is your
different computers running the
processes and distributed storage here
is a quick demo on setting up Cloudera
quick start VM in case you are
interested in working on a standalone
cluster you can download the Cloudera
quick start VM so you can just type in
download cloud data quick start VM and
you can search for package now this can
be used to set up a quick start VM which
single node cloud data based cluster so
you so you can click on this link and
then basically based on the platform
which you would be choosing to install
such as using a VM box or which version
of Cloudera you would install so here I
can select a platform so I can choose
virtual box and then you can click on
get it now so give your details and
basically then it should allow you to
download the quick start we
would look something like this and once
you have the zip file which is
downloaded you can unzip it which can
then be used to set up a single node
cloud
so once you have downloaded the zip file
that would look something like this so
you would have a quick start virtual box
and then a virtual boss disk now this
can be used to set up a cluster ignore
these files which are related to Amazon
machines and we you don't
this can be used to
use to Cluster so for this to be set up
you can click on file import Appliance
and here you can choose your quick start
VM by looking into downloads quick start
VM select this and click on open now you
can click on next and that shows you the
specifications of CPU Ram which we can
then change later and click on import
this will start importing virtual disk
image dot vmdk file into your VM box
once this is done we will have to change
the specifications or machines to use
two CPU cores minimum and give a little
more RAM because Cloudera quick start VM
is very CPU intensive and it needs good
amount of ram so to survive I will give
two CPU cores and 5gb RAM and that
should be enough for us to bring up a
quick start VM which gives us a Cloudera
distribution of Hadoop in a single node
cluster setup which can be used for
working learning about different
distributions in Cloudera cluster
working with sdfs and other Hadoop
ecosystem components let's just wait for
this importing to finish and then we
will go ahead and set up a quick start
VM for our practice here the importing
of Appliance is done and we see Cloudera
quick start machine is added to my list
of machines I can click on this and
click on settings as mentioned I would
like to give it more RAM and more CPU
cores so click on system and here let's
increase the ram to at least five and
click on processor and let's give it two
CPU cores which would at least be better
than using one CPU core Network it goes
for Nat and that's fine click on OK and
we would want to start this machine so
that it uses two CPU cores 5gb RAM and
it should bring up my Cloudera quick
start VM now let's go ahead and start
this machine which has our quick start
VM it might take initially some time to
start up because internally there will
be various Cloudera Services which will
be starting up and those Services need
to be up for our Cloudera quick start VM
to be accessible so unlike your Apache
Hadoop cluster where we start our
cluster and we will be starting all our
processes in case of cloud era it is
your Cloudera SCM server and agents
which take care of starting off of your
services and starting up of your
different roles for those Services I
explained in my previous session that
for a Cloudera cluster it would be these
Services let me show you that so in case
of Apache cluster we start our services
that is we start our cluster by running
script and then basically those scripts
will individually start the different
processes on different nodes in case of
cloud era we would always have a
Cloudera CM server which would be
running on one machine and then
including that machine we would have
cloud and ICM agents which would be
running on multiple machines similarly
if we had a hortonworks cluster we would
have ambari server starting up on the
first machine and then ambari agents
running on other machines so pure server
component knows what are the services
which are set up what are their
configurations and agents running on
every node are responsible to send
heartbeats to the server receive
instructions and then take care of
starting and stopping off of individual
roles on different machines in case of
our single node cluster setup in quick
start VM we would just have one SCM
server and one ICM agent which will
which will then take care
all the role which need to be started
for your different services so we will
just wait for our machine to come up and
basically have clouded SCM server and
agent running and once we have that we
need to follow few steps so that we can
have the Cloudera admin console
accessible which allows you to browse
the cluster look at different Services
look at the roles for different services
and also work with your cluster either
using command line or using the web
interface that is Hue now that my
machine has come up and it already is
accessible at this point of time you can
click on Terminal and check if you have
access to the cluster so here type in
hostname and that shows you your host
name which is quickstart.cloud we can
also type in hdfs command to see if we
have access and if my cluster is working
these commands are same as you would
give them in a Apache Hadoop cluster or
in any other distribution of a loop
sometimes when your cluster is up and
you have access to the terminal it might
take few seconds or few minutes before
there is a connection established
between cloud and ICM server and cloud
and ICM agent running in the background
which takes care of your cluster I have
given hdfs DFS list command which
basically should show me what by default
exists on my sdfs let's just give it a
couple of seconds before it shows us the
output we can also check by giving a
service Cloud error SCM server status
and here it tells me that if you would
want to use cloud era Express free run
this command it needs 8GB of RAM and it
gives two virtual CPU cores and it also
mentions it may take several minutes
before Cloudera manager has started I
can log in as root here and then give
the command service Cloud error SCM
server status remember the password for
root is clouded out so it basically says
that if you would want to check the
settings it is good to have Express
Edition running so we can close this my
sdfs access is working fine let's close
the terminal and here we have launch
Cloudera Express click on this and that
will give you that you need to give a
command which is Force let's copy this
command let's open a different terminal
and let's give this command like this
which will then go ahead and shut down
your Cloudera Based Services and then it
will restart it only after which you
will be able to access your admin
console so let's just give it a couple
of minutes before it does this and then
we will have access to our admin console
here if you see it is starting the
Cloudera manager server again it is
waiting for Cloudera manager API then
starting the cloud data manager agents
and then configuring the deployment as
per the new settings which we have given
as to use the express edition of
Cloudera once all this is done it will
say the cluster has been restarted and
the admin console can be accessed by ID
and password as clouded up we'll give it
a couple of more minutes and once this
is done we are ready to use our admin
console now that deployment has been
configured client configurations have
also been deployed and it has restarted
the Cloudera Management Service a it
gives you an access to Quick Start admin
console using username and password as
Cloud error let's try accessing it so we
can open up the browser here and let's
change this to 7180 that's the default
port and that shows the admin console
which is coming up now here we can log
in as Cloud era Cloud error and then
let's click on login now as I said
Cloudera is very CPU intensive and
memory intensive so it would slow down
since we have not given enough G Ram to
our Cloud error cluster and thus it will
be advisable to stop or even remove the
services which we don't need now as of
now if we look at the services all of
them look in a stop status and that's
good in one way because we can then go
ahead and remove the services which we
will not use in the beginning and later
we can anytime add services to the
cluster so for example I can click on
key value store here and then I can
scroll down where it says delete to
remove this service from the admin
console now anytime you are removing a
particular service it will only remove
the service from the management by Cloud
attack manager all the role groups under
this service will be removed from host
templates so we can click on delete now
if this service was depending on some
other service it would have prompted me
with a message that remove the relevant
services on which this particular
service depends if the service was
already running then it would have given
me a message that the service has to be
stopped before it can be deleted from
the Cloudera admin now this is my admin
console which allows you to click on
Services look at the different roles and
processes which are running for this
service we anyways have access to our
Cloud era cluster from the terminal
using our regular sdfs or yarn or mapred
commands now I removed a service I will
also remove solar which we will not be
using for the beginning but then it
depends on your choice so we can here
scroll down to delete it and that says
that before deleting the solar service
you must remove the dependencies on this
service from the configuration of
following services that is Hue now Hue
is a web interface which allows you to
work with your sdfs and that is
depending on this so click on configure
serves dependency and here we can make
sure that our Hue service does not
depend on a particular service we are
removing so that then we can have a
clean removal of the service so I'll
click on none and I will say save
changes once this is done then we can go
ahead and try removing the solar service
from our admin console which will reduce
some load on my Management console which
will also allow me to work faster on my
cluster now here we have removed the
dependency of hue on solar so we can
click on this and then we can delete
remember I am only doing this so that my
cluster becomes little lighter and I can
work on my focus services at any point
of time if you want to add more services
to your cluster you can anytime do that
you can fix different configuration
issues like what we see here with
different warning messages and and here
we have these Services which are already
existing now if we don't need any of the
service I can click on the drop down and
click on delete again this says that
scope 2 also has relevance to Hue so Hue
as a web interface also depends on scope
2. as of now we'll make it none at any
point of time later you can add the
services by clicking the add service
option now this is a cluster to which
you have admin access and this is a
quick start VM which gives you a single
node Cloudera cluster which you can use
for Learning and practicing so here
we'll click on scoop 2 and then we will
say delete as we have configured the
dependency now and we will remove scoop
2 also from the list of services which
your admin console is managing right so
once this is done we have removed three
services which we did not need we can
even remove scoop as a client and if we
need we can add that later now there are
various other alerts which your cloud in
our admin console shows and we can
always fix them by clicking on the
health issues or configuration issues we
can click here and see what is the
health issue it is pointing to if that
is a critical one or if that can be
ignored so it says there is an issue
with a clock offset which basically
relates to an ntp service network time
protocol which makes sure that one or
multiple machines are in the same time
zone and are in sync so for now we can
click on suppress and we can just say
suppress for all hosts and we can say
look
it later and confirm so now we will not
have that health issue reported that
probably the ntp service and the
machines might not be in sync now that
does not have an impact for our use case
as of now but if we have a Kerberos kind
of setup which is for security then
basically this offset and time zone
becomes important so we can ignore this
message and we are still good to use the
cluster we also have other configuration
issues and you can click on this which
might talk about the Heap size or the
ram which is available for machines it
talks about zookeeper should be in odd
numbers Hue does not have a load
balancer sdfs only has one data node but
all of these issues are not to be
worried upon because this is a single
node cluster
if you want to avoid all of these
warnings you can always click on
suppress and you can avoid and let your
cluster be in all green status but
that's nothing to worry so we can click
on cluster and basically we can look at
the services so we have removed some
Services which we don't intend to use
now I have also suppressed a offset
warning which is not very critical for
my use case and basically I am good to
start the cluster at any point of time
as I said if you would want to add
services this is the actions button
which you can use ad service so we will
just say restart my cluster which will
restart all the services one by one
starting from zookeeper as the first
service to come up we can always click
on this Arrow Mark and see what is
happening in the services what services
are coming up and in which order if you
have any issues you can always click on
the link next to it which will take you
to the logs and we can click on close to
let it happen in the background so this
will basically let my services restart
one by one and my cluster will then
become completely accessible either
using Hue as a web interface or quick
start terminal which allows you to give
your commands now while my machines are
coming up you can click on host and you
can have a look at all the hosts we have
as of now only one which will also tell
you how many rolls or processes are
running on this machine so that is 25
roles it tells you what is the disk
usage it tells you what is the physical
memory being used and using this host
tab we can add new host to the cluster
we can check the configuration we can
check
in Diagnostics you can look at the logs
which will give you access to all the
logs you can even select the sources
from which you would want to have the
logs or you can give the hostname you
can click on
you can build your own charts you can
also do the admin stuff by adding
different users or enabling security
using the administration tab so since we
have clicked on restart of a cluster we
will slowly start seeing all the
services one by one coming up starting
with zookeeper to begin with and once we
have our cluster up and running whether
that is showing all services in green or
in a different status we still should be
able to access the service now as we saw
in Apache Hadoop cluster even here we
can click on hdfs and we can access the
web UI once our hdfs service is up by
clicking on quick links so the service
is not yet up once it is up we should be
able to see the web UI link which will
allow you to check things from sdfs web
interface similarly yarn as a service
also has a web interface so as soon as
the service comes up under your quick
links we will have access to the yarn UI
and similarly once a service comes up we
will have access to Hue which will give
you the web interface which allows you
to work with your stfs which allows you
to work with your different other
components within the cluster without
even using the command line tools or
command line options so we will have to
give it some time while the cloud era
SCM agent on every machine will
restart the roles which are responsible
for your cluster to come up we can
always click here which tells that there
are some running commands in the
background which are trying to start my
and go to the terminal and we can switch
ssdfs user remember sdfs user is the
admin user and it does not have a
password unless you have set one so you
can just log in as hdfs which might ask
you for password initially which we do
not have so the best way to do this is
by logging in as root where the password
is cloud error and then you can log in
as hdfs so that then onwards you can
give your sdfs commands to work with
your file system now since my services
are coming up right now when I try to
give a sdfs DFS command it might not
work or it might also say that it is
trying to connect to the name node which
is not up yet so we will have to give it
some time and only once the name node is
up we will be able to access our sdfs
using commands so this is how you can
quickly set up your quick start and then
you can be working using the command
line options from the terminal like what
you would do in Apache Ado cluster you
could use the web interfaces which allow
you to work with your cluster now if
this usually takes more time so you will
have to give it some time before your
services are up and running and for any
reason if you have issues it might
require you to restart your cluster
several times in the beginning before it
gets accustomed to the settings what you
have given and it starts up the services
at any point of time if you have any
error message then you can always go
back and look in logs and see what is
happening and try starting your cluster
so this is how we set up a quick start
VM and you can be using this to work
with your Cloud error cluster we'll
start with the big data challenges and
the first thing with the big data is you
can see here we have a nice chaotic
image with all these different inputs
server racks all over the place graphs
being generated just about everything
you can imagine and so the problems that
come up with big data is one storing it
how do you store this massive amount of
data and we're not talking about a
terabyte or 10 terabytes we're talking a
minimum of 10 terabytes up to petabytes
of data and then the next question is
processing and so the two go hand in
hand because when you're storing the
data that might take up a huge amount of
space or you might have a small amount
of data that takes a lot of processing
and so either one will drive a series of
data or processing into the Big Data
Arena so with storing data storing Big
Data was a problem due to its massive
volume just straight up people would
have huge backup tapes and then you'd
have to go through the backup tapes for
hours to go find your data and a simple
query could take days and then
processing processing Big Data consumed
more time and so the Hadoop came up with
a cheap way to process the data it used
to be like I've had some processes that
if I ran on my computer without trying
to use multiple cores and multiple
threads would take years to process just
a simple data analysis can get that
heavy in the data processing and so
processing can be as big of a problem as
the size of the data itself so Hadoop as
a solution this is a solution to Big
Data and big data storage storing Big
Data was a problem due to its massive
volume so we take the Hadoop file system
or the hdfs and now we're able to store
huge data across a large number of
machines and access it like this one
file system and processing Big Data
consumed more time we talked about some
processes you can't even do on your
computer so would take years now the
Hadoop with the map reduce processing
Big Data was faster and I'm going to add
a little notation right here that's
really important to note that Hadoop is
the beginning when we talk about data
processing they've added new processes
on top of the map reduce that even
accelerate it your spark set up and some
other different functionalities but
really the basis of all of it where it
all starts with the most basic concept
is your Hadoop mapreduce let us now look
into the hdfs in detail in the
traditional approach all the data was
stored in a single Central database with
the rise of Big Data a single database
was not enough for storage and I
remember the old Sun computers or the
huge IBM machines with all the flashing
lights now all the data on one of those
can be stored on your phone it's almost
a bit of humor how much this has
accelerated over the years the same
thing with our rise of Big Data no
longer can you just store it on one
machine no longer can you go out and buy
a sun computer and put it on that one
Sun computer no more can you buy a Craig
machine or an Enterprise IBM server it's
not going to work you're not going to
fit it all into one server the solution
was to use distributed approach to store
the massive amount of data data was
divided and distributed amongst many
individual databases and you can see
here where we have three different
databases going on so you might actually
saw this in one where they divided up
the user accounts a through G so on by
the letter and so the first query would
say what's the first letter of this
whatever ID it was and then it would go
into that database to find it so it had
a database telling it which database to
look for stuff that was a long time ago
and to be honest it didn't work really
well nowadays so that was a distributed
database you'd have to track which
database you put it in hello Learners
simply brings you a postgraduate program
in data engineering developed in
partnership with Purdue University and
IBM to learn more about this course you
can find the course Link in the
description box below what is hdfs
Hadoop distributed file system hdfs is
specially designed file system for
storing huge data sets in commodity
hardware and commodity is an interesting
term because I mentioned Enterprise
versus commodity and I'll touch back
upon that it has two core components
name node and data node name node is the
master Daemon there's only one active
name node it manages the data nodes and
stores all the metadata so it stores all
the mapping of where the data is on your
Hadoop file system now the name node is
usually an Enterprise machine you spend
a lot of extra money on it so you have a
very solid name known machine and so
then we have our data nodes our data
node data node data node we have three
here the data node is the slave there
can be multiple data nodes and it stores
the actual data and this is where your
commodity Hardware comes in the best
definition I've heard of commodity
Hardware is the cheap knockoffs this is
where you buy you can buy 10 of these
and you expect one of them not to work
because you know when they come in
they're going to break right away so
you're looking at Hardware that's not as
high end so where you might have your
main Master node is your Enterprise
server then your data nodes are just as
cheap as you can get them with all the
different features you need on them as
said earlier name node stores the
metadata metadata gives information
regarding the file location block size
and so on so our metadata in the hdfs is
maintained by using two files it has the
edit log and the fs image the edit log
keeps track of the recent changes made
on the Hadoop file system only recent
changes are tracked here the fs image
keeps track of every change made on the
hdfs since the beginning now what
happens when the edit log file size
increases the name node fails these are
big ones so what happens we are at a log
it just keeps so big until it's too big
or our main Enterprise computer that we
spent all that money on so it would
break actually fails because they still
fail the solution we make copies of the
edit log and the fs image files so
that's pretty straightforward you just
copy them over so you have both the most
recent edits going on in the long-term
image of your file system and then we
also create a secondary name node
there's a node that maintains the copies
of the edit log and the fs image it
combines them both to get an updated
version of the fs image now the
secondary name node only came in the
last oh I guess two three years where it
became as part of the main system and
usually your secondary name node is also
an Enterprise computer and you'll put
them on separate racks so if you have
three racks of computers you would have
maybe the first two racks would have the
name node and the second rack would have
the secondary name node and the reason
you put them on different racks is you
can have a whole rack go down you can
have somebody literally trip over the
power cable or the switch that goes
between the racks is most common goes
down well if the switch goes down you
can usually switch to the secondary name
node and while you're getting your
switch replaced and replacing that
Hardware because of the way the hdfs
works it still is completely functional
so let's take a look at the name node we
have our edit log our FS image and we
have our secondary name node and you
take that it copies the edit log over
and it copies the fs image and you can
see right here you have all the
different contents on your main name
node now also on your secondary name
node and then your secondary name node
will actually take these to your edit
log and your FS image and it will make a
copy so you have a full FS image that
contains its current it's up to date the
secondary nade node creates a periodic
checkpoint of the files and then it
updates the new FS image into the name
node now it used to be this all occurred
on the name node before you had a
secondary name node so now you can use
your secondary node to both back up
everything going on the name node and it
does that lifting in the back where
you're combining your edit log and
bringing your FS image so it's current
and then you end up with a new edit log
and a new you have your FS image updated
and you start a fresh edit log this
process of updating happens every hour
and that's how it's scheduled you can
actually change those schedules but that
is the standard is to update every hour
here we will learn on Apache Spark
history of spark what is spark Hadoop
which is a framework again West spark
components of Apache spark that is spark
core spark SQL spark streaming spark ml
lib and Graphics then we will learn on
spark architecture applications of spark
spark use cases so let's begin with
understanding about history of Apache
spark it all started in 2009 as a
project at UC Berkeley amp Labs by
matissah area in 2010 it was open source
under a BST license in 2013 spark became
an Apache top level project and in 2014
used by data bricks to sort large-scale
data sets and it set in New World Record
so that's how Apache spark started and
today it is one of the most in-demand
processing framework or I would say in
memory Computing framework which is used
across the Big Data industry so what is
Apache spark let's learn about this
Apache spark is a open source in memory
Computing framework or you could say
data processing engine which is used to
process data in batch and also in real
time across various cluster computers
and it has a very simple programming
language behind the scenes that is Scala
which is used although if users would
want to work on spark they can work with
python they can work with Scala they can
work with Java and so on even R for that
matter so it supports all these
programming languages and that's one of
the reasons that it is called polyglot
wherein you have good set of libraries
and support from all the programming
languages and developers and data
scientists incorporate spark into their
applications or build spark based
applications to process analyze query
and transform data at a very large scale
so these are key features of Apache
spark now if you compare Hadoop Wes
spark we know that Hadoop is a framework
and it basically has mapreduce which
comes with Hadoop for processing data
however processing data using mapreduce
in Hadoop is quite slow because it is a
batch oriented operation and it is time
consuming if you if you talk about spark
spark can process the same data 100
times faster than mapreduce as it is a
in-memory Computing framework well there
can always be conflicting ideas saying
what if my spark application is not
really efficiently coded and my
mapreduce application has been very
efficiently coded well then it's a
different case however normally if you
talk about code which is efficiently
written format reduce or for spark based
processing spark will win the battle by
doing almost 100 times faster than
mapreduce so as I mentioned Hadoop
performs batch processing and that is
one of the paradigms of mapreduce
programming model which involves mapping
and reducing and that's quite rigid so
it performs batch processing the
intermittent data is written to sdfs and
written right back from sdfs and that
makes hadoops mapreduce processing
slower in case of spark it can perform
both batch and real-time processing
however a lot of use cases are based on
real-time processing take an example of
Macy's take an example of retail giant
such as Walmart and there are many use
cases who would prefer to do real time
processing or I would say near real time
processing so when we say real time or
near real time it is about processing
the data as it comes in or you're
talking about streaming kind of data now
Hadoop or hadoop's map reduce obviously
was started to be written in Java now
now you could also write it in Scala or
in Python however if you talk about
mapreduce it will have more lines of
code since it is written in Java and it
will take more times to execute you have
to manage the dependencies you have to
do the right declarations you have to
create your mapper and reducer and
Driver classes however if you compare
spark it has few lines of code as it is
implemented in Scala and Scala is a
statically typed dynamically inferred
language it's very very concise and the
benefit is it has features from both
functional programming and object
oriented language and in case of Scala
whatever code is written that is
converted into byte codes and then it
runs in the jvm now Hadoop supports
Kerberos authentication there are
different kind of authentication
mechanisms Kerberos is one of the
well-known ones and it can really get
difficult to manage now spark supports
authentication via a shared it's secret
it can also run on yarn leveraging the
capability of calculus companies use big
data to draw meaningful insights and
take business decisions and Big Data
Engineers are the people who can make
sense out of this enormous amount of
data now let's find out who is a big
data engineer a big data engineer is a
professional Who develops maintains
tests and evaluates the company's Big
Data infrastructure in other words
develop Big Data Solutions based on a
company's requirements they maintain
these solutions they test out these
Solutions as to the company's
requirements they integrate this
solution with the various tools and
systems of the organization and finally
they evaluate how well the solution is
working to fulfill the company's
requirements next up let's have a look
at the responsibilities of a big data
engineer now they need to be able to
design Implement verify and maintain
software systems now for the process of
ingesting data as well as processing it
they need to be able to build highly
scalable as well as robust systems they
need to be able to extract data from one
database transform it as well as load it
into another data store with the process
of ETL or the extract transform load
process and they need to research as
well as propose new ways to acquire data
improve the overall data quality and the
efficiency of the system now to ensure
that all the business requirements are
met they need to build a suitable data
architecture they need to be able to
integrate several programming languages
and tools together so that they can
generate a structured solution they need
to build models that reduce the overall
complexity and increase the efficiency
of the whole system by mining data from
various sources and finally they need to
work well with other teams ones that
include data Architects data analysts
and data scientists now all of you might
be aware that Hadoop or Apache Hadoop is
the core distribution of Hadoop and then
you have different vendors in the market
which have packaged the Apache Hadoop in
a cluster management solution which
allows everyone to easily deploy manage
monitor upgrade your clusters so here
are some vendor specific distributions
we have Cloudera which is the dominant
one in the market we have hortonworks
and now you might be aware that clouder
and hortonworks have merged so it has
become a bigger entity you have mapper
you have Microsoft is your IBM's
infosphere and Amazon web services so
these are some popularly known
vendor-specific distributions if you
would want to know more about the Hadoop
distributions you should basically look
into Google and you should check for
Hadoop different distributions Wiki page
so if I type Hadoop different
distributions and then I check for the
Wiki page that will take me to the
distributions and Commercial support
page and this basically says that the
sold products that can be called a
release of Apache Hadoop come from
apache.org so that's your open source
community and then you have various
vendors specific distributions which
basically are running in one or the
other way Apache Hadoop but they have
packaged it as a solution like a
installer so that you can easily set up
clusters on set of machines so have a
look at this page and read through about
different distributions of Hadoop coming
back let's look at our next question so
what are the different Hadoop
configuration files now whether you're
talking about Apache Hadoop Cloudera
hortonworks map r or no matter which
other distribution these config files
are the most important and existing in
every distribution of Hadoop so you have
Hadoop environment.sh wherein you will
have environment variables such as your
Java path what would be your process ID
path where will your logs get stored
what kind of metrics will be collected
and so on your core hyphen site file has
the hdfs path now this has many other
properties like enabling trash or
enabling High availability or discussing
or mentioning about your zookeeper but
this is one of the most important file
you have hdfs hyphen site file now this
file will have other information related
to your Hadoop cluster such as your
replication Factor where will name node
store its metadata on disk if a data
node is running where would data node
store its data if a secondary name node
is running where would that store a copy
of name nodes metadata and so on your
mapred hyphen site file is a file which
will have properties related to your map
reduce processing you also have Masters
and slaves now these might be deprecated
in a vendor-specific distribution and in
fact you would have a yarn hyphen site
file which is based on the yarn
processing framework which was
introduced in Hadoop version 2 and this
would have all your resource allocation
and resource manager and node manager
related property again if you would want
to look at default properties for any
one of these for example let's say hdfs
hyphen site file I could just go to
Google and type in one of the properties
for example I would say DFS dot name
node.name dot directory and as I know
this property belongs to hdfs hyphen
site file and if you search for this it
will take you to the first link which
says stfs default XML you can click on
this and this will show you all the
properties which can be given in your
stfs hyphen site file it also shows you
which version you are looking at and you
can always change the version here so
for example if I would want to look at
2.6.5 I just need to change the version
and that should show me the properties
similarly you can just give a property
which belongs to say core hyphen site
file for example I would say FS dot
default fs and that's a property which
is in core hyphen sci-fi and somewhere
here you would see core minus default
dot XML and this will show you all the
properties so similarly you could search
for properties which are related to yarn
hyphen site file or mapred hyphen site
file so I could say yarn dot resource
manager and I could look at one of these
properties which will directly take me
to yarn default XML and I can see all
the properties which can be given in
yarn and similarly you could say map
reduce dot job dot reduces and I know
this property belongs to mapreduce
hyphen site file and this takes you to
the default XML so these are important
config files and no matter which
distribution of Hadoop you are working
on you should be knowing about these
config files whether you work as a
Hadoop admin or you work as a Hadoop
developer knowing these config
properties would be very important and
that would also showcase your internal
knowledge about the configs which drive
your Hadoop cluster let's look at the
next question so what are the three
modes in which Hadoop can run so you can
have Hadoop running in a standalone mode
now that's your default mode it would
basically use a local file system and a
single Java process so when you say
Standalone mode it is as you downloading
Hadoop related package on one single
machine but you would not have any
process running that would just be to
test Hadoop functionalities you could
have a pseudo distributed mode which
basically means it's a single node
Hadoop deployment now Hadoop as a
framework has many many services so it
has a lot of services and those Services
would be running irrespective of your
distribution and each service would then
have multiple processes so your pseudo
distributed mode is a mode of cluster
where you would have all the important
processes belonging to one or multiple
Services running on a single node if you
would want to work on a pseudo
distributed mode and using a Cloudera
you can always go to Google and search
for cloudera's quick start VM you can
download it by just saying Cloud era
quick start VM and you can search for
this and that will allow you to download
a quick start VM follow the instructions
and you can have a single node Cloudera
cluster running on your virtual machines
for more information you can refer to
the YouTube tutorial where I have
explained about how to set up a quick
start VM coming back you could have
finally a production setup or a fully
distributed mode which basically means
that your Hadoop framework and its
components would be spread across
multiple machines so you would have
multiple services such as hdfs yarn
Flume scope Kafka hbase Hive Impala and
for these Services there would be one or
multiple processes distributed it across
multiple nodes so this is normally what
is used in production environment so you
could say Standalone would be good for
testing sudo distributed could be good
for testing and development and fully
distributed would be mainly for your
production setup now what are the
differences between regular file system
and hdf
So when you say regular file system you
could be talking about a Linux file
system or you could be talking about a
Windows based operating system so in
regular file system we would have data
maintained in a single system so the
single system is where you have all your
files and directories so it is having
low fault tolerance right so if the
machine crashes your data recovery would
be very difficult unless and until you
have a backup of that data that also
affects your processing so if the
machine crashes or if the machine fails
then your processing would be blocked
now the biggest challenge with regular
file system is the seek time the time
taken to read the data so you might have
one single machine with huge amount of
disks and huge amount of ram but then
the time taken to read that data when
all the data is stored in one machine
would be very high and that would be
with least fault tolerance if you talk
about sdfs your data is distributed so
sdfs stands for Hadoop distributed file
system so here your data is distributed
and maintained on multiple systems so it
is never one single machine it is also
supporting reliability so whatever is
stored in sdfs say a file being stored
depending on its size is split into
blocks and those blocks will be spread
across multiple nodes not only that
every block which is stored on a node
will have its replicas stored on other
nodes replication Factor depends but
this makes sdfs more reliable in cases
of your slave nodes or data nodes
crashing you will rarely have data loss
because of Auto replication feature now
time taken to read the data is
comparatively more as you might have
situations where your data is
distributed across the nodes and even if
you are doing a parallel read your data
read might take more time because it
needs coordination for multiple machines
however if you are working with huge
data which is getting stored it will
still be beneficial in comparison to
reading from a single machine so you
should always think about its
reliability through Auto replication
feature its fault tolerance because of
your data getting stored across multiple
machines and its capability to scale so
when you talk about sdfs we are talking
about horizontal scalability or scaling
out when you talk about regular file
system you are talking about vertical
scalability which is scaling up now
let's look at some specific sdfs
questions what is this why is sdfs Fault
tolerant now as I just explained in
previous slides your sdfs is Fault
tolerant as it replicates data on
different data nodes so you have a
master node and you have multiple slave
nodes or data nodes where actually the
data is getting stored now also have a
default block size of 128 MB that's the
minimum since Hadoop version 2. so any
file which is up to 128 MB would be
using one logical block and if the file
size is bigger than 128 MB then it will
be split into blocks and those blocks
will be stored across multiple machines
now since these blocks are stored across
multiple machines it makes it more fault
tolerant because even if your machines
fail you would still have a copy of your
block existing on some other machine now
there are two aspects here one we talk
about the first rule of replication
which basically means you will never
have two identical blocks sitting on the
same machine and the second rule of
replication is in terms of rack
awareness so if your machines are placed
in racks as we see in the right image
you will never have all the replicas
placed on the same rack even if they are
on different machines so it has to be
fault tolerant and it has to maintain
redundancy so at least one replica will
be placed on some other node on some
other rack that's how as DFS is Fault
tolerant now here let's understand the
architecture of sdfs now as I mentioned
earlier you would in a Hadoop cluster
the main service is your hdfs so for
your sdfs service you would have a name
node which is your master process
running on one of the machines and you
would have data nodes which are your
slave machines getting stored across
marketing or the processes running
across multiple machines
processes has an important role to play
when you talk about sdfs whatever data
is written to hdfs that data is split
into blocks depending on its size and
the blocks are randomly distributed
across nodes with auto replication
feature these blocks are also Auto
replicated across multiple machines with
the first condition that no two
identical blocks will sit on the same
machine now as soon as the cluster comes
up your data modes which are part of the
cluster and based on config files would
start sending their heartbeat to the
this would be every
tickets what does name node do with that
name node will store this information in
its Ram so name node starts building a
metadata in its lab and that metadata
has information of what are the data
nodes which are available in the
beginning now when a data writing
activity starts and the blocks are
distributed across data nodes data nodes
every 10 seconds will also send a block
report to name node so name node is
again adding up this information in its
Ram or the metadata in Ram which earlier
had only data node information now name
node will also have information about
what are the files the files are split
in which blocks the blocks are stored on
which machines and what are the file
permissions now while name node is
maintaining this metadata in Ram name
node is also maintaining metadata in
disk so that is what we see in the red
box which basically has information of
whatever information was written to hdf
so to summarize your name node has
metadata in Ram and metadata in disk you
or data is actually getting stored and
then there is a auto replication feature
which is always existing unless and
until you have disabled it and your read
and write activity is a parallel
activity however replication is in
sequential activity now this is what I
mentioned about when you talk about name
node which is the master process hosting
metadata in disk and RAM so when we talk
about disk it basically has a edit log
which is your transaction log and your
FS image which is your file system image
right from the time the cluster was
started this metadata in disk was
existing and this gets appended every
time read write or any other operations
happen on stfs metadata in Ram is
dynamically built every time the cluster
comes up which basically means that if
your cluster is coming up name node in
the initial few seconds or few minutes
would be in a safe mode which basically
means it is busy registering the
information from data nodes so name node
is one of the most critical processes if
name node is down and if all other
processes are running you will not be
able to access the cluster name nodes
metadata in disk is very important for
name node to come up and maintain the
cluster name nodes metadata in Ram is
basically for all or satisfying all your
client requests now when we look at data
nodes as I mentioned data nodes hold the
actual data blocks and they are sending
these block reports every 10 seconds so
the metadata in name nodes Ram is
constantly getting updated and metadata
in disk is also constantly getting
updated based on any kind of write
activity happening on the cluster now
data node which is storing the block
will also help in any kind of read
activity whenever a client requests so
whenever a client or an application or
an API would want to read the data it
would first talk to name node name node
would look into its metadata on
rack and confirm to the client which
machines could be reached to get to that
data data that's where your client would
try to read the data from sdfs which is
actually getting the data from data
nodes and that's how your read write
requests are satisfied now what are the
two types of metadata in name node
server holds as I mentioned earlier
metadata in disk very important to
remember edit log NFS image metadata in
Ram which is information about your data
nodes files files being split into
blocks blocks residing on data nodes and
file permissions so I will share a very
good link on this and you can always
look for more detailed information about
your metadata so you can search for sdfs
metadata directories explained now this
is from hortonworks however it talks
about the metadata in disk which name
node manages and details about this so
have a look at this link if you are more
interested in learning about metadata on
disk coming back let's look at the next
question what is the difference between
Federation and high availability now
these are the features which were
introduced in Hadoop version 2. both of
these features are about horizontal
scalability of name node so prior to
version 2 the only possibility was that
you could have one single Master which
basically me means that your cluster
could become unavailable if name node
would crash so Hadoop version 2
introduced two new features Federation
and high availability however High
availability is a popular one so when
you talk about Federation it basically
means any number of name nodes so there
is no limitation to the number of name
nodes your name nodes are in a Federated
cluster which basically means name nodes
still belong to the same cluster but
they are not coordinating with each
other so whenever a write request comes
in one of the name node picks up that
request and it guides that request for
the blocks to be written on data nodes
but for this your name node does not
have to coordinate with other name node
to find out if the block ID which was
being assigned was the same one as
assigned by other name node so all of
them belong to a Federated cluster they
are linked via a cluster ID so whenever
an application or an API is trying to
talk to Cluster it is always going via
an cluster ID and one of the name node
would pick up the read activity or write
activity or processing activity so all
the name nodes are sharing a pool of
metadata in which each name node will
have its own dedicated pool and we can
remember that by a term called namespace
or name service so this also provides
High fault tolerance suppose your one
name node goes down it will not affect
or make your cluster unavailable you
will still have your cluster reachable
because there are other name nodes
running and they are available now when
it comes to heartbeats all your data
nodes are sending their heartbeats to
all the name nodes and all the name
nodes are aware of all the data nodes
and you talk about high availability
this is where you would only have two
name nodes so you would have an active
and you would have a standby now
normally in any environment you would
see a high availability setup with
zookeeper so zookeeper is a centralized
coordination service so when you talk
about your active and stand by name
notes election of a name node to made as
active and taking care of a automatic
failover is done by your zookeeper High
availability can be set up without
zookeeper but that would mean that a
admins intervention would be required to
make a name known as active from standby
or also to take care of failover
now at any point of time in high
availability a active name node would be
taking care of storing the edits about
whatever updates are happening on sdfs
and it is also writing these edits to a
shared location standby name node is the
one which is constantly looking for
these latest updates and applying to its
metadata which is actually a copy of
whatever your active name node has so in
this way your standby name node is
always in sync with the active name node
and if for any reason active name node
fails your standby name node will take
over and become the active remember
zookeeper plays a very important role
here it's a centralized coordination
service one more thing to remember here
is that in your high availability
secondary name node will not be allowed
so you would have a active name node and
then you will have a standby name node
which will be configured on a separate
machine and both of these will be having
access to a shared location now that
shared location could be NFS or it could
be a quorum of Journal nodes so for more
information refer to the tutorial where
I have explained about sdfs high
availability and Federation now let's
look at some logical question here
have a input file of
of 3MB which is obviously bigger than
128 MB how many input splits would be
created by sdfs and what would be the
size of each input split so for this you
need to remember that by default the
minimum block size is 128 MB now that's
customizable if your environment has
more number of larger files written on
an average then obviously you have to go
for a bigger block size if your
environment has a lot of files being
written but these files are of smaller
size you could be okay with 128 MB
remember in Hadoop every entity that is
your directory on sdfs file on sdfs and
a file having multiple blocks each of
these are considered as objects and for
each object hadoops name notes Ram 150
bytes is utilized so if your block size
is very small then you would have more
number of blocks which would directly
affect the name nodes of M if you keep a
block size very high that will reduce
the number of blocks but remember that
might affect in processing because
processing also depends on splits more
number of splits more the parallel
processing so setting of block size has
to be done with consideration about your
parallelism requirement and your name
node slam which is available now coming
to the question if you have a file of
350 MB that would be split into three
blocks and here two blocks would have
128 MB data and the third block although
the block size would still be 128 it
would have only 94 MB of data so this
would be the split of this particular
file now let's understand about rack
awareness how does rack awareness work
or why do we even have racks so
organizations always would want to place
their nodes or machines in a systematic
way there can be different approaches
you could have a rack which would have
machines running on the master processes
and the intention would be that this
particular rack could have higher
bandwidth more cooling dedicated power
supply top of rack switch and so on
second approach could be that you could
have one master process running on one
machine of every rack and then you could
have other slave processes running now
when you talk about your rack awareness
one thing to understand is that if your
machines are placed within racks and we
are aware that Hadoop follows Auto
replication the rule of replication in a
rack aware cluster would be that you
would never have all the replicas placed
on the same rack so if we look at this
if we have block a in blue color you
will never have all the three blue boxes
in the same rack even if they are on
different nodes because that makes us
that makes it less fault tolerant so you
would have at least one copy of block
which would be stored on a different
track on a different note now let's look
at this so basically here we are talking
about replicas being placed in such a
way now somebody could ask a question
can I have my block and its replicas
spread across three racks and yes you
can do that but then in order to make it
more redundant you are increasing your
bandwidth requirement so the better
approach would be two blocks on the same
rack on different machines and one copy
on a different track hello Learners
simply learn brings you a postgraduate
program in data engineering developed in
partnership with Purdue University and
IBM to learn more about this course you
can find the course Link in the
description box below now let's proceed
how can you restart name node and all
demons in Hadoop so if you were working
on an Apache Hadoop cluster then you
could be doing a start and stop using
Hadoop demon scripts so there are these
Hadoop demon scripts which would be used
to start and stop your Hadoop and this
is when you talk about your Apache
Hadoop so let's look at one particular
file which I would like to show you more
information here and this talks about
your different clusters so let's look
into this and
so let's look at the start and stop and
here I have a file let's look at this
one and this gives you highlights so if
you talk about Apache Hadoop this is how
the setup would be done so you would
have it download the Hadoop tar file you
would have to unturn it edit the config
files you would have to do formatting
and then start your cluster and here I
have said using scripts so this is in
case of Apache Hadoop you could be using
a start all script that internally
triggers start DFS and start yarn and
these scripts start DFS internally would
run Hadoop demon multiple times based on
your configs to start your different
processes then your start yarn would run
yarn demon script to start your
processing related processes so this is
how it happens in Apache Hadoop now in
case of cloud era or hortonworks which
is basically a vendor-specific
distribution you would have say multiple
Services which would have one or
multiple demons running across the
machines let's take an example here that
you would have machine 1 Machine 2 and
machine 3 with your processes spread
across however in case of Cloudera and
hot remarks these are cluster Management
Solutions so you would never be involved
in running a script individually to
start and stop your processes in fact in
case of Cloudera you would have a
Cloudera SCM server running on one of
the machines and then Cloudera SCM
agents running on every machine if you
talk about hortonworks you would have
ambari server and ambari agent running
so your agents which are running on
every machine are responsible to monitor
the processes send also their heartbeat
to the master that is your server and
your server is the one one or a service
which basically will give instructions
to the agents so in case of vendor
specific distribution your start and
stop of processes is automatically taken
care by these underlying services and
these Services internally are still
running these commands however only in
Apache Hadoop you have to manually
follow these to start and stop coming
back we can look into some command
related questions so which command will
help you find the status of blocks and
file system health so you can always go
for a file system check command now that
can show you the files for a particular
sdfs path it can show you the blocks and
it can also give you information on
status such as under replicated blocks
over replicated blocks misreplicated
blocks default replication and so on so
your fsck file system check utility does
not repair if there is any problem with
the blocks but it can give you
information of blocks related to the
files on which machines they have stored
if they are replicated as per the
replication factor or if there is any
problem with any particular replica now
what would happen if you store too many
small files in a cluster and this
relates to the block information which I
gave some time back so remember Hadoop
is coded in Java so here every directory
every file and file Creator block is
considered as an object and for every
object within your Hadoop cluster name
nodes Ram gets utilized so more number
of blocks you have more would be usage
of name node slam and if you're storing
too many small files it would not affect
your disk it would directly affect your
name node slam that's why in production
clusters admin guys or infrastructure
specialist will take care that everyone
who is writing data to hdfs follows a
quota system so that you could be
controlled in the amount of data you
write plus the count of data and
individual writes on hdfs now how do you
copy data from local system onto sdfs so
you can use a put command or a copy from
local and then given your local path
which is your source and then your
destination which is your sdfs path
remember you can always do a copy from
local using a minus F option that's a
flag option and that also helps you in
writing the same file or a new file to
hdfs so with your minus F you have a
chance of overwriting or rewriting the
data which is existing on sdfs so copy
from local or minus put both of them do
the same thing and you can also pass an
argument when you're copying to control
your replication or other aspects of
your file now when do you use DFS admin
refresh notes or RM admin refresh notes
so as the command says this is basically
to do with refreshing the node
information so your refresh notes is
mainly used when say a commissioning or
decommissioning of nodes is done so when
in node is added into the cluster or
when a node is removed from the cluster
you are actually informing Hadoop master
that this particular node would not be
used for storage and would not be used
for processing now in that case you
would be once you are done with the
process of commissioning or
decommissioning you would be giving
these commands that is refresh notes and
other I might win refresh notes so
internally when you talk about
commissioning decommissioning there are
include and exclude files which are
updated and these include and exclude
files will have entry of machines which
are being added to the cluster or
machines which are being removed from
the cluster and while this is being done
the cluster is still running so you do
not have to restart your master process
however you can just use this refresh
commands to take care of your commenting
decommissioning activities now is there
any way to change replication of files
on sdfs after they are already written
and the answer is of course yes so if
you would want to set a replication
Factor at a cluster level and if you
have admin access then you could edit
your sdfs hyphen site file or you could
say Hadoop hyphen site file and that
would take care of replication Factor
being set at a cluster level however if
you would want to change the replication
after the data has been written you
could always use a set rep command so
set rep command is basically to change
the replication after the data is
written you could also write the data
with a different replication and for
that you could use a minus D DFS dot
replication and give your application
Factor when you are writing data to the
cluster so in Hadoop you can let your
data be replicated as per the property
set in the config file you could write
the data with a different replication
you could change the replication after
the data is written so all these options
are available now who takes care of
replication consistency in a Hadoop
cluster and what do you mean by under
over replicated blocks now as I
mentioned your fsck command can give you
information of over or under replicated
blocks now in a cluster it is always and
always name node which takes care of
replication
so for example if you have set up a
replication of three and since we know
the first rule of replication which
basically means that you can not have
two replicas residing on the same node
it would mean that if your application
is 3 you would need at least three data
nodes available now say for example you
had a cluster with 3 notes and
replication was set to 3 at one point of
time one of your name node crashed and
if that happens your blocks would be
under replicated that means there was a
replication Factor set but now your
blocks are not replicated or there are
not enough replicas as per the
replication Factor set this is not a
problem your master process or name node
will wait for some time before it will
start the replication of data given so
if a data road is not responding or if a
disk has crashed and if name node does
not get information
replica name node will wait for some
time and then it will start
re-replication of those missing blocks
from the available nodes however while
name node is doing it the blocks are in
under replicated situation now when you
talk about over replicated this is a
situation where name node realizes that
there are extra copies of block now this
might be the case that you had three
nodes running with the replication of
three one of the node went down due to a
network failure or some other issue
within few minutes name node
re-replicated the data and then the
failed node is back
set of blocks
so it is smarter
instead that this is a over replication
situation and it will delete set of
blocks from one of the nodes it might be
the node which has been recently added
it might be your old node which has
joined your cluster again or any node
that depends on the load on a particular
node now we discussed about Hadoop we
discussed about sdfs now we will discuss
about mapreduce which is the programming
model and you can say processing
framework what is distributed cash in
map
you know that
we processed might be existing on
multiple nodes so when you would have
your mapreduce program running it would
basically read the data from the
underlying disks
now this could be a costly operation if
every time the data has to be read from
disk so distributed cash is a mechanism
wherein data set or data which is coming
from the disk can be cached and
available for all worker nodes now how
will this benefit so when a map reduce
is running instead of every time reading
the data from disk it would pick up the
data from distributed cache and this
this will benefit your map reduce
so distributed Cache can be set in your
job conf where you can specify that a
file should be picked up from
distributed cache now let's understand
about these roles so what is a record
reader what is a combiner what is a
partitioner and what kind of roles do
they play in a map reduce processing
Paradigm or map reduce operation
record reader communicates with the
input split and it basically converts
the data into key value Pairs and these
key value pairs are the ones which will
be worked upon by the mapper your
combiner is an optional phase it's like
mini reduce so combiner does not have
its own class it relies on the reducer
class basically your combiner would
receive the data from your map tasks
which would have completed works on it
based on whatever reducer class mentions
and then passes its output to the
reduced surface partitioner is basically
a phase which decides how many reduced
tasks would be used aggregate or
summarize your data so partitioner is a
phase which would decide based on the
number of keys based on the number of
map tasks your partitioner would decide
if one or multiple reduced task
so either it could be partitioner which
decides on how many reduce tasks would
run or it could be based on the
properties which we have set within the
cluster which will take care of the
number of reduced tasks which would be
used always remember your partitioner
decides how outputs from combiner are
sent to reducer and to how many reducers
it controls the partitioning of keys of
your intermediate map outputs so map
phase whatever output it generates is an
intermediate output and that has to be
taken by your partitioner or by a
combiner and then partitioner to be sent
to one or multiple reduce tasks this is
one of the common questions which you
might face why is mapreduce slower in
processing so we know mapreduce goes for
parallel processing we know we can have
multiple map tasks running on multiple
nodes at the same time we also know that
multiple reduced tasks could be running
now why does then mapreduce become a
slower approach first of all your map
reduce is a batch oriented operation now
mapreduce is very rigid and it strictly
uses mapping and reducing phases so no
matter what kind of processing you would
want to do you would have to still
provide the mapper function and the
reducer function to work on data not
only this whenever your map phase
completes
the output of your map phase which is an
intermittent output would be written to
hdfs and thereafter underlying disks and
this data would then be shuffled and
sorted and picked up for reducing phase
so every time your data being written to
htfs and retrieved from sdfs makes
mapreduce a slower approach the question
is for a map release job is it possible
to change the number of mappers to be
created Now by default you cannot change
the number of map tasks because number
of map tasks depends on the input splits
however there are different ways in
which you can either set a property to
have more number of map tasks which can
be used or you can customize your code
or make it use a different format which
can then control the number of map tasks
by default number of map tasks are equal
to the number of splits of file you
so if you have a 1GB of file that is
split into eight blocks or 128 MB there
would be eight map tasks running on the
cluster these map tasks are basically
running your mapper function if you have
a hard coded properties in your mapred
hyphen site file to specify more number
of map tasks then you could control the
number of map tasks let's also talk
about some data types so when you
prepare for Hadoop when you want to get
into Big Data field you should start
learning about different data form
now there are different data formats
such as Avro par k
have a sequence file or binary format
and these are different formats which
are used now when you talk about your
data types in Hadoop these are
implementation of your writable and
writable comparable interfaces so for
every data type in Java you have a
equivalent in Hadoop so end in Java
would be in writable in Hadoop float
would be float writable long would be
long writeable double writeable Boolean
writable array writable map writable and
object
these are your different data types that
could be
your mapreduce program and these are
implementation of writable and writable
comparable interfaces what is
speculative execution
now imagine you have a cluster which has
huge number of nodes and your data is
spread across multiple slave machines or
multiple nodes now at a point of time
due to a disk degrade on network issues
or machine heating up or more load being
on a particular node there can be a
situation where your data node will
execute task in a slower manner now in
this case if speculative execution is
turned on there would be a shadow task
or a another
similar task running on some other node
for the same processing so whichever
task finishes first will be accepted and
the other task would be killed
speculative execution could be good if
you are working in an intensive workload
kind of environment where if a
particular node is slower you could
benefit from a unoccupied or a node
which has less load to take of your
processing going further this is how we
can understand so node a which might be
having a slower task you would have a
scheduler which is maintaining or having
knowledge of what are the resources
available so if speculative execution as
a property is turned on then the task
which was running slow a copy of that
task or you can say shadow task would
run on some other node and whichever
task completes first will be considered
this is what happens in your speculative
execution now how is identity mapper
different from chain mapper now this is
where we are getting deeper into
mapreduce Concepts so when you talk
about mapper identity mapper is the
default mapper which is chosen when no
mapper is specified in mapreduce driver
class so for every mapreduce program you
would have a map class which is taking
care of your mapping phase which
basically has a mapper function and
which would run one or multiple map
tasks right your programming your
program would also have a reduce class
which would be running a reducer
function which takes care of reduced
tasks running on multiple nodes now if a
mapper is not specified within your
driver class so driver class is
something which has all information
about your flow what's your map class
what is your reduce class what's your
input format what's your output format
what are the job configurations and so
on so identity mapper is the default
mapper which is chosen when no mapper
class is mentioned in your driver class
it basically implements an identity
function which directly writes all its
key pairs into output and it was defined
in old map reduce API in this particular
package but when you talk about chaining
mappers or chain mapper this is
basically a class to run multiple
mappers in a single map task or
basically you could say multiple map
tasks would run as a part of your
processing the output of first mapper
would become as an input to Second
mapper and so on and this can be defined
in the under mentioned class or
what are the major configuration
parameters required in the map reduce
program obviously we need to have the
input location we need to have the
output location so input location is
where the files will be picked up from
and this would preferably on sdfs
directory output location is the path
where your job output would be written
by your mapreduce program you also need
to specify input and output formats if
you don't specify the defaults are
considered then we need to also have the
classes which have your map and reduce
functions and if you intend to run the
code on a cluster you need to package
your class in a jar file export it to
your cluster and then this jar file
would have your mapper reducer and
Driver classes so these are important
configuration parameters which you need
to consider for a map reduce program now
what is the difference or what do you
mean by map site join and reduce side
join map site join is basically when the
join is performed at the mapping level
or at the mapping phase or is performed
by the mapper so each input data which
is being worked upon has to be divided
into same number of partitions
input to each map is in the form of a
structured partition and is in sorted
order so Maps I join you can understand
it in a simpler way that if you compare
it with rdbms Concepts where you had two
tables which were being joined it will
always be advisable to give your bigger
table as the left side table or the
first table for your join condition and
it would be your smaller table on the
left side and your bigger table on the
right side which basically means the
smaller table could be loaded in memory
and could be used for joining so map
side drawing is a similar kind of
mechanism where input data is divided
into same number of parties
when you talk about reduced side join
here the join is performed by the
reducer so it is easier to implement
than map side join as all the sorting
and shuffling will send the values or
send all the values having identical
keys to the same reducer so you don't
need to have your data set in a
structured form so look into your map
site drawing or reduce side join and
other joints just to understand how
mapreduce Works however I would suggest
not to focus more on this because
mapreduce is still being used for
processing but the amount of mapreduce
based processing has decreased overall
or across the industry now what is the
role of output committer class in a
mapreduce job so output committer as the
name says describes the commit of task
output for a mapreduce job so we would
have this as mentioned our Apache Hadoop
map reduce output committer you could
have a class which extends output
committer class
class so mapreduce relies on this map
reduce relies on the output committer of
the job to set up the job initialization
cleaning up the job after the job
completion that means all the resources
which were being used by a particular
job setting up the task temporary output
checking whether a task needs a commit
committing the task output and
discarding the
so this is a very important class and
can be used within your mapreduce job
what is the process of spilling in
mapreduce what does that mean so
spilling is basically a process of
copying the data from memory buffer to
disk when obviously the buffer usage
reaches a certain threshold so if there
is not enough memory in your buffer in
your memory then the content which is
stored in buffer or memory has to be
flushed out so by default a background
thread starts spilling the content from
memory to disk after 80 percent of
buffer size is filled now when is the
buffer being used so when your mapreduce
processing is happening the data from
data is being read from the disk loaded
into the buffer and then some processing
happens same thing also happens when you
are writing data to the cluster so you
can imagine for a hundred megabytes size
buffer the spilling will start after the
content of buffer reaches 80 megabytes
this is customizable how can you set the
mappers and reducers for a mapreduce job
so these are the properties so number of
mappers and reducers as I mentioned
earlier can be customized so by default
your number of map tasks depends on the
split and number of reduced tasks
depends on the partitioning phase which
decides number of reduced tasks which
would be used depending on the key
word we can set these properties either
in
in config files or provide them on the
command line or also make them part of
our code and this can control the number
of map tasks or reduce tasks which would
be run for a particular job let's look
at one more interesting question what
happens when a node running a map task
fails before sending the output to the
reducer so there was a node which was
running a map task and we know that
there could be one or multiple map tasks
running on one or multiple nodes and all
the map tasks have to be completed
before the further stages that such as
combiner or reducer come into existence
so in a case if in node crashes where a
map task was assigned to it the whole
task will have to be run again on some
other node so in Hadoop version 2 yarn
framework has a temporary demon called
application
so your application Master is taking
care of execution of your application
and if a particular task on a particular
node failed due to unavailability of
node it is the role of application
Master to have this task scheduled on
some other node now can we write the
output of mapreduce in different formats
of course we can so Hadoop supports
various input and output formats so you
can write the output of mapreduce in
different formats so you could have the
default format that is text output
format wherein records are written as
line of text you could have sequence
file which is basically to write
sequence files or your binary format
files where your output files need to be
fed into another mapreduce jobs
go for a map file output format to write
output as map files you could go for a
sequence file as a binary output format
so that's again a variant of your
sequence file input format it basically
writes keys and values to a c
SC so when we talk about binary format
we are talking about a non-human
readable format DB output format now
this is basically used when you would
want to write data to say relational
databases or say no SQL databases such
as hbase so this format also sends the
reduce output to a SQL table now let's
learn a little bit about yarn yarn which
stands for yet another resource
negotiator it's the processing framework
so what benefits did yarn bring in
Hadoop version 2 and how did it solve
the issues of mapreduce version one so
map reduce version 1 had major issues
when it comes to scalability or
availability because sorry in Hadoop
version 1 you had only one master
process for processing layer and that is
your job tracker so your job tracker was
listening to all the task trackers which
were running on multiple machines so
your job tracker was responsible for
resource tracking and job scheduling in
yarn you still have a processing Master
but that's called resource manager
instead of job tracker and now with
Hadoop version 2 you could even have
resource manager running in high
availability mode you have node managers
which would be running on multiple
machines and then you have a temporary
demon called application master
so in case of Hadoop version 2 your
resource manager or Master is only
handling the client connections and
taking care of tracking the resources
the jobs scheduling or basically taking
care of execution across multiple nodes
is controlled by application Master till
the application completes
so in yarn you can have different kind
of resource allocations that could be
done and there is a concept of container
so container is basically a combination
of RAM and CPU cores yarn can run
different kind of workloads so it is not
just map reduce kind of workload which
can be run on Hadoop version 2 but you
would have graph processing massive
parallel processing you could have a
real-time processing and huge processing
applications could run on a cluster
based on yarn so when we talk about
scalability in case of your Hadoop
version 2 you can have a cluster size of
more than 10 000 nodes and can run more
than 100 000 concurrent tasks and this
is because for every application which
is launched you have this temporary
demon called application
so if I would have 10 applications
running I would have 10 app Masters
running taking care of execution of
these applications across multiple nodes
compatibility so Hadoop version 2 is
fully compatible with whatever was
developed as per hadoopers version 1 and
all your processing needs would be taken
care by yarn
so Dynamic allocation of cluster
resources taking care of different
workloads
allocating resources across multiple
machines and using them for execution
all that is taken care by yarn
multi-tenancy which basically means you
could have multiple users or multiple
teams
you could have open source and
proprietary data access engines and all
of these could be basically hosted using
the same cluster now how does yarn
allocate resources to an application
with help of its architecture so
basically you have a client or an
application or an API which talks to
resource manager resource manager is as
I mentioned managing the resource
allocation in the Clusters when you talk
about resource manager you have its
internal two components one is your
scheduler and one is your applications
manager so when we say resource manager
being the master is tracking the
resources The Source manager is the one
which is negotiating the resources with
slave it is not actually resource
manager who is doing it but these
internal components so you have a
scheduler which allocates resources to
various running applications so
scheduler is not bothered about tracking
your resources or basically tracking
your applications so we can have
different kind of schedulers such as
feed
[Music]
first out you could have a fair
scheduler or you could have a capacity
scheduler and these schedulers basically
control how resources are allocated to
multiple applications when they are
running in parallel so there is a queue
mechanism so scheduler will schedule
resources based on requirements of
application but it is not monitoring or
tracking the status of applications
your application's manager is the one
which is accepting the job submissions
it is monitoring and restarting the
application Masters so it's application
manager which is basically basically
then launching a application Master
which is responsible for an application
so this is how it looks so whenever a
job submission happens we already know
that resource manager is aware of the
resources which are available with every
node manager so on every node which has
fixed amount of RAM and CPU cores some
portion of resources that is RAM and CPU
course I allocated to node manager
resource manager is already aware
of how much resources are available
across
mode so whenever a client request comes
in resource manager will make a request
to node manager it will basically
request node manager to hold some
resources for processing node manager
would basically approve or disapprove
this request
holding the sources and these resources
that is a combination of RAM and CPU
cores are nothing but containers we can
allocate containers of different sizes
within yarn hyphen site file so your
node manager based on the request from
resource manager guarantees the
container which would be available for
processing that's when your resource
manager starts a temporary demon called
application Master to take care of your
execution so your app Master which was
launched by resource manager or we can
say internally applications manager will
run in one of the containers because
application Master is also a piece of
code so it will run in one of the
containers and then other containers
will be utilized for execution this is
how yarn is basically taking care of
your allocation your application Master
is managing resource needs it is the one
which is interacting with scheduler and
if a particular node crashes it is the
responsibility of App Master to go back
to the master which is resource manager
and negotiate for more resources so your
app Master will never ever negotiate
Resources with node manager directly it
will always talk to resource manager and
the source manager is the one which
negotiates the resources
container as I said is a collection of
resources like your RAM CPU Network
bandwidth and your container is
allocated based on the availability of
resources on a particular node so which
of the following has occupied the place
of a job tracker of mapreduce so it is
your resource manager so resource
manager is the name of the master
process in Ado version 2. now if you
would have to write yarn commands to
check the status of an application so we
could just say yarn application minus
status and then the application ID and
you could kill it also from the command
line remember your yarn has a UI and you
can even look at your applications from
the UI you can even kill your
applications from the UI however knowing
the command line commands would be very
useful can we have more than one
resource manager in a yarn-based cluster
yes we can that is what Hadoop version 2
allows as have so you can have a high
availability yarn cluster where you have
a active and standby and the
coordination is taking care by your
zookeeper at a particular time there can
only be one active resource manager and
if active resource manager fails your
standby resource manager comes and
becomes active however zookeeper is
playing a very important role remember
zookeeper is the one which is
coordinating the server State and it is
doing the election of active to standby
failover what are the different
schedulers available in yarn so you have
a fee for scheduler that is first in
first out and this is not a desirable
option because in this case a longer
running application might block all
other small running applications Your
Capacity scheduler is basically a
scheduler where dedicated queues are
created and they have fixed amount of
resources so you can have multiple
applications accessing the cluster at
the same time and they would be using
their own queues and the resources
allocated to them if you talk about Fair
scheduler you don't need to have a fixed
amount of resources you can just have a
percentage and you could decide what
kind of fairness is to be followed which
basically means that if you were
allocated 20 gigabytes of memory however
the cluster has 100 gigabytes and the
other team was assigned 80 gigabytes of
memory then you have 20 access to the
cluster another team has 80 percent
however if the other team does not come
up or does not use the cluster in a fair
scheduler you can go up to maximum 100
percent
to find out more information about your
schedulers you could either look in
Hadoop definitive guide or what you
could do is you could just go to Google
and you could type for example yarn
scheduler let's search for yarn
scheduler and then you can look in
Hadoop definitive guide and so this is
your Hadoop definitive guide and it
beautifully explains about your
different schedulers how do multiple
applications run and that could be in
your fifo kind of scheduling it could be
in capacity scheduler or it could be in
a fair scheduling so have a look at this
link it's a very good link you can also
search for yarn untangling and this is a
Blog of four or this is a series of four
blocks where it's beautifully explained
about your yarn how it works how the
resource allocation happens what is a
container and what runs within the
container so you can scroll down you can
be reading through this and you can then
also search for part two of it which
talks about allocation and so on so
coming back
we basically have these schedulers what
happens if a resource manager fails
while executing an application in a high
availability cluster so so in a high
availability cluster we know that we
would have two resource managers one
being active one being standby and
zookeeper which is keeping a track of
the server States so if a RM fails in
case of high availability the standby
will be elected as active and then
basically your resource manager or the
standby would become the active one and
this one would instruct the application
Master to a bot in the beginning then
your resource manager recovers its
running state so there is something
called as RM State Store where all the
applications which are running their
status is stored so resource manager
recovers its running state by looking at
your state store by taking advantage of
container statuses and then continues to
take care of your process now in a
cluster of 10 data nodes each having 16
GB and 10 cores what would be total
processing capacity of the cluster take
a minute to think 10 data nodes 16 GB
Ram per node 10 cores so if you mention
the answer as 160 GB RAM and 100 cores
then you went wrong now think of a
cluster which has 10 data nodes each
having 16 GB RAM and 10 cores remember
on every node in a Hadoop cluster you
would have one or multiple processes
running those processes would need RAM
the machine itself which has a Linux
file system would have its own processes
so that would also be having some RAM
usage which basically means that that if
you talk about 10 data nodes you should
deduct at least 20 to 30 percent towards
the overheads towards the cloud database
Services towards the other processes
which are running and in that case I
could say that you could have 11 or 12
GB available on every machine for
processing and say six or seven course
one multiply that by 10 and that's your
processing capacity remember the same
thing applies to the disk usage also so
if somebody asks you in a 10 data node
cluster where each machine has 20
terabytes of disks what is my total
storage capacity available for sdfs so
the answer would not be 200 you have to
consider the overheads and this is
basically which gives you your
processing capacity now let's look at
one more question so what happens if
requested memory or CPU cores Beyond or
goes beyond the size of container now as
I said you can have your configurations
which can say that in a particular data
node which has 100 GB Ram I could
allocate say 50 GB for the processing
like out of 100 cores I could say 50
cores for processing so if you have 100
GB RAM and 100 cores you could ideally
allocate 100 for processing but that's
not ideally possible so if you have 100
GB Ram you would go for 50 GB and if you
have 100 cores you would go for 50 cores
now within this RAM and CPU course you
have the concept of content
right so container is a combination of
RAM and CPU cores so you could have a
minimum size container and maximum size
content now at any point of time if your
application starts demanding more memory
or more CPU cores and this cannot fit
into a container location your
application will
your application will fail because you
requested for a memory or a combination
of memory and CPU cores which is more
than the maximum container size so look
into this yarn tangling website which I
mentioned and look for the second blog
in those series which explains about
these allocate
allocate now here we will discuss on
hive Peg hbase and these components of
Hadoop which are being used in the
industry for various use cases let's
look at some questions here and let's
look how you should prepare for them so
first of all we will learn on hive which
is a data warehousing package so the
question is what are the different
components of a hive architecture now
when we talk about Hive we already know
that Hive is a data warehousing package
which basically allows you to work on
structured data or data which can be
structurized so normally people are well
versed with querying or basically
processing the data using SQL queries a
lot of people come from database
backgrounds and they would find it
comfortable if they know structured
query language hi is one of the data
warehouse in package which resides
within a Hadoop ecosystem it uses
hadoop's distributed file system to
store the data and it uses rdbm mess
usually to store the metadata although
metadata can be stored locally also so
what are the different components of a
hive architecture so it has a user
interface so user interface calls the
execute interface to the driver this
creates a session to the query and then
it sends the query to the compiler to
generate an execution plan for it
usually whenever Hive is set up it would
have its metadata stored in an rdbms now
to establish the connection between
rdbms and Hadoop we need odbc or jdbc
connector jar file and that connected
jar file has a driver class now this
driver class is mandatory to create a
connection between Hive and Hadoop so
user interface creates this interface
using the driver now we have metastore
metastore stores the metadata
information so any object which you
create such as database stable indexes
their metadata is stored in metastore
and usually this meta store is stored in
an rdbms so that multiple users can
connect to Hive so your meta store
stores the metadata information and
sends that to the compiler for execution
of a query what does the compiler do it
generates the execution plan it has a
tag now tag stands for direct cycle
craft so it has a tag of stages where
each stage is either a metadata
operation a map or reduced job or an
operation on sdfs and finally we have
execution engine that acts as a bridge
between Hive and Hadoop to process the
query so execution engine communicates
bi-directionally with meta store to
perform operations like create or drop
tables so these are four important
components of Hive architecture now what
is the difference between external table
and manage stable and Hive so we have
various kinds of table in Hive such as
external table manage table partition
table the major difference between your
managed and external table is in respect
to what happens to the data if the table
is dropped usually whenever we create a
table in Hive it creates a manage table
or we could also call that as an
internal table now this manages the data
and moves it into warehouse directory by
default whether you create a manage
stable or external table usually the
data can reside in hive's default
Warehouse directory or it could be
residing in a location chosen however
when we talk about manage table if one
drops a manage stable not only the
metadata information is deleted but the
tables data is also deleted from sdfs if
we talk about external table it is
created with an external keyword
explicitly and if an external table is
dropped nothing happens to the data
which resides in sdfs so that's the main
difference between your managed and
external table what might be the use
case if somebody asks you there might be
a migration kind of activity or you are
interested in creating a lot of tables
using your queries so in that case you
could dump all the data on sdfs and then
you could create a table by pointing to
a particular directory or multiple
directories now you could then do some
testing of your tables and would decide
that you might not need all the tables
so in that case it would be advisable to
create external tables so that even if
the table is later dropped the data on
sdfs will be intact unlike your manage
table where dropping of table will
delete the data from sdfs Also let's
learn a little bit on partition so what
is partition And Hive and why is
partitioning required in high life if
somebody asks you that now normally in
world of rdbms partition is the process
to group similar type of data together
and that is usually done on basis of a
column or what we call as partitioning
key now each table usually has one
column in context of rdbms which could
be used to partition the data and why do
we do that so that we can avoid scanning
the complete table for a query and
restrict the scan to set of data or to a
particular partition in Hive we can have
any number of partition keys so
partitioning provides granularity in
Hive table it reduces the query latency
by scanning only relevant partition data
instead of whole data set we can
partition at various levels now if I
compare rdbms with Hive in case of rdbms
you could have one column which could be
used for partitioning and then then you
could be squaring the specific partition
so in case of rdbms your partition
column is usually a part of the table
definition so for example if I have an
employee table I might have employee ID
employee name employee age and employee
salary has four columns and I would
decide to partition the table based on
salary column now why would I partition
it because I feel that employee table is
growing very fast it is or it will have
huge amount of data and later when we
query the table we don't want to scan
the complete table so I could split my
data into multiple partition based on a
salary column giving some ranges in Hive
it is a little different in Hive you can
do partitioning and there is a concept
of static and dynamic partitioning but
in Hive the partition column is not part
of table definition so you might have an
employee table with employee ID name a
each and that that's it that would be
the table definition but you could then
have partitioning done based on salary
column which will then create a specific
folder on sdfs in that case when we
query the data we can see the partition
column also showing up so we can
partition the transaction data for a
bank for example based on month like
giant Feb Etc and any operation
regarding a particular month will then
allow us to query that particular folder
that is where partitioning is useful now
why does Hive not store metadata
information in a CFS if somebody asks
you so we know that hives data is stored
in sdfs which is Hadoop distributed file
system however the metadata is either
stored locally and that mode of I would
be called as embedded mode or you could
have hives metadata stored in rdbms so
that multiple clients can initiate the
connection now this metadata which is
very important for Hive would not be
stored in sdfs so we already know that
sdfs read and write operations are time
consuming it is a distributed file
system and it can accommodate huge
amount of data so Hive stores metadata
information in meta store using rdbms
instead of sdfs so this allows to
achieve low latency and faster data
access
no if somebody asks what are the
components used in Hive query processor
so usually we have the main components
are your parser your execution engine
logical plan generation Optimizer and
type checking so whenever a query is
submitted it will go through a parser
and parser would check the syntax it
would check for objects which are being
queried and other things to see if the
query is fine now internally you have a
semantic analyzer which will also look
at the query you have an execution
engine which basically will work on the
execution part that is the best
generated execution plan which could be
used to get the results for the query
you could also have user defined
functions which a user would want to use
and these are normally created in Java
or Java programming language and then
basically these user defined functions
are added to the class path now you
would have a logical plan generation
which which basically looks at your
query and then generates a logical plan
or the best execution path which would
be required to get to the results
internally there is a physical plan
generated which is then looked in by
Optimizer to get the best path to get to
the data and that might also be checking
your different operators which you are
using within your query finally we would
also have type checking so these are
important components in Hive so somebody
might ask you if you are querying your
data using Hive what are the different
components involved or if you could
explain what are the different
components which work when a query is
submitted so these are the components
now let's look a scenario based question
Suppose there are a lot of small CSV
files which are present in a is DFS
directory and you want to create a
single Hive table from these files so
data in these files have Fields like
registration number name email address
so if this is what needs to be done what
will be your approach to solve it where
will you create a single Hive table for
lots of small files without degrading
the performance of the system so there
can be different approaches now we know
that there are a lot of small CSV files
which are present in a directory so we
know that when we create a table in Hive
we can use a location parameter so I
could say create table give a table name
give the column and their data types I
could specify the delimiters and finally
I could say location and then point it
to a directory on sdfs in this directory
might be the directory which has lot of
CSV files so in this case I will avoid
loading the data in the table because
table being Point table pointing to the
directory will directly pick up the data
from one or multiple files and we also
know that Hive does schema check on read
so does not do a schema check on write
so in case there were one or two files
which did not follow the schema of the
table it would not prevent data loading
data would anyways be loaded only when
you query the data it might show you
null values if data which was loaded
does not follow the schema of the table
this is one approach what is the other
approach so let's look at that you can
think about sequence file format which
is basically a smart format or a binary
format and you can group these small
files together to form a sequence file
now this could be one other smarter
approach so we could create a temporary
table so we could say create table give
a table name give the column names and
their data types we could specify the
delimiters as it shows here that is row
format and Fields terminated by and
finally we can store that as text file
then we can load data into this table by
giving a local file system path and then
we can create a table that will store
data in sequence file format so my point
one is storing the data in this text
file 0.3 would be storing the data in
sequence file format so we say create
table give the specifications we say row
format delimited fields are terminated
by comma stored as sequence file then we
can move the data from test table into
test sequence file table so I could just
say insert overwrite my new table as
select star from other tape remember in
Hive you cannot do insert update delete
however if the table is existing you can
do a insert overwrite from an existing
table into a new table so this could be
one approach where we could have lot of
CSV files or smaller files club together
as one big sequence file and then store
it in the table now if somebody asks you
write a query to in insert a new column
that is integer data type into a hive
table and the requirement might be that
you would want to insert this table at a
position before an existing column now
that's possible by doing an alter table
giving your table name and then
specifying change column giving you a
new column with the data type before an
existing column this is a simple way
where you can insert a new column into a
hive table what are the key differences
between Hive and pick now some of you
might have heard High Visa data
warehousing package and Peg is more of a
scripting language both of them are used
for data analysis or Trend detection
hypothesis testing data transformation
and many other use cases so if we
compare Hive and pick Hive uses a
declarative language called Hive ql that
is Hive querying language similar to SQL
and it is for reporting or for data
analysis even for data transformation or
for your data extraction big uses a high
level procedural language called Pig
Latin for programming both of them
remember use mapreduce processing
framework so when we run a query in Hive
to process the data or when we create
and submit a big script both of them
trigger a mapreduce job unless and until
we have set them to Local mode Hive
operates on the server side of the
cluster and basically works on
structured data or data which can be
structuralized pig usually works or
operates on the client side of the
cluster and allows both structured
unstructured or even I could say
semi-structured data Hive does not
support Avro file format by default
however that can be done by using the
write serializer deserializer so we can
have Hive table related data stored in
Avro format in sequence file format in
parquet format or even as a text file
format however when we are working on
smarter formats like Avro or sequence
file or parquet we might have to use
specific serializers deserializers for
Avro this is the package which allows us
to use Avro format Pig supports Agro
format by default Hive was developed by
Facebook and it supports partitioning
and Peg was developed by Yahoo and it
does not support partitioning so these
are high level differences there are
lots and lots of differences remember
Hive is more of a data warehousing
package and Peg is more of a scripting
language or a strictly procedural flow
following scripting language which
allows us to process the data now let's
get more and let's get more deeper and
learn about Pig which is as I mentioned
a scripting language which can be used
for your data processing it also uses
map reduce although we can even have
pick run in a local mode let's learn
about pig in the next section now let's
learn on some questions about Pig which
is a scripting language and it is
extensively used for data processing and
data analysis so the question is how is
Apache pick different from mapreduce now
we all know that mapreduce is a
programming model it is it's quite rigid
when it comes to processing the data
because you have to do the mapping and
reducing you have to write huge code
usually mapreduce is written in Java but
now it can also be written in Python it
can be written in Scala and other
programming languages so if we compare
pick with mapreduce pig obviously is
very concise it has less lines of code
when compared to mapreduce now we also
know that big script internally will
trigger a mapreduce job however user
need not know about mapreduce
programming model they can simply write
simple scripts in Pig and that will
automatically be converted into
mapreduce however mapreduce has more
lines of code Peak is high level
language which can easily perform join
operations or other data processing
operations map reduce is a low level
language which cannot perform job join
operations easily so we can do join
using mapreduce however it's not really
easy in comparison to Pig now as I said
on execution every Pig operator is
converted internally into a mapreduce
job so every big script which is run
which would be converted into mapreduce
job now map reduce overall is a batch
oriented processing so it takes more
time to compile it takes more time to
execute either when you run a mapreduce
job or when it is triggered by Pink
script big works with all versions of
Hadoop and when we talk about mapreduce
program which is written in one Hadoop
version may not work with other versions
it might work or it might not it depends
on what are the dependencies what is the
compiler you are using what programming
language you have used and what version
of Hadoop you are working on so these
are the main differences between Apache
Pig and mapreduce what are the different
ways of executing pick script so you
could create a script file store it in
dot pick or dot text and then you could
execute it using the pick command you
could be bringing up the grunt shell
that is Pig's shell now that usually
starts with mapreduce mode but then we
can also bring it up in a local mode and
we can also run pick embed it as an
embedded script in other programming
language so these are the different ways
of executing your pick script now what
are the major components of pig
execution environment this is this is a
very common question interviewers would
always want to know different components
of Hive different component currents of
pig even different components which are
involved in Hadoop ecosystem so when we
want to learn about major components of
big execution environment here also so
you have pick scripts now that is
written in pig latin using built-in
operators and user-defined functions and
submitted to the execution environment
that's what happens when you would want
to process the data using pick now there
is a parser which does type checking and
checks the syntax of the script the
output of parser is a tag direct a
cyclic graph so look in Wikipedia for
tag so tag is basically a sequence of
steps which run in One Direction then
you have an Optimizer now this Optimizer
performs optimization using merge
transform split Etc it aims to reduce
the amount of data in the pipeline
that's the whole purpose of Optimizer
you have a internal compiler so pick
compiler converts the optimized code
into a mapreduce job and here user need
not know the mapreduce programming model
or how it works or how it is written
they all need to know about running the
pick script which would be internally
converted into a mapreduce job and
finally we have an execution engine so
mapreduce jobs are submitted to the
execution engine to generate the desired
results so these are major components of
pick execution environment now let's
learn about different complex data types
in big big supports various data types
the main ones are Tuple bag and map what
is Tuple or Tuple as you might have
heard a tuple is an ordered set of
fields which can contain different data
types for each field so in Array you
would have multiple elements but that
would be of same types list can also
have different types your Tuple is a
collection which has different fields
and each field can be of different type
now we could have an example is 1 comma
3 or 1 comma 3 comma a string or a float
element and all of that form a tuple bag
is a set of tuples so that's represented
by curly braces so you could also
imagine this like a dictionary which has
various different correction elements
what is a map map is a set of key value
pairs used to represent data so when you
work in Big Data field you need to know
about different data types which are
supported by Peg which are supported by
Hive which are supported in other
components of Ado so pupil pack map
array array buffer you can think about
list you can think about dictionaries
you can think about map which is key
value pair so these are your different
complex data types other than the
primitive data types such as integer
character string Boolean float and so on
now what are the various diagnostic
operators available in Apache pick so
these are some of the operators or
options which you can give in a pick
script you can do a thumb now dump
operator runs the pig latin scripts and
displays the result on the screen so
either I could do a dumb and see the
output on the screen or I can even do a
dump into and I could store my output in
a particular file so we can load the
data using load operator in Pig and then
Pig also has different internal storage
like Json loader or pick storage which
can be used if you are working on
specific kind of data and then you could
do a dump either before processing or
after processing and dump would produce
the result the result could be stored in
a file or seen on the screen you also
have a describe operator now that is
used to view the schema of a relation so
you can load the data and then you can
view the schema of relation using
describe operator explain as we might
already know displays the physical
logical and mapreduce execution plans so
normally in rdbms when we use X-Plane we
would like to see what happens behind
the scenes when a particular script or a
query runs so we could load the data
using load operator as in any other case
and if we would want to display The
Logical physical and mapreduce execution
plans we could use explain operator
there is also an illustrate operator now
that gives the step-by-step execution of
sequence of statements so sometimes when
we would want to analyze our script to
see how good or bad they are or would
that really serve our purpose we could
use illustrate and again you can test
that by loading the data using load
operator and you could just use a
illustrate operator to have a look at
the step-by-step execution of the
sequence of statements which you would
want to execute so these are different
diagnostic operators available in Apache
pick now if somebody asks State the
usage of group order by and distinct
keywords in big script so as I said big
is a scripting language so you could use
various operators so group basically
collects various records with the same
key and groups the data in one or more
relations here is an example you could
do a group data so that is basically a
variable or you can give some other name
and you can say group relation Name by H
now say I have a file where I have field
various fields and one of the field is a
relational name so I could group that by
a different feed order by is used to
display the contents of relation in a
sorted order whether ascending or
descending so I could create a variable
called relation 2 and then I could say
order relation name one by ascending or
descending order distinct basically
removes the duplicate records and it is
implemented only on entire records not
on individual records so if you would
like want to find out the distinct
values and relation name field I could
use distinct what are the relational
operators in pig so you have various
relational operators which help data
scientists or data analysts or
developers who are analyzing the data
such as go Group which joins two or more
tables and then performs group operation
on the join table result you have cross
it is used to compute the cross product
that is a Cartesian product of two or
more relations for each is basically to
do some iteration so if it will iterate
through tuples of a relation generating
a data transformation so for example if
I say variable a equals and then I load
a file in a and then I could create a
variable called B where I could say for
each a I would want to do something say
group join is to join two or more tables
in a relation limit is to limit the
number of output tuples or output
results split is to split the relation
into two or more relations Union is to
get a combination that it will merge the
contents of two or more relations and
order is to get a sorted result so these
are some relational operators which are
extensively used in pig for analysis
what is the use of having filters in
Apache pick now say for example I have
some data which has three Fields here
product quantity and this is my phone
sales data so filter operator could be
used to select the required values from
a relation based on a condition it also
allows you to remove unwanted records
from data file so for example filter the
products where quantity is greater than
thousand so I see that I have one row
wherein or multiple rows where the
quantity is greater than thousands such
as fifteen hundred Seventeen hundred
twelve hundred so I could create a
variable called a I would load my file
using pick storage as I explained
earlier big storage is an internal
parameter which can be used to specify
the delimiters now here my delimiter is
comma so I could say using pick storage
as and then I could specify the data
type for each field so here being
integer product being character array
and quantity being integer then B I
could say filter a whatever we have in a
by quantity greater than thousand so
it's very concise it's very simple and
it allows us to extract and process data
in a simpler way now Suppose there is a
file called test dot txt having 150
records in sdfs so this is a file which
is stored on sdfs and it has 150 records
where we can consider every record being
one line and if somebody asks you to
write a pick command to retrieve the
first 10 records of the file first we
will have to load the data so I could
create a variable called test underscore
data and I would say load my file using
pick storage specifying the delimiter S
comma as and then I could specify my
Fields what whatever Fields our file
have and then I would want to get only
10 records for which I could use the
limit operator so I could say limit on
test data and give me 10 records this is
very simple and we can extract 10
records from 150 records which are
stored in the file on sdfs now we have
learned on Pig we have learned some
questions on hive you could always look
more in books like programming in Hive
or programming in pick and look for some
more examples and try out these examples
on a existing Hadoop setup now let's
learn on hbase which is a nosql database
now edgebase is a four dimensional
database in comparison to your rdbms
which usually are two dimensional so
rdbms have rows and columns but hbase
has four coordinates it has row key
which is always unique column family
which can be any number column
qualifiers which can again be any number
per column family and then you have a
version so these four coordinates make H
base a four dimensional key value store
or a column family store which is unique
for storing huge amount of data and
extracting data from hbase there is a
very good link which I would suggest
everyone can look at if you would want
to learn more on edgebase and you could
just say hbase mapper and this basically
brings up a documentation which is from
mapper but then that's not specific to
map R and you can look at this link
which will give you a detailed
explanation of HP is how it works what
are the Architectural Components and how
data is stored and how it makes edgebase
a very powerful nosql database so let's
learn on some of the important or
critical questions on hbase which might
be asked by the interviewer in an
interview when you are applying for a
Big Data admin or a developer position
role so what are the key components of
edgebase now as I said this is one of
the favorite questions of interviewers
where they would want to understand your
knowledge on different components for a
particular service edgebase as I said is
a nosql database and that comes as a
part of service with Cloudera or
hortonworks and with Apache Hadoop you
could also set up Edge base as an
independent package so what are the key
components of hbase edgebase has a
region server now edgebase follows the
similar kind of topology like Hadoop now
Hadoop has a master process that is name
node and slave processes such as data
nodes and secondary name node in the
same way edgebase also has a master
which is Edge master and the slave
processes are called region servers so
these region servers are usually
co-located with data nodes however it is
not mandatory that if you have 100 data
nodes you would have 100 region servers
so it purely depends on admin so what
does this region server contain so
region server contains hbase tables that
are divided horizontally into regions or
you could say group of rows is called
regions so in edgebase you have two
aspects one is group of columns which is
called column family and one is group of
rows which is called regions now these
regions or these rows are grouped based
on the key values or I would say row
Keys which are always unique when you
store your data in edgebase you would
have data in the form of rows and
columns so group of rows are called
regions or you could say these are
horizontal partitions of the table so a
region server manages these regions on
the Node where a data node is running a
region server can have up to thousand
regions it runs on every node and
decides the size of region so region
server as I said is a slave process
which is responsible for managing HPS
data on the Node each region server is a
worker node or a worker process
co-located with data node which will
take care of your read write update
delete request from the clients now when
we talk about more components of
edgebase as I said you have HP H master
so you would always have a connection
coming in from a client or an
application what does H Master do it
assigns regions it monitors the region
servers it assigns regions to region
servers for load balancing and it cannot
do that without the help of Zookeeper so
if we talk about components of hbase
there are three main components you have
zookeeper you have etch master and you
have region server region server being
the slave process your Edge Master being
the master process which takes care of
all your table operations assigning
regions to the region servers taking
care of read and write requests which
come from client and for all of this
Edge Master will take in help of
Zookeeper which is a centralized
coordination service so whenever a
client wants to read or write or change
the schema or any other metadata
operations it will contact H Master Edge
Master internally will contact zookeeper
so you could have edgebase setup also in
high availability mode where you could
have a active Edge master in a backup
Edge Master you would have a zookeeper
Quorum which is the way zookeeper works
so zookeeper is a centralized
coordination service which will always
run with a quorum of processes so
zookeeper would always run with odd
number of processes such as 3 5 and 7
because zookeeper works on the concept
of maturity consensus now zookeeper
which is a centralized coordination
service is keeping a track of all the
servers which are alive available and
also keeps a track of their status for
every server with zookeeper is
monitoring zookeeper keeps a session
alive with that particular server Edge
Master would always check with zookeeper
which region servers are available alive
so that regions can be assigned to the
region server at one end you have region
server which are sending their status to
the Zookeeper indicating if they are
ready for any kind of read or write
operation and at other end Edge Master
is querying the Zookeeper to check the
status now zookeeper internally manages
a meta table now that meta table will
have information of which regions are
residing on which region server and what
rookies those regions contain so in case
of a read activity Edge Master will
zookeeper to find out the region server
which contains that meta table once etch
Master gets the information of meta
table it can look into the meta table to
find out the row keys and the
corresponding region servers which
contain the regions for those row Keys
now if we would want to understand row
key and column families in hbase let's
look at this and it would be good if we
could look this on an Excel sheet so row
key is always unique it acts as a
primary key for any hbase table it
allows a logical grouping of cells and
make sure that all cells with the same
row key are co-located on the same
server so as I said you have four
coordinates for hbase you have a row key
which is always unique you have column
families which is nothing but group of
columns and when I say column families
one column family can have any number of
columns so when I talk about hbase hbase
is four dimensional and in terms of H
base it is also called as a column
oriented database which basically means
that every Row in one column could have
a different data type now you have a row
key which uniquely identifies the row
you have column families which could be
one or many depending on how the table
has been defined and a column family can
have any number of columns or I could
say for every row within a column family
you could have different number of
columns so I could say for my Row 1 I
could just have two columns such as name
and City within the column family for my
Row 2 I could have name City age
designation salary for my third row I
could have thousand columns and all that
could belong to one column family so
this is a horizontally scalable database
so column family consists of group of
columns which is defined during table
creation and each column family can have
any number of column qualifiers
separated by a delimiter now a
combination of row key column family
column qualifier such as name City age
and the value within the cell is makes
the hbase a unique four dimensional
database for more information if you
would want to learn on hbase please
refer this link which is hbase mapper
and this gives a complete edgebase
architecture that has three components
of name node three components that is
name node region servers and zookeeper
how it works how hbase H Master
interacts with zookeeper what zookeeper
does in coordination how are the
components working together and how does
hbase take care of read and write coming
back and continuing why do we need to
disable a table so there are different
table operations what you can didn't do
in hbase and one of them is disabling a
table now if you would want to check the
status of table you could check that my
is disabled and giving the table name or
is enabled and the table name so the
question is why do we need to disable a
table now if we would want to modify a
table or we are doing some kind of
Maintenance activity in that case we can
disable the table so that we can modify
or changes settings when a table is
disabled it cannot be accessed through
the scan command now if we have to write
a code to open a connection in hbase now
to interact with hbase one could either
use a graphical user interface such as
Hue or you could be using the command
line hbase shell or you could be using
hbase admin API if you are working with
Java or say happy base if you are
working with python where you may want
to open a connection with hbase so that
you can work with database
programmatically in that case we have to
create a configuration object that is
configuration my conf and then create a
configuration object and then you can
use different classes like Edge table
interface to work on a new table you
could use each column qualifier and many
other classes which are available in
hbase admin API what does replication
mean in terms of hbase so edgebase as I
said Works in a cluster way and when you
talk about cluster you could always set
up a replication from one hbase cluster
to other edgebase cluster so this
replication feature in hbase provides a
mechanism to copy data between clusters
or sync the data between different
clusters this feature can be used as a
disaster recovery solution that provides
High availability for hbase so if I have
hbase cluster 1 where I have one master
and multiple region servers running in a
Hadoop cluster I could use the same
Hadoop cluster to create a hbase replica
cluster or I could have a totally
different hbase replica cluster where my
intention is that if things are changing
in a particular table in cluster one I
would want them to be replicated across
different cluster so I could alter the
edge base table and set the replication
scope to 1. now a replication scope of 0
indicates that table is not replicated
but if we set the replication to 1 we
basically will have to set up ahbs
cluster where we can replicate hbase
tables data from cluster 1 to Cluster so
these are the commands which can be used
to enable replication and then replicate
the data of table across clusters can we
Import and Export in hbase of course we
can it is possible to Import and Export
tables from one edgebase cluster to
other hbase cluster or even within a
cluster so we can use the hbase export
utility which comes in this particular
package give a table name and then a
Target location so that will export the
data of hbase table into a directory on
sdfs then I could create a different
table which would follow some kind of
same definition as the table which was
exported and then I could use import to
import the data from the directory on
sdfs to my table if you would want to
learn more on hbase Import and Export
you could look at hbase import
operations let's search for the link and
this is the link where you could learn
more about hbase import export utilities
how you could do a bulk import bulk
export which internally uses mapreduce
and then you could do a Import and
Export into edgebase tables moving
further what do we mean by compaction in
edgebase now we all know that hbase is a
nosql database which can be used to
store huge amount of data however
whenever a data is written in hbase it
is first written to what we call as
write ahead log and also to mem store
which is write cache now once the data
is written involved and your mem store
it is offloaded to form an internal
edgebase format file which is called H5
and usually these edge files are very
small in nature so we also know that
sdfs is good when we talk about few
number of larger files in comparison to
large number of smaller files due to the
limitation of name nodes memory
compaction is process of merging hbase
files that is the smaller edge files
into a single large file this is done to
reduce the amount of memory required to
store the files and number of disk seeks
needed so we could have lot of H files
which get created when the data is
written to hbase and these smaller files
can then be compacted through a major or
minor compaction creating one big Edge
file which internally would then be
written to sdfs and sdfs format of
blocks that is the benefit of compaction
there is also a feature called Bloom
filter so how does Bloom filter work so
Bloom filter or hbase Bloom filter is a
mechanism to test whether a h file
contains a specific row or a row column
cell Bloom filter is named after its
creator button covered Blue it is a data
structure which predicts whether a given
element is a member of a set of data it
provides an in-memory index structure
that reduces the disk that reads and
determines the probability of finding a
row in a particular file this is one of
very useful features of edgebase which
allows for faster access and avoids disk
syncs thus edgebase have any concept of
namespace so namespace is when you have
similar elements grouped together so
namespace yes HB supports such names
space so namespace is a logical grouping
of tables analogous to a database in
rdbms so you can create hbase namespace
to the schema of rdbms database so you
could create a namespace by saying
create namespace and giving it a name
and then you could also list the tables
within a namespace you could create
tables within a specific namespace now
this is usually done in production
environment where a cluster might be
multi-tenant cluster and there might be
different users of the same nosql
database in that case admin would create
specific namespace and for specific
namespace you would have different
directories on hdfs and users of a
particular business unit or a team can
work on their edgebase objects within a
specific name space this is a question
which is again very important to
understand about the rights or reads so
how does right ahead log wall hell when
a region server crashes now as I said
when a write happens it will happen into
mem store and wall that is your edit log
or write ahead log so whenever a write
happens it will happen in two places mem
store which is the right cache and wall
which is a edit log only when the data
is written in both these places and
based on the limitation of mem store the
data will be flushed to create an
edgebase format file called H5 these
files are then compacted and created
into one bigger file which will then be
stored on sdfs and sdfs data as we know
is stored in the form of blocks on the
underlying data nodes so if a region
server hosting a mem store crashes now
where is region server running that
would be co-located with data node so if
a data node crashes or if a region
server which was hosting the mem store
Write cash crashes data in memory the
data that in memory which was not not
persisted is lost now how does edgebase
recover from this as I said your data is
written into wall and mem store at the
same time hbase recovers against that by
writing to wall before the write
completes so whenever a write happens it
happens in mem store and wall at the
same time HBS cluster keeps a wall to
record changes as they happen and that's
why we call it as also an edit log if
hbase goes down or the node that goes
down the data that was not flushed from
mem store to Edge file can be recovered
by replaying the right ahead log and
that's the benefit of your edit log or
write ahead log now if we would have to
write hbase command to list the contents
and update the column families of a
table I could just do a scan and that
would give me complete data of a table
if you are very specific and if you
would want to look at a particular row
then you could do a get table name and
then give the rocky however you could do
a scan to get the complete data of a
particular table you could also do a
describe to see what are the different
column families and if you would want to
alter the table and add a new column
family it is very simple you can just
say alter give the HB stable name and
then give you a new column family name
which will then be added to the table
what are catalog tables in hbase so as I
mentioned your zookeeper knows the
location of this internal catalog table
or what we call as The Meta table now
catalog tables in edgebase have two
tables one is hbase meta table and one
is hyphen root the catalog table Edge
base meta exists as an hbase table and
is filtered out of hbase shells list
command so if I give a list command on
edgebase it would list all the tables
which h space contains but not the meta
table it's an internal table this meta
table keeps a list of all regions in the
system and location of hbase method
stored in Zookeeper so if somebody wants
to find out or look for particular rows
they need to know the regions which
contain that data and those regions are
located on region server to get all this
information one has to look into this
meta table however we will not be
looking into meta table directly we
would just be giving a write or a read
operation internally your hbase master
queries the Zookeeper zookeeper has the
information of where the meta table
exists and that meta table which is
existing on region server contains
information of row keys and the region
servers where those rows can be found
your root table keeps a track of
location of The Meta table what is hot
spotting in hbase and how to avoid hot
spotting now this is a common problem
and always admin guys or guy who are
managing the infrastructure would think
about it so one of the main idea is that
edgebase would be leveraging the benefit
of sdfs you are all read and write
requests should be uniformly distributed
across all of the regions and region
servers otherwise what's the benefit of
having a distributed cluster so you
would have your data stored across
region servers in the form of regions
which are horizontal partitions of the
table and whenever read and write
requests happen they should be uniformly
distributed across all the regions in
the region servers now hotspotting
occurs when a given region serviced by a
region server receives most or all of
read and write request which is
basically a unbalanced way of read write
operations now hotspot can be avoided by
designing the row key in such a way that
data being written should go to multiple
regions across the cluster so you could
do techniques such as salting hashing
reversing the key and many other
techniques which are employed by users
of hbase we need to just make sure that
when the regions are distributed across
region servers they should be spread
across region servers so that your read
and write request can be satisfied from
different region servers in parallel
rather than all read and write request
hitting the same region server
overloading the region server which may
also lead to the crashing of a
particular page and server so these were
some of the important questions of hbase
and then there are many more please
refer to the link which I specified in
during my discussion and that gives you
a detailed explanation of how edgebase
works you can also look into hbase
definitive guide by O'Reilly or hbase in
action and these are really good books
to understand about hbase internals and
how it works now that we have learned on
hive which is a data warehousing package
we have learned on Pig which is a
scripting or a scripting language which
allows you to do data analysis and we
have learned some questions on a nosql
database just note it that there are
more than 225 nosql databases existing
in market and if you would want to learn
and know about more nosql databases you
can just go to Google and type no SQL
databases org and that will take you to
the link which is for nosql databases
and this shows there are more than 225
nosql databases existing in market and
these are for different use cases used
by different users and for with
different features so have a look at
this link now when you talk about data
ingestion so let's look at data
ingestion and this is one good link
which I would suggest to have a look at
which lists down around 18 different
injection tools so when you talk about
different data ingestion tools some are
for structured data some are for
streaming data some are for data
governance some are for data ingestion
and transformation and so on so have a
look at this link which also gives you a
comparison of different data ingestion
tools so here let's learn about some
questions on scope which is one of the
data ingestion tools mainly used for
structured data or you could say data
which is coming in from rdbms or data
which is already structured and you
would want to ingest that you would want
to store that on sdfs which could then
be used for Hive which could be used for
any kind of processing using mapreduce
or Hive or pig or spark or any other
processing Frameworks or you would want
to load that data into say hi by regime
Stables scoop is mainly for structured
data it is extensively used when
organizations are migrating from rdbms
to a big data platform and they would be
interested in ingesting the data that is
doing Import and Export of data from
rdbms to sdfs or vice versa so let's
learn about some important questions on
scope which you may be asked by an
interviewer when you apply for a big
data related position how is scoop
different from Flume so this is a very
common question which is asked scoop
which is mainly for structured data so
scoop works with rdbms it also works
with nosql databases to Import and
Export data so you can import data into
sdfs you can import data into Data
browsing packets such as Hive directly
or also in edgebase and you could also
export data from Hadoop ecosystem to
your rdbms however when it comes to flow
Flume is more of a data ingestion tool
which works with streaming data or
unstructured data so data which is
constantly getting generated for example
log files or metrics from server or some
chat messenger and so on so if you are
interested in working on capturing and
storing the streaming data in a storage
layer such as sdfs or edgebase you could
be using Flume there could be other
tools also like Kafka or storm or chukwa
or some nifi and so on scoop however is
mainly for structured data you're
loading data in scope is not event
driven so it is not based on event it
basically works on data which is already
stored in rdbms in terms of Flume it is
completely event driven that is as the
messages or as the events happen as the
data is getting generated you can have
that data ingested using flow Zoom scoop
works with structured data sources and
you have various scope connectors which
are used to fetch data from external
data structures or rdbms so for every
rdbms such as MySQL Oracle db2 Microsoft
SQL Server you have different connectors
which are available Flume it works on
fetching streaming data such as tweets
or log files or server metrics from your
different sources where the data is
getting generated and if you are
interested in not only ingesting that
data which is getting generated in a
streaming fashion but if you would be
interested in processing the data as it
arrives scoop can import data from rdbms
onto sdfs and also export it back to
rdbms Flume is then used for streaming
data now you could have one to one one
to many or many to one kind of relation
so in terms of Flume you have components
such as your Source sync and channel
that's the main difference between your
scoop and flow what are the different
file formats to import data using scope
well there are lots and lots of formats
in which you can import data into Scope
when you talk about scoop you can have
delimited text file format now that's
the default import format it can be
specified explicitly using as text file
argument so when I want to import data
from an rdpms I could get that data in
hdfs using different compression schemes
or in different formats using the
specific arguments so I could specify an
argument which will write string based
representation of each record to Output
files with delimiters between individual
columns and rows so that is the default
format which is used to import data in
using scoop so to learn more about your
scoop and different arguments which are
available you can click on scoop dot
apache.org you can look into the
documentation and I would suggest
choosing one of the versions and looking
into the user guide and here you can
search for arguments and look for
specific control arguments which show
how you can import data using scoop so
here we have common arguments and then
you also have import control arguments
wherein we have different options like
getting data as Avro as sequence file as
text file or parquet file these are
different formats you can also get data
in default compression scheme that is
gzip or you can specify compression
codec and then you can specify What
compression mechanism you would want to
use when you are importing your data
using school when it comes to default
format for Flume we could say sequence
file which is a binary format that
stores individual records in record
specific data types so these data types
are manifested as Java classes and scoop
will automatically generate these data
types for you so scoop does that when we
talk about your sequence file format in
terms of your scoop you could be
extracting storage of all data in binary
representation so as I mentioned you can
import data in different formats such as
Avro parquet sequence file that is
binary format or machine readable format
and then you could also have data in
different compression schemes let me
just show you some quick examples here
so if I look in uh the content and here
I could search for a scoop based file
where I have listed down some examples
so if I would want to use different
compression schemes here are some
examples I will look at these so I'm
doing a scope import I'm also giving an
argument so that scope which also
triggers a map reduced job or I would
say map only job so when you run a scoop
import it triggers a map only job no
reduce happens here and you could
specify this parameter or this argument
on the command line mapreduce dot
framework.name so that you could run
your map only job in a local mode to
save time or that would interact with
yarn and run a full-fledged map only job
we can give the connection and then
connect to whatever rdbms we are
connecting mentioning the database name
give your username and password give the
table name give it Target directory or
it would create a directory same as the
table name which would work only once
and then I could say minus Z to get
Theta in a compressed format that is
gzip or I could be specifying
compression codec and then I could
specify What compression codec I would
want to use say Snappy is lz4 default I
could also run a query by giving a scope
import and when I am specifying a query
I if you notice I have not given any
table name because that would be
included in the query I can get my data
as a sequence file format which is a
binary format which will create a huge
file so we could also have compression
enabled and then I could say the output
of my map job should use a compression
at record level for my data coming in
sequence file so sequence file or a
binary format supports compression at
record level or at Block Level I could
get my data in a Avro file where data
has embedded schema within the file or a
parquet file also so these are different
ways in which you can set up different
compression schemes or you can even get
data in different formats and you could
be doing a simple scope import for these
looking further what is the importance
of eval tool in scope so there is
something called as eval tool so scoop
eval tool allows users to execute user
defined queries against respective
database servers and preview the result
in the console so either I could be
running a straight away query to import
the data into mystfs or I could just use
scoop eval connect to my external rdbms
specify my username and password and
then I could be giving in a query to see
what would be the result of the query
which we intend to import now let's
learn about how scoop imports and
exports data between rdbms and sdfs with
its architecture so rdbms as we know has
your database structures your tables
which all of them are logical and
internally there is always metadata
which is stored your scoop import
connects to an external rdbms and for
this connection it uses an internal
connector jar file which has a driver
class so that's something which needs to
be set up by admin but they need to make
sure that whichever rdbms you intend to
connect to they need to have the jdbc
connector for that particular rdbms
stored within the scoop lib folder so
scoop import gets the metadata and then
for your scoop command it converts that
into a map only job which might have one
or multiple map tasks now that depends
on your scoop command you could be
specifying that you would want to do a
import only in one task or in multiple
tasks these multiple map tasks will then
run on a section of data from rdbms and
then store it in sdfs so at high level
we could say scoop will introspect
database to get gather the metadata it
divides the input data set into splits
and this division of data into splits
mainly happens on primary key column of
the table now if somebody might ask what
if my table in rdbms does not have a
primary key column then when you are
doing a scope import either you will
have to import it using one mapper task
by specifying hyphen hyphen m equals one
or you would have to say split by
parameter to specify a numeric column
from rdbms and that's how you can import
the data let me just show you a quick
example on this so I could just look in
again into the scoop command file and
here we could be looking at an example
so if you see this one here we are
specifying minus minus m equals 1 which
basically means I would want to import
the data using one map task now in this
case whether the table has a primary key
column or does not have a primary key
column will not matter but if I say a
minus minus ms6 where I am specifying
multiple map tasks to be imported then
this will look for a primary key column
in the table which you are importing now
if the table does not have a primary key
column then I could be specifying a
split by and then specify the column so
that the data could be split into
multiple chunks and multiple map tasks
would take it now if the second scenario
is your table does not have a primary
key column and it does not have a
numeric column on which you could do a
split by in that case and if you would
want to use multiple mappers you could
still say split by on a textual column
but you will have to add this property
so that it allows splitting the data
which is
non-numeric all of these options are
given in the scoop apache.org link going
further how scoop imports and exports
data between rdbms and sdfs with its
architecture so as I said it submits the
map only job to the cluster and then it
basically does a import or export so if
we are exporting the data from sdfs in
that case again there would be a map
only job it would look at multiple
splits of the data which is existing
which your map only job would process
through one or one table map task and
then export it to rdbms suppose you have
a database test DB in MySQL we if
somebody asked you to write a command to
connect this database and import tables
to scoop so here is a quick example as I
showed you in the command file so you
could say scoop import this is what we
would want to do you connect using jdbc
now this will only work if the jdbc
connector already exists within your
scope lib directory admin has to set up
that so you can connect to your rdbms
you can point to the database so here
our database name is test underscore DB
I could give username and then either I
could give password on the command line
or just say capital P so that I would be
prompted for the password and then I
could give the table name which I would
want to import I could also be
specifying minus minus M and specify how
many map tasks do I want to use for this
import as I showed in previous screen
how to export a table back to rdbms now
for this we need the data in a directory
on hdfs so for example there is a
departments table in retail database
which is already imported into scoop and
you need to export this table back to
rdbms so this is the content of the
table now create a new Department table
in rdbms so I could create a table
specifying the column names whether that
supports null or no if that has a
primary key column which is always
recommended and then I can do a scope
export I can connect to the rdbms
specifying my username and password
specify the table into which you want to
export the data and then you give export
directory pointing to a directory on
hdfs which contains the data this is how
you can export data into table seeing
example on this so I could again look
into my file and here I have an example
of import this is where you are
importing data directly into Hive and
you have scoop import where you are
importing data directly into hbase table
and you can then query your hbase table
to look at the data you could also do a
export by running your map only job in a
local mode connecting to the rdbms
specifying your username specifying the
table where you would want to export and
the directory on scfs where you have
kept the relevant data this is a simple
example of export looking further what
is the role of jdbc driver in scoop
setup so as I said if you would want to
use scoop to connect to an external
rdbms we need the jdbc odbc connector
jar file now one or admin could download
the jdbc connector jar file and then
place the jar file within the scoop lib
directory wherever scoop is installed
and this jdbc connector jar file
contains a driver now jdbc driver is a
standard Java API which is used for
accessing different databases in rdpms
so this connector jar file is very much
required and this connector jar file has
a driver class and this driver class
enables the connection between your
rdbms and your Hadoop structure each
database vendor is responsible for
writing their own implementation that
will allow communication with the
corresponding database and we need to
download the drivers which allow our
scope to connect to external rdpms so
your jdbc driver alone is not enough to
connect to scope we also need connectors
to interact with different database so a
connector is a pluggable piece that is
used to fetch metadata and allow scope
to overcome the differences in SQL
dialects so this is how connection can
be established so normally your admins
would when they are setting up scoop and
Hadoop they would download say MySQL
jdbc connector and this is how they
would go to the MySQL connectors if you
are connecting to mySQL similar early
for your other rdbms you could be say
going in here you could be looking for a
previous version depending you could be
going for platform independent and then
you could be downloading the connected
jar file now if you want our this jar
file you would see a MySQL connector jar
and if we look in com dot
mysql.jdbc.com.mysql.jdbc.river so this
is the package which is within the
connector jar file and this has the
driver class which allows the connection
of your scoop with your rdpms so these
things will have to be done by your
admin so that you can have your scoop
connecting to an external rdbms now how
do you update the columns that are
already exported so if I do a export and
I put my data in rdbms can I really
update the columns that are already
exported yes I can using a update key
parameter so scoop export command
Remains the Same the only thing I will
have to specify now is the table name
your Fields terminated by if you have a
specific delimiter and then you can say
update key and then the column name so
this allows us to update the columns
that are already exported in rdbms what
is code gen so scoop commands translate
into your mapreduce job or map only job
so code gen is basically a tool in scoop
that generates data access objects Dao
Java classes that encapsulate an
interpret imported records so if I do a
scoop code gen connect to an rdbms using
my username and give a table this will
generate a Java code for employee table
in in the test database so this code gen
can be useful for us to understand what
data we have in this particular table
finally can be used to convert data in
different formats I think I already
answered that right if no which tools
can be used for this purpose so scoop
can be used to convert data in different
formats and that depends on the
different arguments which you use when
you do a import such as avrofile parquet
file binary format with record or Block
Level compression so if you are
interested in knowing more on different
data formats then I think I can suggest
a link for that and we can say Hadoop
for match
I think it is Tech Mackie afro parquet
Let's see we can find out the Link tech
Maggie yeah this is a very good link
which specifies or talks about different
data formats which you should know such
as your text file format different
compression schemes how is data
organization what are the common formats
what do you have in text file structured
binary sequence files with compression
without compression what is record level
what is Block Level what is a Avro data
file what is a sequel what is a parquet
data file or a columnar format another
formats like orc RC and so on so please
have a look at this so with this we have
come to the end of this basics of Big
Data if you have found the session
informative and interesting please
consider subscribing to our YouTube
channel let us know your queries in the
comment section below our experts will
happily answer you until next time thank
you and keep learning staying ahead in
your career requires continuous learning
and upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in Cutting Edge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
thank you