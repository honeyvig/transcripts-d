did you know that Global public Cloud
spending is projected to reach an
astonishing 590 billion dollars in 2023
while this represents a remarkable 20.7
increase from the previous year
highlighting the immense growth and
popularity of cloud computing in recent
years the world's social economics
landscape has experienced significant
changes driving the adoption of cloud
computing the pandemic and remote work
requirements have fueled companies
digital transformation leading to the
rapid adoption of cloud-based
applications and the launch of new cloud
services to support remote workers and
satisfy clients so as we move forward in
2023 initiatives for digital
transformation continue to accelerate
the cloud computing Trends projected for
2023 indicate that this upward
trajectory will not slow down anytime
sooner so to help you capitalize on this
growing Trend simply learn is thrilled
to present our cloud computing full
course in this engaging video we will
delve into the fundamental concepts of
cloud computing providing you with an
in-depth understanding of this
revolutionary technology join our
experienced cloud computing specialist
Sam and Rahul as they guide you through
the Keynotes of cloud computing you will
discover the basic of cloud computing
including its definitions and core
principles then you will explore the
various types of cloud computing delved
into cloud computing architecture and
unravel the different cloud computing
service models with a focus on VMware
and virtualization but that's not all we
will also introduce you to the leading
cloud computing platforms AWS Azure and
gcp you will gain insights into the
fascinating services offered by AWS
discover how Azure differentiates itself
and explore gcp and its competition with
AWS and Azure furthermore we will
provide you with valuable information on
cloud computing certifications
empowering you to embark on a successful
career as a cloud engineer so if you
want to learn about the eligibility
criteria qualification and essential
skills required to pave your way in this
exciting field then this is it by the
end of this video all your cloud
computing related questions and doubts
will be cleared our experts are eagerly
waiting to assist you ensuring a
comprehensive understanding of the
topics covered and if you are looking
for a more detailed approach on cloud
computing with hands-on experience then
simply learns Caltech postgraduate
program in cloud computing will be the
right way to go this comprehensive
program will equip you with the
knowledge and skills needed to navigate
the cloud landscape with confidence dive
deep into the cloud architecture
deployment models secure T and migration
strategy you will also explore platforms
like AWS Microsoft Azure and gcp to
build your Cloud expertise so don't miss
out on this chance to transform your
career and join the ranks of successful
Azure professionals click the link in
the description to discover more about
this course here's an inspiring success
story from one of our satisfied Learners
who have unlocked the full potential of
cloud computing the future is in the
cloud and it's waiting for you to seize
it you can also unleash the magic of
cloud computing just like them and its
platforms like AWS Azure and gcp and
witnessed the Boost in your graph of
your career so let's get started
I will be permanently moving to Europe
with my family show simply learn this
course has helped me back a great job
offered there will be the 60 salary high
we cannot wait to start our new life in
Finland we are also excited hey I am
and I am currently living in Algeria I'm
working as Chief Information Technology
officer at Aries tires after working for
23 years in I.T sector in Algeria I felt
that my career had become stagnant here
I wanted to experience what is like to
live in Europe so I started applying to
various jobs there as someone who is
responsible for making the decision I
wanted to upgrade my skills I also
realized that to get a new job I needed
to have a professional certification in
cloud computing so last year I started
the postgraduate program in cloud
computing in association with Caltech
constant radio
it's not easy to get access to Caltech
but simply didn't provided that with
this course I had the choice to study
while I continued with my job
either Technologies like ews and Azure
during the course luckily my new
employers were looking for someone who
knew these Technologies I have just
copulated the course but my new lawyers
were aware that I was taking this course
and that is how I got this new job offer
with 60 percent salary high in Finland I
will be leading the cloud computing unit
of my new company
I am really excited not just for myself
but for my family as well I have 30
dollars I think I will be able to help
them with their higher studies in Europe
as a father you always want to make
decision that have a positive impact on
your children as for me I am going to
live my dream now I think dreams come
true when you work hard to achieve them
also don't miss out on this incredible
opportunity subscribe to our YouTube
channel and hit the Bell icon to stay
updated with all the latest content from
Simply learn imagine you're the owner of
a small software development firm and
you want to scale your business up
however a small team size the
unpredictability of demand and limited
resources are roadblocks for this
expansion that's when you hear about
cloud computing but before investing
money into it you decide to draw up the
differences between on-premise and
cloud-based Computing to make a better
decision when it comes to scalability
you pay more for an on-premise setup and
get lesser options too
once you've scaled up it is difficult to
scale down
and often leads to heavy losses in terms
of infrastructure and maintenance costs
cloud computing on the other hand allows
you to pay only for how much you use
with much easier and faster Provisions
for scaling up or down
next let's talk about server storage
on-premise systems need a lot of space
for their servers notwithstanding the
power and maintenance hassles that come
with them on the other hand cloud
computing Solutions are offered by cloud
service providers who manage and
maintain the servers saving you both
money and space then we have data
security on-premise systems offer less
data security thanks to a complicated
combination of physical and traditional
I.T security measures whereas cloud
computing systems offer much better
security and lets you avoid having to
constantly Monitor and manage security
protocols in the event that a data loss
does occur the chance for data recovery
with on-premise setups are very small
in contrast cloud computing systems have
robust disaster recovery measures in
place to ensure faster and easier data
recovery
finally we have maintenance on-premises
systems also require additional teams
for hardware and software maintenance
loading up the costs by a considerable
degree
cloud computing systems on the other
hand are maintained by the cloud service
providers reducing your costs and
resource allocation substantially so now
thinking that cloud computing is a
better option you decide to take a
closer look at what exactly cloud
computing is
cloud computing refers to the delivery
of on-demand Computing Services over the
internet on a pay-as-you-go basis in
simpler words rather than managing files
and services on a local storage device
you'll be doing the same over the
internet in a cost-efficient manner
cloud computing has two types of models
deployment model and service model
there are three types of deployment
models public private and hybrid Cloud
imagine you're traveling to work you've
got three options to choose from one you
have buses which represent public clouds
in this case the cloud infrastructure is
available to the public over the
internet
these are owned by cloud service
providers two then you have the option
of using your own car this represents
the private cloud with the private Cloud
the cloud infrastructure is exclusively
operated by a single organization this
can be managed by the organization or a
third party and finally you have the
option to Hell a cab this represents the
hybrid cloud
a hybrid cloud is a combination of the
functionalities of both public and
private clouds
next let's have a look at the service
models there are three major service
models available es pass and SAS
compared to on-premise models where
you'll need to manage and maintain every
component including applications data
virtualization and middleware cloud
computing service models are hassle free
is refers to infrastructure as a service
it is a cloud service model where users
get access to basic Computing
infrastructure they are commonly used by
it administrators if your organization
requires resources like storage or
virtual machines is the model for you
you only have to manage the data runtime
middleware applications and the OS while
the rest is handled by the cloud
providers next we have pass
pass or platform as a service provides
Cloud platforms and runtime environments
for developing testing and managing
applications this service model enables
users to deploy applications without the
need to acquire manage and maintain the
related architecture
if your organization is in need of a
platform for creating software
applications pass is the model for you
pass only requires you to handle the
applications and the data the rest of
the components like runtime middleware
operating systems servers storage and
others are handled by the cloud service
providers
and finally we have SAS SAS or software
as a service involves cloud services for
hosting and managing your software
applications
software and Hardware requirements are
satisfied by the vendors so you don't
have to manage any of those aspects of
the solution
if you'd rather not worry about the
hassles of owning any it equipment the
SAS model would be the one to go with
with SAS the cloud service provider
handles all components of the solution
required by the organization what is
cloud computing as we learn what is
cloud computing we will also be learning
about how things were before cloud
computing and benefits of cloud
computing different types of cloud
computing available and some of the
famous companies that are using cloud
computing and the getting benefited out
of it we're going to learn all that
before cloud computing existed if we
need any it servers or application let's
say a basic web server it does not come
easy now here is an owner of a business
and I know you would have guessed it
already that he's running a successful
business by looking at the hot and fresh
brewed coffee in his desk and lots and
lots of paperwork to review and approve
now he had a smart not only smart
looking but a really smart worker in his
office called Mark and on one fine day
he called Mark and said that he would
like to do business on online in other
words he would like to take his business
online and for that he needed his own
website as the first thing and Mark puts
all his knowledge together and comes up
with this requirement that his boss
would need lots of servers database and
softwares to get his business online
which means a lot of investment and Mark
also adds that his boss will need to
invest on acquiring technical expertise
to manage the hardware and software that
they will be purchasing and also to
monitor the infrastructure and after
hearing all this his boss was close to
dropping his plan to go online but
before he made a decision he chose to
check if there are any alternatives
where he don't have to spend a lot of
money and don't have to spend acquiring
technical expertise now that's when Mark
opened this discussion with his boss and
he explained his boss about cloud
computing and he explained his boss the
same thing that I'm going to explain to
you in some time now about what is cloud
Computing what is cloud computing cloud
computing is the use of a network of
remote servers hosted on the internet to
store manage and process data rather
than having all that locally and using
local server for that cloud computing is
also storing our data in the internet
from anywhere and accessing our data
from anywhere throughout the internet
and the companies that offer those
services are called Cloud providers
cloud computing is also being able to
deploy and manage our applications
services and network throughout the
globe and manage them through the web
management or configuration portal in
other words cloud computing service
providers give us the ability to manage
our applications and services through a
Global Network or Internet example of
such providers are Amazon web servers
and Microsoft Azure now that we have
known what cloud computing is let's talk
about the benefits of cloud computing
now I need to tell you the Cloud
benefits is what is driving Cloud
adoption like anything in the recent
days if you want an I.T resource or a
service now with Cloud it's available
for me almost instantaneously and it's
ready for production almost the same
time now this reduces the go live date
and the product and the service hit the
market almost instantaneously compared
to the Legacy environment and because of
this the companies have started to
generate Revenue almost the next day if
not the same day planning and buying the
right size Hardware has always been a
challenge in Legacy environment and if
you're not careful when doing this we
might need to live with a hardware
that's undersized for the rest of our
lives with Cloud we do not buy any
hardware but we use the hardware and pay
for the time we use it if that Hardware
does not fit our requirement release it
and start using a better configuration
and pay only for the time you use that
new and better configuration in Legacy
environments forecasting demand is an
full-time job but with Cloud you can let
the monitoring and automation tool to
work for you and to rapidly scale up and
down the resources based on the need of
that R not only that the resources
Services data can be accessed from
anywhere as long as we are connected to
the internet and even there are tools
and techniques now available which will
let you to work offline and will sync
whenever the internet is available
making sure the data is stored in
durable storage and in a secure fashion
is a talk of the business and Cloud
answers that million dollar question
with Cloud the data can be stored in a
highly durable storage and replicated to
multiple regions if you want and the
data that we store is encrypted and
secured in a fashion that's beyond what
we can imagine in local data centers now
let's bleed into the discussion about
the types of cloud computing very lately
there are multiple ways to categorize
cloud computing because it's ever
growing now we have more categories out
of all these six sort of stand out you
know categorizing cloud based on
deployments and categorizing cloud based
on services and again under deployments
categorizing them based on how they have
been implemented you know is it private
is it public or is it hybrid and again
categorizing them based on the service
it provides is it infrastructure as a
service or is it platform as a service
or is it software as a service let's
look at them one by one let's talk about
the different types of cloud based on
the deployment models first in public
Cloud everything is stored and accessed
in and through the internet and any
internet users with proper permissions
can be given access to some of the
applications and resources and in public
Cloud we literally own nothing beat the
hardware or software everything is
managed by the provider AWS Azure and
Google are some examples of public Cloud
private Cloud on the other hand with
private Cloud the infrastructure is
exclusively for an single organization
the organizations can choose to run
their own cloud locally or choose to
Outsource it to a public cloud provider
as managed services and when this is
done this service the infrastructure
will be maintained on a private Network
some examples are VMware cloud and some
of the AWS products are very good
example for private Cloud hybrid cloud
has taken things to the whole new level
with hybrid Cloud we get the benefit of
both public and private Cloud
organizations will choose to keep some
of their applications locally and some
of the application will be present in
the cloud one good example is NASA it
uses hybrid Cloud it uses private Cloud
to store sensitive data and uses public
Cloud to store and share data which are
not sensitive or confidential let's now
discuss about cloud based on service
model
the first and the broader category is
infrastructure as a service here we
would rent the service network storage
and we'll pay for them in an hourly
basis but we will have access to the
resources we provision and for some we
will have root level access as well ec2
in AWS is a very good example it's a VM
for which we have root level access to
the OS and admin access to the hardware
the next type of service model would be
platform as a service now in this model
the providers will give me a pre-built
platform where we can deploy our codes
and our applications and they will be up
and running we only need to manage the
codes and not the infrastructure here in
software as a service the cloud
providers sell the end product which is
a software or an application and we
directly buy the software on a
subscription basis it's not the infra or
the platform but the end product or the
software or a functioning application
and we pay for the hours we use the
software and in here the client
maintains full control of the software
and does not maintain any equipment
Amazon and Azure also sell products that
are software assets this chart sort of
explains the difference between the four
models starting from on-premises to
infrastructure as a service to platform
as a service to software as a service
this is self-explanatory that the
resource managed by us are huge in
on-premises that towards your left as
you watch and it's little less in
infrastructure as a service as we move
further towards the right and further
reduced in platform as a service and
there's really nothing to manage when it
comes to software as a service because
we buy this software not any
infrastructure component attached to it
there are lots of famous well-known
organizations around the world they have
moved or have migrated to the cloud
environment painting rest is one company
which is one of the world's largest
visual book marketing tool with more
than 70 million monthly active users now
Pinterest was able to scale its business
because it was all built on Amazon web
services with a company with a less
number of employees Pinterest decided
that they do not want a dedicated staff
time to manage a data center instead
Pinterest uses AWS or they used AWS to
manage their high performance social
application and store more than 8
billion objects and 400 terabytes of
data in AWS Cloud using a service called
Amazon simple storage service or in
short Amazon S3 and not only that they
are running 2 lakh 25 000 instance hours
a month and they run that on Amazon
compute service called Amazon ec2 or
Amazon elastic compute Cloud Spotify is
an another company that got benefited
using AWS it's an online music service
offering and it offers instant access to
over 16 million licensed songs Spotify
needed a peculiar requirement it needed
a storage solution that could scale very
quickly without incurring long lead
times for upgrades long story short
Spotify used Amazon S3 which is Amazon
simple storage service and Amazon S3
gave them the confidence in their
ability and to expand their storage
quickly while also providing High data
durability the third biggest company I
want to mention is Netflix it's the
world's largest internet television
network with more than 100 million
members in more than 190 countries
Netflix uses Amazon web services for
nearly all its Computing and storage
needs now that includes database
analytic recommendation Engines video
transcoding and a lot more not only that
this company is planning to use AWS
Lambda to build an automated or an
rule-based self-managing infrastructure
and replace some of their inefficient
process to reduce the rate of errors and
replace inefficient processes to reduce
the rate of errors and save their
valuable time Expedia is another famous
company that got benefited by using
Cloud Expedia provides travel booking
service through its Flagship site called
expedia.com and not only that they also
run 200 plus travel booking sites around
the world Xperia is all on AWS and by
using AWS Xperia has become more
resilient with an high scalable
infrastructure and better cloud services
because it had offloaded the it
management to the cloud provider itself
in this case AWS and because of this
Xperia's developers have been able to
innovate faster while saving the company
millions and millions of dollars types
of cloud computing we can categorize the
different types of cloud computing based
on two wide categories one being a
deployment model and the other one being
service model let's talk about the
deployment model first deployment model
is categorized into three types first
one is public and then private and then
hybrid cloud in other words public Cloud
private cloud and hybrid Cloud it'll be
easy for me to explain and also it'll be
easy for you to understand if I walk you
through this example consider the
different types of vehicles we use to
commute from one place to another for
example if I want to travel I can pick a
bus which is accessible to anyone I get
in and I pay for the seed that I occupy
and I pay for the time that I will be
traveling in it and I'm done cost is
very less here a similar kind of thing
happens in the public Cloud I pay only
for the resource that I use and I pay
for how long I use it if I use less I
pay less if I use more I pay more for
that month simple on the other hand
private cloud is like buying your own
car and using it for commuting purpose
first here I pay a huge amount upfront
and it is all owned only by me I do not
pay for it in an hourly fashion but
completely and all upfront the cost here
is very huge and thirdly if I want the
best of both types like the comfort of
the own car and still don't want to pay
all upfront otherwise one only to pay
for the time that I use the service I
can rent a car similarly I can have it
in an hybrid environment meaning if I
already have a DC I can integrate it
with the cloud and use both the DCS and
that would become an hybrid environment
all right so that was good of in
learning let's summarize the types of
cloud based on deployment models and as
we know now about the public Cloud
public cloud is in Cloud infrastructure
that's made available to the general
public over the internet and it is owned
by the cloud provider some of the major
players as Cloud providers are AWS
Microsoft Azure IBM's blue cloud and Sun
Cloud and private Cloud now this Cloud
infrastructure is exclusively operated
by a single organization it can be
managed by organizations or third party
and may exist on-premises or
off-premises doesn't matter but the
point here is this is exclusively
operated for a single organization and
some companies that provide private
Cloud are AWS and VMware and hybrid
Cloud gives the best of both public and
the private Cloud for example the
federal agencies they opt for private
clouds for storing and developing
personal data and they use public Cloud
to share the non-sensitive data with the
general public or with other government
departments now let's talk about
different clouds based on the service
model if we need to categorize them
broadly we can categorize them as
infrastructure as a service IAS or
platform as a service Pas or software as
a service SAS they sometimes are
referred to as is pass and says now at
this moment you could be like this guy
thinking Sam I thought you're done
categorizing the cloud now you're going
to talk about three more categories
which one should I pick well let me
explain if all that you want is just in
VM and you have all the expertise to
install the software on top of it and
make it work then go for is if you only
want a platform or an interfaced program
or an interface to upload a program and
make it run then pick pass or if all
that you want is a finished product
hosted in the cloud and be able to
access it through the internet then go
for SAS here you get a username and
password for an application and you can
begin to customize the application based
on your needs all right let's talk about
is in a bit more detail I as it gives
basic Computing infra it's based on a
pay for what you use model and some of
the cloud providers who are big players
are AWS Azure and Google and here the
users generally will be it admins in Pas
the provider gives you a platform or a
runtime environment for developing
testing and managing application it's
platform ready you buy the platform you
upload your code and you start working
on it and it allows the software
developers to deploy applications
without running the underlying
infrastructure and as you might have
guessed by now the interesting
candidates who would use Paz is software
developers and in SAS everything is
managed by the vendor be it the hardware
or the software it's managed by the
vendor and we pay for the service and we
pay for it through a pay as you go
subscription model and as you might have
guessed the end users here would be End
customer itself all right let's put
together everything in the same page and
compare and contrast the different types
of service models in this chart it
explains the difference between the four
models starting from on-premises to is
and then pass and says it is
self-explanatory that the resources
managed by us are huge in on-premises
and little less in is and further less
or reduced in Paz and nothing to manage
when it comes to SAS let me also explain
the different types of cloud services
through an example like this let's say
that you have a crush on cake and you're
planning to bake one yourself now let's
look at the options you can have you can
make all the ingredients yourself be the
floor butter and you know put together
and bake the whole thing yourself using
your own oven and you know the needed
water and the rest you get an idea right
everything is yours and that's on
premises all that you use is owned by
you and nothing is managed by the vendor
the other options you can have is buy
the ingredients and mix and bake the
cake yourself now this would be like is
here the infra is managed by the
provider and we get to use it and
customize it the way we want it here the
clouds are service is then shared
responsibility the other options you
still have on hand is simply pick a
phone and order a cake now this is a lot
simpler than the rest we discussed so
far you know it's simply picking the
phone and ordering the cake and pay for
it when it arrives simple and when it
reaches home you will have to arrange
the table garnish the cake if that's
needed and then enjoy the cake it's the
same way with pass just get the platform
in which you would run your code and
upload your code and start running your
application here you and the vendors
still share the responsibility you still
have one more option left that is simply
go out and dine this is a lot lot
simpler that it requires no effort from
us at all you buy the fully finished and
garnished cake and pay for it and walk
out no responsibility on making the cake
it's the same way with SAS we buy the
finished product and pay for the
finished application now let's
understand what do we mean by the cloud
computing architecture
so this diagram represents the cloud
computing architecture where you can see
there is a cloud infrastructure which we
call as a front-end front end means the
infrastructure that is Network facing
that is internet facing and the back end
is where actually the application the
services and the devices are like
storage servers and all along with the
management and the security portals so
primarily the cloud computing
architecture has two components one is
the front end and the other one is the
back end
now with respect to the architecture the
cloud computing is architecture as we
discussed earlier is divided into two
parts front end and the back end it
provides applications and interfaces
that are required for the cloud-based
services
so front end would be anything which is
facing the internet for example the
websites that you deploy they are front
end because they are accessed by the
public over the public network
these applications basically such as web
browser Google Chrome and Internet
Explorer over which you can basically
open up your web applications your
websites and also it includes client and
mobile devices along with some apis
so this is with respect to the front-end
architecture now let's look into the
furthermore concepts related to the
front end so in the front end the cloud
infrastructure consists of a hardware
and the software components that
includes a data storage server
virtualization software Etc
also it provides the GUI we call it as a
graphical user interface to the end
users so that they can perform the
respective task so it's a GUI based
access and those who are not from the
coding background or they don't have
familiarity with the CLI it is an
advantage
now let's understand the back end
architecture
so the back end is basically it manages
all the programs that runs
the application on the front end so
whatever is required to support the
applications for example web
applications which are running on a
front end
would be available in the back end
it has the larger number of data storage
systems and server so it is basically it
comprises of the entire infrastructure
of a cloud provider
and it can also be a software or it can
be a platform as well
So based on the requirement the
application provides output to the end
users in the back end
also it is one of the most important
components in the cloud because it is
you can say a backbone of a cloud
computing architecture
and its task is to provide utility in
the architecture and the few services
that are widely used among the end users
are storage application development
environments and the web services
in the back end with respect to the
storage it maintains and manages any
time of any amount of data over the
internet
some of the examples of storage services
on the cloud are S3 which is called as a
simple storage service Oracle cloud
storage and Microsoft Azure storage
the storage capacity varies depending
upon the service providers available in
the market
and also it allocates specific resources
to a specific task it handles functions
of cloud environment so this is with
respect to the management in the backend
architecture
the management in the backend
architecture helps thin the management
of components like applications tasks
service security data storage and the
cloud infrastructure
and in simple terms it establishes
coordination among the resources
coming to the security aspect of a
back-end architecture
it is an integral part of a crowd
infrastructure because since you it
mostly relies on the storage and the
databases so it has to be more secure it
helps in protecting Cloud resources
system files and the infrastructures
also it provides security to the cloud
servers with virtual firewalls and
results in the prevention data loss
now the different components of the
cloud computing architecture one is the
hypervisor second is the management
software
the third one is the deployment software
fourth one is the network
fifth one is the Cloud Server
and you have the cloud storage
now let's look into what we have with
the hypervisor hypervisor as the name
suggests is a virtual operating platform
and that is basically used by every user
uh it runs a separate virtual machine on
the back end which consists of a
software and the hardware
also it maintain objective its main
objective is to divide and allocate
resources so basically the hypervisor is
primarily used to virtualize the
physical machines so that it can be
shared across with many resources or
many users
the management software its
responsibility is to manage and monitor
the cloud operations so all the
operational tasks are basically done by
the management software
it helps in improving the performance of
the cloud since you can do the admin
task as well for example high security
flexibility full-time access and the
access given to other users who would be
working on the cloud platform
the deployment software consists of all
the mandatory installations and
configurations that are required to run
a cloud service
every deployment of cloud services
performed using a deployment software so
it is primarily used for deploying the
applications or a software so that
basically saves the time of a developer
in terms of deploying the codes
now the three different models which can
be deployed are the software as a
service which we call as a SAS so the
software as a service means it hosts the
software and manage the application of
the end user example is Gmail so Gmail
is an example of a SAS service where it
acts as a mailing service and the users
just have to create their email accounts
and start using the service
the pass is basically a platform as a
service and it helps to develop and
build and deploy the applications on the
software primarily used by the coders
they can concentrate more on their
development activities and less focus on
the infrastructure part
example is Microsoft Azure and the
infrastructure as a service IIs is
basically to provide services on pay as
you go pricing model
coming to the component of a cloud
computing architecture that is Network
so what is with respect to network why
do we need a network so it connects the
front end and the back end together
so the network can be the internal
private network of a cloud computing
provider as well as the public network
and also it allows every user to access
the cloud resources over the internet
it helps users to connect and customize
the route and protocol so let's say an
admin wants a particular IPS to be
blocked so you can specify them and
customize them in the route and the
protocols
it is a virtual server which is hosted
on the cloud computing architecture so
it is just like a virtual networking
that is available on the cloud computing
platform
it is highly flexible secure and cost
effective and it is basically you can
say the backbone in terms of connecting
multiple Services together
the cloud storage is basically where
every data is stored and that can be
accessed by a user from anywhere over
the Internet of course it depends on how
do you provide the access to it
it is scalable at runtime and it is
automatically accessed
and the data can be modified and
retrieved from the cloud storage over
the Internet itself
for a better understanding let's
consider a scenario consider baking
lasagna you have four options you can
either bake it at home or you can go to
a restaurant to eat lasagna or walk into
an event and bake or get it delivered to
your doorstep but how is this related to
cloud computing well let's take a look
we have four boxes we have the
traditional method we have the
infrastructure as a service we have
platform as a service and we have
software as a service in the traditional
method everything is made at home and
all the components are managed by us
that is Kitchen electricity microwave
lasagna sheets then toppings meat and
cooking the lasagna but when it comes to
infrastructure service the main kitchen
appliances are managed by the vendor and
the other things are managed by us so
basically the infrastructure is managed
by the vendor when we look at platform
as service the infrastructure and the
main ingredient for the software is
managed by the vendor and the other
things are managed by us but when we
look at software as a service everything
from the infrastructure till the end
product is managed by the vendor now
let's get into brief of all this when we
look at the traditional method
everything is done from the scratch from
choosing the right ingredients to the
mode of preparation and you have the
full control of the toppings this is
similar to traditional method where all
the hardware and software components are
built by a choice of requirements when
you look at infrastructure as a service
we can say kitchen electricity and
microwave is the infrastructure and this
is where the code runs the next step is
to alter the software as per our
requirements when you look at platform
as a service it's similar to going to
event and baking so you have the main
ingredient that is the lasagna sheet and
then you have all the appliances that is
Kitchen electricity and microwave and
the rest is managed by us you can either
put toppings or you don't want to put
meat or you want it to be Veg it's all
up to us so the components are altered
as per our requirements finally in
software as a service the entire thing
is modeled by the vendor so this is
where you just given your requirements
and you get the software delivered to
you it's basically where the deployment
and framework of the project is already
set now that we have a brief of all
these Services let's dive deep into each
one of them first we have infrastructure
as a service infrastructure cloud
provider gives a variety of
infrastructures such as storage Services
Network hardware and so on it also
maintains and supports these
infrastructures customers can access
these resources over the internet
next let's look at the benefits of
infrastructure as a service
the first feature is that these
resources can easily be scaled up and
down next the cost depends on the
consumption so basically it's a pay as
you go pricing and you pay for what you
use you pay for the services you use a
single piece of server can give out a
lot of information to many users and
finally the client has complete control
over the architecture now let's look at
the pros of infrastructure as a service
model firstly it is highly flexible this
is because only the infrastructure is
provided and the rest depends on the
customer requirements next like I said
before it is cost effective and you pay
for what you use it is easy to use as
all the updates are deployed and all the
hardware is deployed automatically
management tasks are virtualized so that
the other employees have time for other
tasks now let's look at few of it's gone
it is a multi-tenant architecture due to
this there is an issue with data
security when a new infrastructure is
introduced team training is required to
learn all about this new infrastructure
and it consumes a lot of time if in case
the server crashes at the vendor site
the customers cannot access the data for
a while and they would have to wait for
quite a lot of time until the vendor
fixes this issue now that we know what
infrastructure as a service is let's
look at platform as a service platform
as a service cloud computing platform is
a developer programming platform which
is used for the programmers to develop
test and run and manage the applications
a developer drives the applications and
deploys it directly into this layer all
the infrastructure to run the
applications will be over the Internet
let's look at the benefits now firstly
the resources can be scaled Up and Down
based on the requirements of the user
multiple users can access the same
application it allows for testing and
hosting apps in the same environment the
web services and databases and servers
are integrated into one and finally the
teams can collaborate very easily next
let's look at the pros of of paas
firstly the development process is quick
and easy as it is a developer
programming platform it is cost
efficient you only pay for those
Services you use when you use platform
as a service the coding is done by the
developer before so less coding is
required and then the migration to
hybrid cloud is very easy next let's
look at few of the cons of platform as a
service similar to iaas data security is
an issue because it has a multi-tenant
architecture there's a compatibility
issue with the existing infrastructure
pass is dependent on the vendor speed
reliability and support now that we know
about platform as a service let's go
ahead and dive deep into software as a
service in software as a service
everything is done by the vendors the
end users are only responsible to give
their requirements and everything is
done by the providers now let's go ahead
and look at its benefits the
installations and updations are done by
the providers resources are scaled Up
and Down based on the requirements of
the users the only requirement is that
there has to be a strong network
connectivity and finally the provider is
responsible for everything now let's go
ahead and look at the pros of SAS
upgrades are automatic firstly then next
it is again a pay as you go model it is
easier to customize in SAS rather than
the other service models it is
accessible from any location the only
constraint is that you need to have a
strong internet connectivity let's see
the cons of SAS the provider has entire
control the end users only have the
control of giving the requirements there
are only few solutions for software
crashes the devices should always be
connected for better performance now
that we've seen all the three models
let's see few of the companies providing
these service models the famous IAS
providers are Amazon web services
Rackspace digitalocean linode and
Microsoft Azure these are only a few
there are many move apart from this
Amazon offers many features such as Auto
scaling Cloud monitoring and load
balancing features when it comes to
Rackspace the cloud computing platform
vendor focuses primarily on Enterprise
level hosting Services when you look at
Famous past providers you have heroko
Apache Stratos open shift Microsoft
Azure and many other providers going
ahead in SAS providers the most
important one is Google Apps then
there's Salesforce there is Cisco WebEx
this Dropbox and many more finally let's
look at the end users of the cloud
computing service models we can picture
this as a pyramid so in iaas the end
uses a system admins who are responsible
for maintaining everything except the
infrastructure then in pass the end
users of the developers who code on the
platform provided by the vendors in SAS
the end users are the customers who give
only the requirements and the software
is built based on those requirements by
the vendors
now what exactly is the need of VMware
now VMware is a kind of virtualization
concept which is available there so
let's try to understand it with a small
story over here
so that we will be able to get a good
understanding about what is the overall
purpose of using a VMware here now
before the virtual machines or the
VMware uh I can use the term logins and
virtualization virtual machines VMware
they all are pointing to the same point
so let's assume like we got a developer
who is uh working on the application and
the application is not working uh is uh
is not working as expected now it could
be like uh due to the poor data recovery
poor maintenance poor performance so the
overall benefits or the overall quality
of the product which we are expecting is
not coming up over here
now uh after the VMware what really
happened over here is like we got a
certain Solutions you can use the
virtual machines to host your
applications the maintenance became a
little bit more easy and a little bit
less cost effective so we got like
reduced uh operation operating cost
we got like a increased ID productivity
uh efficiency and Agility there so we
are trying to see like how exactly uh we
are performing the executions we are
performing uh the setup here so we also
got like a faster provisioning of
applications so that also something we
are getting over here when we talk about
from the VMware so these are some couple
of benefits which we get uh as a
solution a complete full flush uh
solution from the VMware here so these
are the some solutions which you are
talking about from the VMware
perspective and uh it also helps us to
go for a greater business or continuity
there because it helps us to provide a
high level virtual machines uh in case
any existing virtual machine gets
deleted we can get a new one spin up
within just minutes of time duration
um it helps us to go for a disaster
recovery like where we can go for a Dr
kind of setup also in addition to
production and it also helps us to go
for an easy management of the data
center that's the biggest benefit
because usually we have so many servers
so many infrastructure there so if you
really want to resolve if you really
want to maintain the things in a much
easier way then VMware or virtualization
is a score for that
right so what is the relationship
between the VMware and the
virtualization here now in simple terms
VMware is something which uh we can call
it as a virtualization software yeah
this is something which we can use as in
virtualizer over here so virtualization
uh software is something which generates
a kind of a virtual environment uh just
on top of an a physical infrastructure
the main focus of the virtualization is
to see like how we can create a virtual
set of resources by consuming or by
using the existing resources the
existing physical resources so in case
of virtualization we can make use of the
elements like RAM memory storage and
these can be actually converted into the
virtual resources so and then we can
create a virtual machines which will be
consuming these virtual resources so
that they will be able to get a
full-fledged environment up and running
so VMware is a kind of a tool which
provides us a virtualization a software
and virtualization is a concept in which
we will be uh you using or reusing the
existing physical infrastructure in such
a way that we will be able to
make a most out of our infrastructure
components so that's the main component
or main reason why we are looking
forward for these kind of virtualization
here
right no
so what exactly is a hypervisor here now
hypervisor we talked about the VMware we
talked about virtualization now
hypervisor is a kind of uh tool it's a
kind of a management tool which is uh
available to manage the virtual machines
here the main focus of hypervisor is to
manage like how the virtual machine can
be done right so that's the main focus
of a particular hypervisor here and if
we talk about the uh mechanisms like how
hypervisor is really going on there so
that's where we will be able to also
understand that this is something which
can take up the request from the virtual
machines and then send it to the host
operating system or to the physical
infrastructure so it acts like
intermediate between the host operating
system and the virtual machine so as per
this diagram you can see like we got
this virtual one virtual machine one
virtual machine two and virtual machine
three in virtual machine one you have
the applications you have the binaries
you have the libraries you even have a
guest operating system in machine chain
two you also have the application
binaries guest operating system third
also is having the same thing now we
also have a hypervisor in between the
physical infrastructure and the virtual
environment which actually helps us as a
kind of a layer which will be doing the
collaboration between how exactly these
communication will be performed and the
overall uh communication or the two info
uh request can be processed over here in
this case so this is something which we
will be really going to see like how the
overall setup and the overall
modifications can be are really
performed here so that's one of the
mechanism which is available there like
how we are going for this specific
approach over here
right so virtual uh machine is a very
important component because the
hypervisor is going to manage the
virtual machine only and what we have
into our virtual machine so we basically
have a full-fledged environment you can
talk about like in that you have a guest
operating system you can have an
application running libraries whatever
things you want to store you want to uh
deploy over there you can actually do it
so that's something we can easily get it
over here with the help of this
component so virtual machine is
something which is really helping you to
see like how you can deploy a certain
resources onto this one right so um it's
it's a kind of a virtual environment so
if we deploy it into a physical machine
or physical server then we call it as an
a kind of physical machine or server but
right now it's actually deployed onto a
virtualization or a virtual environment
that's why we call it as in Virtual
machines because it's not physically
available it's present as an on to a
virtual machine here
right and uh whatever applications you
feel application binders libraries
whatever those components you want to
deploy you can actually get those things
deployed as such on this one so that's
the main benefit which you get over here
that with the help of these components
or with the help of uh these uh virtual
machines you can Hast host a lot of
boundaries and a lot of
softwares applications as such on this
one so it could be like you can use
these three virtual machines as a
different different environment so a
single server you have split up into
three environments machine one you can
use it as some kind of a Dev environment
machine two you can use it like a test
environment and machine three you can
use it like a prod environment or a
pre-prod so this really helps you to uh
have most out of your infrastructure now
earlier we used to have only one server
because we are using physical uh
machines but now we are talking about
virtual machines so here we have got
like different different set of
resources being utilized and we can have
multiple virtual machines created and
that's a very good benefit also you can
think about like okay I have just pay
I'm just paying for a cost of one server
and instead of that I'm getting three
different environments you can have it
like as an environment you can have it
like three different applications
deployed so uh it's completely up to you
like how you are going to perform the
customization there but it's all
something which you can handle it at the
level of virtualization that's a main
focus which we are trying to explain
over here
right and like have I have already
explained over here hypervisor is
something which is uh going to deploy to
see like how uh the specific
modifications and the changes can be uh
really deployed over here in this case
right so this is a very important thing
to be considered because it's uh
something which where we will be able to
see like how uh we can go for the
overall implementation of this complete
virtualized environment here
right so now next thing is that we have
got like hypervisor also configured in
two ways you can have a hypervisor type
1 which is a kind of hypervisor which is
available there for example hyperkit for
Mac OS hyper-v for Windows and KVM for
Linux so this is something which is
available there for some couple of
operating systems which we have commonly
like Windows Mac OS and Linux so there
we can install these type 1 kind of
hypervisor same so these are actually
deployed on top of the host operating
system
then we have got like the type 2 which
is again a little bit more complex
hypervisors now these are like VMware
and the virtualbox these
also provides you an additional features
like the Taiwan uh hypervisors are
little bit uh limited there because they
are just trying to give you a flavor of
virtual machines but if you are looking
forward for a kind of a production
environment or a kind of a real
environment so in that case the type 2
infrastructure or type 2 hypervisor
makes more sense for you in that
situation so depending on the situations
and depending on the utilization the
selection between the type 1 or type 2
hypervisor can be decided there
right now uh let's say that uh you know
which situations we can use over here
now you can consider an example where
you are running like two applications uh
onto your server and you want it to be
done into complete isolation now
this will basically require a two
operating system because if you want to
deploy the single application on the
same same operating system or the same
server machine uh definitely they cannot
be hosted isolated there so the only
thing only reason why we can have a
complete or total isolation is by a
splitting by having two different guest
operating systems there so in that case
only you can go for a true isolation on
this case right so this is the main
benefit like you know you can have a
different environments created onto the
virtual environments and uh you can have
uh create a total isolated environment
and that's the biggest benefit so you
save the cost you have saved the
infrastructure you have made the uh
particular maintainers easy on top of
that you made the proper a true
isolation for each and every environment
so these are the main thing which is
being done by the hypervisor so let's go
ahead with the benefits of hypervisor
again one by one cost efficient that's
totally correct because when we talk
about hypervisor it's just telling that
we are converting the physical machines
or physical servers in case of virtual
machines so which means as for the
previous diagram we were converting a
single machine into three different
virtual machines so you're getting most
out of your infrastructure and at the
same time that reduces the cost also now
rather than going for three different
servers you are just using the single
server as in 3 virtual machines to come
up with that environment so that's the
reason why it's uh kind of a cost
efficient over here
flexibility that's great you have got a
create flexibility over here in order to
work with how the maintenance of these
virtual machines can be done you want to
create the virtual machines you want to
delete the virtual machines you want to
do this top start all that activities
you can actually do it with the help of
hypervisor here so that's a great
flexibility which we get or which we are
getting from the hypervisor here
portability these virtual machines which
we are creating in VMware it's not like
I cannot use it in another system if I
have the VMware into one machine and I I
have another virtual VMware in another
environment I could literally use the
existing virtual machines there also so
it helps me to save my time because I
don't have to rebuild my virtual
machines again and again I can easily
Port it from one environment to another
environment so I can do a clone also
like I have got one virtual machine and
I want to clone it like into five six
virtual machines I can do that also so
it it can be extended to any level so
these are all possibilities only
possible by the hypervisor
easy setup of maintainers a single
software maintaining all the activities
all the calls between the physical and
the virtual environment so you don't
have to do any kind of Maintenance as
such over there the backup restoration
all those features are already there in
the hypervisor
better resource allocation so you are
using 100 utilization of your resources
and you're getting max out of your
existing resources so the wastage of
resources is also quite less when we
talk about the hypervisor or the
virtualization here
so now what are the different components
which we get with the virtualization
like how exactly the virtual
virtualization gets into the work
so we have got like three components CPU
memory and Storage
now what is the CPU all about so CPU is
something where exactly we are getting
the cores the internal process of the
system so the virtual machines when we
create onto our physical environment so
these cores actually got uh shared to
the virtual environment also so
hypervisor is capable of taking or
having a virtual CPU created out of this
physical CPU there but it's not like
that that if you are having four four
cores or four CPU you can create eight
virtual machine of One Core each it's
not like that you cannot multiply that
the number of cores which you can use
for the virtual machine should always be
less than what you have physically
available there so that's the true
virtualization which we are talking
about
of course um you know these cores can
actually be a shared across multiple
virtual machines because uh let's say
like I have got two cores and I can
create three virtual machines so uh
definitely these two cores can be shared
across these three virtual machines but
you have to limit the number of cores
because it's not like you want to expose
your host operating system host system
and uh you should be into the limits
when you allocate these cores or these
CPUs and memories
right so uh the hypervisor is actually
taking all the requests from the virtual
environment and then it send it to the
CPU there so the communication between
the virtual machine and the host is not
Direct in fact it's through the
hypervisor
now virtual machine is just a ramp in
the in case of the system in case of the
hardware term lodges when we use the
virtual machine it actually requires
certain memory also so we can get it uh
there from the host memory just like in
CPU here also the limit applies so if we
have the host machine of 8GB Ram we
could use closely to 6 GB of RAM over
there so that we can have one or two GB
left out for the host environment also
so we should not be compromising the
host environment the number of virtual
machines which we can create on a
particular server or a piece of Hardware
is also limited by the resources which
is allocated physically there so that's
something which we need to take care on
that
so the memory size is available there so
you can uh accordingly see like what
kind of resources you have and then
according to that you can allocate it to
the virtual machine so you need to be
careful about that how many resources
you actually allocate to a specific
virtual machine there so that should
always be in a particular limit and this
is something which we can actually
modify whenever we feel we can modify it
also right so that's the main reason
that's the main resource component which
is available there which we can use then
we have the storage ultimately after CPU
and memory it comes the storage also so
we have this as a different storage
devices which we can use with the
virtualization with the virtual machines
here so uh using the uh VMware or any
kind of hypervisor which you are using
you can actually manage the storage also
like what where exactly you want to
store these files or you want to uh you
know consume the which which file system
you want to consume so all those things
we can actually assign we can configure
over there
and in fact like if you want to increase
or if you want to decrease their uh
specific size so that also we can do
there now there are some couple of
extensions which is available there now
these extensions May differ from
different hypervisor which you're using
so vdi vhdx then we have got vmdk then
we have got like HDD these are the
different uh uh kind of files extensions
which is available there when we talk
about the storage like where exactly the
virtual machines are getting stored so
these are getting stored in these uh
different different subsequent
extensions of the files here so these
are the different components which is
available there and of course uh
depending on different hypervisors these
configurations or these parameters may
change over a period of time but uh it's
something which we can see like how
these things can be really configured
here we are going to talk about like how
we can create a new virtual machine with
the help of VMware hypervisor VMware
Workstation and also we are going to
talk about like if you have an existing
virtual machine how we are going to do
that clone activity over here so this is
the official page of VMware where we can
download the VMware Workstation 16
player here now you can have this uh
particular software for Linux and
windows platform both there so let's go
for the windows platform so I have
already got this installed over here so
you can see like in my downloads it's
going to show me like this player is
available there so it's close to 200 and
250 MB package which is available there
and all we need to do is like we need to
install this package as such so I'm
going to just open this installer here
so
it's going to be installed into my
system so once the installation is done
I will be able to have this specific
hypervisor over here installed and then
I can use it for implementing the
virtualization into my system so all I
need to do is like I need to just
install this software it may take a
little bit more time because it's in a
heavy software which is there and it's
doing a lot of installations components
there so that's what it's happening
there and apart from the installation
over here in this case like apart from
installing this uh particular hypervisor
or VMware software virtualization
software I also have to go ahead with
the installation of an ISO image and a
kind of image file I require whatever
operating system like if you want to go
for a Windows operating system
deployment or you want to go for Ubuntu
Linux any kind of platform if you feel
like you want to install so in that case
you have to actually go ahead with the
particular installer so that is
something which we are looking forward
and we are trying to see like how uh we
can download that ISO image
um if it's Windows you can download it
from Windows website if it's of Ubuntu
sentos anywhere you can do it so what
I'm going to do is like I'm going to
download a specific small kind of a tiny
Linux over here which will be very easy
for us to do the installation I could
easily go for Ubuntu and those kind of
iso or image files also but they will
take like a lot of time for us to do it
and uh for this demo purpose we can
start with the tiny Linux and we can go
for that customized operating system but
when we actually require the
implementation when we feel like we want
to go for a real implementation at that
moment of time we can go for the image
like Ubuntu and all that stuff but it's
all the same only the kind of setup
which you are looking for but it's all
the same only
so my installer is done for the
particular VMware so let's see like how
we can download the essay file so
I can go for an installer like I can
download the image file for my tiny
Linux
so now I'm going to download the
tiny Linux ISO file so this is the
website which is available there now the
tiny core Linux micro Linux there are
different names which is available there
right so you have multiple links which
is available there from which you can
actually download it right so it's a
basic a very small complete version
which is available there so you can just
download it right and this is also this
is the official page which is there for
tiny core Linux this is the distributor
link which is available from which way
you can download this one so let's go
ahead and let's download this ISO file
so core tiny core core plus there are
different kind of images which is there
so I'm just going for the tiny core
Linux over here it's just a kind of a
version which is available there which
is having a little bit in uh like medium
level of executables available so that's
what I'm going to download so it's
actually there
um you can see like it's having a size
of 20 MB alone that's the only size
which is available there so which means
like we will be able to easily download
it and get it on ported onto our virtual
machine here so let's open the VMware
meanwhile so that I can go for the basic
configurations when I want to do that so
let's go with that
so I have opened the workstation 16
player so I'm going for the free
non-commercial use over here otherwise
if you have a license you can go with
that I'm just going for this login here
so this is the application which comes
up over here right so let me maximize it
so here you can actually create and
virtual machine you can open an existing
virtual machine also if you if for some
reason you got the virtual machine
already built up and you want to import
it over here you can simply open it up
there right so I'm going to just uh
create a a new virtual machine here very
simple options go to the player then
file and the new virtual machine here
now in this case you just have to
provide the iso image so I have already
downloaded now so I can just go with the
particular downloads option over here
the tiny core this is the a ISO image
which is available there now it could be
a possibility since it's a very small
compressed one so VMware may not be able
to load it up but the whole idea is like
we just require an ISO image that can be
Ubuntu or that can be anything there now
Ubuntu will take a lot of time because
it's more than one GB of size so I'm
just trying with the basic ISO image so
that we can actually get started uh much
faster over here in this one so which
operating system is this one so this is
like a Linux operating system which is
available there and uh
we can see into our options here uh if
anything related to Tiny Linux is
available there but if it's not there so
we can just select for the uh Ubuntu 64.
that's what we can select and we can say
like next
whatever the version you have like if
you are going for the specific one
virtual machine you can select the size
accordingly now here we were talking
about the disk size so I'm just giving
620 GB because that's the maximum size I
don't have to worry about uh the
particular size there 20 GB is more than
sufficient if you feel like you want to
create a single file a single disk file
for the complete virtual machine you can
just select with this one if you want to
split it in multiple files so that it
would be easy for you to uh you know it
provides a better efficiency and if in
future you want to transfer the files
from one system to another that will
also be much easier so depending on the
situation you can see like whether you
want to go for a single file or you want
to go for multiple files
and uh the customizations like I said uh
in this one the course everything is
available there so we are allocating
like 2GB but you can see that there is a
maximum recommended memory which is
available there because my system is
having like 6 GB sorry 8GB of RAM
so it's saying that till 6.1 you should
be going for that particular memory
because if we say 8 GB then 8GB means
like your host machine is also
compromised so the maximum recommend
memory is 6 DB now the recommended
memory is 2GB so that's what we are
trying to use here and the processor
definitely we have like 32 cores which
is available there so I can actually use
like whatever the course I want I feel I
can use that according to my uh
particular configuration so I I can
actually use it accordingly and if you
want to enable the virtualization CPU
all these couple of options are there
then ISO image you have already
configured it like uh if you feel like
you can change it you can change it also
uh the network is there you got the
bridge host only there are different
network which is available there usually
Bridge connection will help you to
replicate uh the you know the connection
so it will directly get connected to the
physical Network and you will be able to
you know share the aspects like you can
get the internet activated and it's all
going on in that part mode there so
these are some couple of options which
is available there using which you will
be able to uh component like you can add
it you can configure it if you feel like
you want to add any one of the hardware
type which is not being covered that
also you can do here right so I'll just
have to say close because this is the
configuration which I am looking forward
but pretty much it picked up the decent
uh basic configuration so I can just go
ahead with the finished part
and now if I feel like I want to edit it
I can do that but I can just start the
play button to get the particular
virtual machine started right so as you
can see like it's just downloading some
couple of VMware tools for Linux so I
can just say like download and install
so these are some VMware tools which is
required now these tools are actually
used when you are dealing with the
virtual machine so at that moment of
time these tools are required here now
in this one you have a multiple options
you can see like a boot tiny core you
want to use the boot core which is only
the command line interface so these
couple of options are available there
but I think it's going to wait for close
to 40 50 seconds it's already passed
like 10 50 seconds so after some time it
will automatically go for boot tiny core
but if I can just click on this window
inside and press enter it will start
proceeding with that tiny core here
right and if you feel like you want to
come out of this so you have to do like
Control Alt like it's hovering over here
so right now you are able to see like my
cursor is inside there but if you want
to go for control alt then you will be
back onto the host machine so when you
click inside you will not be able to you
see my cursor is not going outside this
window it's limited over here
so if you press Ctrl alt then only you
will be able to see your cursor back
here
right and uh
you can go into the full maximize mode
again Ctrl alt to come out
right and this is the particular window
which is coming up for you so let this
Tools installation okay it's already
going on so let it get completed so
these are some tools which is getting
installed so that it will became very
easy for you to interact with the
virtual machine so for that purpose this
is getting installed
now um you can do a lot of work from the
power you can actually shut down it you
can restart it you can stop it all that
stuff you can do it any removable
devices if you have so that you can over
the fly can disconnect it you can see
like you can disconnect over here right
and you want to make it uh like a full
screen you want to uh you know do any
kind of virtual machine settings like
here you will be able to see like all
these settings you can again recover it
so these are some couple of options
which is available there now inside this
one so you have the options here like in
the bottom one you have the exit option
editor control panel then apps run
program Mount tool terminal so terminal
is there so you can see like it's a
little bit uh
different terminal which you're getting
but of course like it's more than
sufficient for you to do that so
you can run some couple of
uh components here
now I can
now the output is very small here
because of this particular one but it's
actually being used like it's using a
specific size of the file system which
is being allocated to this one and this
is where my operating system is
basically being used over here in this
case right so
that's how the commands are going to
work
it's it's going to execute some couple
of commands some executions some
commands may not be available there so
you may have to execute that so that's
how it it happens over there
right now if I have to come out control
alt same and I'll just go here say that
uh
let me stop it up so I can just shut
down this virtual machine here
and once this is on so I have to open
the VMware Player again to get it on
there
so this is what we have got over here in
this one this is a kind of virtual
machine which is available there so
again I can edit the configurations
processor hard disk everything is
available there right so if you see this
is a very important aspect about this
one that the maximum size which is being
allocated is 20 GB
now it's not like the complete 20 GB is
being allocated uh yes 2.6 MB is being
allocated right now the current size so
gradually it will be increasing up to 20
GB but yes it's something which is
available and we can now see like how
exactly we can uh you know what amount
of size is being utilized and what is
being used over here
right and this is just in a virtual
machine you can have multiple virtual
machines which you can create here so
it's just a matter of one virtual
machines or 20 Mutual machines you can
just have it all available there and
these are the different prefaces which
is available there which you can perform
and you can go ahead with that
right you can delete uh this one
completely from the specific uh file
system here so if you feel like you want
to completely wipe a top from the file
system so that also you can do here so
these are the different options which is
possible over here just to see like how
exactly we can go ahead with this
implementations all together
right again in the file you have all
these options which is available there
and you can also go for the preferences
which is available there like how what
are the different options which is uh
being provided over here in this case
so that's a mechanism how exactly we
work with the virtual machines uh
Creations we can manage it and we can
get it uh implemented all together if
you are looking for a more detailed
approach on cloud computing with
hands-on experience then simply learns
Caltech postgraduate program in cloud
computing will be the right way to go
this comprehensive program will equip
you with the knowledge and skills needed
to navigate the cloud landscape with
confidence dive deep into the cloud
architecture deployment models security
and migration strategy you will also
explore platforms like AWS Microsoft
Azure and gcp to build your Cloud
expertise so don't miss out on this
chance to transform your career and join
the ranks of successful Azure
professionals click the link in the
description to discover more about this
course what exactly virtualization is
let's take a look at an example so beat
Jake he works as a software developer in
an IT firm and he often has to work on
different projects involving multiple
operating systems according to the
requirements of the project during which
you often come across cases where the
managing of data becomes problematic due
to the compatibility issues with the
different operating systems which makes
it hard for the completion of the
project so what can Jake do to overcome
such a problem
well to overcome such a problem he
decided to use the way of virtualization
but what is virtualization and how does
it help us
well let's take a look
to begin with we'll take a look at what
exactly virtualization is and how it can
help us then we'll understand what
virtual machines are after that we learn
about the role and different types of
hypervisor that are involved during the
process of virtualization after that we
learn what different types of
virtualizations are available and how
differently they affect our systems and
lastly we will see what benefits
virtualization provides us with to begin
with we'll understand what
virtualization is
virtualization is nothing but utilizing
a software to create a virtual layer
over the hardware which allows the
system Hardware to be used more
efficiently and allows appropriate
return for a hardware cost the software
hypervisor also allows the elements of
the system like storage memory processor
and Etc to be distributed among multiple
separate and secure virtual computers
known as virtual machines to understand
the situation of virtualization much
better let's take a look at an example
this is a system installed with the
Windows operating system which is
officially known as the host OS where
the virtualization software known as the
hypervisor will run
and then using the hypervisor software
we can have multiple instances of
different OS including Unix Mac and
Linux which are known as virtual systems
or guest OS
the working of the virtualization is
only possible by using a software known
as a hypervisor and later in the video
we will also learn about the working and
different types of hypervisor involved
now that we have understood what
virtualization is let's take a look at
what a virtual machine is so as the name
suggests virtualization is nothing but
an emulation or a virtual representation
of a physical operating system on a
hardware device
the virtual machines are also known as
guest OS whereas a physical system that
they run on is also known as the host OS
now let's take a look at the software
that makes the virtualization possible
hypervisor is a software layer that
manages the virtual machines
it forms an interface between the
physical system and the virtual machine
which ensures the proper access of the
resources
it also manages the virtual machine so
that they don't interfere with each
other's resources
let's take a look at what different
types of hypervisors are there the first
type of hypervisor is known as the type
1 hypervisor or the bare metal
hypervisor this type of hypervisor
directly interacts with the hardware
system and user resources
the other type of hypervisor is known as
the type 2 hypervisor which runs as an
application on the host operating system
and the hypervisor also coordinates with
the virtual machine for resource
management
let's take a look at the type 1
hypervisor they run directly on top of
the host operating system and utilize
the hardware resources and that is why
they're also known as the bare metal
hypervisor they take up the place of the
host operating system and works as the
own operating system
and since this type of hypervisor Works
directly on the hardware system they are
highly efficient now that we understand
type 1 hypervisor let's take a look at
the type 2 hypervisor
this type of hypervisor doesn't directly
work on the operating system Hardware
but instead it works as an application
in the host operating system where they
are suitable for running individual
systems users can also have different
multiple OS installed in the physical
system by using this type of hypervisor
due to the application-based working of
the type 2 hypervisor they are also
known as virtual machine monitors or in
short vmms
now that we understand what different
types of hypervisors there are let's
take a look at what types of
virtualizations are present
the first type of virtualization is the
desktop virtualization then the network
virtualization storage virtualization
and lastly the application
virtualization let's take a look at them
one by one for desktop virtualization as
the name suggests in this type of
virtualization we can run multiple
operating systems on a single Hardware
system let's take a look at the
different types of desktop
virtualization the first one is virtual
desktop infrastructure or in short vdi
which runs numerous virtual machines on
a central server and then hosted to the
user according to the user's requirement
in this way the user can access any
operating system without having to
physically install the particular
operating system in its Hardware system
the second one is known as local desktop
virtualization as the name suggests it
uses a hypervisor software on a local
system which allows a user to run
multiple operating systems
simultaneously without having to affect
the host operating system
the next type of virtualization is
Network virtualization
in this type of virtualization the
software creates a virtual instance of
the network that can be used to manage
from a single console and it also forms
the abstraction of the hardware
components and functions including
switches routers and Etc which
simplifies the network management
different types of network
virtualization are software-defined
virtualization which virtualizes the
hardware that controls the network
traffic routing and the other one is
Network function virtualization
virtualize is the Hardware Appliances
that provide Network specific functions
easier to configure and manage for
example a firewall let's take a look at
the third type of virtualization known
as the storage virtualization this
virtualization enables all the storage
devices on the system to be accessed and
be managed as a single storage unit the
storage virtualization collects all the
storage into a single pool from which
they can allot another virtual machine
on the same network as required and this
makes it easier to assign storage for
multiple virtual machines with maximum
efficiency and the last virtualization
that we'll discuss is the application
virtualization in this type of
virtualization process the application
runs directly without the need of
installing it into the system as they
run on a virtual environment different
types of application virtualization app
the local application virtualization and
this type of application runs on the
host device but runs in a different
virtual environment but not in the
hardware
the second one is application
virtualization and in this the
application is on the server side and it
sends some of the components to the host
device according to the requirement and
last is the server-based application
virtualization this application runs
directly on the server side and sends
only the interface to the client system
now let's take a look at the benefits of
virtualization the first one is resource
efficiency
as the name suggests before
virtualization each application server
used its own Hardware resources which
were being underused but with having
multiple virtual machines maximum
utilization of the hardware capacity
occurs
then we have minimum downtime which
refers to the crashes of operating
systems and applications which can cause
a halt in the user productivity by using
virtualization the admin can run
multiple similar virtual machines
simultaneously and change over the
working instances in case of a crash
instead of having multiple dedicated
servers
and then we have time management
buying installing and configuring a new
system is not only costly but also a
waste of time
in such a case virtualization can solve
the problem provided that the existing
Hardware resources are sufficient for
running the virtualization software
otherwise it can be configured for the
same next thing let's talk about the
different Cloud providers Amazon web
services AWS is a cloud computing
service provided by Amazon it provides a
mix of infrastructure as a service is
platform as a service pass and package
software as a service called SAS
offerings Microsoft Azure formerly known
as Windows Azure is a cloud computing
service by Microsoft and it sort of
specializes in using Cloud for building
testing deploying and managing the
applications through the service
throughout a Global Network that
Microsoft manages it also provides
software as a service platform as a
service infrastructure as a service and
it supports lots of different
programming languages and tools and
Frameworks including both Microsoft and
third-party software and systems IBM M
cloud is a cloud computing service
offered by IBM IBM Cloud includes
infrastructure as a service software as
a service platform as a service now the
difference is here it offers through
public private and hybrid Cloud delivery
models VMware on the other hand is an
subsidiary of Dell Technologies and
provides cloud computing and platform
virtualization software and services it
was the first commercially successful
company to virtualize the x86
architecture Google Cloud platform on
the other hand is offered by Google it's
a suit of Cloud Computing Services that
run on the same infrastructure that
Google uses internally for its end user
products such as the Google search and
the YouTube you're familiar with
alongside a set of managed tools it also
provides cloud services including
Computing Services data storage Services
data analytics and machine learning
services digitalocean on the other hand
is headquartered in New York City with
data centers worldwide digital ocean
provide provides developers cloud
services that help to deploy and scale
applications that run simultaneously on
multiple computers as of January 2018
digitalocean was the third largest
hosting company in the world in terms of
web facing computers let's talk about
cloud computing in AWS Amazon web
services AWS is a cloud computing
service provided by Amazon and these
services are accessible over the
internet and because AWS provides
infrastructure as a service it's a
flagship offering we can create and
deploy any type of application in the
cloud on top of the is that Amazon
provides and you know the best part here
is the subscriptions are pay as you go
type you use less and pay less and only
for what you have used you use more pay
more but still less per unit for the
service used attractive isn't it now
let's talk about the life cycle of the
cloud computing solution the very first
thing in the life cycle of so solution
or a Cloud solution is to get a proper
understanding of the requirement I
didn't say get the requirement but said
get a proper understanding of the
requirement it is very vital because
only then we will be able to properly
pick the right service offered by the
provider getting a sound understanding
the next thing would be to define the
hardware meaning choose the compute
servers that will provide the right
support where you can resize the compute
capacity in the cloud to run application
programs getting a sound understanding
of the requirement helps in picking the
right Hardware one size does not fit all
there are different services and
Hardwares for different needs you might
have like ec2 if you're looking for is
and Lambda if you're looking for
serverless computing and ECS that
provides container rest service so there
are a lot of Hardwares available pick
the right Hardware that suits your
requirements the third thing is to
define the storage choose the
appropriate storage service where you
can backup your data and a separate
storage service where you can archive
your data locally within the cloud or
from the internet and choose the
appropriate storage there is one
separately for backup called S3 and
there is one separately for archival
that's for glaciers so you know you
knowing the difference between them
really helps in picking the right
service for the right kind of need
Define the network Define the network
that securely delivers data video and
applications Define and identify the
Network Services properly for example
VPC for Network Route 53 for DNS and
direct connection for private P2P line
from your office to the AWS data center
set up the right Security Services IM
for authentication and authorization and
KMS for a data encryption at rest so
there are variety of security products
available we got to pick the right one
that suits our need and there are a
variety of deployment and Automation and
monitoring tools that you can pick from
for example Cloud watch is for
monitoring Auto scaling is for being
elastic and cloud formation is Define
the management process and tools you can
have complete control of your Cloud
environment if you define the management
tools which monitors your AWS resources
and all the custom applications running
on AWS platform there are a variety of
deployment Automation and monitoring
tools you can pick from like cloudwatch
for monitoring Auto scaling for
Automation and the cloud formation for a
deployment so knowing them will help you
in defining the life cycle of the cloud
computing solution properly and
similarly there are a lot of tools for
testing a process like code star and
code build and code pipeline these are
tools with which you can build a test
and deploy your code quickly and finally
once everything is set and done click
the analytics service for analyzing and
visualizing the data using the analytics
Services where we can start querying the
data instantly and get a result now if
you want to visually view the happenings
in your environment you can pick a
antenna and other tools for analytics
are EMR and which is elastic mapreduce
and Cloud search all right enough of
theory and let's have a quick look at
how two services in AWS ec2 and S3 work
together and benefits us here are two ID
professionals talking to each other one
says I have an application which takes a
lot of storage and works only on Linux
system which I do not have at the moment
and the other one is a smart guy and he
immediately replies that he could use S3
to store data and retrieve data and use
ec2 for all its compute needs and then
the Curious conversation builds up and
the first person wants to know what is
easy to and S3 and the second guy starts
to explain that AWS ec2 is a web service
that provides a secure and resizable
compute capacity in their cloud and ec2
it can also be used to launch as many
virtual servers as we need and about S3
he explains AWS S3 is a simple storage
service provided by AWS and about S3 he
explains further saying that using
Amazon S3 we can store and retrieve any
amount of data at any time on the web
alright so the very first thing is to
have an AWS account so create an AWS
account and second thing is to create an
AWS S3 bucket and upload the files there
you know the files that ec2 server is
going to pull from will get stored in S3
as first thing all right so the first
thing is to create an S3 bucket so let's
create an S3 bucket and let's call it as
a website bucket and these names will
have to be unique so let's see if this
bucket name is available it says pocket
name already taken or already exist so
let me top it up with SL meaning simply
learn all right so we were able to
create a bucket and in that bucket let
me upload a Content that basically is
going to be my index file that's going
to go and set it in my web server so let
me also make it public and view it
alright so this is the content that I
have stored in my S3 and this is the
content I'm going to push to my ec2
server right Second Step create an AWS
S3 bucket and upload files we're done
with that third is to create an ec2
instance that's in other words create an
virtual machine in the cloud let's go
and create it alright the third thing is
to create an ec2 instance so let's
create an ec2 instance it's plain and
it's very simple I'm going to keep
everything as defaults here alright so
I've launched an ec2 instance as you see
it's running I also have an IP address
public IP address here with that I have
logged into this ec2 instance and I'm in
the folder slash War slash
www.html and as you see there is nothing
in there at the moment and This Server
also has Apache installed in it as you
can see and now I'm browse to the IP
address on the ec2 instance and it's
showing a page which is the default
Apache page all right if I had any files
in this folder that would get shown as
the web page instead of the default page
our task is to save files in S3 and move
them to the ec2 instance so S3 is going
to act as a storage or a repository or a
source code control in this example all
right so let's do that all right the
third step is to create your ec2
instance and that's what we have done
and the ec2 instance has no files in it
and the fourth step is to synchronize
the source code bucket with the easy to
instance let's do that the actual
command to do that would be AWS S3 sync
and then the name of the bucket from
which we're going to pull the code and
the folder in which we're going to put
the code or the data all right so it has
downloaded something look at that and
now if we go back to the test page and
do refresh there you go it's showing the
page page that it pulled from S3 so here
I can use S3 as the source code control
bucket and any information that I put in
there will get reflected in ec2 instance
once I do a sync every day and that's
how I use S3 as my storage for ec2 all
right finally we were able to
synchronize S3 bucket and the ec2
instance and we were also able to view
the results in the web browser of the
ec2 instance the data was copied from S3
to the ec2 instance and that we were
able to view from the web browser meet
Rob he runs an online shopping portal
the portal started with a modest number
of users but has recently been seeing a
surge in the number of visitors on Black
Friday and other holidays the portal saw
so many visitors that the servers were
unable to handle the traffic and crashed
is there a way to improve performance
without having to invest in a new server
wondered rob a way to upscale or
downscale capacity depending on the
number of users visiting the website at
any given point well there is Amazon web
services one of the leaders in the cloud
computing Market before we see how AWS
can solve Rob's problem let's have a
look at how AWS reached the position it
is at now
AWS was first introduced in 2002 as a
means to provide tools and services to
developers to incorporate features of
amazon.com to their website in 2006 its
first Cloud Services offering was
introduced in 2016 AWS surpassed its 10
billion Revenue Target
and now AWS offers more than 100 cloud
services that span a wide range of
domains thanks to this the AWS cloud
service platform is now used by more
than 45 percent of the global market now
let's talk about what is AWS AWS or
Amazon web service is a secure cloud
computing platform that provides
computing power database networking
content storage and much more
the platform also works with a PSU go
pricing model which means you only pay
for how much of the service is offered
by AWS you use
some of the other advantages of AWS are
security AWS provides a secure and
durable platform that offers end-to-end
privacy and security
experience you can benefit from the
infrastructure management practices born
from Amazon's years of experience
flexible it allows users to select the
OS language database and other services
easy to use users can host applications
quickly and securely
scalable depending on user requirements
applications can be scaled up or down
AWS provides a wide range of services
across various domains what if Rob
wanted to create an application for his
online portal AWS provides compute
services that can support the app
development process from start to finish
from developing deploying running to
scaling the application up or down based
on the requirements the popular Services
include ec2 AWS Lambda Amazon light cell
and elastic Beanstalk for storing
website data Rob could use AWS storage
services that would enable him to store
access govern and analyze data to ensure
that costs are reduced agility is
improved an innovation accelerated
popular services within this domain
include Amazon S3 EBS S3 Glacier and
elastic file storage
Rob can also store the user data in a
database with aw Services which he can
then optimize and manage popular
services in this domain include Amazon
RDS dynamodb and redshift if Rob's
businesses took off and he wanted to
separate his Cloud infrastructure or
scale up his work requests and much more
he would be able to do so with the
networking Services provided by AWS some
of the popular networking Services
include Amazon VPC Amazon Route 53 and
elastic load balancing other domains
that AWS provide services in are
analytics blockchain containers machine
learning internet of things and so on
and there you go that's AWS for you in a
nutshell
foreign
let me introduce myself as Sam a
multi-platform cloud architect and
trainer and I'm so glad and I'm equally
excited to talk and walk you through the
session about what AWS is and talk to
you about some services and offerings
and about how companies get benefited by
migrating their applications and infra
into AWS so what's AWS let's talk about
that now before that let's talk about
how life Works without any cloud
provider and in this case how life was
without AWS so let's walk back and
picture how things were back in 2000
which is not so long ago but a lot of
changes a lot of changes for better had
happened since that time now back in
2000 a request for a new server is not
an happy thing at all because a lot of
money a lot of validations a lot of
planning are involved in getting a
server online or up and running and even
after we finally got the server it's not
all said and done a lot of optimization
that needs to be done on that server to
make it worth it and get a good return
on investment from that server and even
after we have optimized for a good
return on investment the work is still
not done there will often be a frequent
increase and decrease in the capacity
and you know even news about our website
getting popular and getting more hits
It's still an Bittersweet experience
because now I need to add more servers
to the environment which means that it's
going to cost me even more but thanks to
the present day Cloud technology if the
same situation were to happen today my
new server it's almost ready and it's
ready instantaneously and with the Swift
tools and technologies that Amazon is
providing uh in provisioning my server
instantaneously and adding any type of
workload on top of it and making my
storage and server secure you know
create getting a durable storage where
data that I stored in the cloud never
gets lost with all that features Amazon
has got our back so let's talk about
what is AWS there are a lot of
definitions for it but I'm going to put
together a simple and a precise
definition as much as possible now let
me iron that out Cloud still runs on an
Hardware all right and there are certain
features in that infrastructure in that
cloud infrastructure that makes cloud
cloud or that makes AWS a cloud provider
now we get all the services all the
Technologies all the features and all
the benefits that we get in our local
data center like you know security and
compute capacity and databases and in
fact you know we get even more cool
features like content caching in various
global locations around the planet but
again out of all the features the best
part is that I get or we get everything
on a pay as we go model the less I use
the less I pay and the more I use the
less I pay per unit very attractive
isn't it right and that's not all the
applications that we provision in AWS
are very reliable because they run on a
reliable infrastructure and it's very
scalable because it runs on an on-demand
infrastructure and it's very flexible
because of the designs and because of
the design options available for me in
the cloud let's talk about how all this
happened AWS was launched in 2002 after
the Amazon we know as the online retail
store wanted to sell their remaining or
unused infrastructure as a service or as
an offering for customers to buy and use
it from them you know sell
infrastructure as a service the idea
sort of clicked and AWS launched their
first product first product in 2006 that
works like four years after the idea
launch and in 2012 they held a big sized
customer even to gather inputs and
concerns from customers and they were
very dedicated in making those requests
happen and that habit is still being
followed it's still being followed as
reinvent by AWS and at 2015 Amazon
announced its Revenue to be 4.6 billion
and in 2015 through 2016 AWS launched
products and services that helped
migrate customer services into AWS well
their web products even before but this
is when a lot of focus was given on and
developing migrating services and in the
same year that's in 2016 Amazon's
revenue was 10 billion and not but not
the least as we speak Amazon has more
than 100 products and services available
for customers and get benefited from all
right let's talk about the server
services that are available in Amazon
let's start with this product called S3
now S3 is a great tool for internet
backup and it's it's the cheapest
storage option in the object storage
category and not only that the data that
we put in S3 is retrievable from the
internet S3 is really cool and we have
other products like a migration and data
collection and data transfer products
and here we can not only collect data
seamlessly but also in a real-time way
monitor the data or analyze the data
that's being received that they're cool
products like AWS data transfers
available that helps achieve that and
then we have products like ec2 elastic
compute Cloud that's an resizable
computer where we can anytime anytime
after the size of the computer based on
the need or based on the forecast then
we have SIMPLE notification Services
systems and tools available in Amazon to
update us with notifications through
email or through SMS now anything
anything can be sent through email or
through SMS if you use that service it
could be alarms or it could be service
notifications if you want stuff like
that and then we have some security
tools like KMS key management system
which uses AES 256-bit encryption to
encrypt our data at rest then we have
Lambda as service for which we pay only
for the time in seconds seconds it takes
to execute our code and we're not paying
for the infrastructure here it's just
the seconds that the program is going to
take to execute the code for the short
program we'll be paying in a
milliseconds if it's a bit bigger
program we'll be probably paying in 60
seconds or 120 seconds but that's a lot
cheap lot simple and lots cost effective
as against paying for service on an
hourly basis which a lot of other
services are well that's cheap but using
Lambda is a lot cheaper than that and
then we have services like Route 53 a
DNS service in the cloud and now I do
not have to maintain and DNS account
somewhere else and my cloud environment
with AWS I can get both in the same
place all right let me talk to you about
how AWS makes life easier or how
companies got benefited by using AWS as
their it provider for their applications
or for the infrastructure now Unilever
is a company and they had a problem
right and they had a problem and they
picked AWS as a solution to their
problem all right now this company was
sort of spread across 190 countries and
they were relying on a lot of digital
marketing for promoting their products
and their existing environment their
legacy local environment proved not to
support their changing ID demands and
they could not standardize their old
environment now they chose to move part
of their applications to AWS because
they were not getting what they wanted
in their local environment and since
then you know rollouts were easy
provisioning your applications became
easy and even provisioning
infrastructure became easy and they were
able to do all that in push button
scaling and needless to talk about
backups that are safe and backups that
can be securely accessed from the cloud
as needed now that company is growing
along with AWS because of their Swift
speed in rolling out deployments and
being able to access secure backups from
various places and generate reports and
in fact useful reports out of it that
helps their business now on the same
lines let me also talk to you about
Kellogg's and how they got benefited by
using Amazon now Kellogg's had a
different problem it's one of its kind
now their business model was very
dependent on an infra that will help to
analyze data really fast right because
they were running promotions based on
the analyzed data that they get so
they're being able to respond to the
analyzed data as soon as possible was
critical or vital in their environment
and luckily sap running on Hana
environment is what they needed and you
know they picked that service in the
cloud and that sort of solved the
problem now the company does not have to
deal with maintaining their legacy infra
and maintaining their heavy compute
capacity and maintaining their a
database locally all that is now moved
to the cloud or they are using Cloud as
their I.T service provider and and now
they have a greater and Powerful it
environment that very much complements
their business I mean AWS services so
essentially AWS services are compute
storage database migration networking
and content delivery developer tools
management tools Media Services machine
learning analytics security identity and
compliance mobile services
application integration AR and VR which
is augmented reality and virtual reality
customer engagement business
productivity desktop and app streaming
internet offering switches iot
now let's look into
the compute service which is one of the
widely used service on the AWS and what
does the compute service do these
Services help developers build deploy
and scale an application in the cloud
platform that is ec2
Lambda elastic container service elastic
load balancer light sail and elastic
bean stock these are the services which
lie in the compute service section only
one of the most widely used service
within the compute section is ec2 which
stands for elastic Cloud compute it is a
web service that allows developer to
rent virtual machines and helped to
resize the compute capacity so here what
you can do is you can run the virtual
machines and you have the Privileges to
select the type of operating system that
you want uh should be running on your
instances or on your virtual machines
and likewise later you can customize as
per your requirement Lambda is a
serverless compute service it is also
responsible to execute code for a
specific application so those who are
from the development background they can
focus more on creation of a code they
don't have to create a server manage it
instead they can use a Lambda where you
they can deploy their code directly onto
the Lambda server then comes this
storage service now couple of storage
services are S3 Glacier EBS which is
elastic block Storage storage Gateway
now AWS provides web data storage
service for archiving data also its main
advantage is disaster data recovery with
high durability let's look into some of
the essential storage services and one
of the most widely used storage which is
S3 which stands for simple storage
service in the simple storage service
what you need to do is you have to
create a bucket and in that bucket you
have to put the files in it so S3 gives
open cloud-based storage service which
is utilized for online data backup then
comes the ABS which is an elastic block
storage now you can understand EBS as a
virtual hard drive also which attaches
with the ec2 and it provides High
availability storage volume for
persistent data it is mainly used by
Amazon ec2 instances then you have the
database Services AWS database domain
service offers cost efficient highly
secure and scalable database instances
in the cloud and some of the database
services are RDS which is a relational
database service
dynamodb the non-relational or nosql or
nosql database service elastic cache and
Amazon redshift now one of the essential
database service is the dynamodb it is a
flexible no SQL database service which
offers fast and reliable performance
with no scalability issues
it is fast reliable highly scalable and
suitable for small scale applications
like mobile applications gaming
applications or anything with respect to
the Internet of Things devices there the
dynamodb is uh most widely used or
suitable then comes the relational
database service that is the RDS which
is a structured database service it is a
managed distributed relational database
cloud service that helps developers to
operate and scale database in a simple
manner so RDS has different vendors
platform with respect to the database
usage and that includes the postgresql
MySQL then you have Oracle Microsoft or
Ms SQL and they have their own
customize
database as well that is called as the
Amazon Aurora
along with that they have a Maria
database or Maria DB as well
so these are couple of vendors that give
their database engines that you can use
on the RDS now coming to the networking
Services it offers a highly secure Cloud
platform and helps in connecting your
physical Network to your private virtual
network with high transfer speed
now some of the services in the
networking and content delivery are VPC
which is a virtual private Cloud a very
important service in order to make your
applications or Services more secure
Route 53 which is a DNS mapping Service
Direct Connect which directly connects
with the AWS services and with your data
centers
and the cloud front that is basically a
Content delivery service now coming to
the VPC or a virtual private Cloud it
helps a developer to deploy AWS
resources such as Amazon ec2 instances
in a private virtual Cloud so that you
can actually make your ec2 isolated or
make it more secure and even you can
make it for a public access also depends
on the administrator that how they want
to customize it so the complete control
of a VPC and its networking is with the
admins then comes the Route 53 service
it is a web service with highly
available domain name system or the DNS
that helps user to Route software by
translating text into IP address and
that is why it is called as a DNS
mapping service and that helps you to
use your domains or the external domains
pointed to the AWS services in case if
you use AWS for hosting your websites or
the applications note DNS translates
text into the IP address now coming to
the developer tool services it helps a
user build deploy and run an application
source code automatically it also
updates the server and instance on the
workload so first is the code star code
build code deploy code pipeline so code
star it is a service designed to manage
application development at a single
place
here developers can quickly develop
build and deploy applications on AWS
so all the manage app development can be
done with the code star code build
removes the hassle of managing physical
servers and helps developer build and
test code with continuous scaling so
security identity and compliance
services
helps in monitoring a safe environment
for your AWS Resources by providing
limited access to specific users so in
case let's assume that you have to give
an access to someone but with limited
privileges you can primarily use IIM in
that case and if you want to make your
applications or
deployments more secure then you can use
these services like KMS IM Cognito
Waf which acts as a firewall now the IM
service which is the identity access
management is a framework that helps in
maintaining access to AWS services in a
secure way so what happens is that the
admin who has the complete access of the
AWS console provide access to users and
there can be different users and they
would have the privileged accesses
defined by the Admin so what type of
permissions the admin gives them they
would have those limited access on the
AWS console KMS enables users to create
and manage the encryption keys that are
used for encrypting data coming to
management tool services with the help
of management tools using the service an
individual can optimize cost minimize
risk and automate all the resources
running on the AWS infrastructure
efficiently so with the management tools
you can
monitor the resources application it's
its tools and the utilizations and along
with that you can scale up scale down
the resources likewise with the help of
the management tools you can also do the
auditing task so one of the essential
services in the management tool services
the cloud watch
it is a monitoring tool for AWS
resources and customer applications
running on AWS platform so let's assume
that you have used ec2 dynamodb S3 RDS
and you want to monitor those resources
you can use the cloud watch that can
give you the results coming to the cloud
formation
this service helps you in monitoring all
your AWS resources at one place so that
you can spend minimum time in managing
those resources and maximum time on
developing the application so with the
help of the cloud formation you can
deploy the entire solution with the help
of creation of a template you just need
to create one template and you have to
deploy it the rest of the things will be
done by the AWS and hence it is a kind
of an automation task only now let's
look into the demo of some of the
essential services
so we'll start with the ec2 and I have
already logged in into my AWS account so
where exactly you can find the ec2 just
click on the services
under the compute section you can find
the ec2 ec2 stands for elastic Cloud
compute service
primarily used for creating the virtual
machines
so I'll click on the service and quickly
I'll show you how the virtual machine is
created and how we can basically access
it
so I would be creating one virtual
machine or an instance with the Windows
operating system
so here you can see there are three
instances which are already running let
me create another one
we have to click on launch instances
and here you have to select some
configuration details
so most of the configuration details
I'll be taking as default
and wherever it is necessary I'll be
making the changes
so first of all you have to select the
operating system in the form of Ami and
I would be looking for Windows
2016 and then you have to select the
type of instance with respect to
the CPUs number of CPUs and the memory
capacity so I'll go with T2 dot micro
which is a free tier eligible instance
in the configure instance details the
rest of the things we'll keep it as
default as of now
click on add storage
this is a basically
the virtual hard drive or the EVS the
elastic block storage that is attached
with the ec2 instance
so primarily it is giving us 30 GB of
space without any additional cost
we can leave the tags as blank
click on configure security groups and
here you can see that the RDP Port is by
default open
which will actually allow us to RDP or
to have a remote connectivity of our
instance
click on review and launch
now to give an authentication or to
provide an authentication we should have
a key pair with us so that the AWS scan
understand or can tally the key pair and
give us the access
so I already have a key pair created
what I'll do is I'll create a new key
pair for this instance and put a random
name let's say I put it something like
demo
download the key pair
and make sure that you keep your key
pair in a safe and secure place
click on launch instances
now you will see that there is an
instance ID that has been created which
is a number alphanumeric number that is
randomly given by the AWS
you can name your instances
so let's assume let's say we name our
instance as
windows 2016.
click on Save
and
the AWS deploys our instance on a
respective infrastructure
it gives us the IP addresses the public
IP and the private IP which is an
internal IP and we have to access the
instance from the public IP only so now
to access that instance we have to open
up the RDP
put the public IP
click on connect
and when it asks you for the
Authentication
you put a username as administrator
and the password you have to generate by
providing the keep so click on connect
and then you have to click on RDP client
here you have to get a password so click
on get password
browse and when you click on browse you
have to provide demo dot pem file so it
should be in the download section here
itself
just provide that key pair and it is
going to give you the password in the
encrypted format
decrypt your password
copy that
and then provide those details
to the RDP client click on OK
and now it should allow you to login to
the instance let's wait for the windows
to appear so here you can see that we
have
logged in into the windows instance the
windows 2016 screen is available in
front of us if getting your learning
started is half the battle what if you
could do that for free visit scale up by
simply learn click on the link in the
description to know more
demonstration I would be explaining you
about
how to use a storage service
specifically the S3 service which is
most widely used service under the
storage section
now the S3 stands for simple storage
service primarily used for storing the
objects and the files
and what we need to do is we just have
to click on the S3 service under the
storage section
so when you click on S3 service you have
to create the buckets inside the S3
and the buckets are the places where you
keep your folders or you upload the
files that are available on your systems
so
here if you'll notice that the S3
service is a global Service that means
it is irrespective of the region
and the buckets when you create they are
created in a specific region now what is
the benefit
the benefit is that if you create a
bucket in different regions all those
buckets in different regions can be
viewed from a single dashboard
and you don't have to change the region
again and again to view the S3 buckets
so what you need to do is you have to
click on create bucket
and here you have to specify the name of
the bucket so let's say I put something
like demo AWS
and uh
that's the bucket name I'm going to
select now make sure that the bucket
name starts with a lowercase and it
should be always unique now why it
should be unique because since S3 is a
global Service so it may be that
somebody else could be using your
the name that you have provided so it
should be always unique otherwise it
will not be allocated
now you have to select a region and that
would be let's say I go with
the Ohio region
and in the bucket settings
you can basically change and configure
these settings according to your
requirement by default when you create a
bucket
it blocks all the public access from the
bucket so when you have all public
access blocked
no object can be viewed from the S3
Bucket from a public network or from or
by anyone else
and hence uh in order to
view the objects or the other files in
the bucket you have to unblock or
uncheck this option so that first of all
you make the bucket accessible from the
public network likewise you can
customize as per the requirement and
then click on acknowledgment the rest of
the things will keep it as default and
click on create bucket now it says that
the bucket with the same name already
exists so that means it is not a unique
name somebody else might have been using
this name so I'll try to keep it more
unique and I'll try to assign some
number so let's say I put something like
987
and it says it is already existed let's
say 9876
and then click on create bucket now if
it creates a bucket that means we have
been allocated with that bucket name so
let's wait for
wait for a minute to get that bucket
created
so that happens quickly and you can see
there are a couple of buckets already
created here and these buckets can be
viewed in a single dashboard
so some of the buckets are in Mumbai
region some of the buckets are in Ohio
region but they are available in the
same or a single dashboard so I'll open
up the bucket that I have recently
created and I'll try to upload some
documents now what you can do is you can
create a folder also inside the S3 and
when you create a folder you can upload
the objects accordingly otherwise you
can directly also upload the objects so
click on upload and we will upload any
random file from a system onto the sket
so I'll basically file
and then click on upload
now it is uploading my object from my
system onto the S3 bucket
and now it has successfully done so it
says the messages successful now in
order to validate I will click on the
bucket from the S3 service and view this
object
now you can see here is my bucket so
I'll just open up this bucket and these
dnsrecords.csv file is available with us
now the S3 is not only limited till
storing the objects or the files it has
many other functionalities and the
features also like you can enable
versioning you can host a static website
on the S3
along with that you can have a cross
region replication enabled so that you
can have a high availability of your
objects or you can have a redundancy of
your critical objects in different
regions so likewise there are more
features that would be covered up in the
details section of an S3 service now
coming to another
section of the essential services that
is the database
now here you can see I have a database
section in the AWS and it has multiple
service within
the RDS is there dynamodb is their
elastication the other database services
are there
so I would be showing a demo on the
dynamodbeam which is a no SQL database
now
when we say it is a nosql database that
means it is a non-relational database
service where we can create a database
table directly from the web console we
don't require a separate database engine
like in the case of RDS
and in the tables you can insert the
values and view those values directly
from the AWS dashboard itself
so it says Amazon dynamodb is a fast and
flexible nosql database service
primarily suitable for iot and web
gaming and other mobile applications
so what you need to do is it is a
straightforward database service which
is which can be accessed
while creating a table itself and it's a
compute based database service that is
the reason that it is more fast
so click on create table
and what you need to do is you just have
to put a table name so I'll just put
something like test
and
in the partition key so these partition
keys are unique entities so what you
need to do is you have to specify a
partition key so I'll put something like
ID
and the string instead of string I'll
use a number
likewise I can add sort Keys Also let's
say I put a name
and the name should be in the string
format
now these are the unique entries these
are the fixed entries in the table and
after that we are going to put the
attributes and in the attributes the
data will be inserted so what you need
to do is rest of the things will keep it
as default as of now click on create
and
here you can see the table has been
created the test table has been created
now if I click on the items so you would
see that it has the sort Keys available
the ID and the sort key associated with
that that is the name but it does not
have any entry because we have not added
any value or the attributes so how we
can add or insert the values in this
table that can be done many ways
you can enter manually you can use the
help of CLI to enter the large chunk of
data directly upload it onto the
dynamodb table and also you can use the
apis also in order to insert the data
inside the table so what you will do is
we will click on create item
and in the ID we will put some value
let's say number one string let's say we
put something like
we will go with a random name so instead
of putting any name we'll put a value
like ABCD that should be fine and then
we will insert some attributes
so let's say I put
string as Rank and I'll put something
like rank 2.
and
then click on Save
so here you will see
just refresh the database table
close it and open it up again
click on the items
now here you can see the id1 name is
ABCD got the rank two likewise you can
click on create items let's say the
serial number or the id2 name let's say
we put something like we can go with XYZ
any random thing put it up as
number rank
and let's say this particular value got
a rank 3
likewise you can add some more items
string let's say you put something like
a JK
and
you put a rank let's say it's a rank
number one holder rank one holder
right so likewise uh this is just an
example likewise you can add the
attributes as per your need and the
table can be filtered out based on the
attributes also so if you have to search
some values in the dynamodb table so you
can always use filters uh to basically
search the values inside the table apart
from that the dynamodb table has lot of
other functionalities
it can be basically you can have a
backup
of a table created in the dynamodb table
in the Dynamo database and then you can
retrieve or recover the data by
restoring the backups from the dynamodb
this database can be created in the
cluster format also
so these are couple of features that you
can use with the dynamodb
now let's move into the next section
now coming to the networking services so
there are
some of the networking Services which
are very useful and that includes a VPC
which is a virtual private cloud cloud
front root 53 API Gateway Etc
now I will basically demonstrate about
the Route 53 service in this demo so in
the root 53 is basically a DNS mapping
service so what you can do is let's
assume that you are hosting any web
application on the server and you want
to Route the domain traffic onto those
servers you need to have the help of
Route 53 to do that
so what you need to do is you just have
to click on Route 53 service
and from this service you can register
your domains also otherwise if you have
domains purchased from any external site
you can point them to the Route 53 name
servers also so first of all
in the Route 53 you have to create a
hosted zone so there is already one
hosted Zone all created
now when you create a hosted Zone you
have to specify the domain name
so let me show you I have one dummy
domain
this is the domain that has been defined
so what you have to do is when you click
on create hosted Zone you have to put a
domain here you can see example.com
likewise you have to put your own domain
and uh
click on the public hosted Zone
click on create hosted Zone and it is
going to give you four name server now
those four name servers have to be
updated
on
the platform from where you have
purchased the domain
so that is mandatory in order to Route
the traffic to the Route 53 service
right so here you can see I already have
a hosted Zone created for a domain
and it has given me
four name servers
these are the name servers and these
name servers have been updated
in a record set
from where the domain has been purchased
right
once it is done then you have to
route the domain traffic to a server so
what you have to do is you have to click
on create record
and in the create record you have to
specify
the IP address or you can use the Alias
also where the traffic should be routed
to where your application is hosted at
so ideally it is a server details
and it exists with certain
policies also
so you can see some view existing
records so here you can see
this particular domain
is routed to a DNS value which is hosted
in the elastic Beanstalk instead of that
you can put an IP address of the ec2
instance also you can use the S3 URL
also you can use the cloudfront URL also
so likewise what would happen is that
the domain traffic will be routed to the
server where actually the application or
a web application is hosted at
now I am using a routing policy as
simple routing policy that means all the
traffic should be routed to that
particular domain
whereas there are other routing policies
also in the Route 53 that includes a
weighted routing policy which acts as a
kind of a load balancing geolocation
routing policy multi-value answer
routing policy and then you have a
redundancy based routing policy which is
a failover one so likewise you can
select as per your requirement
so what you need to do is you just have
to click on create records uh you have
to specify you have the domain so you
have to specify any uh particular info
you want to put before the domain
otherwise you can leave it as blank and
in the record type let's assume that you
are using an IP address of a server
where your web application is hosted at
so you can basically use a routes
traffic to an ipv for address and put an
IP address make sure that you put a
public IP address or the elastic IP
address attached to your instance in
case if you are not using any particular
IP address or the URL you can use the
Alias also so likewise the records can
be created along with that the Route 53
Services used for domain verifications
also like if you want the email services
on onto your web application then you
can basically verify your domain
directly from the root 53 service you
can get the verification done for the
SSL certificate creation for that also
the records will be directly created
from the root 53 service because it is
actually managing or hosting your domain
now with respect to the security
services
the most widely used service is the IM
which is identity access management
that lies under security identity and
compliance
so in from the IM you can create the
users whom you want to give an access to
your AWS console you can create groups
and you can add multiple users in that
group give them the permissions you can
create the rules also so that multiple
Services can interact or integrate it
together
so how the IM is used we click on the IM
service
and I'll show you how a particular user
can be created
and how the user is basically uses the
credentials to access the AWS console
now the IM dashboard is open and when
you open up the IM dashboard it gives
you the URL so this is the URL through
which the user has to actually
access the AWS console by providing the
user credentials now how the users are
created just click on the users
and
here you just have to click on add user
so I'll create one sample user let's say
I put something like sample user
and what type of an access you want to
give that particular user do you want to
give a programmatic access which is
primarily or the CLI access or you want
to give the AWS Management console so
right now we'll go with the AWS
Management console access
now do you want an auto generation of
the password or do you want to customize
the password so let's assume that we
customize the password so put any value
make sure the password meets all the
criterias
and uh
then click on next permissions now what
kind of permissions you want to give to
that user let's assume that you want to
give an access of a particular service
only to that user so you have to
actually search you have to actually
search a policy for that particular
service that can be given an access to a
user otherwise if you want to give an
admin policy or the admin access to that
user you can search an admin policy
there so the permissions are important
otherwise the user would not have
privileges to access
the AWS console
so
we'll attach the existing policies
directly so click on it
and here let's assume that I want to
give an admin access to that user so
I'll just click on the admin access now
to a particular user you can give
multiple policies also it is not
necessary that you have to give only a
single policy
so you can provide a multiple policies
to the users also now click on next tags
let let's make the tags blank click on
review and create a user
now you'll see that our sample user
would be created and to access the
console we'll just copy this URL which
has the account information as well that
is the account number
so copy the link or you can copy the
complete URL log out from the root
account
and then
paste the URL that was copied from the
console
and then we have to give the user
credentials
so we'll put sample user as the username
and the password that we provided while
creating the user
click on sign in and if it is correct
then it should allow us to login into
the AWS console while asking the
password change
so we'll change the password
confirm password change
and now it should allow us to login into
the root accounts AWS console with the
admin privileges
now the next service is the monitoring
services and under the monitoring
Services primarily the cloud watch the
cloud trail the cloud formation these
are some of the services that are most
widely used
now the cloud watch is a service which
is primarily used for monitoring the
metrics primarily of the servers like
for example if you create an ec2
instance and you want to basically
watch out for the
metrics associated with the CPU
utilization or the storage utilization
the network in network out all those
information you can get it from the
cloud watch
now cloudwatch is not only
related to monitoring the metrics it can
generate the alarms also and here you
can get the events also generated which
or the events that can be created which
can trigger the Lambda function as well
so what you have to do is you just have
to
click on a dashboard so I'll centering
metrics are viewed
with the help of the cloud watch so
first of all you have to create a
dashboard
now you have to put a dashboard name
let's say I put something like
monitoring
ec2
click on create dashboard now how do you
want the reports to be published you
have to actually select a widget I want
that okay the report should be visible
in the numeric form so I'll select the
number
now click on the dashboard that was
created and uh
so again add the widget
and here uh we'll select the metrics
primarily for the ec2
for instance metric so we have one
single instance running now there are 14
metrics available for that particular
instance now I would be looking for the
CPU utilization for this particular
instance so I'll just select for the CPU
utilization and it is going to give me
some information about what is the
current CPU utilization of that
particular instance so that is somewhere
around 29.3 percent
the CPU utilization has been done for
the single instance that is running in
my ec2 dashboard so likewise you can add
some more metrics and view them in this
particular dashboard and the cloud watch
will keep on publishing the data at a
refresh interval of five minutes that is
the default value if you are looking for
a more detailed approach on cloud
computing with Hands-On Expo experience
then simply learns Caltech postgraduate
program in cloud computing will be the
right way to go this comprehensive
program will equip you with the
knowledge and skills needed to navigate
the cloud landscape with confidence dive
deep into the cloud architecture
deployment models security and migration
strategy you will also explore platforms
like AWS Microsoft Azure and gcp to
build your Cloud expertise so don't miss
out on this chance to transform your
career and join the ranks of successful
Azure professionals click the link in
the description to discover more about
this course
hi guys so today's session is on AWS
stage maker but before proceeding with
this topic I would request you guys to
like our video also subscribe to our
Channel you can find the link just below
at the right side of this video
let's look into what we have in our
today's session so what's in it for you
we would be covering what is AWS
why do we need AWS sagemaker what is AWS
sagemaker services
what are the benefits of using the AWS
sagemaker machine learning with AWS
sagemaker how to train a model with AWS
stagemaker how to validate a model with
AWS and the companies that are using AWS
stagemaker along with that we will be
covering up one live demo on the AWS
platform
now let's understand what is AWS so what
is AWS it's an Amazon web services it's
a largest or most widely used public
Cloud platform
offered by Amazon it provides services
over the internet
AWS Services can be used to build
Monitor and deploy any type of
application in the cloud
AWS also uses the subscription pricing
model that means you only pay for
whatever the services you use for
now why do we need AWS sagemaker let's
look into it so let's consider an
example of one of the company that is
proquest now before AWS stagemaker the
proquest is a Global Information content
and technology company
that provides valuable content such as
ebooks newspapers Etc to the users
before AWS sagemaker the proquest
requirement was to have a better user
experience maximum relevant search
results now after awch maker they were
able to achieve those results so they
achieved more appealing media user
experience they achieved more relevant
search results for the users now what do
we mean by AWS sagemaker why this
service is primarily used so Amazon
sagemaker is a cloud machine learning
platform that helps users in building
training tuning and deploying machine
learning models in a production ready
hosted environment
so it's kind of a machine learning
service which is already hosted on the
AWS platform now what are the benefits
of using AWS sagemaker uh the key
benefits of using aw sagemaker are it
reduces machine learning data cost so
you can do the cost optimization
while running this particular service on
the AWS
all ml components are stored in a
particular place in a dashboard so they
can be managed together
highly scalable so it can be scalable on
you can scale this particular service on
the Fly
it trains the models quite faster
maintains the uptime so you can be
assured that your workloads will be
running all the time it will be
available all the time
High data security so security becomes a
major concern on the cloud platforms and
it ensures that you have the high data
security along with that you can do a
data transfer to different AWS services
like S3 bucket and all with the simple
data transfer techniques
now machine learning with AWS sagemaker
let's look into it
so machine learning with AWS sagemaker
is a three-step function
so one is to build second is to test and
tune the model and third is to deploy
the model now with the build it provides
more than 15 widely used ml algorithms
for training purpose now to build a
model you can collect and prepare
training data or you can select from the
Amazon S3 bucket also choose and
optimize the required algorithm so some
of the algorithms that you can select
are k-means linear regressions logistic
regression sagemaker helps developers to
customize ml instances with the Jupiter
notebook interface in the test and tune
you have to set up and manage the
environment for training so you would
need some sample data to train the model
so train and tune a model
with the Amazon sagemaker sagemaker
implements hyper parameter tuning by
adding a suitable combination of
algorithm parameters also it divides the
training data and stores that in the
Amazon S3 S3 is a simple storage service
which is primarily used for storing the
objects and the data hence it is used
for storing and recovering data over the
internet and Below you can see that AWS
stage maker uses Amazon S3 to store data
as it's safe and secure also it divides
the training data and stores in Amazon
S3 where the training algorithm code is
stored in the ECR ECR stands for elastic
container registry which is primarily
used for containers and Dockers ECR
helps users to save Monitor and deploy
Docker and the containers later
sagemaker sets up a cluster for the
input data train set and stores it in
the Amazon S3 itself so this is done by
the sagemaker itself after that you need
to deploy it so suppose you want to
predict limited data at a time
you use Amazon sagemaker hosting
services for that okay but if you want
to get prediction for an entire data set
prefer using Amazon sagemaker batch
transform now the last step that is to
deploy the model
so once tuning is done models can be
deployed to sagemaker endpoints
and in the endpoint real-time prediction
is performed so you would have some data
which you would reserve and
validate your model whether it is
working correctly or not now evaluate
your model and determine whether you
have achieved your business goals now
the other aspect is how we can train a
model with AWS stage maker
so this is basically a flow diagram
which shows you how to train a model
with the AWS stage maker and here we
have used couple of Services of an AWS
to get that done
so model training in AWS sagemaker is
done on machine learning compute
instances and here we can see there are
two machine learning compute instances
used as helper code and the training
code
along with that we are using two S3
buckets and the ECR for the container
registry
now let's look into what are the ways to
train the model as per the slides
So Below are the following requirements
to train a model so here in the diagram
you can see these are the following
requirements to train a model
the URL of an Amazon S3 bucket where the
training data is stored that is
mentioned
the computer sources on machine learning
compute instances so these are all your
machine learning compute instances
then the URL of an Amazon S3 bucket
where the output will be stored
and the path of AWS elastic container
registry where the code data is safe the
inference code image lies in the elastic
container registry now what are these
calls these are called as the training
jobs
now when a user trains a model in Amazon
sagemaker he she creates a training job
so we need to First create a training
job and then the input data is fetched
from the specified Amazon S3 bucket once
the training job is built Amazon Sage
maker launches the ml compute instances
so these compute instances will be
launched once the training job is built
then it trains the model with the
training code and the data set and it
shows the output and model articrafts in
the AWS S3 bucket so this is done
automatically now here the helper code
performs a task when the training code
fails the interference code which is in
the elastic container registry consists
of multiple linear sequence containers
that process the request for inference
on data the ec2 container registry is a
container registry that helps users to
say Monitor and deploy container images
whereas container images are the ready
applications once the data is trained
the output is stored in the specified
Amazon S3 bucket so here you can see the
output will be stored here to prevent
your algorithm being deleted save the
data in Amazon's sagemaker critical
system
which can process you on your ml compute
instances now how to validate a model
let's look into it so you can evaluate
your model
using offline or using the historical
data so first thing is that you can do
the offline testing to validate a model
you can do an online testing with the
live data so if you have a live data
coming or real-time streams coming you
can validate a model from there itself
you can validate using a holdout set and
also you can validate using the k-fold
validation now use historical data to
send requests to the model
through the jupyter notebook in Amazon
sagemaker for the evaluation online
testing with live data deploys multiple
models into the endpoints of Amazon
sagemaker and directs live traffic to
the model for validation validating
using a holdout set is part of the data
is set aside where which is called hold
outset so the part of the data is left
which is basically called as the holdout
set
this data is not used for the model
training so later when the model is
trained
with the remaining input data and
generalize the data based on what is
learned initially so whatever the data
which is left out will be used for
validating a model because we have not
used the data while training a model the
k-fold validation is the input data is
split into two parts one part is called
K which is the validation data for
testing the model and the other part is
K minus 1. which is used as a training
data
now based on the input data the machine
learning model evaluates the final
output now the companies that are using
AWS stage maker one is the ADP so you
must be knowing about ADB zalando Dow
Jones which is the stock market proquest
and the Intuit now let's look into the
demo that how we can actually run the
AWS stage maker so we'll use the r
algorithm and then package the algorithm
as a container for building training and
deploying a model
we are going to use the jupyter notebook
for that for model building for model
training for model deployment and the
code for the demo is in the below link
so you can see here that from this link
you can get the code for the demo let's
try to
do a demo on the AWS
now I would be using a link which is uh
provided by Amazon to build train and
deploy the machine learning model on the
sagemaker as you can see on my screen
and in this tutorial you would have some
steps where you can put those steps and
the code python codes into your AWS
stage maker Jupiter lab
so in this tutorial you will learn how
to use Amazon sagemaker to build train
and deploy a machine learning model and
for that we will use the popular XT
boost ml algorithm for this exercise
so first of all what you need to do is
you have to go to the AWS console and
there you have to create a notebook
instance so in this tutorial you will be
creating a notebook instance you will
prepare the data train the model to
learn from the data deploy the model
evaluate your ml models performance and
once all those activities are done then
we'll see how we can actually remove all
the resources in order to prevent the
extra costing
now the first step is we have to enter
to the Amazon sagemaker console so here
you can see I'm already logged in into
the sagemaker console you can click on
the services
search for the sagemaker here
and here you get the Amazon sagemaker
service
now the next step is
that we have to create a notebook
instance so we will select the notebook
instance from the sagemaker service and
then after the notebook instance is
selected we put a name to our instance
and we'll create a new IM role for that
so let's wait for the sagemaker studio
to open
so here you can see the studio is open
and you just have to click on the
notebook instances
and here you have to create a notebook
instance so here you can see couple of
notebook instances have already been
created one of them is in service so
this is The Notebook instance that we
are going to use for creating the demo
model
I'll show you how you can create a
notebook instance you just have to click
on create notebook instance button and
put your notebook instance name so you
can put something like demo Dash
sagemaker 987 or we can put it as model
we'll go with notebook instance type as
default which is mlt2.media
and in the permission and encryptions
under the IM role we'll click on create
a new IM role
now why we are creating a new IM rule so
that we can allow the sagemaker to
access any S3 bucket that has been
created on our account
just click on create a role and here you
would see that the new IM role will be
created with the set of permissions
then rest of the things will keep it as
default and then you just have to click
on create a notebook instance
The Notebook instance creation takes
some time so you just have to wait for a
couple of minutes to get that in service
we already have one of the notebook
instance that has been created so we
will be using that to create a demo
now going back to the steps so these are
the steps that we have already performed
now once the notebook instance is
created then we have to prepare the data
so in this step we will be using the
sagemaker notebook instance to
pre-process the data that would require
to train the machine learning model
and for that we would be opening up the
jupyter notebook and then we have to
select an environment a kernel
environment in the Jupiter notebook that
would be conda underscore python 3. so
let's follow these steps go back to the
sage maker click on the notebook
instances
select the running notebook instance and
here you would select the open Jupiter
lab
now here you would see that the
sagemaker would try to open up the
Jupiter Notepad
and we would be performing all our
inputs into that jupyter notebook
and executing the results there itself
so just wait for the notebook to open
now here you can see the Jupiter lab
notebook has been open so I would be
selecting one of the notebook that has
been created
so this one so likewise you can create
your own notebook also how you can do
that
first of all let me select the kernel
environment so I would be selecting
underscore Python 3 and just click on
select so how you can create your own
notebook just have to click on file
click on new and here you can select the
notebook just name your notebook select
the environment conda underscore Python
3 to run this demo
so I have my notebook open so in the
tabs I would be putting up the python
codes and I would be executing those
codes to get the output directly
so the next step is
to prepare the data train the ml model
and deploy it we will need to import
some libraries and Define a few
environment variables in the jupyter
notebook environment so I would be
copying this code
which you can see that would try to
import numpy pandas these are all
required to run the python
syntax so just
copy this code
and paste it into your
notebook
right so once you do that execute your
code
and here you can see that you get the
output which is
that it has imported all the necessary
libraries that have been defined in the
code
now the next step is we would create an
S3 bucket
into the S3 service and for that you
have to copy this python code just that
you have to edit it so you have to
specify this bucket name that you want
to get created
so here I would
provide the bucket name which should be
unique
should not overlap
so something like Sage maker
Dash
demo
is the name that I have selected for the
bucket and now you have to execute that
code
it says that the S3 bucket has been
created successfully with the name
sagemaker Dash demo 9876 so this is
something which you can verify so you
can go to the S3 service and there you
can verify whether the bucket has been
created or not now the next task is that
we need to download the data
to the AWS sagemaker instance and load
it into the data frame and for that we
have to follow this URL so from this URL
which is buildtrain deploy machine
learning model would have a data in the
form of bank underscore clean.csv and
this will be deployed onto our sagemaker
instance we'll copy this code
and paste it here
and execute the code
so change that it has successfully
downloaded Bank underscore clean.csv
which is which has the data inside it
and that has to be loaded into the
sagemaker data frame successfully
now we have a data to build and train
our machine learning model
so what we are going to do we are going
to shuffle the data and we are going to
split it one into the training data set
and the other one into test data set
so for the training data set we are
going to use 70 of the customers that
are listed in the CSV file and 30 of the
customers in the CSV file data we will
be using it as a test data to train the
model
so we'll copy the following code into a
new code cell and then we are going to
run that code cell
so I'll just copy it
for training the data so that we can
segregate the data model 70 for building
the model and 30 for
testing the data
so click on
run the execution and here you can see
that we got the output successfully
now
we have to train the model from the data
so how we are going to train that model
and for that we will use sagemaker
pre-built xgboost model which is an
algorithm
so you will need to reformat the header
and First Column of the training data
and load the data from the S3 bucket so
what I'll do is I'll copy
this syntax
and paste it
in the node cell
so it has the train data it would train
the model
click on run execution
now it is changing the S3 input class
which will be renamed to training input
because now we are training the model
with the training data so
so we just have to wait for some time
till it gets executed completely
now the next thing is that we need to
set up the amazon sagemaker session to
create an instance of the XT boost model
so here we are going to create this
sagemaker session say we are going to
create an instance of the XT boost model
which is an estimator
so just copy that copy that code
and paste it here
executed
and here you can see that it will start
it has basically changed the parameter
image name to the image underscore URI
in the sagemaker python SDK V2
now we'll follow the next step that is
with the data loaded in the XT boost
estimator we'll set up drain the model
using gradient optimization
and uh we'll copy the following code and
that would actually start the training
of the model
so copy this code
and this would actually
start training the model using our input
data that we have reserved 70 of the
data that we have reserved for training
the model so just copy that
again initiate the execution and it will
start the training job
now we'll deploy the model and for that
I would copy the deploy code put that in
the cell
and execute it
so
it says parameter image will be renamed
to image URI and using already existing
models so xgboost was deployed already
if you have not done that if you're
doing it the first time so it will
initiate another XT post instead so
where you can find your XT boosts
endpoints created I just have to scroll
down and here under the inference click
on the endpoints and you should find the
XT boost endpoints defined here
so here you can see that today I have
created one XT boost
endpoint and that is now in process of
creating so just refresh it
so it is still created it is going to
take some time to get that in service
now our endpoint is in service state so
now we can use it so going forward with
the next steps we will try to predict
whether the customer in the test data
enroll for the bank product or not for
that we are going to copy this code put
that in the Jupiter cell function
and execute it
so here it gives you the output that it
has actually evaluated and the same
output we got in the screenshot of the
demo as well
now we are going to evaluate the model
performance so what we are going to do
we are going to get the prediction done
so based on the prediction we can
conclude that you predicted a customer
that will enroll for a certificate of
deposit accurately for 90 of the
customers in the test data with the
Precision of 65 percent for enrolled and
90 which are which haven't enrolled for
it so for that we are going to copy this
code
and execute it here in the cell
so if it is predicted correctly that
means our model is working absolutely
fine so here you can say the overall
classification rate is 89.5 percent and
there is the accurate
prediction that has been made by the
model and that's what the output we can
see here
in the screenshot of a model so that
means our model is absolutely working
fine it has been built deployed and
trained correctly
now the next thing is that once you are
done with that you terminate your
resources and for that you just have to
copy
uh this code and put that in the cell
function so that the additional
resources and the endpoints and the
buckets that have been created by the
Jupiter notepad should be uh terminated
so that you would not be incurred with
the extra costing so just execute it and
here you would see that it is prior to
it would try to terminate all the
additional resources that we have
created from the Jupiter AWS let me
start this session with this scenario
let's imagine how life would have been
without Spotify for those who are
hearing about Spotify for the first time
a Spotify is an online music service
offering and it offers instant access to
over 16 million licensed songs Spotify
now uses AWS Cloud to store the data and
share it with their customers but prior
to AWS they had some issues imagine
using spotify before AWS let's talk
about that back then users were often
getting errors because Spotify could not
keep up with the increased demand for
storage every new day and that led to
users getting upset and users canceling
the subscription the problem Spotify was
facing at that time was their users were
present globally and were accessing it
from everywhere and they had different
latency in their applications and
Spotify had a demanding situation where
they need to frequently catalog the
songs released yesterday today and in
the future and this was changing every
new day and the songs coming in rate was
about 20 000 a day and back then they
could not keep up with this requirement
and needless to say they were badly
looking for a way to solve this problem
and that's when they got introduced to
AWS and it was a perfect fit and match
for their problem AWS offered a
dynamically increasing storage and
that's what they needed AWS also offered
tools and techniques like storage life
cycle management and trusted advisor to
properly utilize the resource so we
always get the best out of the resource
used AWS addressed their concerns about
easily being able to scale yes you can
scale the AWS environment very easily
how easily one might ask it's just a few
view button clicks and AWS sold
spotify's problem let's talk about how
it can help you with your organization's
problem let's talk about what is AWS
first and then let's bleed into how AWS
became so successful and the different
types of services that AWS provides and
what's the future of cloud and AWS in
specific let's talk about that and
finally we'll talk about a use case
where you will see how easy it is to
create a web application with AWS all
right let's talk about what is AWS AWS
or Amazon web services is a secure cloud
service platform it is also pay as you
go type billing model where there is no
upfront or Capital costs we'll talk
about how soon the service will be
available well the service will be
available in a matter of seconds with
AWS you can also do identity and access
management that is authenticating and
authorizing a user or a program on the
Fly and almost all the services are
available on demand and most most of
them are available instantaneously and
as we speak Amazon offers 100 plus
services and this list is growing every
new week now that would make you wonder
how AWS became so successful of course
it's their customers let's talk about
the list of well-known companies that
has their ID environment in AWS Adobe
Adobe uses AWS to provide multi-terabyte
operating environments for its customers
by integrating its system with AWS Cloud
Adobe can focus on deploying and
operating its own software instead of
trying to you know deploy and manage the
infrastructure Airbnb is another company
it's an Community Marketplace that
allows property owners and travelers to
connect each other for the purpose of
renting unique vacation spaces around
the world and the Airbnb Community users
activities are conducted on the website
and through iPhones and Android
applications Airbnb has a huge
infrastructure in AWS and they're almost
using all the services in AWS and are
getting benefited from it another
example would be Autodesk Autodesk
develops software for engineering
designing and entertainment Industries
using services like Amazon ideas or
rational database service and Amazon S3
or Amazon simple storage service
Autodesk can focus on deploying or
developing its machine learning tools
instead of spending that time on
managing the infrastructure AOL or
American online uses AWS and using AWS
they have been able to close data
centers and decommission about 14 000
in-house and co-located servers and move
Mission critical workload to the cloud
and extend its Global reach and save
millions of dollars on energy resources
bitdefender is an internet security
software firm and their portfolio of
softwares include antivirus and
anti-spyware products bitdefender uses
ec2 and they're currently running few
hundred instances that handle about 5
terabytes of data and they also use
elastic load balancer to load balance
the connection coming in to those
instances across availability zones and
they provide seamless Global delivery of
service because of that the BMW group it
uses AWS for its new connected Car
application that collects sensor data
from BMW 7 Series cars to give drivers
dynamically updated map information
Canon's offers Imaging products division
benefits from faster deployment times
lower cost and Global reach by using AWS
to deliver cloud-based services such as
mobile print the office Imaging products
division uses AWS such as Amazon S3 and
Amazon Route 53 Amazon cloudfront and
Amazon IM for their testing development
and Production Services Comcast it's the
world's largest cable company and the
leading provider of internet service in
the United States Comcast uses AWS in a
hybrid environment out of all the other
Cloud providers Comcast chores AWS for
its flexibility and scalable hybrid
infrastructure Docker is a company
that's helping redefine the way
developers build ship and run
applications this company focuses on
making use of containers for this
purpose and in AWS the service called
the Amazon ec2 container service is
helping them achieve it the esa or
European Space Agency although much of
esa's work is done by satellites some of
the programs data storage and Computing
infrastructure is built on Amazon web
services Esa shows AWS because of its
economical pay as you go system as well
as its quick startup time the Guardian
newspaper uses AWS and it uses a wide
range of AWS services including Amazon
Kinesis Amazon redshift that power and
analytic dashboard which editors use to
see how stories are trending in real
time Financial Times FD is one of the
world's largest leading business news
organization in and they used Amazon
redshift to perform their analysis A
Funny Thing Happened Amazon Redshirt
performed so quickly that some analysis
thought it was malfunctioning they were
used to running queries overnight and
they found that the results were indeed
correct just as much faster by using
Amazon redshift FD is supporting the
same business functions with costs that
are 80 percentage lower than what was
before general electric GE is at the
moment as we speak migrating more than
9000 workloads including 300 desperate
Erp systems to AWS while reducing its
data center footprint from 34 to 4 over
the next three years similarly Howard
Medical School HTC IMDb McDonald's NASA
Kellogg's and lot more are using the
services Amazon provides and are getting
benefited from it and this huge success
and customer portfolio is just the tip
of the iceberg and if we think why so
many adapt AWS and if we let AWS answer
that question this is what AWS would say
people are adapting AWS because of the
security and durability of the data and
end-to-end privacy and encryption of the
data and storage experience we can also
rely on AWS way of doing things by using
the AWS tools and techniques and
suggested best practices built upon the
years of experience it has gained
flexibility there is a greater
flexibility in AWS that allows us to
select the OS language and database easy
to use swiftness in deploying we can
host our applications quickly in AWS be
it a new application or migrating an
existing application into AWS
scalability the application can be
easily scaled up or scaled down
depending on the user requirement cost
saving we only pay for the compute power
storage and other resources you use and
that too without any long-term
commitments now let's talk about the
different types of services that AWS
provides the services that we talk about
fall in any of the following categories
you see like you know compute storage
database Security customer engagement
desktop and streaming machine learning
developers tools stuff like that and if
you do not see the service that you're
looking for it's probably is because AWS
is creating it as we speak now let's
look at some of them that are very
commonly used within compute Services we
have Amazon ec2 Amazon elastic bean
stock Amazon light sale and Amazon
Lambda Amazon ec2 provides compute
capacity in the cloud now this capacity
is secure and it is resizable based on
the user's requirement now look at this
the requirement for the web traffic
keeps changing and behind the scenes in
the cloud ec2 can expand its environment
to three instances and during no load it
can shrink its environment to just one
resource elastic Beanstalk it helps us
to scale and deploy web applications and
it's made with a number of programming
languages elastic Beanstalk is also an
easy to use service for deploying and
scaling web application locations and
services deployed a beat in java.net PHP
node.js python Ruby Docker and lot other
familiar services such as Apache
passenger and IIs we can simply upload
our code an elastic Beanstalk
automatically handles the deployment
from capacity provisioning to load
balancing to Auto scaling to application
Health monitoring and Amazon light sale
is a virtual private server which is
easy to launch and easy to manage Amazon
light cell is the easiest way to get
started with AWS for developers who just
need a virtual private server light sail
includes everything you need to launch
your project quickly on a virtual
machine like SSD based storage a virtual
machine tools for data transfer DNS
management and a static IP and that too
for a very low and predictable price AWS
Lambda has taken Cloud Computing
Services to a whole new level it allows
us to pay only for the compute time no
need for provisioning and managing
servers an AWS Lambda is a compute
server that lets us run code without
provisioning or managing servers Lambda
executes your code only when needed and
scales automatically from few requests
per day to thousands per second you pay
only for the compute time you consume
there is no charge when your core is not
running let's look at some storage
services that Amazon provides like
Amazon S3 Amazon Glacier Amazon abs and
Amazon elastic file system Amazon S3 is
an object storage that can store and
retrieve data from anywhere websites
mobile apps iot sensors and so on can
easily use Amazon S3 to store and
retrieve data it's an object storage
built to store and Destroy any amount of
data from anywhere with its features
like flexibility in managing data and
the durability it provides and the
security that it provides Amazon simple
storage service or S3 is a storage for
the internet and Glacier Glacier is a
cloud storage service that's used for
archiving data and long-term backups and
this Glacier is an secure durable an
extremely low cost cloud storage service
for data archiving and long-term backups
Amazon EVS Amazon elastic blog store
provides Block store volumes for the
instances of ec2 and this elastic Block
store is highly available and a reliable
storage volume that can be attached to
any running instance that is in the same
availability Zone ABS volumes that are
attached to the ec2 instances are
exposed as storage volumes that persist
and independently from the lifetime of
the instance an Amazon elastic file
system or EFS provides an elastic file
storage which can be used with AWS cloud
service and resources that are on
premises and Amazon elastic file system
it's an simple it's scalable it's an
elastic file storage for use with Amazon
cloud services and for on-premises
resources it's easy to use and offers a
simple interface that allows you to
create and configure file systems
quickly and easily Amazon file system is
built to elastically scale on demand
without disturbing the application grow
and shrinking automatically as you add
and remove files to your application
have the storage they need and when they
need it now let's talk about databases
the two major database flavors are
Amazon RDS and Amazon redshift Amazon
RDS it really eases the process involved
in setting up operating and scaling a
rational database in the cloud Amazon
RDS provides cost efficient and
resizable capacity while automating time
consuming administrative tasks such as
Hardware provisioning database setup
patching and backups it sort of frees us
from managing the hardware and sort of
helps us to focus on the application
it's also cost effective and resizable
and it's also optimized for memory
performance and input and output
operations not only that it also
automates most of the services like
taking backups you know monitoring stuff
like that it automates most of those
Services Amazon redshift Amazon redshift
is a data warehousing service that
enables users to analyze the data using
SQL and other business as intelligent
tools Amazon redshift is an fast and
fully managed data warehouse that makes
it simple and cost effective analyze all
your data using standard SQL and your
existing business intelligent tools it
also allows you to run complex analytic
queries against petabyte or structured
data using sophisticated query
optimizations and most of the results
they generally come back in seconds
alright let's quickly talk about some
more services that AWS offers there are
a lot more services that AWS provides
but are we going to look at some more
services that are widely used AWS
application Discovery Services help
Enterprise customers plan migration
projects by gathering information about
their on-premises data centers in a
planning a data center migration can
involve thousands of workloads they are
often deeply interdependent server
utilization data and dependency mapping
are important early first step in
migration process and this AWS
application Discovery service collects
and presents configuration usage and
behavior data from your servers to help
you better understand your workloads
Route 53 it's a network and content
delivery service it's an highly
available and scalable Cloud domain name
system or DNS service and Amazon Route
53 is fully compliant with IPv6 as well
elastic load balancing it's also a
network and content delivery service
elastic load balancing automatically
distributes incoming application traffic
across multiple targets such as Amazon
ec2 instance containers and ipu
addresses it can handle the varying load
of your application traffic in a single
availability zones and also across
availability zones AWS Auto scaling it
monitors your application and
automatically adjusts the capacity to
maintain steady and predictable
performance at a lowest possible cost
using AWS Auto scaling it's easy to set
up application scaling for multiple
resources across multiple services in
minutes Auto scaling can be applied to
web services and also for DB Services
AWS identity and access management it
enables you to manage access to aw AWS
services and resources securely using IM
you can create and manage AWS users and
groups and use permissions to allow and
deny their access to AWS resources and
moreover it's a free service now let's
talk about the future of AWS well let me
tell you something cloud is here to stay
here's what in store for AWS in the
future as years pass by we're gonna have
a variety of cloud applications Bond
like iot artificial intelligence
business intelligence serverless
Computing and so on cloud will also
expand into other markets like
healthcare banking space automated cars
and so on as I was mentioning some time
back lot or greater Focus will be given
to artificial intelligence and
eventually because of the flexibility
and advantage that cloud provides we're
going to see a lot of companies moving
into the cloud all right let's now talk
about how easy it is to deploy an web
application in the cloud so the scenario
here is that our users like a product
and we need to have a mechanism to
receive input from them about their
likes and dislikes and you know give
them the appropriate product as per
their need alright though the setup and
the environment it sort of looks
complicated we don't have to worry
because AWS has tools and Technologies
which can help us to achieve it now
we're going to use services like Route
53 so it's just like Cloud watch ec2 S3
and lot more and all these put together
are going to give an application that's
fully functionable and an application
that's going to receive the information
like using the services like Route 53
cloudwatch ec2 and S3 we're going to
create an application and that's going
to meet our need so back to our original
requirement all I want is to deploy a
web application for a product that keeps
our users updated about the happenings
and the newcomings in the market and to
fulfill this requirement here is all the
services we would need ec2 here is used
for provisioning the computational power
needed for this application and ec2 has
a vast variety of family and and types
that we can pick from for the types of
workloads and also for the intents of
the workloads we're also going to use S3
for storage and S3 provides any
additional storage requirement for the
resources or any additional storage
requirement for the web applications and
we are also going to use cloudwatch for
monitoring the environment and
cloudwatch monitors the application and
the environment and it provides trigger
for scaling in and scaling out the
infrastructure and we're also going to
use Route 53 for DNS and Route 53 helps
us to register the domain name for our
web application and with all the tools
and Technologies together all of them
put together we're going to make an
application a perfect application that
caters our need all right so I'm going
to use elastic Beanstalk for this
project and the name of the application
is going to be as you see GSG sign up
and the environment name is GSG signup
environment one let me also pick a name
let me see if this name is available yes
that's available that's the domain name
so let me pick that and the application
that I have is going to run on node.js
so let me pick that platform and launch
now as you see elastic Beanstalk this is
going to launch an instance it's going
to launch the monitoring setup or the
monitoring environment it's going to
create a load balancer as well and it's
going to take care of all the security
features needed for this application
all right look at that I was able to go
to that URL which is what we gave and
it's now having an default page shown up
meaning all the dependencies for the
software is installed and it's just
waiting for me to upload the code or in
specific the page required so let's do
that
let me upload the code I already have
the code saved here
so that's my code
and that's going to take some time all
right it has done its thing and now if I
go to the same URL look at that I'm
being thrown an advertisement page all
right so if I sign up with my name email
and stuff like that you know it's going
to receive the information and it's
going to send an email to the owner
saying that somebody had subscribed to
your service that's the default feature
of this app look at that email to the
owner saying that somebody had
subscribed to your app and this is their
email address stuff like that not only
that it's also going to create an entry
in the database and dynamodb is the
service that this application uses to
store data there's my dynamodb and if I
go to tables right and go to items I'm
going to see that a user with name
Samuel and email address so and so has
said ok or has shown interest in the
preview of my site or product so this is
where this is how I collect those
information right and some more things
about the infrastructure itself is it is
running behind and load balancer look at
that it had created a load balancer it
had also created an auto scaling group
now that's the feature of elastic load
balancer that we have chosen it has
created an auto scaling group and now
let's put this URL you see this it's
it's not a fancy URL all right it's an
Amazon given URL a dynamic URL so let's
put this URL behind our DNS let's do
that
so go to Services go to Route 53
go to hosted Zone and there we can find
the DNS name right so that's a DNS name
all right
all right let's create an entry
and map that URL to our load balancer
right
and create now technically if I go to
this URL it should take me to that
application all right look at that I
went to my custom URL and now that's
pointed to my application previously my
application was having a random URL and
now it's having a custom URL so now what
is azure what's the big cloud service
provider all about so Azure is a cloud
computing platform provided by Microsoft
now it's basically an online portal
through which you can access and manage
resources and services now resources and
services are nothing but you know you
can store your data and you can
transform the data using services that
Microsoft provides again all you need is
the internet and being able to connect
to the Azure portal then you get access
to all of the resources and their
services in case you want to know more
about how it's different from its rival
which is AWS I suggest you click on the
top right corner and watch the AWS
versus Azure video so that you can
clearly tell how both these cloud
service providers are different from
each other now here are some things that
you need to know about Azure it was
launched in February 1st 2010 which is
significantly later than when AWS was
launched it's free to start and has a
pay-per-use model which means like I
said before you need to pay for the
services you use through Azure and one
of the most important selling points is
that 80 percent of Fortune 500 companies
use Azure Services which means that most
of the bigger companies of the world
actually recommend using Azure and then
Azure supports a wide variety of
programming languages the C sharp
node.js Java and so much more another
very important selling point of azure is
the amount of data centers it has across
the world now it's important for a cloud
service provider to have many data
centers around the world because it
means they can provide their services to
a wider audience now Azure has 42 which
is more than any cloud service provider
has at the moment it expects to have 12
more in a period of time which brings
its total number of regions it covers to
54. now let's talk about Azure Services
now Azure Services have 18 categories
and more than 200 services so we clearly
can't go through all of them it has
services that cover compute aim machine
learning integration management tools
identity devops web and so much more
you're going to have a hard time trying
to find a domain that Azure doesn't
cover and if it doesn't cover it now you
can be certain they're working on it as
we speak so first let's start with the
compute Services first virtual machine
with this service what you're getting to
do is to create a virtual machine of
Linux or Windows operating system it's
easily configurable you can add RAM you
can decrease RAM you can add storage
remove it all of it is possible in a
matter of seconds now let's talk about
the second service cloud service now
with this you can create a application
within the cloud and all of the work
after you deploy it deploying the
application that is is taken care of by
Azure which includes you know
provisioning the application load
balancing ensuring that the application
is in good health and all of the other
things are handled by Azure next up
let's talk about service fabric now with
service fabric the process of developing
a micro service is greatly simplified so
you might be wondering what exactly is a
microservice now a micro service is
basically an application that consists
of smaller applications coupled together
next up functions now with functions you
can create applications in any
programming language that you want
another very important part is that you
don't have to worry about any hardware
components you don't have to worry what
Ram you require or how much storage you
require all of that is taken care of by
Azure all you need is to provide the
code to Azure and it will execute it and
you don't have to worry about anything
else now let's talk about some
networking Services first up we have
Azure CDN or the content delivery
Network now the azure CDN service is
basically for delivering web content to
users now this content is of high
bandwidth and can be transferred or can
be delivered to any person across the
world now these are actually a network
of servers that are placed in strategic
positions across the world so that the
customers can obtain this data as fast
as possible next up we have expressr now
with this you can actually connect your
on-premise network onto the Microsoft
cloud or any of the services that you
want through a private connection so the
only communication that happens is
between your on-premise network and the
service that you want then you have
virtual Network now with virtual Network
you can have any of the Azure Services
communicate with each other in a secure
manner in a private manner next we have
Azure DNS so Azure DNS is a hosting
service which allows you to host their
DNS or domain name system domains in
Azure so you can host your application
using Azure DNS now for these storage
services first up we have this storage
with this storage you are given a cost
effective option of choosing HDD or
solid state drives to go along with your
virtual machines based on your
requirements then you have blob storage
now this is actually optimized to ensure
that they can store massive amounts of
unstructured data which can include Text
data or even binary data next you have
file storage which is a manage file
storage and can be accessible via the
SMB protocol or the server message block
protocol and finally you have queue
storage now with Q storage you can
provide durable message queuing for an
extremely large workload and the most
important part is that this can be
accessed from anywhere in the world now
let's talk about how Azure can be used
firstly for application development it
could be any application mostly web
applications then you can test the
application see how well it works you
can host the application on the internet
you can create virtual machines like I
mentioned before with the service you
can create these virtual Machines of in
any size or Ram that you want you can
integrate In Sync features you can
collect and store matrices for example
how the data Works how the current data
is how you can improve upon it all of
that is possible with these services and
your virtual hard drives which is an
extension of the virtual machines where
these services are able to provide you a
large amount of storage where data can
be stored let's talk about Azure
Services as I told you Azure provides
services for a wide range of domains now
let's have a look at some of these
domains there's Ai and machine learning
compute containers database identity
management tools networking Security
Storage and so much more now let's have
a look at some of the individual
services within these domains firstly
you have Azure virtual machines with
Azure virtual machines what you get is
the opportunity to create Windows or
Linux virtual machines now all of this
is possible in a matter of seconds with
a large amount of customization now
let's have a look at some of its
features firstly you can choose from a
wide variety of virtual machine options
then you have a large amount of
optimization available to you for
example what size of operating system do
you want how much size do you want to
locate it to it what version of the
system is it and so much more then it
provides low cost and permanent billing
now Azure provides you per minute
billing which means that you're only
charged for how much time you use the
service and finally you have enhanced
security and protection for your virtual
machines next we have service fabric now
with service fabric you have platform
which enables you to create micro
Services now this also makes the process
of application lifecycle management a
whole lot easier as a direct result you
can create applications with a faster
time to Market it supports Windows Linux
on-premises or other clouds and it
enables you to do a tremendous amount of
scaling up depending on your requirement
and finally we have functions now with
functions you can build applications
with the help of serverless computing
here the users only pay for the amount
of resources that they've used you can
create application locations in any
language that you want and the only
thing you need to worry about is the
code of the application everything other
than that that is the hardware
requirements are taken care of by Azure
now let's have a look at the networking
Services firstly we have the Azure CDN
or the content delivery network with
Azure CDN what you get is the ability to
deliver your content with reduced load
times fast responsiveness and less
bandwidth now CDN can be integrated with
several other Azure services so that the
process can move at a faster rate it can
handle heavy loads and traffic spikes
with ease it also provides a robust
security system now with the content
that's delivered you can get Advanced
analytic data with which you can
understand how customers are using your
content next we have express route with
express route you can connect your
on-premises network to Azure through a
private Network Now by default this
lowers latency it increases the emphasis
on reliability and speed and it can be
of great use when you have to transfer
the large amounts of data between
networks now another way this can be
useful is if it's used to add compute or
storage capacity to Data Centers next we
have Azure DNS domain name service or
Azure DNS can be used to host your
domains on Azure this provides High
availability and great performance it
provides fast responses to DNS queries
by taking advantage of Microsoft's
Global Network it also provides High
availability next we have virtual
Network Azure virtual Network allows the
Azure resources to communicate with each
other or other on-premise networks via
the Internet and all of this is kept
extremely secure now with this users can
create their own private Network for
communication it provides users with an
isolated and extremely secure
environment for their applications to
run now all of the traffic stays
entirely within the Azure Network and it
also allows users to design their own
networks next we have traffic manager
now with traffic manager you can route
incoming traffic to improve your
performance and availability now one
thing it provides is multiple failover
options so if a particular situation
goes wrong there's always an option to
consider to salvage the situation it
helps reduce application runtime and
enables the distribution of user traffic
across multiple locations it also helps
the people who are using it to know
where the customers connecting from
across the world next we have load
balancer with this you have provided the
ability to instantly scale applications
at the same time providing High
availability and improved Network
performance for users applications it
can be integrated into virtual machines
and cloud services it provides highly
reliable applications it also allows
users to secure and integrate security
groups finally we have Azure VPN Gateway
now this allows users to connect their
on-premise networks to Azure using a
side-to-site VPN now this allows users
to connect their virtual machine to
anywhere in the world through a point to
site VPN and also it's very easy to
manage and is highly available now let's
talk about the storage Services first we
have data Lake storage now with this
what you get is a scalable data storage
with an emphasis on cost Effectiveness
and scalability now it comes with
maximum use when you integrate it with
other services so that you can get
analytics on how the data is being used
it is also integrated with other
services like the Azure blob storage now
it is also optimized for Big Data
analysis tools like Apache spark and
Hadoop next up we have blob storage The
Blob storage provides a storage capacity
for data now depending on how often a
particular data is used it is classified
into different tiers now all the data
that is within the blob storage is
unstructured data now it has a way of
ensuring the the data Integrity is
maintained every time a particular
object is being changed or the data has
been accessed and it also helps improve
app performance and reduces bandwidth
consumption next we have q storage now
with this you have a message queuing
system for large workloads this allows
users to build flexible applications and
separate functions not to mention with
this you can be sure that your
individual components will not fail it
also makes sure that your application is
scalable
Q storage provides queue monitoring
which helps ensure that the customer's
demands are met then we have files
stored now with file storage you can
perform file sharing with the help of
the SMB protocol or the server message
block protocol now this data is
protected by SMB 3.0 and the https
protocol in this case like we mentioned
in functions Azure takes care of all the
hardware needs and the operating system
deployments on its own it also improves
on-premises performance and other
capabilities lastly we have table
storage with table storage you can
deploy semi-structured data sets and
nosql key value store now this is used
for creating applications which have a
flexible data schema and also
considering how it has a very strong
consistency model it's mainly aimed for
end prices next let's have a look at
some web and mobile services first we
have the Azure search now with Azure
search you get a cloud search service
which is powered by artificial
intelligence with this you can develop
web application as well as mobile
applications now one big Advantage is
that you don't have to set up or manage
your search indices Azure takes care of
that and by extension it increases your
development speed the artificial
intelligence also will provide insights
and structured information that you can
use to improve the search as structured
information next we have logic apps now
with this you can create integration
Solutions which can connect applications
that are important to your business now
with this you can visually create
business processes and workflows you can
integrate SAS or software as a service
applications and Enterprise applications
and more importantly it allows you to
unlock data within a firewall and
securely connect to services next we
have web applications now with web apps
you can create deploy and scale web
applications according to business
requirements now it supports both
windows and Linux platforms and it helps
with continuous integration or
deployment abilities another very
important aspect of this is that the
data can be deployed and hosted across
multiple locations in the world and
finally we have mobile apps with mobile
apps you can create applications for iOS
Android and Windows platforms one
advantage is that it automatically
scales Up and Down based on your
requirements now in situations where you
have network issues offline data syncing
ensures that your applications work
anyway and you can create cross-platform
applications or native applications for
iOS Android and Windows next let's have
a look at some container services first
let's talk about ACS or Azure container
services it is also known as the Azure
kubernetes Services as it's a fully
managed kubernetes container
orchestration service now what this
means is that it eases the process of
container integration and deployment it
also can be used with other resources
from security like virtual networks
cryptographic keys and so much more to
ensure that your container is kept
secure next we have container instances
now this is similar to functions in a
way just that in this we're using
containers without having to manage
servers now applications can be
developed here without managing virtual
machines or learning new tools all that
is azure's problem to take care of and
it enables building applications without
having to manage the infrastructure that
is all you need to worry about is
running the container next let's have a
look at some database Services first we
have the SQL database now with SQL
database what you get is a relational
cloud data based service now this means
that it helps accelerate your app
development and makes it easier for you
to maintain your application now SQL
database is also used extensively in
migrating workloads to the cloud and
hence saves time and cost it also helps
improve your performance by integrating
machine learning and Adaptive
Technologies into your database next we
have Azure Cosmos DB now this is a
globally distributed multi-model
database service now what this means is
that with this you can create
application with support nosql it
provides a high grade security system
has high availability and low latency
now this is usually used in situations
where you have a diverse and highly
unpredictable workload now let's have a
look at some security and identity
Services firstly we have the Azure
active directory now if you want to know
more about Azure active directory I
suggest you click on the top right
corner and watch our video on the Azure
active directory this is just an intro
production so with this you can manage
user identities and you can make sure
the resources are kept safe with the
help of access policies most of these
are intelligence driven now one of the
main features is that you can have
access to your applications from any
location or device it helps increase
your efficiency and helps down cutting
costs when it comes to having a help
desk it can also help improve security
and can respond to Advanced threats in
real time next you have Azure active
directory b2c it helps provide customer
identity and access management in the
cloud now protecting customer identity
is extremely important for an
organization and that's what Azure adb2c
does now it also enables the application
to be scaled to great amounts even
billions of customers next we have the
Azure security Center this is basically
like a command post with which you get a
complete view of the security across
users on your on-premises and Cloud
workloads so with this you are given
threat protection method that adapts to
situations and helps reduce exposing you
to threats it also has rapid threat
response and makes the process of
finding and fixing vulnerabilities a
whole lot easier next up let's talk
about monitoring and Management Services
so first let's have a look at as your
advisor now Azure advisor is basically a
guide for the best practices when it
comes to Azure now when you follow these
it improves performance security cost
and increases availability now it also
learns from how you use the services on
your configuration and usage pattern and
the adjustments that it suggests can be
implemented very quickly and easily next
we have Network Watcher now with this
you can monitor diagnose and understand
the working of your network now you can
monitor your network without actually
having to login to your virtual machine
now you can also use something known as
network security flow logs to understand
the traffic pattern how much traffic is
coming toward you how much you're giving
and so much more it also helps diagnose
VPN problems that you might have with
detailed logs and finally you have the
Azure resource manager now with this you
can ensure that the resources that you
have are managed and deployed at a
consistent rate now this makes it
extremely easy for you to manage and
visualize your resources that are used
in your applications or some other
requirements and you can control who can
access your resources as well as perform
actions on it if you are looking for a
more detailed approach on cloud
computing with hands-on experience then
simply learns Caltech postgraduate
program in cloud computing will be the
right way to go this comprehensive
program will equip you with the
knowledge and skills needed to navigate
the cloud landscape with confidence dive
deep into the cloud architecture
deployment models security and migration
strategy you will also explore platforms
like AWS Microsoft Azure and gcp to
build your Cloud expertise so don't miss
out on this chance to transform your
career and join the ranks of successful
Azure professionals click the link in
the description to discover more about
this course now let's have a look at how
we can use one of these services to
satisfy a particular requirement so the
friend asks that she needs one last bit
of help she wants her friend's help to
host the website on Azure and here's how
we can do it so now we're going to put
one of these Services into action so
this is the Azure dashboard so from here
we are going to create a virtual machine
within which we are going to create a
website a very simple HTML website
nonetheless so let's get started so
first what we're going to do is create a
resource a resource Group so what a
resource Group is is actually a
repository of all the resources like
virtual machines or storage that you're
going to use in your project so in this
we need a virtual machine so let's get
that created so first we'll name it
demo
ecos and will create it now this will
take a few seconds
and that's it so now that that's done we
go on ADD
so we're going to be using a Windows
operating system so we'll just search
for virtual machine
you select this
and create
okay so here we are going to set up
we're going to select the resource Group
the one just that we just created a
virtual machine name
okay
Let's uh storage name
[Music]
passwords
and then just Breeze through the rest of
it we can introduce the size we don't
need this big
and create
now that the validation is done the next
step is to
create
now this will take a few minutes so
we'll wait and as you can see here the
virtual machine has been created we'll
go to resource
so here all we need to do is connect and
download the RDP file let's see if we
run this file and see what happens
now this error shows up in a whole lot
of systems that I've used so this is a
very easy way to troubleshoot this
problem so here's what you need to do
now so these are the problems there's
remote access to the server is not
enabled the remote computers turned off
the remote computer is not available on
the network
so what we're going to do is go to
networking
and add an inbound Port Rule now we're
trying to access a computer remotely
right so to remotely access the computer
we would need to add an inbound Port
rule so that's what we're going to do
right now
so we can keep the sources any port
range as any and our destination Port as
3389 if you've noticed there we'd
actually have used the 3389 as the
inbound port and then we change the name
as 3389 and this rule is added
now after this is done all we need to do
you can see here that we refresh the
page this would be here
a new rule has been added now so we'll
go to overview
and try to connect again
do the same thing again
and this time it worked so here we'll
input the name
and our password
and that's it now we just press yes
and there you go the system has started
now your virtual machine is started now
in a while the server manager will pop
up
so there's some things that we need to
do so that we can host the website so
the first thing is to create an inbound
Port so we'll turn off the virtual
machine for now
come back to that later first we'll go
to a resource Group
check the name of our resource
and then we'll look at all its resources
here
so here we need to find at the network
security group
and add an inbound security rule press
add we take any Source any Source Port
range
and then we add Port 80. which is the
one that we're going to use and we'll
put name as port
80.
we'll add the security rule
which will take a few seconds
and that's that now it's a lot now let's
go back to our virtual machine
here you go
and let's get it started
let me log in again
and there you have it it's on again now
we'll go to server manager so we need to
add roles and features
so most of this we can just run through
we'll leave them at the default values
same and here we need to add a web
server which is the IIs so we'll select
this
we add the features next so we can add
the.net 3.5 features as well next
and
we'll install now
we'll wait for the installation to
finish
and now the installation is done so now
what we're going to see is if the
website can be reached I will go to the
public address which is given here so
we've come back to our original system
click
and you can see that the is Page has
been created now we need to work more to
create the HTML page
here's what we'll need to do to run our
website so right now what we're getting
is this the IIs page now let's make it
our own HTML page a very simple HTML
page from there on we can create the
website so go back to the virtual
machine and then we go to
C drive
so the IIs is located in a location
which is in C drive in it Pub and www
root so if you click on this actually
what you'll get is the page I showed you
earlier which is this one so now we'll
go to the server manager
click on tools
and find the IIs manager
so we click on this
and sites now there's a default website
we can now add a new one
a site name
demo
we need a physical path we'll put it
inside the folder we just saw which is
right here C drive
in a pub WWE root within which we'll
create a new folder it's for simply
learn demo
I'll save it and put 80 and okay
now this is assigned to another site so
we'll change that but we'll wait for
that okay
now for this we'll just change it we
click on this
so since both of them have the same port
we'll change that here edit binding
you change this
to something else
okay and close
so now both are different and now we can
start our website
it's already started now
now we'll start the website right click
this
it's a manage website
and start
and that's it
now we will go to the
to the inner Pub ww root and we have
simply learned demo so here we will
create the
HTML website or the HTML the document
so
index Dot HTML
okay
open this
it's a very simple website now so
we keep it save it
and so here what we're going to do is
save as
an HTML file but for all
all files
we close this now to make sure this is
recognized as a DOT HTML file it goes
View
options here will hide extensions from
known file types
rename this
[Music]
and there you have it it's a DOT HTML
file now let's go back
to our original computer
and run this page
and there you have it now this is just
the initial version more work has to be
done on it but your first step is
already done with the greatest debate of
the century today I am joined by two
giants of the cloud computing industry
they'll be going head to head with each
other to
who amongst them is better it's going to
be one hell of fight now let's meet our
candidates on my left we have AWS who's
voiced by a bake shop hi guys and on my
ride we have Microsoft Azure whose voice
by Anjali hey there so today we'll be
deciding who's better on the basis of
their origin and the features they
provide their performance in the present
day and comparing them on the basis of
pricing market share and options free
tier and instance configuration now
let's listen to their opening statements
let's start with AWS launched in 2006
AWS is one of the most commonly used
cloud computing platforms across the
world companies like Adobe Netflix
Airbnb HTC Pinterest and Spotify have
put their faith in AWS for their proper
functioning it also dominates the cloud
computing domain with almost 40 percent
of the entire market share so far
nobody's even gotten close to beating
that number AWS also provides a wide
range of services that covers a great
number of domains domains like compute
networking storage migration and so much
more now let's see what Azure has to say
about that Azure was launched in 2010
and is trusted by almost 80 percent of
all Fortune 500 companies the best
companies the world choose to work only
with Azure Aja also provides its
services to more regions than any other
cloud service provider in the world
Azure covers 42 regions already and 12
more are being planned to be made Azure
also provides more than 100 Services
spanning a variety of domains now that
the opening statements are done let's
have a look at the current market status
of each of our competitors this is the
performance route here we have the stats
for the market share of AWS Azure and
other cloud service providers this is
for the early 2018 period Amazon web
services takes up a whopping 40 percent
of the market share closely followed by
sgr at 30 percent and other cloud
services adding 30 this 40 indicates
most organizations clear interest in
using AWS VR number one because of our
years of experience and Trust we've
created among our users sure you're the
market leader but we are not very far
behind let me remind you more than 80
percent of the Fortune 500 companies
trust Azure with their cloud computing
needs so it's only a matter of time
before Azure takes the lead the rest of
the 30 person that is in AWS or Azure
accounts to the other cloud service
providers like Google Cloud platform
Rackspace IBM software and so on now for
our next round the comparison round
first we'll be comparing pricing we'll
be looking at the cost of a very basic
instance which is a virtual machine of
two virtual CPUs and 8 GBS of RAM for
AWS this will cost you approximately
0.0928 US dollars per hour and for the
same instance in Azure it'll cost you
approximately 0.096 US dollars per hour
next up let's compare market share and
options as I mentioned before AWS is the
Undisputed market leader when it comes
to the cloud computing domain taking up
40 of the market share by 2020 AWS is
also expected to produce twice its
current Revenue which comes close to 45
4 billion dollars not to mention AWS is
constantly expanding its already strong
roaster of more than 100 services to
fulfill the shifting business
requirements of organizations all that
is great really good for you but the
research company Gartner has released a
magic quadrant that you have to see you
see the competition is now neck to neck
between Azure and AWS it's only a matter
of time before Azure can increase from
its 30 market share and surpass AWS this
becomes more likely considering how all
companies are migrating from AWS to
Azure to help satisfy their business
needs Azure is not far behind AWS when
this comes to Services as well azure's
service offerings are constantly updated
and improved on to help users satisfy
their cloud computing requirement now
let's compare AWS and azure's free
offerings AWS provides a significant
number of services for free helping
users get Hands-On XP experience with
the platform products and services the
free tier Services fall under two
categories services that will remain
free forever and the others that are
valid only for one year the always free
category offers more than 20 services
for example Amazon SNS sqs cloudwatch
Etc and the valid for your category
offers approximately 20 services for
example Amazon S3 ec2 elastic cache Etc
both types of services have limits on
the usage for example storage number of
requests compute time Etc but users are
only charged for using services that
fall under the valid for a year category
after a year of their usage a show
provides a free tier as well it also
provides services that belong to the
categories of free for a year and always
free there are about 25 plus always free
services provided by Azure these include
app service functions container service
active directory and lots more and as of
the valid for a year there are eight
services offered there's Linux or
Windows Virtual machines blob storage
SQL database and few more Azure also
provides the users with credits of 200
US dollars to access all their services
for 30 days now this is a unique feature
that Azure provides with users can use
their credits to utilize any service of
a choice for the entire month now let's
compare instance configuration the
largest instance that AWS offers is that
of a whopping 256 GBS of RAM and 16
virtual CPUs the largest that Azure
offers isn't very far behind either 224
GBS of RAM and 16 virtual CPUs and now
for the final round now each of our
contestants will be shown facts and they
have to give explanations for these
facts we call it the rapid fire round
first we have features in which AWS is
good and Azure is better AWS does not
cut down on the features it offers its
users however it requires slightly more
Management on the user's part Azure goes
slightly deeper with the services that
fall under certain categories like
platform as a service and infrastructure
as a service
next we have hybrid Cloud where AWS is
good and Azure is better OK although AWS
did not emphasize on hybrid Cloud
earlier they are focusing more on
technology now Azure has always
emphasized on hybrid cloud and has
features supporting it since the days of
its inception for developers AWS is
better and Azure is good of course it's
better because AWS supports integration
with third-party applications well Azure
provides access to data centers that
provide a scalable architecture for
pricing both AWS and Azure are at the
same level it's good for AWS because it
provides a competitive and constantly
decreasing pricing model and in the case
of azure it provides offers that are
constantly experimented upon to provide
its users with the best experience and
that's it our contestants have finished
giving their statements now let's see
who won surprisingly nobody each cloud
computing platform has its own pros and
cons choosing the right one is based
entirely on your organization's
requirements let's understand why Google
Cloud platform so Google Cloud platform
is popular for many reasons now let us
see few of most important reasons why it
stands out
when you talk about pricing pricing is
one of the significant factors that make
Google Cloud Stand Out Among the other
Cloud providers
it offers a monthly pricing plan which
is billed according to monthly usage and
when we talk about billing here the
billing can be in hours it can be in
minutes and it can also be in seconds
there are different options when you
talk about your pricing which can be
found from your Google Cloud web page
now pricing could be based on preemptive
machines pricing could be based on
reserved instances or reserved resources
I'll show you the link where you can
find more details on pricing part of it
so Google Cloud really has various
pricing options which help customers in
their different requirements whether
they would go for any of the service
models such as infrastructure as a
service platform as a service or even
software as a service one more
attractive thing about Google Cloud
pricing is that it provides committed
use discounts
for example under this scheme you can
purchase a specific amount of virtual
CPU cores and memory for up to 57
discount off regular prices if you
commit usage for either one or three
years now this is just one option there
are various such options which you can
learn about from the Google clouds page
and that really suits different
customers for their different
requirements
now when we talk about speed we don't
need to really challenge this aspect
when it comes to Google services so
Google provides its Google cloud and
Google app customer speed up to 10
terabytes because of its faster cable
system there are different kind of
machines which can be used if you are
talking about computation if you are
talking about memory hungry applications
or even storage intensive workloads okay
all in all speed is one of the defining
characteristics of Google cloud services
the cable has connections over us West
Coast main cities in Japan and even
major hubs in Asia this speed enhances
performances and leads to customer
satisfaction now when we talk about
customers any or every customer would
prefer to have low latency High
throughput Based Services they would
want to use higher speeds to process
their data in as less time as possible
Google provides a low latency Network
infrastructure in fact you can say that
when you are using Google cloud services
you are using the services from the same
infrastructure which Google uses for its
popular services such as Google search
or even YouTube which is one of the
second largest repository which can be
accessed for videos now when we talk
about Big Data big data is data which is
very complex has lot of other
characteristics such as your volume you
have velocity you have variety you have
velocity validity volatility virility
and so on so if an organization has is
working on Big Data Google Cloud can be
a better choice because Google has many
Innovative tools for cloud warehousing
for example such as bigquery and even
real-time data processing tools such as
data flow bigquery is a data warehouse
that allows massive processing of data
at high speeds basically working on your
structured data Google also has launched
some new machine learning from
artificial intelligence tools now there
are various other services which we can
which we can use from Google Cloud
platform but let's understand what is
Google Cloud platform and what are some
of the services even the services which
are not less tested here can be found in
your console from Google Cloud so what
is Google Cloud platform gcp it is a set
of Cloud Computing Services provided by
Google that runs on the same
infrastructure as I mentioned that
Google uses for its end user products
like YouTube Gmail and even Google
search the various set of services
offered by Google Cloud platform are so
you have Services which are specific to
Computing requirements and again in
Computing you have various different
options available you have machines
which are compute optimized machines
which are memory optimized machines
which are storage optimized and also we
have certain machines such as preemptif
which basically means that you could get
a machine to work on at a far lesser
price than any other machine but when we
talk about preemptive these are the
machines which can be requested on
demand and they can be taken back by
Google at any time you have networking
related Services which can be very
useful when you are setting up your
applications or your services across
Globe you also have different Services
which are specific to machine learning
and organizations which would be
interested in working on machine
learning or artificial intelligence
would be really interested in using
these Services there are also innovative
solutions to work on big data and Big
Data related Technologies now when we
talk about Google Cloud platform domains
so we can break down these Services into
specifics such as you have compute now
the compute service allows for computing
and hosting the cloud
now when you talk about Computing here
there are different services such as app
engine you have compute engine you have
kubernetes you have Cloud functions and
Cloud run when you talk about storage
and database so the storage and database
service allows application to store
media files backups or other file like
objects now various Services Under This
are as follows you have cloud storage
Cloud SQL Cloud bigtable for
unstructured data you have Cloud spanner
cloud data store persistent disks and
Cloud memory store when we talk about
networking the networking service allows
us to load balanced traffic across
resources now as I mentioned earlier
resources could be your different
resources which you would be using from
a cloud platform such as your devices
your instances memory optimize or CPU
optimized instances us or other
resources creating DNS records and much
more so various Services Under This are
VPC that stands for virtual private
Cloud you have Cloud load balancing
Cloud armor Cloud CDN you have Cloud
interconnect DNS and network service
tiers when we talk about the big data
service this allows us to process and
query big data in Cloud now various
Services under these are as follows you
have bigquery cloud cloud data proc
Cloud composer cloud data lab cloud data
prep Cloud Pub sub which is publishing
subscribing system you have cloud data
studio now you also have the developer
tools and developer Tools service
includes tools related to development of
an application now various Services
under these are as follows that is you
have Cloud SDK software development kit
you have deployment manager Cloud Source
Repository and Cloud test lab when we
talk about identity and security which
is one of the primary concerns for any
organization or any user who would be
interested in using a cloud platform
Google Cloud really has taken care of
this so when you talk about identity and
security domain this deals with Security
Services now various Services here are
Cloud identity you have identity and
access management that is cloud IAM you
have identity aware proxies you have
cloud data loss prevention API security
key enforcement Key Management Service
and many more you also have cloud
services which are related to internet
of things so very much would be used by
organizations who would be working on
iot devices or the data generated by
these devices so various Services here
are Cloud iot Core you have Edge TPU and
Cloud iot also when we talk about Cloud
AI that is artificial intelligence this
comprises of services related to machine
learning and much more so you have Cloud
Auto ml Cloud TPU Cloud machine learning
engine job Discovery dialogue flow
Enterprise natural language Cloud text
to speech and much more
if you would be interested in Services
related to API platform then there are
different Services Under This category
or this domain so you have Maps platform
you have APG API platform monetization
developer portal analytics that is API
analytics APG sends Cloud endpoints and
service infrastructure so these are some
of the listing of services under each
Cloud platform domain now let's look at
Ferrero use case and let's also
understand what was done here so Ferrero
is one of the famous chocolate and ranks
third among worldwide chocolate and
confectionary producers it was found in
1946 in Italy I'm sure you would have
seen the Ferrero chocolates when you
would have gone out to buy some
chocolates so the challenges here was
that Ferrero as we know is sold in every
supermarket and is known for its quality
once the prisoners grew some issues
arose right and that's what happens when
the business grows you have issues
popping up which could be related to the
volume of data the speed with which the
data is getting generated the variety of
data and also looking at your platforms
which support different Dynamic
applications or your scalability
requirements performance requirements
and so on
so it needed data storage processing and
Analysis system for a wide customer
database
there was a huge gap between the company
and the people who bought its goods
because the company relied on data given
sales outlets now that's one of the
challenge Ferrero wanted to create a
digital ecosystem where there was a
point of contact with its customers and
also a foundation for an Innovative
data-driven marketing strategy what was
the solution here so one of the service
of cloud platforms or Google Cloud
platform is bigquery and this was an
answer to ferrero's challenges since it
was capable of hyper fast and efficient
data analysis as a solution now as I
mentioned bigquery is a data warehouse
which allows you to store structured
data now this could directly be used as
a service where you could store in any
amount of data and you would not be
paying for storage so there are
different pricing models and for data up
to one terabytes you would not be
charged anything and if you would be
accessing the data that is reading the
data or processing the data from
bigquery that's where the pricing model
kicks in
Now using Google Cloud's bigquery
business analysts of Ferrero were able
to store and analyze massive data sets
in a very reliable fast and affordable
manner
consumer behavior and sales pattern data
reports were easy to build and automate
and the analysis also followed Ferrero
to adopt advertising across various
marketing channels to serve the customer
needs in a better way what was the
result
they could divide their database into
real-time actionable consumer clusters
to generate more accurate user profiles
Ferrero was also able to personalize its
marketing strategies to match the user
needs
now Google Cloud platform completely
tailored the website mobile content and
advertising and created a very cost
effective media by strategy now these
were some Basics on Google Cloud now as
I said you can always find lot of
details about pricing about services and
also all the services are accessible in
your free trial account now let me just
walk you through here so one is you
could always find details on
documentation on each of these services
so if I would click on getting started
you have quick start which basically
shows you short tutorials you have
trainings and you have also
certifications now if we click on quick
start now that takes me to this page
which shows me
quick starts or if you would want to
understand about different projects if
you would want to look into the
documentation of creating a Linux
virtual machine or storing a file and
sharing it deploying a container Docker
container image and so on so you have a
lot of quick starts here you can also
look at Cloud minute
on the same page on the right side you
have docs and once you click on that it
takes you to these links now this shows
you build Solutions which shows your top
use cases best practices all the
solutions it's always good to learn from
these use cases which are available and
here you have different feature products
and all the different Services which
Google Cloud offers now you can click on
featured products and that basically
shows you a list of these products now
here we have all the solutions which you
can look at architecture database
Enterprise level big data and Analysis
gaming related internet of things and so
on now if you go down here it shows you
featured products and that shows you
some of the important products such as
compute engine which is belonging to the
compute domain you have Cloud run you
have anthos which is for migration and
basically Cloud adoption when
organizations would want to move from
on-premise to cloud-based Solutions you
have Vision AI you have cloud storage
now that basically allows you to store
any kind of data whether that's an
object so this is acting as an object
storage you have Cloud SQL which is
basically a ready to use service where
you would be using MySQL postgres or any
other SQL Server database Services you
have bigquery which is a data warehouse
which basically allows you to store your
structured data and then you have your
AI and machine learning related products
so you have automl Vision AI video AI
text to speech speech to text and so on
now you also have different platform
accelerators which can be used and in
any of these cases for example if I
would click on compute engine which is a
featured product from Google Cloud now
that shows you basically your quick
starts using your Linux machines how-to
guides which tells you completely
working on a VM instance
or working on storage working on
persistent disks and so on and it shows
you the documentation here now you also
have products and pricing option which
you can see for GCB pricing and you
could straight away go to the pricing
you could be looking at Solutions which
talks about infrastructure modernization
now this is something which
organizations are interested in when
they would want to move from their
on-premise solution to your Google Cloud
now here I can say for example I click
on infrastructure modernization you can
always find some case studies what are
the different solutions we have here
right when you talk about Google Cloud
so you can always click on see Solutions
and you can look at VM migration or sap
Cloud right why Google Cloud what it can
be used for VMware as a service or HPC
that is high performance Computing and
about these services we will learn in
detail later now I can go back on the
same page where I was clicking on the
services so you have quick starts you
have how-to guides you have deep
understanding of different concepts
right and here if I click on all how to
guides so that shows me what are the
different ways in which you can work
with compute engine and working with
different instances although it's quite
exhaustive content but then if you
follow steadily then you can learn a lot
about Google Cloud now here I can just
go back so this is just giving you some
idea on the different products which are
available from Google Cloud looking into
different sections finding right
documentation here right and you can
always look into each one of these in
detail for each product which Google
Cloud offers
now if we scroll down we could see all
the options or all the domains which we
see here right let me just go back
because we got into Solutions now we
have quick starts right and then you can
basically scroll all the way down to
look at Cloud SDK which can be set up on
your Windows machine like I have set it
up on my Windows machine plus when you
use Google console you have a GUI which
I'll show you in a couple of minutes and
also a cloud shell where you can use
your command line options to work with
Google Cloud platform you have Cloud
console which is nothing but your GUI to
access your resources now you can also
look at in-depth tutorials pricing and
you have different other options so
let's just click on pricing here and
then we have your price list which
basically gives you details of different
services what are available and what are
the services or what are the prices so
for example if I click on compute engine
right and this shows me the pricing
aspect of compute engine which belongs
to compute domain and here you can see
you have VM instance pricing you have
networking pricing sold tenant nodes
which are specific to particular
organizations or if organizations would
want to have dedicated nodes you have
GPU based pricing right so General
processing units you have disk and image
pricing and you can click on any of the
links and you can see pricing which also
shows you your different kind of machine
types so here you have different kind of
machine types which says N1 N2 n2d E2
you have memory optimized machine types
you have compute optimized you have
premium images and you can basically
look at all the categories you can look
at disk pricing which involves your
persistent disk pricing for ssds or sdds
what are the kind of images what are the
different network Services right and you
can always look at your machine types
you can choose a particular region right
there is the concept of region and
availability zones here and as per
region you could look at the prices
you can also look at standard prices you
could be looking for what are the free
tier machines if I'm specifically
looking for VM instance pricing I can
click on this one and that takes me to
the VM instance pricing what is the
billing module what is the instance
uptime what is the resource base pricing
and then you can also look at different
kind of discounts such as sustained use
discounts committed use discounts
discounts for preemptable VM instances
and so on and this is how you can be
looking at pricing for all different
resources and you can choose the
resource which you are interested in and
then look for the pricing benchmarks
reservations if you would want to use
and you would want to benefit if you
would want to look at what are the
quotas and limits and so on so please
explore this link and you can find lot
of information when it comes to
technical options looking at different
Google Cloud products which we briefly
discussed when it comes to domains and
what each product can be used for you
can always come back to the main page
where I was showing you different
Services we went into pricing straight
away right now you can always come back
to this page on your cloud.google.com
and then you have your Solutions
which talks about different products
right and you can click on these or you
can look into technical documentation so
this talks about your different featured
Solutions infrastructure Solutions you
have data center migration related and
so on so I could be talking more and
more about Google Cloud platform it's a
ocean it's a huge chunk of different
services for different organizational
requirements so look into this link and
also what you can do is create a account
on Gmail I mean you could create a free
account or you could go to
cloud.google.com and then you could
create a free account like I have
created here and I can just click on
Console now that's my GUI or Google
Cloud console which basically allows me
to work with Google Cloud now there are
a lot of options here by default when
you create an account now in my case on
the top it says it's a free trial
account I have three hundred dollars
credit and out of that 251 dollars is
left and 237 days out of a year are left
for my account now here I can basically
see the project right and by default we
can create an project or by default
there is a project existing for any user
so when you log in it creates a
particular project and you could create
a new project and project would be to
dedicate your different services or
different resources per different
project so this is your dashboard which
basically shows your project which shows
you a project number and a project ID
which is always unique now you can click
on this and this if you would want you
can hide this information you can also
look into the documentation part of it
then it shows you the different services
or a graphical information of services
which you might have used in past so you
have compute engine which shows you how
how much percentage of CPU was used you
can always go to compute engine as a
service you can look at your Google
route platform status and look at the
Google Cloud status now this is billing
now since this shows me the billing
period for April month and I can always
look at detailed charges I can look at
reporting I can look at different apis
which Google offers and for your
different kind of work you can always go
to the API overview or the API link and
enable or disable any API now once you
enable or disable any API then you can
use that so you have new section you
have documentation and you have getting
started guides which tells you how to
work or enable different apis if you
would want to deploy a pre-built
solution at Dynamic logging monitoring
errors deploying a Hello World app and
so on now this is your dashboard I can
click on activity and that that would
show me what kind of activity I would
have done in my cloud platform I can
always choose the kind of activities by
saying activity type I can choose the
resources now it shows me that I created
some VM instances I deleted them I
updated some metadata I basically worked
on instances I changed some firewall
rules here I have then also worked on
some other services or apis so I created
some buckets which is for object storage
and again I have been working with some
instances and creating some firewall
rules here I have granted some
permissions I am setting up a policy
right so this activity gives me a
history of what things I have done over
past couple of months while working on
Google Cloud platform now
on the top left corner you have the
hamburger menu which you can click on
this so that's your navigation menu and
when you click on this this shows you
home it takes you to the marketplace it
specifically takes you to billing you
can always look at apis and services so
here in API Services I can pretty much
go to a dashboard and I can see a report
on the traffic or errors or latency and
the kind of apis which are available so
you have compute engine API you have
bigquery API bigquery data transfer API
bigquery storage cloud data proc so
these are some of the apis or Services
I've used in past might be I was
evaluating a product might be I was
using a particular product so you have
whenever you would want to use a
particular service from Google Cloud
platform you would be enabling these
apis so here we see some apis for data
proc you have logging monitoring you
have have resource manager API you have
Cloud SQL which allows you to directly
use MySQL or postgres you have cloud
storage right and so many apis so if I
would want to enable a particular API
which is not right now required but then
it's good to know you can always click
on this one you can search for an API so
for example if I would say data proc
right and that shows me the cloud data
proc API which manages Hadoop based
clusters and jobs on Google Cloud
platform so I can spin up a Hadoop
cluster and I can start running some
jobs on a Hadoop cluster on demand and
as I'm done with it I can just get rid
of it so this is an API which I would
have to enable and then for every
particular service you also have an
admin API which you would enable
now going back so coming out from this
API Library I got into API and services
now there is you can look into the
library you can look at credentials and
you can also look at different time
intervals for which you can see the
usage of your different apis now going
back to the menu you have support
wherein you can always reach out to the
Google support team if you are using a
paid version even with free trial you
can try to reach out and you will find
someone to help but it is always good
when you have a billing cycle reaching
out the customer care you have identity
access management and admin and this is
required when you're working with
different apis or Services when you
would want to have relevant access you
have getting started you have security
related options you have anthos which is
mainly for migration now here you can
look at your different domains such as
which we discussed so you have of
compute and in compute you have
different services so you have app
engine you have compute engine
kubernetes you have Cloud functions and
Cloud run you can look into the storage
aspect which shows you bit table which
is usually and mainly for your
unstructured data big table which is
something which gave rise to popular
nosql databases like edgebase and
Cassandra you have data store which can
be used you have firestore file store
storage which can be used to put in data
of any kinds you have SQL for structured
data you have spanner as a service you
have memory store and you have data
transfer now when it comes to the
networking domain you have all these
services such as VPC virtual private
network network related services for
your load balancing for using a
cloud-based DNS or Cloud defined Network
you have hybrid connectivity network
service tiers security and intelligence
then you have other options for your
operations which can be used and here
you have different other tools which can
be used such as Cloud build you have
Cloud tasks containers registry
deployment manager and many more big
data specific you have the services here
so you have data proc which can be used
to spin up your clusters you have
published subscribe messaging systems
such as now Kafka which you might have
heard of originates its idea from here
so Pub sub you have data flow you have
iot core bigquery which is a data
warehouse package or data warehousing
solution from Google Cloud now then you
have your artificial intelligence
related services and other Google
Solutions so you can always find a huge
list of services or you can say at high
level Solutions offered by Google cloud
and we can use these to basically test
some of the solutions use them and work
on them so I'll give you a quick demo on
different Services which can be used
here from your Google Cloud platform now
this is your Google Cloud platform and
this is your console now this also gives
you a cloud shell which can be activated
so that you can work from command line
and you can always find a lot of
documentation on that so you can also
set up your Cloud shell that is SDK on
your Windows machine and if you have set
it up then basically you could be doing
something like gcloud right if the cloud
has been set up so in my case I had set
up the cloud SDK and basically I can get
into that by looking at what path I have
set up so Cloud SDK and then I can be
using it from my Windows machine as per
my comfort I can also activate the cloud
shell here which will basically open up
a terminal and at any point of time you
can open up this one Cloud cell in a
different window it is preparing the
cloud shell where it will by default set
up your project it will set up the
metadata and now I am logged into my
Google Cloud account from command line
and here I can basically use gcloud
right and that basically shows you the
different options which are available
which you can be using so if you would
want to use a particular service you can
also try giving a help and that shows
you how do you manage your Google Cloud
platform right so you have different
options for billing to work with your
different services and basically you
could be working on Google Cloud using
this Cloud shell from your command line
usually for people who are learning old
cloud in the beginning it is always good
to go for console and use your different
services from here in an easier way but
as I said you can always use the command
line for example if I would just go
ahead and type gcloud create instances
and that will directly take me to the
cloud console documentation which shows
you different options which you can use
to work with instances so I could do a
gcloud compute instances create and then
I can give my instance name and so on
and I'll show you some examples on that
so you can be using your Cloud console
right that is your Cloud shell and you
can straight away start working from
command line however I would suggest
using console in the beginning and when
you are well experienced then you can
start using Cloud shell to do things
from command line and when you are fully
experienced you can always switch and
certain things are usually useful or
easier when done from command line and
some are easier when you do it from the
console and you can use any one of these
options now we can go to Google Cloud
console now as of now I can close this
one I still have my cloud cell open if I
would want to look into it I can click
on this one and I can straight away go
to compute end region where I would want
to work on creating some instances on
Google Cloud platform using the compute
engine service and then basically
connecting to those instances and
basically trying out some basic things
so you can click on VM instances and
then once this comes up I can always
create some instances now if you see
here I have some instances already
created right and I can continue working
on those I can create new instances I
can use different options while creating
instances and I'll show you that in a
demo in quick seconds so let's have a
quick demo on setting up gcp instances
now before that let's have a quick recap
so when you talk about instance or a
virtual machine it is hosted on Google's
infrastructure right and you can create
your Google Cloud instance using this
Google Cloud console that is by clicking
on this and then going to compute engine
and clicking on VM instances now you can
also do that from Google Cloud command
line tool that is cloud shell and you
can be doing that using compute engine
API so compute engine instances can run
the public images for Linux or Windows
servers that Google provides you also
have option of creating or using custom
images that you can create and import
from your existing systems you can
deploy Docker containers which are
automatically launched on instance
running containers optimized OS now when
you talk about instances and projects
always remember that in each instance
belongs to a Google Cloud console
project and a project can have one or
more instances
when you talk about instances and
storage options each instance has a
small boot persistent disk which I will
show you in for this queens that
contains the OS you can add more storage
space if needed and when you talk about
instances and network a project can have
up to five VPC networks so VPC network
is virtual private networks wherein you
can virtual private Cloud networks where
you can have your resources within your
own subnet and each instance belongs to
one VPC Network
now instances in same network can
communicate with each other through
local area network protocol an instance
uses internet to communicate within any
machine so that could be virtual
physical or outside its own network when
you talk about instances and containers
you should remember that compute engine
instances support a declarative method
for launching your application using
containers now you can create a VM
instance or a VM instance template you
can provide a Docker image and launch
the configuration
so there are different ways in which you
can create these instances and once you
create these instances you can say for
example you're creating a Linux instance
you can associate SSH keys with your
Google account or your G Suite account
and then manage your admin non-admin
access to the instance using IM roles if
you connect to your instance using
gcloud or SSH from console which we will
see later compute engine can generate
SSH keys for you and apply that to your
Google cloud or G Suite account now what
we can do is we can see how we can
create your Google Cloud instances from
your console or from the command line
let's have a look in creating instances
using GCB console now that we have
learned on some basics of Google Cloud
platform what are the different Services
what are different domains basically
looking at your Google Cloud console or
even Cloud Shell let's go ahead and
create some VM instances now that's from
your compute engine service and let's
try connecting to these instances and
see how this works let's also see what
are the different options which are
available when you would want to create
the virtual machine instances here so
when you click on your drop down from
the top left and choose compute engine
so it brings you to this page so I can
show that again so you can click on
compute engine click on VM instances and
that basically brings you to this page
now here it tells you compute engine
lets you use Virtual machines that run
on Google's infrastructure so we can
create micro VMS or large instances
running different distributions of Linux
or Windows using standard images that is
public images and you can also have your
own image so let's create a VM instance
by clicking on create
now that's basically helping me to
create an instance here so it shows me
the instance name I can give it
something so let me say C1 now I can add
labels so what is label it basically
allows you to organize your project add
arbitrary labels as key value pairs to
your resources this so this is basically
categorizing your labels and projects if
you have multiple projects now remember
if you have your Cloud console and if
you have created your free trial account
it will allow you to do these things if
not you may have to go back to the
billing section and see if the billing
is enabled which also means that when
you are creating your Google Cloud
account it will ask you to clean your
credit card details but they do not
charge anything or they might charge
might be one dollar or one rupee
depending on your location and that's
also refunded but that's just to verify
your card now once I've given the name I
can choose a region so I would basically
choose Europe since I'm in Europe I
would be clicking on Europe West 3 and
here I can choose an availability Zone
availability zone is basically to make
your services or instances or any other
resources highly available so you can
choose one of the availability zones now
I would choose save S3 now this one we
can scroll down and here it says machine
configurations you have general purpose
machines you have memory optimized
machines which is M1 series and you can
always go back to the Google Cloud page
and see what a particular kind of
machine specializes in so I would click
on General's purpose and in general
purpose you have different categories so
you have N1 which is powered by Intel
Skylake CPU platform or you have E2
which is CPU platform selected selection
based on availability so let's have N1
selected now this one shows me the
machine type and if you are using your
free tier account then you can start
with correcting a micro machine which is
one virtual CPU core you can go for a G1
small which is one virtual CPU core and
1.7 GB Ram you can even go up to a
high-end machine and then you can
basically see if you are using a free
account how many of these machines you
can use if I would be selecting eight
virtual CPU cores and 32 gigabyte of
memory then it would allow me to create
at least two instances by this
configuration we will use N1 standard
which is one virtual CPU core 3.75 GB
memory now then we can also deploy a
container image to this VM instance if
you would be interested in deploying a
container image let's not get into that
right now here it shows you the port
disk and it shows you dbn gnu 9 Linux 9.
what I would do is I can go for this
distribution or I can choose a Linux
distribution of my child choice so here
you have public images you have custom
images you also have snapshots if you
have created backup of your previous
images here I can choose for example
Ubuntu and then I can choose a
particular version so let's go for
Ubuntu 18.04 you can go for the latest
one also 18.04 would be good enough and
here it tells me what is a boot disk so
you have ssds or you have standard
persistent disks now ssds are little
expensive in comparison to your standard
persistent disks or your sdds but then
ssds are faster so as of now we can
choose standard persistent disk as it is
and we can let the gigabyte be 10 now
depending on your requirement you can
increase this you can even add disks
later that's not a problem click on
select now here I have access Scopes so
here I will say allow default access you
can also set access for each API or you
can give full access to all Cloud apis
so that based on your requirement you
can any time change later also we will
also say allow HTTP traffic and we can
also choose allow https traffic that
basically allows me to access this
machine or Services which are HTTP based
accessible from this machine now I can
just click on create however it would be
good to basically enable connectivity to
this machine now we can do that in
different ways one is when you bring up
your machine it will have an SSH access
which you can log in from the cloud
platform here itself or what you can do
is you can create a private and a public
key using some softwares like putty or
puttygen so for example if you do not
have that on your machine you can
download you can just type in download
putty that takes you to the putty.org
page and here you can click on download
putty and scroll down which shows you
for your 64-bit machine which I have in
my case you can download putty.exe which
is basically your SSH internet client to
connect to your machines you can also
use putty gen which will be allowing you
to create a private and a public key
which I have done in my case let me show
you how so what you can do is you can go
to puttygen to begin with and here you
can click on generate now then to have
this key generated just move your cursor
on the top here in the empty space and
that creates your key you can give it a
name so for example let's give a
username I would give hdu now I can give
a password for this one so let me give a
simple password
and what I can also do is I can copy
this public key from here and for my
later usage I can just keep it in my
notepad file which I can use later and
I'll show you when so now we can save
this private key and this will basically
allow me to save my private key I can
choose desktop and I can give it a name
so let's say new key new key and that
will be getting saved in a DOT PPK file
so let's save it and that's done so we
also have our public key and we have
saved our private key now we know that
when you would want to connect using SSH
you need your private keys to the client
and public key also has to be existing
so let's take this public key so let's
do a control a I'm going to copy this
and I'm going to come back to my Google
Cloud console and here you can click on
security so you can click on the
security tab here here scroll down and
just give your public key once you give
that it resolves and shows you the name
and this is good enough so that I can
use my SSH client to connect to this
machine I can click on Create and this
will basically create my VM instance it
will take some time and then your
instance will have an internal IP which
will show up here external IP and it
will also show you options to connect to
these machines so this is my internal IP
this is my external IB which I can use
to connect from a client I can easily
connect from the option here which says
SSH and I can say open in a browser
window I can even open this in a custom
Port I can look at the gcloud command
which you can give from cloud shell or
you can use another SSH so let's first
do a open in browser window and let's
see if this connects so we can easily
connect to our instance the Ubuntu 18
instance which I just set up here easily
in couple of seconds now this is trying
to establish a connection using your SSH
keys and when you do that it also
basically brings up this web browser so
I'm already connected it shows my
username which is my cloud account
username and I have been connected to
this machine here using SSH it did not
ask me for any password and basically
now I can just check what do I have in
my Linux file system right and I can
anytime login as root by doing a sudo Su
for example let's try installing a
package and I can say aptkit install Vim
or app get install wget or aptkit
install open SSH and all these packages
are already existing so not an issue now
I can start using this instance I can
just look at the disk what is available
okay now we gave an 10 gigabyte out of
which we see 8.3 gigabyte here for the
dev sda1 and then 1.8 gigabyte available
and you can continue using this machine
this was the easiest way of connecting
to a VM instance using SSH
now what we can also do is I can just
leave this and I will now try to connect
using an external SSH client right and
here you can copy the public IP so when
you want to connect to an instance you
will have to get the public IP also
remember that if you select and stop
this machine which will stop your
billing counter and if you start it
again the internal IP will remain same
but the external IP is the one which
will change
I can obviously select this machine
anytime and I can do a cleanup and I can
delete it I can do a start of the
machine if it is stopped and I can even
import the virtual machine to be used
later so there are different options
which you can always use so this is my
instance and if you would want to look
at the details click on this one C1 and
that should basically allow you to look
at the details so it shows you what is
the instance ID what is the machine type
is it reservation specific what is the
CPU platform what's the zone and all the
other details
at any point of time if you would want
to edit you can always click on edit and
you can change the details as you need
now you can also look at the equivalent
rest command to basically use the rest
API to connect to this instance for now
I have copied the public IP and I would
want to connect it using putty so let's
go in here and let's give the hostname
so I can give Ubuntu I can give my IP
address so that's in my session now I
will click on SSH I would go into
authentication I'll click on browse and
this is where I need to choose the PPK
file so this is the one which we created
new key let's select this
and then I can come back to session I
can even save this and I can call it as
my instance one let's save it and you
can create any number of instances so
you see I have created different
instances here for my Google cloud or
Amazon related instances and I can click
on open it says the service host key is
not cached in the registry that's fine
just click on yes
and basically it says no authentication
method supported now this could be
because we have not enabled your SSH
access so let's look at that so now
let's see if we were trying to connect
using putty what was the issue here so
if I go back to my putty select my
instance load it it says Port 22 I am
giving the username which is Ubuntu or
sorry that's the wrong username we gave
and that might be the reason we had set
up the user as sdu so let's save this
again and now let's try connecting to
this one and it asks for my password and
you are able to connect this right now
if there was any other issue related to
network connectivity then we could look
at the rules the inbound and outbound
rules which allow us to look into the
machine now we are connected to our
machine here using hdu user I can log in
as root and I can continue working so
not only from the SSH within the cloud
console but you can use an external SSH
client and connect to your machine so
this is your Ubuntu machine and we can
basically look at the space and that
basically confirms we are connecting to
the same machine which shows 8.3
gigabytes here and 1.8 gigabytes here
which we were seeing from the ssh in the
browser now let's close this and let's
go back to our instance page now here
you can always look at the network
details and this will show you your
different kind of rules that is ingress
or egress rules which basically allows
to connect to this machine from an
external network or for this machine to
connect to an external network so here
we have different firewall rules which
shows default allow HTTP that's your
Ingress Rule and it tells you apply to
all it shows me what are the IP ranges
where I can specifically give the ipf my
machine it shows the protocols it shows
what are the different ports which you
have used for these services for example
RDP or SSH which shows 22 you have icmp
HTTP and https now anytime if you would
want to make a change to these rules
that can be done by going into your
network details and say for example you
would want to work on firewall rules so
we have these firewall rules here you
can click on this one so right now we
are looking into the network domain and
we are looking into VPC Network right
and this shows me what are the different
rules we have now if I would want to
create a different firewall rule for a
different protocol I can always click on
create firewall rule I can give it a
name okay I can say what if you would
want to turn on the firewall logs you
can basically say what is the kind of
traffic so Ingress applies to incoming
traffic and egress applies to outgoing
traffic and you can then basically
choose what are the IP ranges from where
you want that connection to be coming in
or to going out you can choose a
particular protocol you can give a
protocol here with comma separated
values and you can create a firewall
rule so this might be required depending
on the services which you are running
wherein you might want to enable your
access to your machine from an external
service or to an external service so you
can always go to network details from
here you can then go into firewall rules
create a firewall rule apply that to
your instance and restart your instance
so as of now we don't need to create any
network details here because my Ingress
or egress rules are already available
right now I can basically then have my
instance stopped and removed so I can
just do a stop or since this is running
the ideal wave would be to do a stop I
could also do a reset now what does
reset do reset does not basically delete
the machine what it does is it does a
cleanup of the machine and it brings it
to the initial state so sometimes we
might have installed certain things on
our machine we would want to clean them
up and at that time reset can be useful
I can basically click on delete right
and I can select this and I can do a
cleanup and this is good always when you
are using a free trial account try to
use different Services play around with
them and then you can clean up so that
you do not waste your free billing
credit right and you can use it for
Meaningful stuff now I have clicked on
delete and within few seconds my
instance which I had created will be
deleted also to remember is if you are
creating multiple instances then you can
connect from one machine to other
machine using SSH by using the private
file so for that we can learn in detail
later so this is just a simple example
of using your compute engine creating
your instances connecting to it from an
internal SSH or from an external SSH
client such as putty where you have
already created your private and public
Keys now I can click on the browser here
and then I can basically come out and I
can basically be looking into any
particular service so we just looked
into compute engine right you have
different other options you have
instance groups you have instance
templates for example let's click on
instance templates here and that
basically shows you that you don't have
any instance template and this basically
facilitates say for example you are
working as a admin and you would want to
create an instance template so that
using that you can describe a VM
instance and then you can basically use
this template to create different
instances you can go for soul tenant
nodes you can look for machine images
you can also look at your disks you can
create a snapshots and you can look at
different options here now let's come
back here and let's click on home and
that should take you back to your home
page which basically shows you if a
particular API which you have used
recently which shows me in the graph
here so I can go to the api's overview
right and that basically shows me if
there were any errors if I was using the
compute engine API to basically create a
VM instance and that's why we were
seeing some spike in the graph so this
is a quick demo of using your compute
engine service provided by Google Cloud
wherein you can create VM instances and
use those VM instances for your
application installation for any other
purpose now that we have seen how you
use your Cloud console to create an
instance and also clean it up let's also
understand how you can do using your
command line options and let's see what
it takes or what are the different
commands which you can use to create
your instances
now you can always when when you're
creating an instance you can use compute
engine which Provisions resources to
start the instance so instance basically
has different states which we can see
when we are creating the instance so you
basically start the instance instance
moves into staging that is prepared for
your first boot finally it boots up and
then it moves into running so when you
look at instant states which we will
create instance in C it will basically
have different states such as
provisioning where resources are being
allocated for instance but instances not
yet running then it goes into staging
where resources have been acquired and
instance is being prepared for your
first boot then instance is booting up
and running and if you are stopping an
instance it goes into being stop status
and it would be moved to the terminated
option you can also do a repairing of
instance and finally you can terminate
your instance or clean it up by stopping
and then deleting it now when you say
stopping and resetting an instance you
can stop the instance as I showed
earlier if you no longer need it but if
you need for future use you can just use
the reset option which will basically
wipe the content tense of instance or
any application State and then finally
you can stop it and have it terminated
now when you would want to do that using
your Cloud console or we have seen the
options now let's also see from the
command line tool how you can do it so
here I have the cloud cell which I
brought up from here and I basically
opened it in a new window so command
line tool enables you to easily manage
your compute engine resources in a
friendlier format than using your
compute engine API now gcloud which is
part of cloud SDK is the main command
here and then you can always auto
complete the different options here so
when you would want to create or work on
gcloud you can just type in gcloud here
and then for example I could just say
help to see different options of gcloud
which will show me the different options
which you can use here now we would be
interested in compute instances so I
could also do a gcloud compute
compute instances and then I can
basically say create and then I can do a
help now that should show me different
options which work with gcloud compute
instances create command which is
expecting an instance name so your
Google Cloud SDK which we can set up on
our Windows machine or even on your
Linux machine a set of tools that helps
you to manage resources and applications
hosted on your gcp that is your Google
Cloud platform now here you have options
such as gcloud which I'm showing you
right now you have Gs util and then you
have BQ so that also can be used so you
can set up a Google Cloud compute if
using gcloud now what we can do is if we
are setting up our SDK on our Windows
machine then we would have to do a
gcloud init which basically initializes
your configurations for GE cloud
now here we are using a cloud shell
which was started from the console and
we don't have to basically give your
gcloud init command because it's already
initialized now you can always look at
your default Zone you can look at your
region what is being used all those
things are coming from the metadata
which is being used so for example I
could be looking at the metadata for my
particular project by just doing a g
cloud and then I can say compute and I
would be interested in Project info so I
can just do a project minus info and
then I can do a describe and then I need
my project ID so I can say minus minus
project and I can get my project ID from
here so you can click on this one and
that's my project ID so I can click
click on this one and here I can just do
a right click and if it does not paste
then you can do a control V and then I
can try to look at my project info so
this basically will give me the metadata
which is by default set and we can
always look at what are the regions or
what is the Zone which has been set so
if it has been set so I'm looking at my
details it is showing me my SSH keys and
then it can be using your default region
and your default available zones it also
shows my username and other details so
this is basically to look at the
metadata which is available now at any
point of time I can basically say add
metadata I can basically choose metadata
option and I can say my default region
should be Europe which I was choosing
earlier so I can just bring up this
command again and what I can do is I can
say here where I have gcloud compute
project info where I did a describe
earlier now what I could also do is I
can just say add metadata
and then I can specify what is the
metadata I would be interested in adding
I can then say Google compute
default
region and then I can basically give a
region for example Europe and then I can
say West 3
and if you are not well experienced you
can always do it from console or you can
be doing it from here so I'm giving
Google Cloud compute Google compute
default region then I can also give a
Google compute default Zone
and then I can pass in a value for
default zone so I can say Europe West 3
and then basically I can give an
availability zone so if it is basically
giving me some error where I'm trying to
pass in these values so I can just give
it this way and then it basically says
that if you can look into particular
help to see what is the command which
you would have to do now I can basically
do a g Cloud init here initial
configuration and this basically says
that it is initializing my default
configuration it says re-initialize this
configuration from cloud shell you want
to create a new configuration so if I
had updated my metadata then basically I
could just do a cloud in it and I could
be re-initializing my default
configuration or properties which I have
passed constant so as of now I will not
activate or change the default region
let's go for a simple way in which you
can create your compute engine so for
example I'll just say one and it is
reinitializing it asks me what is the
username and let's select that it says
what is the project and let's select
that and do you want to configure a
default compute region and zone right
and I will just say Yes And basically
then it shows me different options which
we have here so there are too many
options here and then here we were
interested in Europe West 3 so let's
choose 21 right and then that basically
allows me to choose my region and my
zone so it also gives you some
specification so it says your project
default compute Zone has been set to
Europe West 3A you can change it by
running gcloud config set and you can
give a compute zone right so I can
always give these commands I can get
help information here so I can just say
gcloud config set and I can be
specifying the compute zone so I can say
compute slash Zone and I can give a Zone
I can say compute slash region and I can
give a region name if I would want to do
it or the easier ways like what I did
right now so you can basically do a
command gcloud in it and then basically
it allows you to to change your
configuration or set default things here
you can all the times you can say gcloud
config unset to basically remove a
compute zone or a compute region now
there are different ways so if you were
working on a Linux machine you could
always use an export command something
like this so you could do export compute
sorry Cloud
SDK and then you can say compute
underscore Zone and then give your Zone
name or compute underscore region and
give your region or you could add it in
your bash RC file right now that is when
you have your Cloud SDK set up on your
Linux machine or on Windows machine and
you would want to specifically set a
Zone and a region for all your compute
related resources so we don't need to do
that the default settings are already
given here right and what we can do is
we can start by quickly looking into
gcloud compute instances options so I
can say gcloud compute instances and
then I can just do a list at any point
of time if you would need help you can
just do a gcloud compute and then say
minus minus help or I could just do a
gcloud compute and that shows me
different options which I have here from
which I use instances I can always type
instances and again hit enter and it
shows me different options what you
would want to do so I can initially just
type list which should show me what are
the list or what are the instances
available and as of now we don't have
any instances I can always do a list and
then I can specify minus minus format
and then try to get the information in a
Json format or EML or minus minus format
text
so you can always do a list you can do a
filter so there are different options
with your list and you can you can try
doing a help here and that basically
shows you what are the options so for
example if I do a help and it shows me
with list what are the things you can do
so you can give a name you can give a
regular expression you can say in
particular Zone you can use a minus
filter so there are different commands
which are available and you can always
find all those options here as I showed
you earlier so now what we would be
interested in is basically going for
your compute instances I can always do a
SSH and I can create an instance I can
add and remove metadata right for my
instances by giving a particular Zone by
giving a particular region and if you
would want to do that now here what we
can do is we can can basically create an
instance by just giving a name to that
particular instance and we can then go
back to our console and see what has it
done so I can say here create so that's
an option let's see what does the create
do so it says okay you are giving a
create option but you would have to give
a name so let's say let's call it E1 and
that will be the name of my instance and
this one is an easy command from your
Cloud shell which basically creates an
instance you see the Zone which has been
set it is the standard machine type it
is not preemptable it has an internal IP
and it has an external IP now I have
created an instance and here I can just
go back and then I can just do a Refresh
on this page and that already shows me
the instance which I have created and
then you can use the same method to
connect to it using an SSH so I have an
instance created from my cloud shell and
what I can do is I can basically look
into instances and see what are the
different options with instance so if we
did a create right you have an option
delete we did a create you have an
option delete you can be in deleting the
instance from the command line or you
can use other options so you can do a
list you can do a stop you can start so
I can basically do a stop and let's say
even let's go ahead and do a stop
commands are pretty easy here to
remember or you can always use the help
option and now I'm trying to stop the
instance and then basically I can go
ahead and delete it so this is a simple
way where I created a instance from the
command line or from the console which I
showed earlier and similarly you can be
working with other options so now I have
created stop the instance let's do a
listing to see if my instance shows up
it shows up right and it says the status
is terminated so you have stopped it
it's in the terminated status and now
what I can do is I can go ahead and
delete it
so it says this will be lost are you
sure you would want to continue just say
yes and that should take care of
deleting your instance
so this is a quick demo on creating your
instance we have already seen how you
can connect to these instances using SSH
now that we have created instances let's
also see how to use Google Cloud's
storage service which can be used to
load data or upload data so for this we
will have to look into your Google cloud
storage options and here you can look in
cloud storage so let's go back here on
the top which shows me different
Services which we have here and let's
look into storage so this one shows me
your storage option now you can click on
browser and that basically shows me your
storage browser which shows me on the
top you have options for creating a
bucket now Google Cloud Storage allows
you to store any kind of data here the
easiest and the simplest way would be by
using Cloud console although you can
again use cloud shell and in that you
can use your GS util command and GS util
basically has different options where
you can be using say for example I would
want to work on buckets so I can be
using MB and then I can use my command
line option to create buckets I can put
in data there I can browse it and I can
access the data from command line so
let's create a bucket here let's click
on this let's give it a name so let's
say my data important that's the name
now I can click on continue straight
away I can look into all of these
options if I'm interested in so you can
basically be looking at your monthly
cost estimate in the beginning now I can
click on continue or you can say choose
where to store your data and this one
shows you different options so you have
region specific
so let's also give the bucket name with
all lower case that's what is required
yeah so coming back to the location type
you can choose multi-region which
basically allows High availability so
your bucket or your storage option will
be accessible across regions you can
also give dual region and high
availability and low latency across two
regions I can also say region specifics
or for our use case we can just keep
region specific which can keep our cost
low but in business use cases you would
be going for multi-region now here you
have location and I would again go for
say Europe and let's choose Europe West
3 Frankfurt it is always a good practice
that when you create your instances when
you create your storage or you use
different Services try to have a
geographical region Chosen and then you
would try to put things on your services
within the particular region within a
particular Zone unless you would want to
make it accessible and available across
regions
now I can then choose a default storage
class so when you say default storage
class there are different storage class
and each one is for a different use case
so you have standard which is best for
short-term storage and frequently
accessed data you have near line so this
is basically best for backups and data
access less than once a month you can
also have code lines so these are
basically like your cold storage or
freezing storage which you might have
heard generically as terms so you can
choose one of these storage classes
depending on what will be the use case
for this particular storage bucket so
let it be standard now how to control
access to objects so you can basically
say specify access to individual objects
by using object level permissions now
you can give permissions to the bucket
you can just say uniform access to all
objects in the bucket by using only
Bucket Level permissions so you can
choose that you can also go into
advanced settings and here you can see
different ways in which you can have
configuration set up now you can also
have a retention policy to specify the
minimum duration where that this
bucket's object must be protected from
deletion and you can set a retention
policy will not get into all that we
will just first try creating a bucket so
just click on Create and that should
create your bucket wherein I have my
bucket now I can click on overview to
see the details what is the region which
region it belongs what is the storage
class anytime you can always click on
edit bucket and make some changes here
you can look at the permissions so
bucket uses fine grained access allowing
you to specify access to individual
objects and then you can basically look
at who has access to this so in my case
editors of the project they basically
are the bucket owners owners of project
viewers of the project right and you can
basically choose what kind of access you
need so for this you can always go into
cloud storage and then you can decide
what kind of access you would want to
give whether that's a storage admin it's
an object admin object Creator object
viewer and so on you can always look at
storage Legacy
and for any other services depending on
what apis you have enabled right you can
always control your permissions so right
now it is storage Legacy bucket reader
and that's fine and this one is bucket
owner and then might be I was using some
other services like data proc which uses
cloud storage and that's why I have
given data's proc service agent also so
these are some of the members you can
remove you can view by specific members
you can view by roles so what are the
different roles which have access to
this so it says storage Legacy bucket
owner there are two owners based on this
particular project so these are Auto
controlled but then you can add or
remove machines you can save storage
cost by adding a life cycle rule to
delete objects after the duration of
current retention policy so you can add
different policies and you can basically
control your bucket now my bucket is
already created right so I can go back
and then I see my bucket is already here
I can click on this option and you can
anytime edit the bucket permissions you
can edit the labels the default storage
class you can just go ahead and delete
the bucket you can export it to Cloud
Pub sub so basically if you would want
to have the content from this bucket
being accessible in a message queuing
system you can go for Pub sub you can
process with Cloud functions and you can
scan with cloud data loss prevention so
there are different options which are
available we can always select this
bucket and delete it now I can click on
the bucket and that basically shows me
different ways in which I can upload
some data here so I can just click on
upload files and here then I can choose
some files so for example I'll go in
here I'll go into data sets and I have
different data sets so for example let's
choose this one which is a CSV file and
then I'm just uploading this to my
storage right it's as simple as this so
you can drag and drop your files or you
can just upload your files and my file
is uploaded here now I can basically
edit permissions for this one right so
this is in the bucket so anyone who has
access to this bucket can basically
download this file it can they can copy
move or rename this you can export it
and you can look at the permissions of
this file so let's look at edit
permissions and it says
for this project whoever is the owner it
has access I have also given a specific
user my Gmail ID and I have given access
so at any point of time you can just say
add item and then you can start giving
different kind of accesses so let's
click on cancel here now my data is
already uploaded into this particular
bucket and that's a simple usage of your
cloud storage now what we can also do is
I can basically select this and I can
delete it I can also create specific
folders and then basically load data
into it I can click on this one I can
just do a download and then I can
download it anywhere on my machine so
let's go to desktop and let's download
this file so we not only uploaded some
content in the bucket but we also
downloaded that in a different location
which will be accessible now what I
could have also done is I could have
created a folder here and I could have
specified say for example immediate
immediate data
okay and I'm clicking on this one so
that's my immediate data I can click on
this one and now I can upload my data
specifically to this particular folder
by using the same mechanism you can just
do upload file let's choose air
passengers let's say open and my file
will be uploaded so you can always
choose what is the retention expiry date
for this one so as of now there is
nothing right but you can be deleted
objects can be deleted or modified until
there is a minimum duration you can
control all of that you can basically
download it right you can copy it and
you can move it and rename it so this is
a simple example where I created a
particular bucket right now if for
example you click on this transfer so it
says cloud storage data transfer
products have moved you can now find
storage transfer service on-premise data
and transfer Appliance in new data
transfer section right so you can always
go back
to cloud storage you can also look at
transfer options which are mainly when
you are using your on-premise service
and you would want to upload data in a
Google Cloud right so this is my bucket
here now I can go to Cloud console and
here what I can do is if I would be
interested in working so I can just say
GS util right and then we have for
example let's try a help okay and that
basically shows me my GS util now you
can always go to Quick Start gsutil tool
and this is one more way wherein
basically you can do it from the command
line so working with buckets so you can
do a GS util MB minus B and then on a
particular Zone if you would be
interested in and then you can say GS
colon and give a name so my awesome
bucket this is how you will create a
bucket which will show you it is
creating a bucket and then basically you
can upload an image or some data from
internet you can just do a w get
download the file and then you can just
do a GS util copy command and that
basically will pick up the file from
your Cloud shell location put it in your
bucket once you have done that you can
always do a copy right so here you are
doing a copy from your local machine
that is your Cloud cell machine to your
bucket and you can do the reverse of
that that is you can do a copy and you
can point your bucket and the file and
download it to your desktops so you can
just download that using the command
line and you can copy the object to a
folder in the bucket so that is by
creating a particular folder and you are
pushing the file into a particular
bucket you can list the contents of a
bucket using LS and then give your
bucket name so GS colon so we can just
try this one to be simple and rest you
guys can try so so here you can just do
a GS util you can do a LS GS colon slash
slash and then I need to give my bucket
name so in my case the bucket name was
my data important so let's try that so
let's save my data important and I'm
trying to list the bucket and the
content it has right and then you have a
folder and then you can look into the
folder and look into the files so this
is a simple quick option wherein you can
use your Cloud cell you could be using
your Google Cloud SDK from your local
machine or you can be using Cloud
console to create a bucket upload some
data on it download the data see if the
data is accessible and that basically
shows you the power of cloud storage
where you can easily upload any kind of
data now what we can also do is as
you're using a free account or even a
paid account unless and until you would
want to keep the bucket you can select
it and you can just go ahead and delete
it so this will basically ask you to key
in the bucket name and let's say my data
important you need to confirm your
bucket name click on confirm and you
could have done that using GS util and a
delete command from the command line
that is from cloud cell so we have now
created a bucket we have uploaded some
data into it we saw how you can download
it or create a folder and upload some
specific data into it and also you can
do the same thing using GS util tools
wherein you can list your bucket create
your buckets delete it create some
folders into it and do everything from
the command line so this is in simple
way you can use your gcp where either
you can spin up your machines or you can
use cloud storage to basically use a
storage or a easy to use instance on
Google Cloud platform if you were
looking for a more detailed approach on
cloud computing with hands-on experience
then simply learns Caltech postgraduate
program in cloud computing will be the
right way to go this comprehensive
program will equip you with the
knowledge and skills needed to navigate
the cloud landscape with confidence dive
deep into the cloud architecture
deployment models security and migration
strategy you will also explore platforms
like AWS Microsoft Azure and gcp to
build your Cloud expertise so don't miss
out on this chance to transform your
career and join the ranks of successful
Azure professionals click the link in
the description to discover more about
this course in this video we will
compare and contrast AWS Azure and gcp
based on a few related Concepts around
these cloud computing platforms it will
help us understand the functioning of
these top Cloud platforms and will also
let us figure out the individuality of
each one of them but before starting
with the comparison let's have a quick
introduction of AWS versus Azure versus
gcp so let's get started
Amazon web services or AWS is a cloud
computing platform that manages and
maintains hardware and infrastructure
reducing the expense and complexity of
purchasing and running resources on site
for businesses and individuals
these resources are available for free
or for a fee per usage
Microsoft Azure is a cloud computing
service that offers a collection of
Cloud Computing Services for building
testing deploying and managing
applications in the cloud including
remotely hosted and managed versions of
Microsoft Technology
Google Cloud platform offers a variety
of Cloud Computing Services for building
deploying scaling monitoring and
operating a cloud
the services are identical to those that
power Google products such as Google
search Gmail YouTube and Google Drive
now let's move on to the comparison
between AWS Azure and gcp
we will be comparing them based on a few
major parameters like origin
service integration
availability Zone
Cloud tools like compute
storage
networking
market share
pricing
and at last who uses them
now let's move ahead and start with the
first comparison origin
in the year 2006 Amazon web services or
AWS was introduced to the market
and in the year 2010 Azure launched its
services whereas on the other hand gcp
was established in the year 2008.
from the start AWS has been supportive
of the open source concept but the open
source Community has a tense
relationship with azure
on the other hand gcp similar to AWS
provides Google cloud with managed open
source services that are tightly linked
AWS offers services on a large and
complex scale that could be manipulated
but Azure support is comparatively low
quality whereas gcp's monthly support
price is almost a hundred and fifty
dollars for the silver class which is
the most basic of services and is quite
expensive
now let's move on to the service
integration of these Cloud platforms
service integration is a set of tools
and technology that connects different
applications systems repositories and
data and process interchange in real
time
AWS makes it simple for users to combine
services such as Amazon ec2 Amazon S3
Beanstalk and others and on the other
hand Azure allows customers to
effortlessly combine Azure VMS as your
app service SQL databases and other
services whereas users can utilize gcp
to combine services such as compute
engine cloud storage and Cloud SQL now
that we know briefly about all these
Cloud platforms let's have a look at the
availability zones of these platforms
because AWS was the first in the cloud
domain they have had more time to build
and extend their network but Azure and
gcp both have various locations around
the world
but the distinction is in the amount of
availability zones they have AWS now
offers 66 availability zones with an
additional 12 on the pipeline close to
it Azure is available in 140 countries
and is available in 54 regions
throughout the world but Google Cloud
platform is now available in 20 Global
areas with three more on the way
now let's move on to the next important
factor which is tools
now let's move ahead and have a look at
the first feature which is compute
elastic compute cloud or ec2 is aws's
compute service which offers a wide
range of features including a large
number of instances support for both
windows and Linux high performance
Computing and more Azure on the other
hand as virtual machines is Microsoft
azure's core cloud-based compute
solution
it includes Linux Windows server and
other operating systems as well as
better security and Microsoft program
integration
in comparison to its competitors
Google's Computing Services catalog is
somewhat smaller compute engine that
companies principal service offers
custom and pre-defined machine types per
second invoicing Linux and Windows
support and carbon neutral
infrastructure that uses half the energy
of traditional data centers
within the compute category Amazon's
different container services are gaining
prominence it has Docker kubernetes and
it's also fargate service which
automates server and cluster management
when using containers as well as other
alternatives
Azure unlike AWS uses virtual machine
scale sets of two container services
Azure container services is based on
kubernetes and container service uses
Docker Hub and Azure container registry
for management
for Enterprises interested in deploying
containers Google offers the kubernetes
engine and it's also worth noting that
Google was significantly involved in the
kubernetes project providing an
extensive knowledge in this field
now let's move on to the next parameter
of comparison which is storage
simple storage service for object
storage elastic block storage for
persistent block storage and elastic
file system for file storage are among
aws's storage offerings
block storage for rest based object
storage of unstructured data queue
storage for a large volume workload file
storage and disk storage are among
Microsoft Azure core storage services
gcp offers an increasing number of
storage options its unified object
storage service cloud storage also has a
persistent disk option relational
database service or RDS dynamodb no SQL
database elastic and memory data store
redshift data warehouse Neptune graph
database and database migration service
are all SQL compatible databases offered
by Amazon
the database choices in Azure are
specifically wide SQL database mySQL
database and postgre SQL database are
the three SQL based choices
now when it comes to databases gcp
offers the sql-based cloud SQL and Cloud
spanner a relational database built for
Mission critical workloads now the next
parameter is networking
AWS uses Amazon virtual private cloud or
VPC
on the other hand Azure uses Azure
virtual Network or v-net
and gcp uses Cloud virtual Network now
let's move on to another factor which is
market share and pricing
all these cloud services are based on
comparative pricing strategies which
means you need to pay on the basis of
its usage
according to Canalis the worldwide Cloud
Market Rose 35 percent to 41.8 billion
in the first quarter of 2021. AWS
accounts for 32 percent of the market
with Azure accounting for 19 and Google
accounting for seven percent
on one hand Amazon charges on a yearly
basis and on the other hand Microsoft
Azure and Google services charge on a
minute basis and also all of them
provide you a standard price for you to
access these services
AWS charges roughly 69 per month for a
very basic instance with two virtual
CPUs and eight gigabytes of RAM
and AWS largest instance with 3.84 TB of
RAM and 128 vcpus will set you back
roughly
3.97 per hour but in Azure the same type
of instance one with two vcpus and eight
gigabytes of RAM cost roughly 70 U.S per
month and azure's largest instance has
3.89 TB of RAM and 128 virtual CPUs and
it cost about
6.79 per hour
compared to AWS gcp will supply you with
the most basic instance which includes
two virtual CPUs and eight gigabytes of
RAM for 25 percent less and as a result
it will set you back roughly 52 dollars
every month and the largest instance
that includes is 3.75 TB of RAM and 160
vcpus and it will cost you about 5.32
cents per hour
Amazon other than this also provides
spot instances reserved instances and
dedicated hosts where you can look for
multiple offers and discounts
but for Azure it provides special prices
to developers based on situations or
even Azure hybrid benefit which benefits
your organization up to 40 percent if it
uses Microsoft software in their data
centers
whereas Google offers quite assorted
pricing to its customer compared to the
other two
it gives you sustained use discounts
which activate if you use the same
instance for a month
the pronable instance which is very
similar to Amazon's spot instances but
one thing is common in all three cloud
services which is that they all offer
long-term discounts
now let's have a look at the last
comparison which is the companies that
are using them
because AWS is the oldest player in the
cloud business it has the largest user
base and Community Support as a result
AWS has the largest number of high
profile and well-known clients including
Netflix Airbnb Unilever BMW Samsung
missinga and others
with time Azure is getting a large
number of high profile customers Azure
currently boasts around 80 percent of
Fortune 500 firms as customers Johnson
Controls polycom Fujifilm HP Honeywell
apple and others are among its key
clients
Google Cloud on the other hand uses the
same infrastructure as Google search and
YouTube and as a result many high-end
Enterprises trust Google Cloud HSBC
PayPal 20th Century Fox Bloomberg
Domino's and other are among Google
Cloud's mini clients the cloud is much
more than just a reliable storage
solution in the year 2021 like the year
before proved to be a watershed even for
cloud computing
it gave businesses more freedom to
operate in the face of covert 19.
cloud computing according to it experts
will be at the Forefront of all
Technologies used to address key
businesses concerns in the coming years
cloud computing has generated more
anticipation excitement and investment
than any other I.T area in the last
decade according to IDC the cloud will
increase at a rate of 22 percent next
year with the market value of 277
billion dollars
as a result there will surely be a
significant demand for cloud
professionals therefore a right
certification in cloud computing can
help you earn a handsome salary of 145
000 to 227 000 in the United States and
close to 16 lakh to 24 lakh rupees in
India
so let's have a look at the top 10
certification in cloud computing
the first one is AWS certified Solutions
architect associate
AWS certified Solutions architect
associate is a set of technical
certifications offered by Amazon web
services for beginners and professionals
working in Enterprise architecture and
solutions architecture
this certificate AIDS in the
identification and development of
personnel with crucial capabilities for
cloud implementation
AWS certified solution architect
associate certification verifies your
competence to build and deploy
distributed systems on AWS
so this certification is for persons who
can do solution architecture such as
deploying and securing web applications
and who have worked with AWS services
for at least a year
and its difficulty level is easy
skills required for this certification
is understanding of specific programming
or scripting languages like Java python
C hash and many other programming
language available data storage
fundamentals networking
AWS service selection Cloud specific
patterns and Technologies
moving on here are a few certification
details you will need to proceed with
its examination
cost of examination is 150 USD
and the average annual salary is 130
883 dollars
the duration of this examination is 130
minutes with number of questions 65.
you have multiple choice and multiple
response questions and the passing score
is 70 percent
and the granule time of the
certification is two years
the company is hiring our Amazon IBM
capgemini mindtree and ikvia these are
the top companies that hire
second is AWS certified Cloud
practitioner
the AWS certified Cloud practitioner
certification is a foundational level
test designed for people who can
successfully show an overall
understanding of the AWS Cloud according
to Amazon
this exam verifies a candidate's
understanding of essential Cloud
infrastructure and Architectural
Concepts as well as key AWS services
the certification provides an overview
of AWS key Services as well as security
and Network
so this is the idle search Cloud
certification for beginners or anyone
interested in learning more about cloud
computing and the AWS Cloud platform
so the difficulty level is easy
the skills required for this
certification is
oversight on creating architecture for
AWS platform ability to deploy AWS
Solutions and applications building
Cloud Solutions
developing plans for the adoption of
cloud Solutions management and
monitoring of cloud platforms
moving on here are a few certification
details you will need to proceed with
this examination
the cost of examination is 100 USD
average annual salary is 113 932 dollars
examination duration is 90 minutes
number of questions is 65.
types of questions are multiple choice
and multiple response question
passing score 70 and again the renewable
time for the certification is two years
and the famous company hiring are Amazon
net app essential Deloitte and solial
LLC
moving on third is AWS certified
developer associate
AWS certified developer associate is an
Amazon web service examination that
measures a person's ability to
successfully demonstrate deploying and
maintaining applications on the AWS
platform
this is the ideal Cloud certification
for programmers and software developers
interested in building Cloud native
applications and the difficulty level is
intermediate
skills required for this certifications
are
use the AWS Services API CLI and
software development kits to write
applications
identify key features of AWS Services
understand the AWS shared responsibility
model user continuous integration of
continuous delivery pipeline to deploy
application on AWS and use and interact
with AWS services
so here are the few certification
details
cost of examination is 150 USD
average annual salary is 130 272 dollars
exam duration is 130 minutes number of
questions is 65
type of questions multiple choice and
multiple response questions
the passing score is 70 renewable time
is two years
and the company hiring our Amazon IBM
cab Gemini TCS and Oracle
fourth is AWS certified
system operation administrator associate
the AWS certified
sysops administrator associate
certification is designed to demonstrate
technical skills for system
administrators in Cloud operations jobs
this certificate assists organizations
in identifying and developing personnel
with crucial abilities for cloud
implementation
the ability to develop manage and
operate workloads on AWS is demonstrated
by earning the certification
this associate certification is for
professionals who have at least one year
of experience with AWS deployment
management and operations
it teaches you how to identify the right
service for your needs and the
difficulty level is intermediate
so these are the skills required for the
certification understanding of AWS
tenants hands-on experience with the AWS
CLI and sdks tools understanding of
Network Technologies understanding of
security concept with hands-on
experience in implementing security
controls and compliance requirements
understanding of virtualization
technology
moving on here are a few important
details related to this certification
so cost of examination is 150 USD
average annual salary is
111
966 dollars
the duration of examination is 130
minutes and the number of questions is
65
which are multiple choice and multiple
response questions
with passing score of 70 and renewable
time of two years
and here are the companies hiring
Amazon IBM capgemini TCS and Oracle
fifth is Microsoft certified Azure
fundamentals
Azure fundamental certifications allows
you to demonstrate your understanding of
cloud Concepts Azure Services Azure
workloads Azure security and privacy and
Azure pricing and support
the Azure Foundation certification
intended for people who already have a
basic understanding of cloud services
it will explain you Cloud Concepts as
well as how to use them and the
difficulty level is easy
skills required for this certification
is the different cloud computing
Concepts
main Azure Services describing Azure
security private privacy compliance and
Trust
Azure pricing and support
and here are a few certification details
required for the certification
cost of examination is 99 USD which can
vary
average annual salary is 110 000 dollars
the duration of this examination is 85
minutes
and the number of questions varies
between 40 to 60. and the type of
questions are multiple choice and
multiple response questions
the passing score for this examination
is 70 and the renewable time is 6 months
and the top companies hiring are
Microsoft Dell Accenture and cognizant
sixth is Microsoft certified Azure
administrator associate
and Azure administrator is in charge of
deploying monitoring and managing
Microsoft Azure Solutions which includes
significant compute storage Network and
Security Services
by earning these this certification you
will be able to demonstrate that you can
deploy and manage Azure compute
resources
this certificate will prepare you to
develop administer and monitor cloud
services such as Storage security and
virtual environments among other things
and the difficulty level is intermediate
so the skills needed for this
certification is
mean Azure services
describing Azure security private and
compliance and Trust
knowledge of azure pricing and support
a knowledge of different cloud computing
Concepts
and here are the further important
details needed for the certification
cost of examination is 165 USD average
annual salary is 107 683 dollars
exam duration is 120 Minutes with number
of questions
varying between 40 to 60.
and the type of questions are multiple
choice and multiple response questions
with the passing score of 70 percent
renewal time is six months and the top
companies hiring are Microsoft Accenture
TCS and Yahoo
seventh is Google associate Cloud
engineer
an associate Cloud engineer delivers and
protects applications
and infrastructure overseas various
projects operations and maintains
corporate solution to ensure that the
fulfill performance goals
this person has worked with both public
clouds and on-premises systems
for programmers developers and software
Engineers this is the greatest Google
Cloud certification its Holder will be
responsible for deploying web
applications in the cloud as well as
monitoring and managing operations and
the difficulty level is intermediate
here are the some important skills
required for you to excel this
certification basic understanding of
Google Cloud platforms products and
services and basic understanding of
cloud Concepts such as virtual machines
containers and networking
and these are the few important
certification details that you need to
take care of
cost of examination is 125 USD with
average annual salary of 109 415 dollars
examination is 120 Minutes with number
of questions 50. and the type of
questions are multiple choice and
multiple response questions with a
passing score of 70 percent
and the renewable time is two years and
the top companies hiring a Google
Goldman Sachs
here comes the eighth one that is
Microsoft certified Azure solution
architecture expert
to implement Solutions and Azure
solution architect work
with Cloud administrators
cloud dbas and clients these this
credential displays the ability to
provide advice to stakeholders and
translate business requirements into
save scalable and dependable Solutions
this certificate is for experienced
programmers developers and devops
Engineers who wish to become Azure
professionals
this is the ideal Cloud certification
and the difficulty level is hard
and these are the skills required for
this Advanced certification
deploying and configuring infrastructure
implementing workloads and security
creating and deploying apps implementing
authentication and securing data
developing for the cloud and for this
Azure storage
and these are the details you need to
remember cost of examination 165 USD
average annual salary
135 000 with exam duration of 150
minutes
and number of questions varies between
40 to 60.
type of questions are multiple choice
and multiple response questions with the
passing score of 70 percent
and the renewable time is six months
and the company's hiring are IBM
Microsoft Infosys and Medline
coming to the ninth one is Google
professional Cloud architect
a Google Certified professional Cloud
architect means you demonstrate the
ability to design and plan A Cloud
solution architecture
manage and provision the Cloud solution
infrastructure
designed for security and compliance
the professional Cloud architect exam
measures your abilities to design and
plan A Cloud solution architecture
manage and provision Cloud solution
infrastructure and design for security
and compliance
this is the ideal Google Cloud
certification for experienced it
professionals interested in becoming
solution architects for Google Cloud
technology such as bigtable bigquery and
other GCB platform services and the
difficulty level is hard
and these are the skills you need to
focus on
proficiency with command command line
Linux operating system and system
operations
and three plus years of Industry
experience
and here are the further details you
need to take care of
cost of examination is 200 USD with
average annual salary of 140 000 dollars
the examination duration is 120 minutes
but number of questions 40. the type of
questions are multiple choice and
multiple response questions with a
passing score of 70 percent
and the renewable time is two years
and the company is hiring are Google and
golden sets
and here comes the last but not the
least 10th certification that is AWS
architect professional
the AWS certified Solutions architect
professional exam verifies Advanced
technical knowledge and experience in
designing distributed applications and
systems on the Amazon web services
platform
this certification AIDS in the
identification and development of
personnel with crucial capabilities for
cloud implementation
the AWS certified Solutions architect
Prudential verifies your ability to
develop Implement and evaluate
applications on AWS under a variety of
conditions
this Advanced certification teaches you
how to create and deploy scalable web
applications on Amazon AWS servers as
well as how to choose the right service
and power for your application and the
difficulty level is hard
and the skills required for this
certification is
familiarity with AWS CLI AWS apis AWS
cloud formation templates
and windows and Linux environments
ability to provide best practice
guidance on the architectural design
across multiple applications and
projects of the Enterprise
ability to evaluate Cloud application
requirements and ability to design a
hybrid architecture using key AWS
Technologies
and here are the details you need to
know before applying for the exam
cost of examination is 300 USD with
average annual salary of 118 266 dollars
the examination duration is 180 minutes
with number of questions 75.
types of questions are multiple choice
and multiple response questions
with the passing score of 70 percent
and the renewable time is two years
and the top companies hiring are Amazon
Central IBM and mind3
so this was all about top 10 Cloud
certification in 2020. let's have a look
at some of the skills required to become
a cloud administrator first off you need
experience with programming languages
like C sharpen.net you'll also need to
be experienced with devops tools like
Jenkins Docker ansible and Chef you'll
need to have an understanding of
database configuration so that you can
take advantage of all the relevant
information about the hardware and
software components used by the
organization you'll also need to have
experience with Cloud infrastructure
systems like servers storage Network and
visualization software finally you'll
also need to configure virtual machines
vpns and Cloud servers now let's have a
look at the salary offered to Cloud
administrators now in the United States
the average salary of a cloud
administrator is approximately sixty
five thousand dollars per annum
similarly in India it's approximately 7
lakh rupees per annum now let's have a
look at some of the companies hiring for
cloud administrators we have companies
like web Pro Infosys Accenture IBM and
so on so here I see a comment which says
I'm a fresher and want to learn about
AWS how can I start so actually any
fresher can make a career in AWS as long
as you have interest towards the subject
now if you are from a non-technical
background you need to put in a lot more
hard work and have a lot of patience so
I would recommend that before you take
up the AWS course you have a look at the
course curriculum types of AWS courses
opportunities in the market for AWS
certified candidates and so on so now
let's go to number six Cloud application
developer now a cloud application
developer mainly focuses on implementing
and maintaining an organization's Cloud
infrastructure now they're involved with
designing building creating analyzing
and maintaining Cloud systems now these
individuals are also involved with
ensuring that there's an effective
design of business processes in the
cloud there also involved with a number
of different tasks like optimizing
efficiency and performance scaling
application components and security
issues now let's have a look at some of
the skills required to become a cloud
application developer first off you need
to have experience with database
languages like MySQL SQL and mongodb
then you need to have experience with
programming languages like python Ruby
and Perl so that you can code and create
cloud computing applications you'll also
need to have experience with Linux now
this is because most Cloud
infrastructures are created with Linux
servers you'll also need to have
experience with cloud service providers
like Amazon web services Google Cloud
platform and Microsoft Azure you'll need
to have experience with information
security with the help of certifications
you'll also need to know how to create
microservices and to create Cloud
applications now let's have a look at
the salary offered to a cloud
application developer in the United
States it's approximately seventy
thousand dollars per annum in India the
average salary of a cloud application
developer is approximately 8 lakh rupees
per annum now let's have a look at some
of the companies hiring Cloud
application developers you have
companies like Bosch McAfee sap and so
on so now let's go to number five Cloud
network engineer a cloud network
engineer is responsible for implementing
supporting maintaining and optimizing
the network Hardware software and
communication Links of the
organization's Cloud infrastructure he
or she basically focuses on Automation
and security why building the cloud
infrastructure enhancing Network tooling
visibility and improving productivity
now let's have a look at some of the
skills required to become a cloud
network engineer first off obviously you
need to have experience with networking
which means the familiarity with the
internet and Van communication
Technologies protocols and best
practices you need to have an
understanding of cloud security and how
you can design a public cloud with
multiple options you'll need to have an
experience with data center
Administration which basically means an
understanding on infrastructure design
operations and life cycle management and
finally you need to have experience with
cloud computing platforms like Amazon
web services Microsoft Azure and Google
Cloud platform next let's have a look at
the salaries offered to a cloud network
engineer firstly the average salary for
cloud network engineer in the United
States is approximately seventy two
thousand dollars per annum in India the
average salary of a cloud network
engineer is approximately 4 lakh rupees
per annum now that we're done with this
let's have a look at the company's
hiring Cloud Network Engineers we have
companies like Amazon web services
Rakuten Cisco and so much more now for
number four we have Cloud automation
engineer a cloud automation engineer
focuses on cloud Automation
orchestration and integration he or she
implements optimizes and supports the
cloud infrastructure and ensures it has
high availability they also have to
increase the cost Effectiveness and
availability of the cloud now let's have
a look at some of the skills required to
become a cloud automation engineer first
off since the role is cloud-based
experience with the cloud service
platform like Microsoft Azure AWS or
Google Cloud platform is an absolute
must you'll also need to have experience
with programming languages like Python
and go that will help you make the
process of automation easier next
experience with devops tools like chef
ansible and puppet are seen as a huge
plus you'll also need to have an
understanding of Docker and containers
experience with databases like postgres
SQL and MySQL that can handle multiple
workloads would be very useful and
finally you need to have experience with
virtualization which means you need to
know how you can work with virtual
servers applications storage and
networks with this you can reduce the
amount of resources required by the
organization now let's have a look at
the salary offered to Cloud automation
engineers in the United States the
average salary for cloud automation
engineer is approximately 79 000 per
annum the average salary for cloud
automation engineer in India is
approximately 5 lakh rupees per annum
now that we're done let's have a look at
some of the companies hiring Cloud
automation Engineers we have companies
like Oracle Google Kappa Gemini Disney
and so on and now to number three
Cloud security manager a cloud security
manager is someone who's responsible for
providing security for cloud-based
digital platforms and protecting the
organization's data he or she may be
involved with creating new security
methods or to analyze existing ones they
may also have to create cloud-based
applications performing threat
simulations and providing security
recommendations now let's have a look at
some of the skills required to become a
cloud security manager first off you'll
need to have experience with at least
one of the popular cloud service
providers like Amazon web services
Microsoft Azure and gcp you'll also need
to have experience in programming
languages like python experience with
commonly used devops tools like Jenkin
so that you can Implement continuous
integration or continuous deployment
models would be very helpful you'll also
need to be well versed with TCP
protocols and other networking Concepts
you need to be well versed with pki SSL
SSH https and so on experience with
web-based Analytics and services would
also be very helpful now let's have a
look at the salary offered to a cloud
security manager in the United States
the average salary of a cloud security
manager is approximately 99 000 per
annum in India the average salary for
cloud security manager is approximately
11 lakh rupees per annum now let's have
a look at some of the companies hiring
Cloud security managers we have
companies like Deloitte VMware Cisco
Dell and so on now let's move on to
number two Cloud engineer a cloud
engineer is an individual who is
responsible for any technological duties
that are associated with cloud computing
including design planning management
maintenance and support they're also
involved with orchestrating and
automating cloud-based platforms
throughout the organization let's have a
look at the skills required to become a
cloud engineer first off you need to
have experience with at least one of the
popular cloud service providers like AWS
Microsoft Azure or the Google Cloud
platform you you'll need to have an
understanding of networking Concepts
like building and accessing servers
virtual networks and so on this enables
Cloud engineers make sure that the
network is responsive to the users next
you need to have experience with
virtualization with virtualization you
can reduce how many Hardware units you
need with the help of virtual machines
it also makes resources scalable and
fault tolerant in an organization
experience with the Linux operating
system would also be very helpful
considering how thirty percent of the
servers that powers your are Linux based
you'll need to know about apis to design
restful Services experience with devops
is also a major skill to have since
you'll be able to help handle work
dependencies between the development and
operation teams and finally you need to
have programming experience with
languages like Java python C plus and
Ruby next let's have a look at the
salary offered to a cloud engineer in
the United States the average salary of
a cloud engineer is approximately 100
five thousand dollars per annum the
average salary in India is approximately
five lakh rupees per annum now let's
have a look at some of the companies
hiring Cloud Engineers we have companies
like McAfee IBM JPMorgan visa and so on
now I see a pretty good question in the
chat how do you become a cloud engineer
so to become a cloud engineer you need
to have experience with the programming
language experience with at least one
cloud service platform which is Amazon
web services or Microsoft Azure or
Google Cloud platform the choice is
yours and you need to specialize in a
particular service be it storage
networking disaster recovery and so on
and now for number one Cloud architect
Cloud architect is responsible for
managing cloud computing architecture in
an organization he or she handles
everything to do with front-end
platforms servers storage delivery and
networks now let's have a look at some
of the skills required to become a cloud
architect
first off you need to have knowledge in
programming knowledge about Perl python
Ruby PHP Java and net can provide users
with the ability to build deploy and
manage applications quickly you'll need
to know about the basics of networking
since most of the work what you'll be
doing would be web related knowledge
about networking can be really helpful
knowledge about data storage
fundamentals are also really important
this provides you with the knowledge to
determine which data storage option to
use and when being experienced with
cloud service platforms is also really
important platforms like AWS Google
Cloud platform and Microsoft Azure are
what you are going to be basing most of
your work on experience with them is
pretty important and finally Cloud
security this will help you make sure
that your data is secure and only
authorized code is run and the right
people are allowed to run it now let's
talk about the salary of a cloud
architect the average salary for cloud
architect is approximately 107 thousand
dollars per and in India the average
salary of a cloud architect is
approximately 16 lakh rupees per annum
and there you go those are the top job
roles in the field of cloud computing
now let's have a look at some of the
companies hiring Cloud Architects we
have companies like eyepro Huawei
Hewlett Packard Enterprise and so on if
you are looking for a more detailed
approach on cloud computing with
hands-on experience then simply learns
Caltech postgraduate program in cloud
computing will be the right way to go
this comprehensive program will equip
you with the knowledge and skills needed
to navigate the cloud landscape with
confidence dive deep into the cloud
architecture deployment models security
and migration strategy you will also
explore platforms like AWS Microsoft
Azure and gcp to build your Cloud
expertise so don't miss out on this
chance to transform your career and join
the ranks of successful Azure
professionals click the link in the
description to discover more about this
course and today I am going to tell you
how you can become a successful cloud
computing engineer now cloud computing
is one of those Technologies that's
rapidly rising and with any technology
that's growing rapidly it comes with
several job opportunities for the people
who's killed in it so before we get into
it let's have a brief look at what is
cloud computing cloud computing refers
to services like storage databases
software analytics machine learning
artificial intelligence and so much more
all of which made accessible via the
Internet the cloud tech services Market
is expected to grow 17.3 percent in the
span of 2018 to 19 which means there's a
growth from
175.8 billion dollars to a whopping 206
billion dollars in 2019 and as of 2020
it's expected that 90 percent of all
organizations in the world would be
using cloud services not to mention
several organizations around the world
suggest that using Cloud Computing
Services has enabled their employees to
experiment a lot more with Technologies
like machine learning and artificial
intelligence so here's what we'll be
going through today firstly we'll be
talking about who is a cloud computing
engineer the steps you need to take to
become a cloud computing engineer and
the cloud computing engineer's salaries
so first off who is a cloud computing
engineer now a cloud computing engineer
is an ID professional who takes care of
all the technical aspects of cloud
computing now be a design planning
maintenance and support now a cloud
computing engineer can take up a number
of different career paths this could be
that of a cloud developer security
engineer a full stack developer cisops
administrator Solutions architect Cloud
architect and so much more now let's
have a look at some of the major Cloud
computer in roles first off we have
Solutions architect now these are
individuals who are responsible for
analyzing the technical environment in
which they are going to produce the
solutions the requirements and the
specifications secondly they are
required to select an appropriate
technology that satisfies set
requirements they need to estimate and
manage the usage and the operational
costs of the solutions they provide and
they need to support project management
as well as solution development next we
have sysops administrators they are
involved in deploying managing and
operating highly scalable and fault
tolerant systems they need to select an
appropriate service based on compute
security or data requirements they need
to estimate and manage usage and
operational costs and they need to be
able to migrate on-premises workloads
onto an appropriate cloud computing
platform so among both of these roles
there are certain requirements that are
remaining constant now let's have a look
at the steps you need to take to become
a cloud computing engineer your first
step is to gain Proficiency in a cloud
Computing platform now the first step is
to become proficient in at least one of
the three major cloud computing
platforms be it AWS Azure or the Google
Cloud platform now there are a huge
number of resources that you can find on
the internet it could be YouTube videos
articles virtual or physical classrooms
and so much more now after you're done
learning you can get certified by
Microsoft Azure AWS or the Google Cloud
platform now for AWS you have a number
of different certifications which can be
divided into three categories which are
the foundational which is just the
basics the associate level
certifications the professional level
certifications and the specialty
certifications similarly with Microsoft
Azure you have certifications that
enable you to become an Azure developer
associate and as your administrator
associate and Azure architect
professional and a devops engineer now
most cloud computing platforms have a
free tier that you can take advantage of
these provide a number of free services
for a period of time some of which are
free forever so you can use these
platforms to your advantage and do as
much practice as you can on them now if
you want to learn more about cloud
computing you can also check out Simply
learns YouTube channel then you can go
on to the playlist section right here
and you can find comprehensive videos on
a number of different cloud computing
platforms AWS and Microsoft Azure our
AWS tutorial videos talk about what
exactly is AWS how you can become an AWS
Solutions architect Amazon ec2 S3 some
of the other services and so much more
we also have detailed tutorials on Azure
which talks about what exactly is azure
the certifications provided by Azure
some of the services like machine
learning Azure active directory and so
much more and now we're at Step 2 being
experienced in at least one programming
language unlike general purpose
programming languages like C C plus plus
C sharp and so on cloud computing
requires ones that are a lot more data
oriented now some of the major
programming languages that are you in
cloud computing are go Python cladour
and Java now as I said before there is a
wealth of resources that you can learn
from there are free websites that you
can practice your code on like quick
code code academy and several others
there's also resources like YouTube
videos as well as that's the option of
online or offline classes now we're at
step 3 specialization you'll also need
to be well versed with a number of key
Concepts these are storage and
networking now with storage you need to
know how data can be stored and where it
can be accessed from you need to know
how it can be accessed from multiple
different resources you'll also need to
have some experience with the services
provided by Azure and AWS like the
Amazon S3 in AWS and the appropriately
named Azure storage from Microsoft Azure
with networking you need to have a
strong understanding of the networking
fundamentals as well as virtual networks
next up we have virtualization and
operating systems with virtualization
you need to know how much networks which
are just a combination of different
virtual machines can be used to emulate
different components in a particular
system with operating systems you need
to have a very strong understanding
operating systems like Windows and Linux
next up we have security and Disaster
Recovery now you need to understand how
data application as well as
infrastructure can be protected from
malicious attacks With Disaster Recovery
you need to be prepared for any
unexpected circumstance by making sure
your systems are always safe and are
regularly backed up to prevent any sort
of loss of data then we have web
services and devops now you need to have
a strong understanding of apis or
application program interfaces and web
services some amount of experience with
web design also can be of great help
with devops you need to have a strong
understanding of how cloud computing is
able to provide a centralized platform
on which you can perform testing
deployment and production for devops
automation moreover with devops you
under understand the Synergy that the
operations as well as the development
teams have with each other and for the
success of any project and finally
you're a cloud computing engineer now
let's have a look at the salaries of
cloud computing engineers in the United
States cloud computing Engineers earn
around 116 000 dollars per annum in
India a cloud computing engineer is paid
approximately 6 lakhs 66 000 rupees per
annum now how can simply learn help you
become a cloud computing engineer so
let's head on to simpleleon's website
here we have the cloud architect Masters
program now this deals with a number of
different courses all of which that can
help you get started in your journey to
becoming a cloud computing engineer this
master's program covers a number of
different courses like AWS technical
Essentials Microsoft Azure fundamentals
AWS developer associate and so much more
it provides you 40 plus in demand skills
and 25 plus Services provides you a
master certification it has 16 plus real
life projects and helps you get a salary
that changes between 15 to 25 lakh
rupees per annum it also covers a
variety of tools like Amazon ec2 Azure
data Factory virtual machines and so
much more so why don't you head on to
simplylearn.com and get started on your
journey to getting certified and getting
ahead and there you have it the
conclusion of our captivating cloud
computing full course we hope you found
this video both useful and entertaining
if you have any questions about this
topic cover please feel free to ask away
in the comment section below experts
will be there to address your concerns
thank you for watching and keep learning
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here