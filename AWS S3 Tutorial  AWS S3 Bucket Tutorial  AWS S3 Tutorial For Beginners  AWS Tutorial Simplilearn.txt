hello everybody welcome to simply learns
aws
s3 tutorial for beginners my name is
kent and today i'm going to be covering
these following points i'm going to be
covering what is
cloud computing i'm going to be
showcasing what aws is in terms of
cloud computing and i'm also going to be
covering the core fundamental service of
the simple storage service which is
a object storage service we're going to
be covering the benefits of the simple
storage service better known as s3 what
objects and buckets are
and we're going to be seeing how things
are working in the background
and implementing that in terms of
lab assisted demonstrations in order for
you to see what those features of s3 are
and then we'll perform a wrap up of the
material we've done in a conclusion
so
let's get started
what is
cloud computing
so here is a definition of cloud
computing
but i'm going to paraphrase it
cloud computing is nothing more than you
gaining access to
infrastructure
through web services with a
pay-as-you-go model now this is a very
very different model than we're
traditionally
involved in on a on-prem
data center so when i mean on-prem
i'm saying on-premise okay so you'll
hear that a lot on-prem
now when we are on on-premise data
center we have to provision lots of
infrastructure even before we get
started with
deploying any type of application
whether it be a mobile application a web
application etc etc
so every application needs some sort of
underlying infrastructure whether that
be bare bone servers databases they're
obviously going to be needing some types
of storage etc etc so all this
is something that we have to kind of
fight against in order just to get
started with deploying our application
so this takes usually traditionally
depending on the size of the company
about three months to set up
so
what if
we could leverage
all this
infrastructure as a service and just
through some api call that's what the
web services is all about
we could provision a server or we could
provision a database within minutes and
then deploy any type of application that
we want to on top of that stack and also
take advantage of any storage that we
may want we may want object level
storage which is what we're going to
cover we may want
an elastic file system
right so there are different types of
storages that we're eventually going to
look at as you go down your learning
path with simply learn so cloud
computing is really about provisioning
of all these kinds of services but not
only at the infrastructure level we're
going to see how we could move up the
stack and get entire platforms as a
service or even softwares as a service
so what is
aws
well aws is really a cloud computing
platform
that will offer us
many many services
through these api calls some are again
just bare infrastructure like a service
other can be
a service as a software for example
email
so here we have an example
of the public data storage this is s3
the simple storage service icon that
you're going to see over and over again
this is a representation
of an actual bucket in s3 so this will
become more clear as we
continue the slide deck and actually
reinforce this with some hands-on lab
but as you can imagine you may want to
provision a
private
software defined network in the cloud
and so we can do that in aws we can
provide load balancing we could provide
scalability in terms of auto scaling
groups that will respond to
increased demand as your application
let's say
becomes more popular you want that
infrastructure to grow elastically so
doing this on premise is traditionally
not only extremely expensive
and thus prohibitive
but very difficult to implement
physically speaking as well
so
by going with a cloud provider like aws
we can severely reduce the expense and
the complexity of setting all of this up
because
we're trading capital expense for
variable expense so capital expense is
about you procuring all the hardware
that you need beforehand
and the budget
being approved before even getting that
hardware in through your door right and
then you have to configure it so there's
that complexity factor
whereas if you're doing this through aws
you can within minutes
just provision all of that very quickly
so you're also not paying for everything
all up front you're paying this with a
pay as you go model so very different
we are now
removing what's called undifferentiated
lifting which is everybody has to
do the same thing but it's not really
bringing any value to their end product
we all have to
buy servers and configure them and scale
them etc etc so if we remove that
element
and have aws handle that we can
concentrate on our business
which is our application and improving
that application
and responding more to our clients
request we can be more agile more
flexible so all of this leads to these
points that we see here on the slide
much easier to use much easier to get
our application to market we're actually
going to be leveraging
the physical security that aws
implements for us they have data centers
all over the world that we're going to
plug into and that physical security is
already
implemented for us there are
varying levels of security that we're
going to get into through our aws
journey
but right off the bat you know that you
have physical
security of course we're going to be
talking about storage in this
demonstration or in this slide deck over
here but the nice thing is is we only
pay for what we use
so that's that variable expense that i
was talking about that we've traded for
capital expense we can easily set up
databases we can easily
automate our backups and have those
backups stored
in the cloud through that s3 storage
that we're going to get into okay
so lots of benefits of going through aws
here
and it's also comprehensive and simple
to use
comprehensive meaning that there are
hundreds and hundreds of services
it's going to be actually quite time
consuming for you to just know what each
service does in general
but
you'll get there through your journey
it's all about sticking with it and
learning something new every day which
is what simple learn is all about
over here we have infrastructure as a
service platform as a service and
software as a service so these are three
different types of distinctions we have
in the cloud industry to basically
cover what's going on here on the right
hand side so on premise you're
traditionally involved in
everything from networking which is you
for example
buying the switches the routers the
cabling everything all the way up the
stack to deploying your application and
you see everything in between there okay
so this is a very time consuming job not
only that but it costs a lot of money
because you have to buy all these
machines up front and you have to have
the
individuals with the right knowledge
to
maintain all of this stack all right so
different individuals in different teams
maintaining this whole on-premise data
center you may also have different data
centers deployed globally so you can
imagine the cost
overhead of this
so everything in green is basically
you're managing it
now we can offset that and ask aws to
handle part
of this stack over here so
on this second column over here we see
infrastructure as a service which
basically says aws i want you to handle
all the networking my software defined
network i want you to handle my storage
elasticity i want you to handle
my compute power my virtual machines i
want you to handle
even
uh administrating those machines at a
physical layer as well and i want you to
physically secure them so this will
allow you more time to focus on all
the green up here
of course we can move up the stack and
say i even want aws to take on more
responsibility
and that would be more of a platform as
a service where aws would also install
and maintain the operating system for
you so you can imagine things like new
versions of operating systems
security patches maintenance patches all
of the like and any middleware or
runtime that you have on top of that
think of something like the java a
virtual machine perhaps that needs to be
updated now you wouldn't be
responsible for maintaining all of
what's in orange over here you would
only be responsible for deploying your
application and
making sure that your data is saved and
secured right so again platform as a
service is a more hands-off approach so
think of it as
you wanted to set up a
cicd deployment environment
for your development team and you wanted
to be very
involved in just handling your
application code and making sure that it
was properly
compiled built tested and deployed right
well this could be implemented as a
platform as a service
or
you could take your code as a zip file
and give it to aws's
service called beanstalk which would
automatically deploy your code and make
sure that it's highly available scalable
and fault tolerant across all of the
orange stack over here right so more of
a hands-off approach
of course we can go all the way up the
stack and tell aws to take care of
everything for us and just ask for a
service much like a utility like
electricity or water we might want to
say we just want an email software
service and we're not
really concerned about how the
underlying operating system or servers
or network are configured for that we
just want to use the software as a
service we don't want to administrate
anything else we just want to be sort of
like an end user to it so aws will
manage everything
underneath the hood so there are varying
types of services you pick and choose
whichever one makes more sense for you
and your team
so what is
s3 s3 stands for simple storage
service and it is an object
storage service in amazon what this
means is that
it doesn't matter what kind of file you
upload to s3 it will be treated as an
abstraction meaning an object so you can
upload a pdf file you can upload a jpeg
file you can upload a database backup it
doesn't really matter all that is
abstracted away
by the use of storing it within an
object we're going to talk more about
what composes an object but by doing so
what happens is s3 allows us to have
industry
leading scalability so it doesn't matter
if you're going to make a request for
that database backup let's say that
object
five times or ten thousand times that
request that demand will scale the
underlying infrastructure that is
implemented behind the scenes is handled
for you so in a sense it's kind of like
a
serverless service a storage service
where you're not
implicated in handling anything
underneath the covers you're just
uploading your objects and accessing
them later on in terms of data
availability very powerful as well
because it takes those objects and will
replicate them across at least three
availability zones of course these
availability zones have
one or more data centers attached to
them so you have lots and lots of copies
of your
objects distributed globally
or at least at a regional in a regional
area you can do this as well globally if
you enable global replication
and you will see that your level of
juror availability will skyrocket and
you're just not going to worry about
losing an s3 object
data security well we can encrypt our
data at rest or objects at rs we can
encrypt it in transit
we can also come up with security
policies at
the bucket level which we're going to
talk about what a bucket is very very
very soon
these are going to be implemented
through what's called i am policies and
you're going to be able to control who
or what has access to your objects
and of course because everything is
handled underneath the covers all the
servers all the storage nodes
are
cloud optimized for the best possible
performance
so let's continue on our journey on what
is s3
we're going to take a look at what is a
bucket and what is an object as you can
see here you have inside this bucket
which is a logical container
for an unlimited amount of objects you
could have objects of different shapes
and sizes like i said you could have
pictures database backups etc so this is
what's being represented here by
different shapes
and you can think of a bucket really as
almost like a folder where objects or
files are placed within them
so there are many ways for us to place
objects within a bucket we can do this
through the aws console which i'll be
showing you very shortly we can do this
via the command line interface or we can
do this to a software development kit at
the end of the day all those three
options go through
an api right so
it's all about picking the right method
that is best suited for whatever kind of
end user or application that needs
access to these buckets and to these
objects now once you've got the data or
objects within this bucket
you could control pretty much how the
data is accessed how it's stored whether
you want it to be encrypted and how it's
even
managed so you can have organizations
that have specific compliance needs for
example any objects
that are placed in or perhaps some pdfs
are not to be modified or not to be
touched for 90 days let's say we can
employ
object locks if we want to
we can imply security guards we can
also for example
record all
api actions
that try to access to list to delete any
kind of api operations on those objects
at the bucket level or on the object
level we can record if that is some sort
of organizational compliance need for
auditing or for whatever internal
auditing reason that you may have
so continuing on here you can see that
there are many organizations that use s3
one in point is amazon itself s3 is so
popular and so durable that amazon
internally uses it to store its own data
you're guaranteed you know almost that
you're not going to lose any object in
there because it has what
we call eleven nines durability now
eleven nine's durability
is um really an extreme amount of
durability where
it's mathematically
almost impossible for you to lose an
object once you placed it in s3 you're
in fact more
liable to get hit by meteorite than you
are to lose an object in s3
statistically speaking which is a pretty
incredible statistic but it's but it's
true
so if we continue here with the benefits
of s3 some of these have already
described but let's talk about the full
picture here we see the performance
scalability availability and durability
um of s3 are really
you know first class in the industry
again durability we were talking about
that 11 9 so what that really means is
99.9999
in total if you count all the nines
that's 11 of those nines so um it's most
durable in the industry by far and again
because everything is handled underneath
the covers it's a serverless
service
they ensure the scalability and
availability and the performance of
the inner workings of that object level
storage now cost is of course always
important and s3 really shows up here
with first class support again
for cost it's got very very low cost we
can have object level storage for
let's say a terabyte of object level
storage for as little as a dollar
a month so again it will
depend on the kind of storage class
we're going to have a discussion on
different types of storage classes that
we might want to transition or move over
our objects from one storage class to
the next depending on how frequently
accessed that data is as you know data
over time seems to get less and less
accessed as you know your needs shift
from
one place to another we're going to talk
about that coming up very soon so we're
going to want to really focus on that to
reduce our end of the month
costs for storage with s3 of course
security is always at the forefront and
we want to make sure that we either
simply secure by encrypting everything
right off the bat
either at rest in transit or we want to
put an object lock or just maintain
security at the
bucket level maintaining let's say who
or what has access to that data because
by default no one has access to the data
unless we open up the door and also we
want to make sure that we don't give
public access
to our bucket for obvious reasons of
giving away important information by
mistake so aws makes it extremely
difficult for you to accidentally do
this and i'm going to show this
in a demonstration lab coming up very
soon
and we can also query in place which is
very interesting so you can imagine you
putting let's say
a file that's in csv format or json
format or par k format some sort of
structured format or semi-structured
format in s3 and you can actually use
sql queries on the spot to filter that
data in place at the bucket level you
could also have other services that may
want to extract some business
intelligence from that data in your s3
bucket directly as well so some other
more advanced querying operations can
take place at the s3 level so this you
will learn
during your journey with simply learn as
you're covering all the aws technology
stacks
and here the most widely used storage
cloud service in the industry
because of all the points that we just
covered it really is the number one
storage or object level storage solution
in the industry
let's now take a look at objects and
buckets and s3 so the objects are really
the fundamental entities that are stored
in s3 or the lowest common denominator
which means that we're not really
interested
in what kind of data is within the
object at the layer of s3 because s3
doesn't have direct access to the data
within an object like i said before it's
an abstraction of
the data so we don't know
if it's a cat's picture if it's a backup
of your database it's just treated as a
fundamental entity aka the object now
every object is associated with metadata
so data about itself things like
what's the name of the object what's the
size the time and date that the object
was uploaded things of that nature are
categorized
as metadata so s3 does have direct
access to that now the
data within that object of course is
accessible by other services so it's not
that once you've uploaded it you've
totally lost the data and it's only can
be treated as an object it's just that
at this layer
it's simply just assigned metadata and a
version id a unique version id and if
you re-upload the exact same object the
second or third time to s3 it will have
its own
version
id number so a new unique version id
number will be generated so really what
buckets are are there logical containers
to store those objects so of course at
an extremely high level a bucket would
be like your
root file system if you want to think
about it like that but that doesn't mean
you can't go into this bucket and create
separate folders
now when you create separate folders in
a bucket because you might want to
logically organize
all your objects
you might be fooled by the fact that you
think this is a hierarchical storage
system when in fact it is not and i'll
talk about that in a second
so you cannot store an object without
first storing it into a bucket so of
course the first step would be for us to
create a bucket and then
upload objects within that bucket so an
object cannot can cannot exist without
its container or its bucket
so there's no windows explorer view like
we're used to in an operating system
because this is not a hierarchical view
no matter if you create
folders within folders within folders in
fact s3's internal architecture is a
flat hierarchy
what we do instead is we assign prefixes
which are treated as folders
in order to
logically organize our objects within
our buckets and i'm gonna showcase
a prefix in one of the demonstrations
coming up very soon
so when you create or when you're
working with s3 first of all you're
working at a regional level you're gonna
have to pick a region
for example you might pick us east 1
region
or u.s east 2 region and the bucket that
you're going to be creating will be
replicated across several availability
zones but within that region
also that data those objects can be
accessed globally because
s3 uses the http protocol http protocol
is very permissive everybody knows how
to administrate it and work with it so
we just need to give access to that
object in terms of a permission policy
and give it the proper
url or
give whoever needs access to it the
proper url and they can access that via
http globally so first when you're
creating a bucket you have to select the
region
and
the object will live within that region
but that doesn't mean that it still
can't be accessed globally
so let's now take a minute go over to
the aws console and actually let me
showcase how to create
objects and buckets
so for our first lab we're going to be
going and creating an s3 bucket and
uploading an object to it so we first
have to log into
the aws console which we have up here in
the address bar
let's click on either create free
account if it's your very first time or
as in as in my case already have an
account we're going to sign in here
you're going to have to pass in your im
username password and your
account id of course
once you've logged in
you can search for
the simple
storage service s3 either
by coming up here in the search box and
typing s3 and you'll find it there
that's probably the easiest way or if
you want you can take a look at all the
services and it'll be under storage
over here the first one okay so pick
whichever method you see best for
yourself
once we're here we want to create
our first bucket
now i've already have a couple buckets
here so we're gonna go on and create a
new bucket the very first thing is you
have to specify a bucket name now this
bucket name has to be globally unique if
you take a look here at the region that
we're in
it doesn't actually select a specific
region
it selects the global
option which means that this bucket will
become globally accessible
so that is why it needs to have a unique
name much like a dns name has to be
unique for your website
right so i'm going to come up here
and pick a name that i think is going to
be unique so i'm going to say simply
learn
s3 demo
all right let's take a look at if that's
going to work out of course we have to
pick a region
but it is globally accessible so you
either pick a region that's closest to
your end users or in our case since
we're just doing a demonstration we can
do whichever is closer to us right now
okay
and we're going to skip these options
for now we're going to come back to them
later on we're just going to create
the bucket
now hopefully that was a unique name and
allowed me to create that and it looks
like it did so if i scroll down you can
see
that it clearly got created over here
now
we're going to click on that
and we're going to start
uploading objects to this bucket
so let me click on the upload button
and you can either select one or more
files
or you can select an entire folder so
i'm going to just go and select a
specific
cat picture that i have here
okay and again we'll go through some of
those other options later on and we'll
just click upload
now this should take no time at all
because of the fact that it's a very
small object or file that's being
uploaded so it is has succeeded we can
close this up
and we see now clearly that the object
is in our bucket if we go to properties
we can see the metadata associated with
this bucket we can see the region that
it's in we can see the arn which is the
amazon resource name which uniquely
identifies this resource this bucket
across the globe so if ever you needed
to reference this let's say
in an im policy or whether other service
needed to communicate with s3 you would
need this arn it's very important piece
of information and of course we have the
creation date so some high level
metadata so objects as we have covered
already consist of not only the data
itself
but the metadata so there's lots of
metadata and there's a lot of other
features that we can go here and enable
very easily and this will be the basis
of the future
demonstrations that i'm going to do all
right so just to
recap what we just did
we created a unique bucket
give it a name of
simply learn s3 demo and uploaded our
first object to it
so let's now take a look at
the inner workings of the amazon s3
when we upload an object into a bucket
we have to select which one of these
storage classes the object will reside
in so you see have six storage classes
here at the bottom and each have their
own characteristics that we're going to
get
into by default
if you don't specify anything it'll get
placed in what's called the s3 standard
storage class which is the most
expensive out of all the storage classes
once your object gets colder and what i
mean by colder is your axis patterns
diminish meaning that you're accessing
that file less and less over the course
of time so it gets colder
you will transition that object from one
tier to the next all the way to for
example s3 deep archive so again deep
archive
signifies extremely cold so maybe you're
only referencing this data once a year
once every couple of years
and so you want to have the cheapest
possible storage
available so right now you can get about
one terabyte of storage per month for
about a dollar a month with s3
glazer deep archive
so you are going to be very interested
in knowing how to transition from one or
more of these storage classes over time
in order to
save on your storage costs that's really
why we're doing this so let's go through
some of the storage classes
by default like i said
whenever you upload an object you
automatically get placed into the
standard storage class so any files that
you're working on frequently daily this
is the best fit for it you've got the
highest level
of accessibility
and
durability as well not that the others
don't have the same level of durability
however we'll see how
when you transition from one
storage tier to the next some
characteristics do change in order for
you to save on some cost we're going to
go through some of those
now this would be considered hot data
right data that's used all the time
maybe just by you maybe by everybody
right so that's the perfect place to
place it in the
standard storage class now over time
like i said you may find yourself
working
less and less perhaps on a document that
was due by the end of the month that
document was submitted and then
afterwards you don't work on that
document anymore perhaps you're only
working on revisions based on
feedback from your colleagues that are
asking you to make some corrections or
some amendments and so only those
corrections or amendments come in
perhaps once a month and so in that case
you might find yourself
finding a justification for moving that
document from the standard tier to the
standard ia or infrequently accessed
here
maybe any objects
not modified for more than 30 days are a
good fit for that and that's really the
criteria for ia in active access is that
s3
or aws itself recommends to only put
objects in there if they haven't been
asked access for at least 30 days so you
get a a price reduction a rebate for
putting objects that are not accessed
frequently in here of course if you
remove objects let's say before the
30-day
limit then you are charged a surcharge
for
retrieving an object that you said was
infrequently accessed but it really was
not so bear that in mind if you're going
to place objects in infrequent access
be
somewhat
reasonably assured that you're not going
to be going there and accessing them you
can still access those files no problem
just as quickly they have the same level
of accessibility and durability however
like i said anything less than 30 days
you'll get a price thing on that
if you want to have long-term storage
we're talking about amazon glacier so
this is more
anything
over 90 days that hasn't been modified
or 180 days there's two subcategories of
amazon glacier that we're going to get
into
and this is the cheapest storage by far
and amazon glacier
doesn't really operate through the aws
console as the same as the standard and
the infrequent access you can't really
upload objects to glacier via the
console in the browser
you can only do so let's say through the
command line interface or through an sdk
the only thing you can do on the
web console with glacier is actually
create what's called a vault which is a
logical container for your archives but
then after that you have to go through
the cli or the sdk to do the rest of the
work there
if we continue on there's some gray
areas between
the s3 standard and the glacier one is
the one zone in frequently accessed
storage class so if we go back to the
regular
standard and ia storage class all of
these objects are stored across a
minimum of three availability zones
if you want a further price reduction
you can store your objects in a one zone
ia storage class which means that
instead of taking that object and
replicating it across three or more
availability zones it will only store it
in a single availability zone therefore
reducing
the level of availability that you have
to that object so in this case here if
that single availability zone would go
down for example you would not have
access to that object once it would come
back up of course you would the other
thing is is if there was a
an immense catastrophe where the actual
availability zone was destroyed well of
course then your object is also gone so
if that's something that doesn't worry
you
because you have already many copies of
this object maybe lying around on
premise then this is a good option for
you because it's data that you're
willing to lose or lose access to for
short periods of time if ever that
single availability zone goes down so
it's about an extra 20 off the price
from already the
normal
ia standard price
there is another one called the standard
reduced redundancy storage
this one is kind of getting phased out
as we speak because
the same price
for this storage class is about the same
amount you're going to pay for the
normal ia standard class what this does
is again is a good fit for
um your objects that you're not really
worried about losing if there is some
sort of catastrophe that happens in an
availability zone there's less copies of
it that are stored and so if that data
center and that availability goes down
then you lose your object so of course
it offered at the time the highest price
reduction
possible but now the difference between
this one and the normal ia standard
storage class is so small in terms of
price that you're probably not going to
migrate to
or navigate to this storage class but it
is still there in the documentation and
it may very well come up still
in the
certification exam so at least be aware
of that
let's now take a look at some individual
features of s3 starting off with
lifecycle management so lifecycle
management is very interesting because
it allows us to come up with a
predefined rule
that will help us automate the
transitioning of objects from one
storage
class to another without us having to
manually copy things over of course you
could imagine how time-consuming that
would be if we had to do this manually
so we're going to see this very soon in
a lab however let me discuss how how
this works
so once we it's basically a graphical
user interface it's very very simple to
use once you come up with these
lifecycle management rules but you're
going to define two things you're going
to define the transition action and the
expiration action so the transition
action is going to be something like
well i want to transition an object
from maybe it's all objects or maybe
it's just a specific type of object in a
folder example that has a specific
prefix
from one storage class let's say
standard to standard inactive
or infrequent access
maybe only after 45 days after at least
a minimum of 30 days like we spoke of
before and then maybe after
90 days you want to transition the
objects in ia to
right away glacier deep archive or 180
days you come up with whatever
combination you see fit okay it doesn't
have to be sequential from s3 to ia to
one zone etc etc because
like we discussed before it depends what
kind of objects that you're interested
in putting in one zone objects that you
don't really mind losing if that one
availability zone goes down so
you're going to be deciding those rules
it ends up that this even is not a
simple task because you have to monitor
your usage patterns to see which data is
hot which data is cold and what's the
best kind of life cycle management to
implement to reap the benefits of the
lowest cost
so you have to put somebody on this job
and
make the best informed
decisions based on your access patterns
and that is something that you need to
consistently monitor
so what we can do is we can instead opt
for something called
s3 intelligent tiering
which basically analyzes your workload
using machine learning algorithms
and after about a good 30 days of
analyzing your access patterns
will automatically be able to
transition your objects from s3 standard
to s3 standard
infrequent access okay it doesn't go
past the ia one doesn't go after the
glacier and whatnot okay so it can then
offer you
um that
at a reduced
price overhead so there is a monitoring
fee
that is introduced in order to
implement this feature it's a very
nominal very very low monitoring fee
and the nice thing is is if ever you
take out an object out of the infrequent
access before the 30-day limit as we
spoke of before
you will not be
charged
an overhead charge because of that why
because you're using the intelligence
tearing you're already paying an
overhead for the monitoring fee so at
least in that sense the intelligent
tearing will take the object out of ia
and put it back into the s3 standard
class if you need access to it before
the 30 days
and in that case you will be charged at
overhead so that is something that is
very
um that is very um good to to to do in
order not to have to put somebody on
that job so yes you're paying a little
bit of overhead for that monitoring fee
but at the other side of the spectrum
you're not investing in somebody uh
working many hours to
monitor and put into place a system to
monitor your data access patterns so
let's take a look at how to do this
right now let's implement our own
management rules
so let's now create a lifecycle rule
inside our bucket first off we're going
to need to go to the management tab
in the bucket that we just created and
right on the top you see right away life
cycle rule
we're going to create lifecycle rule and
we're going to name it so i'm just going
to say something very
simple like simply
learn
lifecycle rule
and we have the option of creating this
rule for every single object in the
bucket or we can limit the scope to a
certain type of file perhaps as a prefix
like
i could see one right now something like
logs so anything
that we categorize
as a log file will transition from one
storage tier to the next as per our
instructions
we're doing this because we really want
to save on costs right it's not so much
of organizing what's your older data
versus your newer data it's more about
reducing that storage cost as your
objects
get less and less used so in this case
logs are a good fit because
perhaps you're using your logs for the
first 30 days you're sifting through
them
you're trying to get insights on them
but then you
kind of
move them out of the way because they
become old data and you don't need them
anymore so we're going to see how we can
transition them to another pricing tier
another storage tier
we could also do this with object tags
which is a very powerful feature
and in the lifecycle rules action you
have to at least pick one of these
options now
since we haven't enabled versioning yet
what i'm going to do is just select
transition the current
version of the object between the
storage classes so as a reminder of what
we already covered in the slides our
storage classes are right over here so
the one that's missing is obviously
the default standard storage class which
all objects are placed in by default so
what we're going to say is this
we want our objects that are in the
default standard storage class to go to
the standard inactive access storage
class after 30 days and that'll give us
a nice discount on those objects being
stored
then we want to add another transition
and let's say we want to transition them
to glacier
after
90 days
and then
as a big finale we want to go to
glacier deep archive you can see the
rest are
grayed out would it make sense to go
back
and maybe after 180 days we want to go
there okay now there's a little bit of
um
a warning or a call to attention here
they're saying if you're going to store
very small files
into glacier not a great idea there's an
overhead in terms of metadata that's
added and also there's an additional
cost associated with storing small files
in glacier so we're just going to
acknowledge that of course for the
demonstration that's fine in real life
you don't want to store very big tar
files or zip files that had you know one
or more log files in there okay that
would bypass that that surcharge that
you would get
and over here you have the timeline
summary of everything we selected
up above so we have here after 30 days
the standard inactive access after 90
days glacier and after 180 days glacier
deep archive so let's go and create that
rule
all right so we see that the rule is
already enabled and at any time you
could go back and disable this if ever
you had a reason to do so we can easily
delete it as well or view the details
and and edit it as well so if we go back
to our bucket
now what i've done
is created that prefix with the slash
logs since we're not doing this from the
command line we're going to create a
logs folder over here that will fit that
prefix so
create logs
create folder and now we're going to
upload our let's say apache log files in
here so we're going to upload
one demonstration apache log file that
i've created with just one line in there
of course just for demonstration
purposes we're going to upload that
and now we have we're just close that
and now we have our apache log file in
there so what's going to happen because
we have that
life cycle rule in place after 30 days
anything any file that has the logs
prefix or basically is placed inside
this folder will be transitioned as per
that lifecycle
row policy that we just created so
congratulations you just created your
first
s3 lifecycle rule policy
let's now move over to bucket policies
so bucket policies are going to
allow
or deny access to not only the bucket
itself but the objects within those
buckets to either
specific
users
or other services
that are inside the aws network
now these policies fall under the
category of i am policy so im stands for
identity
and access management and this is a
whole other topic that deals with
security at large so there are no
services in aws
which are allowed to access other
services or data for example within s3
without you explicitly
allowing it through these i am policies
so one of the ways we do that is by
attaching one of these policies which
are written in a json format so it's a
text file that we write
at the end of the day that's the
artifact and that's a good thing because
we can use that artifact
and we can configuration control it in
our source control and version it and
put it alongside our source code
so we deploy everything it is part of
our deployment package
so in this case here we have several
ways of doing this we can use what's
called the policy generator which is a
graphical user interface that allows us
to simply click and point and populate
certain text boxes which will then
generate that json document that will
allow us to attach that to our s3 bucket
and that will determine like i said
which users or services have access to
whatever api actions are available for
that resource so we might say we want
certain users to be able just to
list the contents
of this bucket not necessarily be able
to delete
or upload new objects into that bucket
so you can get very fine-grained
permissions
based on
the kind of actions you want to allow on
this resource so in order to really
bring this home let's
go and perform our very own lab on this
let's now see how to create an s3
bucket policy
going back to our bucket we're now going
to go into permissions
so the whole point of coming up with a
bucket policy is that we want to control
who or what the what being other
services have access to our bucket and
our objects within our bucket so there
are several ways we can go about doing
this let's edit a bucket policy
one we can go and look at a whole bunch
of pre-canned examples which is a good
thing to do
two we could
actually go in here and code the json
document ourselves which is much more
difficult of course so what we're going
to do is we're going to look at a policy
generator which is really
a form based graphical user interface
that allows us to generate
through the
answers that we're going to give here
the json document for us
first question is we got to select the
policy type of course we're dealing with
s3 so it makes sense for us to create an
s3 bucket policy
the two options available to us are
allowing or denying
access to
our s3 bucket now in this case here we
could get really
fine-grained and specify certain kinds
of services or certain kinds of users
but for the demonstration we're just
going to select star which means
anything or anybody can access
this s3 bucket all right now depending
on also the actions that we're going to
allow so in this case here we can get
very fine-grained and we have all these
check boxes that we can check off to
give access to certain kind of api
action so we can say we want to give
access to you know just deleting the
bucket which obviously is something very
powerful
but you can get more fine green as you
can see you have more of the getters
over here
and you have more of the
the the listing and the putting new
objects in there as well so you can get
very fine grain now for demonstration
purposes we're going to say all action
so this is a very
broad and wide ranging permission
something that you really should think
twice about before doing we're basically
saying we want to allow everybody
and anything any service
all api actions on this s3 bucket so
that's no
small thing
we need to specify the amazon resource
name the arn of that bucket specifically
so what we're going to do is go back to
our bucket
and you can see here
the bucket arn okay so we're just going
to copy this
paste it in this policy generator and
just say
add statement you can see here kind of a
resume of what we just did and we're
going to say generate policy
and this is where it creates for us and
make this a little bit bigger for us
it creates that json document so we're
going to take this we're going to copy
it
and we're going to paste it into
the
generator okay now of course we could
flip this and change this to a deny
right which would basically say we don't
want anybody to have access or any thing
any other service to have access to this
s3 bucket we could even say
slash star
to also encapsulate all the objects
within that bucket so if i save this
right now
you have a very ironclad
s3 bucket policy which basically denies
all access
to this bucket and the objects within of
course this is on the other side of the
spectrum very very secure so we might
want to for example
host a static website
through our s3 bucket so in this case
here allowing access would make more
sense
right so if i save changes
you see that we get an error here saying
that we don't have permissions to do
this
and the reason for that is because it
realizes that this is extremely
permissive so in order to give access to
every single object within this bucket
as in the case that i was stating of a
static website being hosted on your s3
bucket it would be much better to also
at first enable that option so i'm just
going to duplicate the tab here
and once you go back to the permissions
tab
one of the first things that shows up is
this blood block public access setting
right right now it's completely blocked
and that's what's stopping us from
saving our policy we would have to go in
here
unblock it and save it right and it's
also kind of like a double clutch
feature you have to confirm that just so
you don't do that by accident
right so now what you've effectively
done
is you've really opened up the
floodgates
to
have public access to this bucket it's
something that can't be accidentally
done it's kind of like
having to perform these two actions
before the public access can be granted
now historically
this was something that aws
was
um
was guilty of was making it too easy to
have public access so now we have this
double clutch now that this is enabled
or turned off we can now save
our changes here successfully
and you can see here that now it's
publicly accessible
which is a big red flag that perhaps
this is not something
that you're interested in doing now if
you're hosting a public website and you
want everybody just to have read access
to every single object in your bucket
yes this is fine however please make
sure that you
pay very close attention to this type of
access flagged over here on the console
so congratulations you just got
introduced to your first
bucket policy a permissive one but at
least now you know how to go through
that graphical user interface through
the policy generator and create them and
paste them inside your s3 bucket policy
pane
so let's continue on with data
encryption so any data that you place in
s3 bucket can be encrypted at rest very
easily
using an aes 256 encryption key so we
can have server side encryption we could
have aws handle all the encryption for
us
and the decryption will also be handled
by aws when we request our objects later
on but we could also have client-side
encryption where we
the client that are uploading the object
have to be responsible for
also passing over our own
generated key that will eventually be
used by aws to then encrypt that object
on the bucket side of course once that
happens then the key is discarded the
client key is discarded and you have to
be very mindful that since you've
decided to handle your own encryption
client-side encryption that if ever you
lose those keys well that data is not
going to be recoverable in that bucket
on the aws network so be very careful on
that point
we can also
have a very useful feature called
versioning
which will allow you to have a history
of all the changes of an object over
time so versioning sounds exactly how
it's named every time you make a
modification to a file and upload that
new version to s3 it will have a brand
new version id associated with that so
over time you get a sort of stack of a
history of all the file changes over
time so you can see here at the bottom
you have an id with all these ones and
then an id with one two one two one two
so
eventually if ever you wanted to revert
back to a previous version you could do
so
by
accessing one of those previous versions
of course
versioning is not an option that's
enabled by default you have to go ahead
and enable that yourself it is an
extremely simple thing to do
and so there may be a situation where
you already have objects within your
buckets
and you only then
enable versioning well versioning would
only apply
to the new objects that would get
uploaded from the point that you enabled
versioning the objects that were there
before that point will not
get a specific version number attached
in fact they will have
a sort of null marker um the version
number that will get attached to them
it's only after that you
modify those objects later on and upload
a new version that they will get their
own version numbers
so right now what we're going to be
doing is a lab on actual
versioning so let's go ahead and do that
right
now this lab we're going to see how to
enable versioning in our buckets
enable versioning is very easy we're
simply going to click on our bucket go
into properties and there is going to be
a bucket versioning section
we're going to click on edit and enable
it
once that's done
any new objects
that are uploaded to that s3 bucket will
now benefit from
being tracked by a version number so if
you upload
objects with the same file name after
that they'll each have a different
version number so you'll have version
tracking a history of the changes for
that object
let's actually go there and upload a new
file
i'll upload one
called index.html so we're going to
simulate a situation where
we've decided we're going to use an s3
bucket
as the source of our static website to
deploy one and in this index.html file
if you take a look uh right now let's
take a look at what's in there you can
see that we have welcome to my website
and we're at version two okay so if i
click on this file right now
and i go to versions
i can clearly see that there's some
version activity that's happening here
okay
we have here um
at 1456 which is the latest one the
latest one is on the top the current
version we have a specific version id
and then we have a sort of history of
what's going on here now i purposely
enabled versioning before
and then try to
delete version or disable versioning but
here's the thing with versioning you
cannot disable it
fully once it's enabled you can only
suspend it
right now suspending means
that
whatever version numbers
those objects had before you decided to
suspend it will remain so you can see i
have an older version here that has
an older version number and at this
point here i decided to suspend
versioning and so what it does instead
of disabling the entire history it puts
what's called a delete marker
okay
you could always roll back to that
version if you want
now in the demonstration when we started
it together i
enabled it again
so you can see this is
actually the brand new version number as
we did it together but you don't lose
the history of
previous versioning if ever you had
suspended it before so that's something
to keep in mind right and it'll come up
in the exam where they'll last can you
actually disable versioning once it's
enabled and the answer is no you can
only suspend it and your history is
still maintained
now we have that version there and let's
say i come to this file
and i want to up grade this i don't know
i say version three right and now it's
going to happen
is
if i click on this version just as the
current one with version two and i open
this we should see
version two which is fine that's that's
that's expected if we go back to
our
bucket and upload that new version file
that has version three in there
don't just modify it
we should now see
in that index.html file a brand new
version that was created
under the versions tab
and there you go
1458 just two minutes after
you can see here we have a brand new
version id
right and if i open this one
you can see version three so now you
have a way
to
enable versioning very easily in your
buckets and you also have seen what
happens when you want to
suspend versioning what happens to the
history of those versions files before
just to actually go back here to the
properties
where we enabled in the first place if i
want to go back in here and disable it
like i said you can't disable you can
only suspend and that's where that
delete marker gets placed but all your
previous versions
retain their version id
so don't forget that because that will
definitely be a question on your exam if
you're interested in taking the
certification exam so congratulations
you just learned how to
enable
versioning
let's move on
to cross region replication or crr as it
is known there will be many times when
you find yourself with objects
in a bucket and you want to share those
objects
with another bucket now that other
bucket could be within the same account
could be within another account within
the same region or could be within a
separate account
in a different region so there's varying
levels of degree there the good thing is
is all of those
combinations are available so crr
if we're talking about cross-region
replication is really about replicating
objects across regions something that is
not enabled by default because that
will incur a replication charge because
it's sinking objects across
regions of course you are spanning a
very wide area network in that case so
there is a surcharge for that
now doing so is quite simple to do
but one of the things that we have to be
mindful of is to give permissions for
the source bucket which has the
originals
to allow for this copying of objects to
the destination bucket so if we're doing
this across regions of course we would
have to come up with im policies and we
would also have to exchange credentials
in terms of
i am user credentials in terms of
account ids and and such
we are going to be doing a demonstration
in the same account in the same region
but
largely this would be the same steps if
we're going to go cross region so this
is something you might find yourself
doing if you want to share data with
other
entities in your company maybe you're
multinational and you want to have all
your lock files copied over to another
bucket in another region for another
team to analyze to extract business
insights from or it might just be that
you want to aggregate data in a separate
data lake
in an s3 bucket in another region or in
a like i said it could be even in the
same region or in the same account so
it's all about organizing moving data
around across objects across these
boundaries and let's actually go through
a demonstration and see how we can do
crr
let's now see how we can perform cross
region replication
we're going to take all the new objects
that are going to be uploaded in the
simply learn s3 demo bucket and we're
going to replicate them into a
destination bucket so what we're first
going to do is create a new bucket
okay
and we'll just tack on the number two
here and this will be our destination
bucket where all those objects will be
replicated too we're gonna demonstrate
this within the same account but it's
the exact same steps
when doing this across regions
one of the requirements when performing
cross region replication is to enable
versioning so if you don't do this you
can do it at a later time but it is
necessary to enable it at some point in
time before
coming up with a cross region
replication rule
all right so let me create that bucket
and now after the bucket is created i
want to go to the source bucket
and i want to configure
under the managements tab here a
replication rule
so i'm going to create a replication
rule
call it
simply learn
rep rule
and i'm going to enable this right off
the bat
the source bucket of course is the
simply learn s3 demo we could apply this
to
all objects in the bucket or perform a
filter once again let's keep it simple
this time and apply to all
objects in the bucket of course caveat
here this will only now apply to any new
objects that are uploaded into this
source bucket and not the ones that are
already pre-existing there
okay
now in terms of the a destination bucket
we want to select the one we just
created so we can choose a bucket in
this account
or if we really want to go cross region
or another account in another region we
could specify this and put in the
account id and the bucket name in that
other account
so we're going to stay in the same
account we're going to browse and select
the newly created bucket
and we're also going to need permissions
for the source bucket to
dump those objects into the destination
bucket so we can either create the role
ahead of time or we can ask this user
interface to create a new role for us so
we'll opt for that
and we'll skip these additional
features over here that we're not going
to talk about in this demonstration i'm
just going to save this so that will
create
our replication rule
that is automatically enabled for us
right now
so let's take a look at the
overview here you can see it's been
enabled just to double check the
destination bucket is the demo
we're talking about
the same region
[Music]
and again here we could opt for
additional um
parameters like
different storage classes in the
destination bucket that that object is
going to be deposited in etc etc for now
we just created a simple rule
now if we go back to
the original source bucket which we're
in right now and we upload a new file
which will be transactions file in a csv
format
once this is uploaded
that
cross region replication rule
will kick in
and will eventually
right it's not immediate but will
eventually copy
the file inside the demo2 bucket now i
know it's not there already so what i'm
going to do is pause the video and come
back in two minutes and when i click on
this the file should be in there
okay so let's now double check and make
sure that object has been replicated
and there it is been replicated as per
our rule so congratulations you just
learned how to
perform your first
same account s3 bucket region
replication rule
let's now take a look at transfer
acceleration
so transfer acceleration is all about
giving your end users the best possible
experience when they're accessing
information
in your bucket so you want to give them
the lowest
latency possible you can imagine if you
were serving a website
and you wanted people to have the lowest
latency possible of course that's
something that's very desirable so in
terms of traversing long distances if
you have your bucket that is in for
example
the u.s east one region in the united
states in the virginia region and you
had users let's say in london that want
to access those objects of course they
would have to traverse a longer
distance than users that were based in
the united states and so if you wanted
to bring
those objects closer
to them in terms of latency then we
could take advantage of what's called
the amazon cloudfront delivery network
the cdn network which extends the aws
backbone by providing what's called
edge location so edge locations are
really data centers that are placed in
major city centers where our end users
mostly are located more densely
populated areas and your objects will be
cached in those locations so
if we go back to the example of your end
users being in london
well
they would be accessing a cached copy of
those objects that were stored in the
original bucket in the for example u.s
east one region
of course
you will get most likely a dramatic
performance
increase by enabling transfer
accelerations very simple to
enable this just bear in mind that when
you do so
that you will incur
a charge for using this feature the best
thing to do is to show you how to go
ahead and do this so let's do that right
now
let's now take a look at how to enable
transfer acceleration on our
simply learn s3 demo bucket
by simply going to the properties tab we
can scroll down
and look for
a heading called transfer acceleration
over here
and very simply just enable it
so what does this do
this allows us to take advantage of
what's called the content delivery
network the cdn
which extends the aws network backbone
the cdn network is
strategically placed into more densely
populated areas for example major city
centers and so if your end users
are situated in these more densely
populated areas they will reap the
benefits of having transfer acceleration
enabled because the latency that they
will experience will be
severely decreased so their performance
is going to be enhanced if we take a
look
at the speed comparison page for
transfer acceleration we can see that
once the page is finished loading it's
going to do a comparison it's going to
perform first of all what's called a
multi-part upload and it's going to see
how fast
that upload was done
with or without transfer acceleration
enabled now this is relative to where i
am running this test so right now i'm
actually running it from europe so you
can see that i'm getting very very good
results
if i would enable transfer acceleration
and my users were based in virginia so
of course now i have
varying
uh differences in percentage
as i go closer
or further away from my
region where my bucket or my browser is
being
is being referenced so you can see here
united states i'm getting pretty good
uh percentages as i go closer to europe
it gets lower of course but still very
very good frankfurt again this is about
us probably at the worst i'm going to be
getting here since i'm situated in
europe
and of course as i go
look more towards you know the asian
regions you can see once again it kind
of scales up in terms of better
performance
so of course this is an optional feature
once you enable it as i just showed over
here
this is a feature that you pay
additionally for
so bear that in mind make sure that you
take a look at the pricing page in order
to figure out
how much this is going to cost you so
that is it congratulations you just
learned how to
simply enable transfer acceleration to
lower the latency
from the end user's point of view
we're now ready to wrap things up in our
conclusion
and go over at a very high level what we
just spoke about so we talked about what
s3 is which is a core service one of the
original services published by aws
in order for us to have
unlimited object storage in a secure
scalable and durable fashion we took a
look at other aspects of s3 in terms of
the benefits we mainly focused on the
cost savings that we can attain
in s3 by looking at different storage
classes now of course s3 is
industry recognized as one of the
cheapest object storage
services out there that has the most
features available we saw what goes into
the object storage in terms of creating
first our buckets which are our
containers high level containers in
order for us to store our objects in
again
objects are really an abstraction of the
type of data that are in there
as well as the metadata associated with
those objects
we took a look at the different storage
tiers
the default being the standard
all the way till the cheapest one which
is the glacier which are meant for long
term archived objects for example lock
files that you may hold on to for a
couple of years
may not need to access routinely and
we'll have the cheapest pricing option
by far so we have many pricing tiers and
if you want to transition from one tier
to the next you would implement a life
cycle policy or use the intelligent
tiering option that can do much of this
for you we took a look at some very
interesting features
starting from the life cycle management
policies that we just talked about all
the way to
versioning cross region replication and
transfer acceleration so
with this conclusion you are now ready
to at least start working with s3 and if
you want to learn more about s3 and all
other services in aws you can refer to
the simply learn website for more
information at
www.simplylearn.com i look forward to
seeing you there thank you and have
yourself a great day
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here