foreign
ER today we have our session on the
basics of machine learning in today's
rapidly evolving technological landscape
machine learning has emerged as a
powerful tool that's transforming
Industries and reshaping our daily lives
whether you are an aspiring data
scientist a business professional or
simply curious about this fascinating
field you have come to the right place I
am abisar and I will be your guide as we
embark on this journey to domestify the
fundamentals of machine learning so what
exactly is machine learning how does it
work and why is it so crucial in today's
world let's find out machine learning is
a branch of artificial intelligence that
focuses on developing algorithms and
models capable of learning and making
predictions or decisions without being
explicitly programmed it enables
computers to automatically analyze and
interpret vast amounts of data uncode
patterns and extract valuable insights
from autonomous vehicles and
personalized recommendations on
streaming platforms to fraud detection
systems and medical Diagnostics machine
learning has a range of applications
across various domains its
revolutionizing Industries such as
Finance Healthcare retail manufacturing
and among others by enabling more
accurate predictions optimizing
processes and enhancing decision making
in this session we'll explore the key
Concepts that form the foundations of
machine learning we'll start by
understanding the types of machine
learning algorithms and their
characteristics we will delve into
supervised learning where models learn
from label data and unsupervised
learning where models identify patterns
in unlabeled data we'll also touch upon
reinforcement learning which involves
training models through interaction with
an environment furthermore we'll discuss
the crucial steps of machine learning
algorithms from lenient aggression to
nav base we will also address project
topics like fake news detection and
object detection which will help you in
resume a building by the end of this
session you will have a solid
understanding of the basics of machine
learning enabling you to comprehend its
applications and challenges better
whether you are looking to pursue a
career in data clients collaborate with
machine learning experts or simply be an
informed participant in today's data
driven world this knowledge will
undoubtedly prove invaluable
according to recent studies AI machine
learning related job postings have
increased by 344 in the past five years
companies across the globe are actively
seeking professionals who can harness
the power of data and build intelligent
systems the average salary is 150 000 in
the US and 15 lakhs per annum in India
Excel radio career in air and machine
learning with our comprehensive
post-graduate program in Ai and machine
learning in expertise in machine
learning deep learning NLP computer
vision and reinforcement learning you
will receive a prestigious certificate
exclusive alumni membership and
hackathons and ask me anything sessions
by IBM with three capstones and 25 plus
industry projects using real data sets
from Twitter Uber and more you will gain
practical experience this program covers
statistics python supervised and
unsupervised learning NLP neural
networks computer vision Gans KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below let's take a
minute to hear from our Learners who
have experienced massive success in
their careers you need to keep updating
your skills on a regular basis no matter
what level you're at I recently
completed the professional certificate
program in Ai and machine learning from
Simply learn in partnership with Purdue
University the course material was
comprehensive and the faculty was
extremely experienced The Faculty was
able to adjust their teaching style in
order to cater to the overall skill set
of the class in the rapidly evolving
world of technology it's important to
keep up Skilling for every working
professional stay relevant continue
learning if you are new to Channel
please make sure to subscribe to our
Channel and hit the Bell icon for
regular updates so why wait let's say
mark on this exciting Adventure into the
basics of machine learning together are
you ready let's dive in should we have
our um it looks a little bit like
Frankenstein or Frankenstein looking
robot today let me tell you what is
machine learning machine learning works
on the development of computer programs
that can access data and use it to
automatically learn and improve from
experience watch a robot builder
construct a house in two days this was
back in July 29
2016. so that's pretty impressive this
amount of time to continue to grow in
its development and it's smart enough to
leave spaces in the brickwork for wiring
and plumbing and can even cut and shape
bricks to size
Amazon Echo relies on machine learning
and with more data it becomes more
accurate play your favorite music order
pizza from Domino's voice control your
home request rides from Uber have you
ever wondered the difference between AI
machine learning and deep learning
artificial intelligence a technique
which enables machines to mimic human
behavior this is really important
because this is how we are able to gauge
how well our computations or what we're
working on works is the fact that we're
mimicking human behavior we're using
this to replace human work and make it
more efficient and make it more
streamlined and more accurate and so the
center of artificial intelligence is the
big picture of all this put together IBM
deep blue chess electronic game
characters those are just a couple
examples of artificial intelligence
machine learning a technique which uses
statistical methods enabling machines to
learn from their past data so this means
if you have your input from last time
and you have your answer you use that to
help prove the next guess it makes for
the correct answer IBM Watson Google
search algorithm email spam filters
these are all part of machine learning
and then deep learning which is a subset
of machine learning composing algorithms
that allow a model to trade itself and
perform tasks alphago natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black box as it learns it changes all
these things that are as a human we
would have a very hard time tracking and
it's able to come up with an answer from
that now let's see how machine Learning
Works first we start with training the
data once we've trained the data the
train we go into the machine learning
algorithm which then puts the data into
a processing which then goes down to
machine another machine learning
algorithm and then we take new data
because you have to test whatever you
did to make sure it works correctly and
we put that into the same algorithm once
we do that we check our prediction we
check our results and from the
prediction if we've set aside some
training date and we find out it didn't
do a good job predicting it and it gets
a thumbs down as you see then we go back
to the beginning and we retrain the
algorithm and a lot of times it's not
just about getting the wrong answer it's
about continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending what domain you're
working in whether it's medical
economical business stocks whatever you
try out your model and if it's not
giving you a good answer you retrain it
if you think you can get a better answer
you retrain it and you keep doing that
until you get the best answer you can
now let's look into the types of machine
learning
machine learning is primarily of three
types first one is supervised machine
learning as the name suggests you have
to supervise your machine learning while
you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled
finally there is reinforcement learning
wherein the system learns on its own
let's talk about all these types in
detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train
this is how the training is done we
provide a data set that contains
pictures of a kind of a fruit say an
apple
then we provide another data set which
lets the model know that these pictures
where that of a fruit called Apple
this ends the training phase now what we
will do is we provide a new set of data
which only contains pictures of Apple
now here comes the fun part the system
can actually tell you what fruit it is
and it will remember this and apply this
knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own
this kind of a model is generally used
into filtering spam mails from your
email accounts as well yes surprise
aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their similarities
so you provide the data to the system
and let the system do the rest of the
work simple isn't it this kind of a
model is used by Flipkart to figure out
the products that are well suited for
you
honestly speaking this is my favorite
type of machine learning out of all the
three and this type has been widely
shown in most of the Sci-Fi movies
lately let's find out how it works
imagine a newborn baby you put a burning
candle in front of the baby the baby
does not know that if it touches the
flame its fingers might get burned so it
does that anyway and gets hurt the next
time you put that candle in front of the
baby it will remember what happened the
last time and would not repeat what it
did that's exactly how reinforcement
learning works
we provide the machine with a data set
wherein we ask it to identify a
particular kind of a fruit in this case
an apple
so what it does as a response it tells
us that it's a mango but as we all know
it's a completely wrong answer so as a
feedback we tell the system that it's
wrong it's not a mango it's an apple
what it does it learns from the feedback
and keeps that in mind
when the next time when we ask a same
question it gives us the right answer it
is able to tell us that it's actually an
apple that is a reinforced response so
that's how reinforcement learning works
it learns from its mistakes and
experiences
this model is used in games like Prince
of Persia or Assassin's Creed or FIFA
wherein the level of difficulty
increases as you get better with the
games
just to make it more clear for you let's
look at a comparison between supervised
and unsupervised learning firstly the
data involved in case of supervised
learning is labeled as we mentioned in
the examples previously
we provide the system with a photo of an
apple and let the system know that this
is actually an apple
that is called label data so the system
learns from the label data and makes
future predictions
now unsupervised learning does not
require any kind of label data because
its work is to look for patterns in the
input data and organize it
the next point is that you get a
feedback in case of supervised learn
that is once you get the output the
system tends to remember that and uses
it for the next operation
that does not happen for unsupervised
learning and the last point is that
supervised learning is mostly used to
predict data whereas unsupervised
learning is used to excel radio career
in air and machine learning with our
comprehensive postgraduate program in Ai
and machine learning the next part is in
machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and hackathons and ask me anything
sessions by IBM with three capstones and
25 plus industry projects using real
data sets from Twitter Uber and more you
will gain practical experience this
program covers statistics python
supervised and unsupervised learning NLP
neural networks computer vision gns KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below so without
much further Ado let's get started so
deep learning is considered to be a part
of machine learning so this diagram very
nicely depicts what deep learning is at
a very high level you have the
all-encompassing artificial intelligence
which is more a concept rather than a
technology or a technical concept right
so it is it's more of a concept at a
very high level artificial intelligence
under the herd is actually machine
learning and deep learning and machine
learning is a broader concept you can
say or a product technology and deep
learning is a subset of machine learning
the primary difference between machine
learning and deep learning is that deep
learning uses neural networks and it is
suitable for handling large amounts of
unstructured data and last but not least
one of the major differences between
machine learning and deep learning is
that in machine learning the feature
extraction or the feature engineering is
done by the data scientists manually but
in deep learning since we use neural
networks the feature engineering happens
automatically so that's a little bit of
a quick difference between machine
learning and deep learning and this
diagram very nicely depicts the relation
between artificial intelligence machine
learning and deep learning now why do we
need deep learning machine learning was
there for quite some time and it can do
a lot of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images Voice or
even text for that matter so traditional
machine learning is not that very good
at doing this traditional machine
learning can handle large amounts of
structured data but when it comes to
unstructured data it's a big challenge
so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition and we need to process
analyze images and voice that's recent
deep learning is required compared to
let's say traditional machine learning
it can also perform complex algorithms
more complex than let's say what machine
learning can do and it can achieve best
performance with the large amounts of
data so the more you have the data let's
say reference data or label data the
better the system will do because the
training process will be that much
better and last but not least with deep
learning you can really avoid the manual
process of feature extraction those are
some of the reasons why we need deep
learning some of the applications of
deep learning deep learning has made
major inroads and it is a major area in
which deep learning is applied is
Healthcare and within Healthcare a
particularly oncology which is basically
cancer related stuff one of the issues
with cancer is that a lot of cancers
today are curable they can be cured they
are detected early on and the challenge
with that is when a Diagnostics is
performed let's say an image has been
taken of a patient to detect whether
there is cancer or not you need a
specialist to look at the image and
determine whether it is the patient is
fine or there is any onset of cancer and
the number of Specialists are limited so
if we use deep learning if we use
automation here or if we use artificial
intelligence here then the system can
with a certain amount of the good amount
of accuracy determine whether a
particular patient is having cancer or
not so the prediction or the detection
process of a disease like cancer can be
expedited the detection process can be
expedited can be faster without really
waiting for a specialist we can
obviously then once the application once
the artificial intelligence detects or
predicts that there is an onset of a
cancer this can be cross-checked by a
doctor but at least the initial
screening process can be automated and
that is where the current focus is with
respect to deep learning in healthcare
what else robotics is another area deep
learning is majorly used in robotics and
you must have seen nowadays robots are
everywhere humanoids the industrial
robots which are used for manufacturing
process you must have heard about Sophia
who got a citizenship with Saudi Arabia
and so on there are multiple such robots
which are knowledge oriented but there
are also industrial robots are used in
Industries in the manufacturing process
and increasingly in security and also in
defense for example image processing
video is fed to them and they need to be
able to detect objects obstacles and so
on and so forth so that's where deep
learning is used they need to be able to
hear and make sense of the sounds that
they are hearing that needs deep
learning as well so robotics is a major
area where deep learning is applied then
we have self-driving cars or autonomous
cars you must have heard of Google's
autonomous car which has been tested for
millions of miles and pretty much
incident free there were of course a
couple of incidents here and there but
it is considered to be fairly safe and
there are today a lot of Automotive
companies in fact pretty much every
automotive company worth its name is
investing in self-driving cars or
autonomous cars and it is predicted that
in the next probably 10 to 15 years
these will be in production and they
will be used extensively in real life
right now they are all in r d and in
test phases but pretty soon these will
be on the road so this is another area
where deep learning is used and how is
it used where is it used within
autonomous driving the car actually is
fed with video of surroundings and it is
supposed to process that information
process that video and determine if
there are any obstacles it has to
determine if there are any cars in the
site will detect whether it is driving
in the lane also it has to determine
whether the signal is green or red so
that accordingly it can move forward or
wait so for all these video analysis
deep learning is used in addition to
that the training overall training to
drive the car happens in a deep learning
environment so again a lot of scope here
to use deep learning couple of other
applications are mission translations
today we have a lot of information and
very often this information is in one
particular language and more
specifically in English and people need
information in various parts of the
world it is pretty difficult for human
beings to translate each and every piece
of information or every document into
all possible languages there are
probably at least hundreds of languages
or if not more to translate each and
every document into every language is
pretty difficult therefore we can use
deep learning to do pretty much like a
real-time translation mechanism so we
don't have to translate everything and
keep it ready but we train applications
or artificial intelligence systems that
will do the translation on the Fly for
example you go to somewhere like China
and you want to know what is written on
a sign board that is impossible for
somebody to translate that and put it on
the web or something like that so you
have an application which is train to
translate stuff on the fly so you
probably this can be running on your
mobile phone on your smartphone you scan
this the application will instantly
translate that from Chinese to English
that is one then there could be web
applications where there may be a
research document which is all in maybe
Chinese or Japanese and you want to
translate that to study that document or
in that case you need to translate so
therefore deep learning is used in such
situations as well and that is again on
demand so it is not like you have to
translate all these documents from other
languages into English in one shot and
keep it somewhere that is again and
pretty much an impossible task but on a
neat basis so you have systems that are
trained to translate on the fly so
Mission translation is another major
area of edit learning is used then there
are a few other upcoming areas where
synthesizing is done by neural nets for
example music composition and generation
of music so you can train a neural net
to produce music even to compose music
so this is a fun thing this is still
upcoming it needs a lot of effort to
train such neural land it has been
proved that it is possible so this is a
relatively new area and on the same
lines colorization of images so these
two images on the left hand side is a
grayscale image or a black and white
image this was colored by a neural net
or a deep learning application as you
can see it's done a very good job of
applying the colors and Excel radio
career in air and machine learning with
our comprehensive postgraduate program
in Ai and machine learning in expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and hackathons and ask me anything
sessions by IBM with three capstones and
25 plus industry projects using real
data sets from Twitter Uber and more you
will gain practical experience this
program covers statistics python
supervised and unsupervised learning NLP
neural networks computer vision Gans
Keras tensorflow and many more skills
enroll now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below human versus
artificial intelligence
are amazing let's just face it we're
amazing creatures we're all over the
planet we're exploring every Nick
Chinook we've gone to the Moon uh we've
got into outer space we're just amazing
creatures we're able to use the
available information to make decisions
to communicate with other people
identify patterns and data remember what
people have said adapt to new situations
so let's take a look at this so you can
get a picture you're a human being so
you know what it's like to be human
let's take a look at artificial
intelligence versus the human artificial
intelligence develops computer systems
that can accomplish texts that require
human intelligence
so we're looking at this one of the
things that computers can do is they can
provide more accurate results this is
very important recently I did a project
on cancer whereas identifying markers
and as a human being you look at that
and you might be looking at all the
different images and the data that comes
off of them and say I like this person
so I want to give them a very good
outlook and the next person you might
not like so you want to give them a bad
Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolve it's very quick and easy and
affordable to do this
what is machine learning and deep
learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world and you're sitting by the
road and you have a whole lot of and a
whole lot of time passes by there's a
few hours
and suddenly you wonder
how many cars buses trucks and so on
passed by in the six hours
now chances are you're not going to sit
by the road for six hours and count
buses cars and trucks unless you're
working for the city and you're trying
to do City Planning and you want to know
hey do we need to add a new truck route
maybe we need a bicycle length we have a
lot of bicyclists here that kind of
thing so maybe City Planning would be
great for this
machine learning well the way machine
Learning Works is we have labeled data
with features okay so you have a truck
or a car a motorcycle a bus or a bicycle
and each one of those are labeled it
comes in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle this is a little bit more
in depth on this
and the model here it actually the
features we're looking at would be like
the tires someone sits there and figures
out what a tire looks like it takes a
lot of work if you try to try to figure
the difference between a car tire a
bicycle tire a motorcycle tire uh so in
the machine learning field this could
take a long time if you're going to do
each individual aspect of a car and try
to get a result on there and that's what
they did do that was a very this is
still used on smaller amounts of data we
figure out what those features are and
then you label them
deep learning so with deep learning one
of our Solutions is to take a very large
unlabeled data set
and we put that into a training model
using artificial neural networks and
then that goes into the neural network
itself when we create a neural network
and you'll see the arrows are actually
kind of backward but uh which actually
is a nice point because when we train
the neural network
we put the bicycle in and then it comes
back and says if it's a truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns
once you've trained the neural network
you then put the new data in and they
call this testing the models you need to
have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
as a truck a motorcycle as a motorcycle
let's just take a little closer look at
that
determining what objects are present in
the data so how does deep learning do
this and here we have the image of the
bike it's 28 by 28 pixels that's a lot
of information there could you imagine
trying to guess that this is a bicycle
image by looking at each one of those
pixels and trying to figure out what's
around it and we actually do that as
human beings it's pretty amazing we know
what a bicycle is and even though it
comes in as all this information and
what this looks like is the image comes
in
it converts it into a bunch of different
nodes in this case there's a lot more
than what they show here and it goes
through these different layers and
outcomes and says okay this is a bicycle
a lot of times they call this the magic
Black Box why because as we watch it go
across here all these weights and all
the math behind this and it's not it's a
little complicated on the math side you
really don't need to know that when
you're programming or doing working with
the Deep learning but it's like magic
you don't know you really can't figure
out what's going to come out by looking
what's in each one of those dots and
each one of those lines are firing and
what's going in between them so we like
to call it the magic box so that's where
deep learning comes in
and in the end it comes up and you have
this whole neural network it comes up
and it says okay we fire all these
different pixels and we connect all
these different dots and gives them
different weights and it says okay this
is a bicycle
and that's how we determine what the
object is present in the data with deep
learning
machine learning we're going to take a
step into machine learning here and
you'll see how these fit together in a
minute the system is able to make
predictions or take decisions based on
past data that's very important for
machine learning is that we're looking
at stuff and based on what's been there
before we're creating a decision on
there we're creating something out of
there we're coloring a beach ball we're
telling you what the weather is in
Chicago
what's nice about machine learning is a
very powerful processing capability it's
quick and accurate outcomes so you get
results right away once you program the
system the results are very fast
and the decisions and predictions are
better they're more accurate they're
consistent you can analyze very large
amounts of data some of these data
things that they're analyzing now are
petabytes and terabytes of data it would
take hundreds of people hundreds of
years to go through some of this data
and do the same thing that the machine
learning can do in a very short period
of time and it's inexpensive compared to
hiring hundreds of people so because a
very affordable way to move into the
future is to apply the machine learning
to whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do
problem solved in an end-to-end method
so instead of having to break it apart
and you have the first piece coming in
and you identify tires and the second
piece is identifying labeling handlebars
and then you bring that together that if
it has handlebars and tires it's a
bicycle and if it has something that
looks like a large Square it's probably
a truck the neural networks does this
all in one network you don't really know
what's going on in all those weights and
all those little bubbles but it does it
pretty much in one package that's why
the neural network systems are so big
nowadays and coming into their own
best features are selected by the system
and it this is important they kind of
put it's on a bullet on the side here
it's a subset of machine learning this
is important we talk about deep learning
it is a form of machine learning there's
lots of other forms of machine learning
data analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test you put in your information
you then have your group of tests and
then you held some aside you see how
does it do it's very quick to test it
and see what's going on with your deep
learning and your neural network
are they really all that different
AI versus machine learning versus deep
learning concepts of AI
so we have concepts of I I you'll see
natural language processing machine
learning and approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep learning
so we look at this we have ai with
machine learning and deep learning and
so we're going to put them all together
we find out that AI is a big picture we
have a collection of books it goes
through some deep learning the Digital
Data is analyzed text mining comes
through the particular book you're
looking for maybe it's a genre books is
identified and in this case we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens so
we'll look at some of the pieces here
this information goes into uh there's as
far as this example the translation of
the handwritten printed data to digital
form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we used a deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the book
so here's a nice summation of what we
were just talking about AI with machine
learning and deep learning deep learning
is a subset of machine learning which is
a subset of artificial intelligence so
you can look at artificial intelligence
as the big picture how does this compare
to The Human Experience in either doing
the same thing as a human we do or it
does it better than us and machine
learning which has a lot of tools is
something that learns from data past
experiences it's programmed it's uh
comes in there and it says hey we
already had these five things happen the
sixth one should be about the same and
then there's a lot of tools in machine
learning but deep learning then is a
very specific tool in machine learning
it's the artificial neural network which
handles large amounts of data and is
able to take huge pools of experiences
pictures and ideas and bring them
together
real life examples
artificial intelligence news generation
very common nowadays as it goes through
there and finds the news articles or
generates the news based upon the news
feeds or the back end coming in and says
okay let's give you the actual news
based on this there's all the different
things Amazon Echo they have a number of
different Prime music on there of course
there's also the Google command and
there's also Cortana there's tons of
smart home devices now where we can ask
it to turn the TV on or play music for
us that's all artificial intelligence
from front to back you're having a human
experience with these computers and
these objects that are connected to the
processing machine learning spam
detection very common machine learning
doesn't really have the human
interaction part
so this is the part where it goes and
says okay that's a Spam that's not a
Spam and it puts it in your spam folder
search engine result refining another
example of machine learning whereas it
looks at your different results and it
Go and it is able to categorize them as
far as this had the most hits this is
the least viewed this has five stars you
know however they want to weight it all
exam good examples of machine learning
and then the Deep learning deep learning
another example is as you have like a
exit sign in this case is translating it
into French sortie I hope I said that
right
neural network has been programmed with
all these different words and images and
so it's able to look at the exit in the
middle and it goes okay we want to know
what that is in French and it's able to
push that out in France French and learn
how to do that
and then we have chat Bots I remember
when Microsoft first had their little
paper clip
um boy that was like a long time ago
they came up and you would type in there
and chat with it these are growing you
know it's nice to just be able to ask a
question and it comes up and gives you
the answer and instead of it being were
you just doing a search on certain words
it's now able to start linking those
words together and form a sentence in
that chat box
types of AI and machine learning
types of artificial intelligence this in
the next few slides are really important
so one of the types of artificial
intelligence is reactive machines
systems that only react they don't form
memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to re-sitter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
short-lived when we're talking about
this and you look at like a neural
network that's been programmed to
identify cars it doesn't remember all
those pictures it has no memory as far
as hundreds of pictures you process
through it all it has is this is the
pattern I use to identify cars as a
final output for that neural network we
looked at
so when they talk about limited memory
this is what they're talking about
they're talking about I've created this
based on all these things but I'm not
going to remember anyone specifically
theory of Mind systems being able to
understand human emotions and how they
affect decision making to adjust their
behaviors according to their human
understanding
this is important because this is our
page mark this is how we know whether it
is an artificial intelligence or not is
it interacting with humans in a way that
we can understand
without that interaction is just an
object so we talk about theory of mind
we really understand how it interfaces
that whole if you're in web development
user experience would be the term I
would put in there so the theory of mine
would be user experience how's the whole
UI connected together and one of the
final things is as we get into
artificial intelligence is systems being
aware of themselves understanding their
internal States and predicting other
people's feelings and act appropriately
so as artificial intelligence continues
to progress we see ones they're trying
to understand well what makes people
happy how would they increase our
happiness how would they keep themselves
from breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based on all
that information predicting which action
would work the best what would help
people if I know that you're having a
cup of coffee first thing in the morning
is what makes you happy as a robot I
might make you a cup of coffee every
morning at the same time to help your
life and help you grow that'd be the
self-awareness of being able to know all
those different things
types of machine learning and like I
said on the last slide this is very
important this is very important if you
decide to go in and get certified in
machine learning or know more about it
these are the three primary types of
machine learning the first one is
supervised learning systems are able to
predict future outcome based on past
data requires both an input and an
output to be given to the model for it
to be trained
so in this case we're looking at
anything where you have 100 images of a
bicycle
and those hundred images you know are
bicycle so they're preset someone
already looked at all hundred images and
said these are pictures of bicycles and
so the computer learns from those and
then it's given another picture
and maybe the next picture is a bicycle
and it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says yeah it's
not a bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
it can look at an image and start
pulling the different pieces of the
image out because they aren't the same
the human all the parts of the human are
not the same as a fuzzy tree behind them
because it's slightly out of focus which
is not the same as the beach ball it's
unsupervised because we never told it
what a beach ball was we never told it
what the human was and we never told it
that those were trees all we told it was
hey separate this picture by things that
don't match
and things that do match and come
together
and finally there's reinforcement
learning systems are given no training
it learns on the basis of the reward
punishment it received for performing
its Last Action it helps increase the
efficiency of a tool function or a
program reinforced learning or
reinforcement learning is kind of give
it a yes or no yes you gave me the right
response no you didn't and then it looks
at that and says oh okay so based on
this data coming in what I gave you was
a wrong response so next time I'll give
you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
were grouping a ton of machine learning
tools all together linear regression K
means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe you have like the case
of one of the things 500 points of data
and 30 different fields it starts
getting really confusing there and
artificial intelligence or machine
learning and the Deep learning aspect
really shines when you get to that
larger data that's really complex
works well on a low end systems so a lot
of the machine learning tools out there
you can run on your laptop with no
problem and do the calculations there
where with the machine learning usually
needs a higher end system to work it
takes a lot more processing power to
build those neural networks and to train
them it goes through a lot of data
we're talking about the general machine
learning tools most features need to be
identified in advanced and manually
coded so there's a lot of human work on
here the machine learns the features
from the data it is provided so again
it's like a magic box you don't have to
know what a tire is it figures it out
for you
the problem is divided into parts and
solved individually and then combined so
machine learning you usually have all
these different tools and use different
tools for different parts
and the problem is solved in an
end-to-end manner so you only have one
neural network or two neural networks
that is bringing the data in and putting
it out it's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem I have a solving one
piece of the puzzle
with regular machine learning emotional
machine learning tools out there they
take longer to test and understand how
they work
and with the Deep learning is pretty
quick once you build that neural network
you test it and you know
so we're dealing with very crisp rules
limited resources you have to really
explain how the decision was made when
you use most machine learning tools but
when you use the Deep learning tool
inside the machine learning tools the
system takes care of it based on its own
logic and reasoning and again it's like
a magic Black Box you really don't know
how it came up with the answer you just
know it came up with the right answer a
glimpse into the future so a quick
glimpse into the future artificial
intelligence be using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe it'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in healthcare
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization in so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant I'm
excited about that Excel radio Korea in
air and machine learning with our
comprehensive postgraduate program in Ai
and machine learning in expertise in
machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and hackathons and ask me anything
sessions by IBM with three capstones and
25 plus industry projects using real
data sets from Twitter Uber and more you
will gain practical experience this
program covers statistics python
supervised and unsupervised learning NLP
neural networks computer vision gns KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below machine
learning is improved our lives in a
number of wonderful ways today let's
talk about some of these I'm Rahul from
Simply learn and these are the top 10
applications of machine learning first
let's talk about virtual personal
assistance Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record what are
you saying send it over to a server
which is usually in a cloud decode it
with the help of machine learning and
neural networks and then provide you
with an output so if you ever noticed
that these systems don't work very well
without the internet that's because the
server couldn't be contacted next let's
talk about traffic predictions now say I
wanted to travel from Buckingham Palace
to Lord's cricket ground the first thing
I'd probably do is to get on Google Maps
so switch it
and let's put it here
so here we have the path you should take
to get to large cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there yellow indicate that they
are slightly congested and red means
they are heavily congested so let's look
at the map a different version of the
same map and here as I told you before
red means heavily congested yellow means
slow moving and blue means clear
so how exactly is Google able to tell
you that the traffic is clear slow
moving or heavily congested so this is
the help of machine learning and with
the help of two important measures first
is the average time that's taken on
specific days at specific times on that
route the second one is the real-time
location data of vehicles from Google
Maps and with the help of sensors some
of the other popular map services are
Bing Maps maps.me and here we go next up
we have social media personalization so
say I want to buy a drone and I'm on
Amazon and I want to buy a DJI mavic Pro
the thing is it's close to one lap so I
don't want to buy it right now but the
next time I'm on Facebook I'll see an
advertisement for the product next time
I'm on YouTube I'll see an advertisement
even on Instagram I'll see an
advertisement so here with the help of
machine learning Google has understood
that I'm interested in this particular
product hence it's targeting me with
these advertisements this is also with
the help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what spam and what's not spam
so Gmail has an entire collection of
emails which have already been labeled
as spam or not spam so after analyzing
this data Gmail is able to find some
characteristics like the word lottery or
winner from then on any new email that
comes to your inbox goes through a few
spam filters to decide whether it's spam
or not now some of the popular spam
filters that Gmail uses is content
filters header filters General Blacklist
filters and so on next we have online
fraud detection now there are several
ways that online fraud can take place
for example there's identity theft where
they steal your identity fake accounts
where these accounts only last for how
long the transaction takes place and
stop existing after that and man in the
middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes is that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags
the one known size and duration now this
is used to predict stock market trends
assist to Medical Technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice chemic stroke lesions
they can also be used in fetal Imaging
and cardiac analysis now some of the
medical fields that machine learning
will help assist in is disease
identification personalized treatment
drug Discovery clinical research and
radiology and finally we have automatic
translation now say you're in a foreign
country and you see Billboards and signs
that you don't understand that's where
automatic translation comes of help now
how does automatic translation actually
work the technology behind it is the
same as the sequence of sequence
learning which is the same thing that's
used with chatbots here the image
recognition happens using convolutional
neural networks and the text is
identified using optical character
recognition furthermore the sequential
sequence algorithm is also used to
translate the text from one language to
the other Excel radio career in Ai and
machine learning with a comprehensive
postgraduate program in Ai and machine
learning in expert is in machine
learning deep learning NLP computer
vision and reinforcement learning you
will receive a prestigious certificate
exclusive alumni membership and
hackathons and ask me anything sessions
by IBM with three capstones and 25 plus
industry projects using real data sets
from Twitter Uber and more you will gain
practical experience this program covers
statistics python supervised and
unsupervised learning NLP neural
networks computer vision gns KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below data dinners
the individual pieces of factual
information collected from various
sources it is stored process and later
used for analysis
and so we see here just a huge grouping
of information a lot of tech stuff money
dollar signs numbers and then you have
your performing analytics to drive
insights and hopefully you have a nice
share your shareholders gather it at the
meeting and you're able to explain it in
something they can understand
so we talk about data's types of data we
have in our types of data we have a
qualitative categorical
and you think nominal or ordinal and
then you have your quantitative or
numerical which is discrete or
continuous
and let's look a little closer at those
data type vocabulary always people's
favorite is the vocabulary words okay
not mine
but let's dive into this what we mean by
nominal nominal they are used to label
various uh label our variables without
providing any measurable value
uh country gender race hair color Etc
it's something that you either mark true
or false this is a label it's on or off
either they have a red hat on or they do
not uh so a lot of times when you're
thinking nominal data labels uh think of
it as a true false kind of setup and we
look at ordinal this is categorical data
with a set order or a scale to it and
you can think of salary range as a great
one movie ratings Etc you see here the
salary range if you have ten thousand to
twenty thousand number of employees
earning that rate is 150 20 000 to 30
000 100 and so forth some of the terms
you'll hear is bucket uh this is where
you have ten different buckets and you
want to separate it into something that
makes sense into those 10 buckets and so
when we start talking about ordinal a
lot of times when you get down to the
breastbones again we're talking true
false so if you're a member of the 10 to
20K Reigns so forth those would each be
either part of that group or you're not
but now we're talking about buckets and
we want to count how many people are in
that bucket
quantitative numerical data falls into
two classes discrete or continuous and
so data with a final set of values which
can be categorized class strength
questions answered correctly and runs
hit and Cricut a lot of times when you
see this you can think integer and a
very restricted integer I.E you can only
have 100 questions on a test so you can
it's very discreet I only have a hundred
different values that it can attain so
think usually you're talking about
integers but within a very small range
they don't have an open end or anything
like that
uh so discrete is very solid simple to
count set number continuous on the other
hand continuous data can take any
numerical value within a range so water
pressure weight of a person Etc usually
we start thinking about float values
where they can get phenomenally small in
their in what they're worth and there's
a whole series of values that falls
right between discrete and continuous
you can think of the stock market you
have dollar amounts it's still discrete
but it starts to get complicated enough
when you have like you know jump in the
stock market from
525.33 to
580.67 cents there's a lot of Point
values in there it'd still be called
discrete but you start looking at it as
almost continuous because it does have
such a variance in it now we talk about
no we did we went over nominal and
ordinal almost true false charts and we
looked at quantitative and numerical
data data which we'll start to get into
numbers discrete you can usually a lot
of times discrete will be put into it
could be put into true false but usually
it's not uh so we want to address this
stuff and the first thing we want to
look at is the very basic which is your
algebra so we're going to take a look at
linear algebra you can remember back
when your euclidean geometry we have a
line well let's go through this we have
a linear algebra as the domain of
mathematics concerning linear equations
and their representations in Vector
spaces and through matrixes I told you
we're going to talk about Matrix is
so a linear equation is simply
2x plus 4y minus 3z equals 10. very
linear 10x plus 12.4 y equals z and now
you can actually solve these two
equations by combining them unless we're
talking about a linear equation
and the vectors we have a plus b equals
c now we're starting to look at a
direction and these values usually think
of an x y z plot so each one is a
direction and the actual distance of
like a triangle a b is C and then your
Matrix can describe all kinds of things
I find matrixes confuse a lot of people
not because they're particularly
difficult but because of the magnitude
and the different things are used for
and a matrix is a chart or a you know
think of a spreadsheet but you have your
rows and your columns and you'll see
here we have a times b equals c very
important to know your counts uh so
depending on how the math is being done
what you're using it for making sure you
have the same rows and number of columns
or a single number there's all kinds of
things that play in that that can make
matrixes confusing but really it has a
lot more to do with what domain you're
working in are you adding in multiple
polynomials where you have like uh a x
squared plus b y plus you know you start
to see they can be very confusing versus
a very straightforward Matrix and let's
just go a little deeper into these
because these are such primary this is
what we're here to talk about is these
different math mathematical computations
that come up
so we're looking at linear equations
let's dig deeper into that one an
equation having a maximum order of one
is called a linear equation
so it's linear because when you look at
this we have ax plus b equals c which is
a one variable
we have two variable ax plus b y equals
c a X plus b y plus z c z equals D and
so forth but all of these are to the
power of one you don't see x squared you
don't see X cubed so we're talking about
linear equations that's what we're
talking about in their addition if you
have already dived into say neural
networks you should recognize this ax
plus b y plus c z
setup plus The Intercept which is
basically your your neural network each
node adding up all the different inputs
and we can drill down into that most
common formula is your y equals MX plus
c
so you have your y equals the M which is
your slope your X Value Plus C which is
your y-intercept they kind of labeled it
wrong here
threw me for a loop but the the C would
be your y-intercept so when you set x
equal to 0 y equals c and that's that's
your y-intercept right there
uh and that's they they just had a
reverse value of y when x equals zero
equals the y-intercept which is C and
your slope gradient line which is your M
so these are y equals two X plus three
and there's lots of easy ways to compute
this this way this is why we always
start with the most basic one when we're
solving one of these problems and then
of course the uh one of the most
important takeaways is the slope
gradient of the line so the slope is
very important that M value in this case
we went ahead and solved this
if you have y equals two X plus three
you can see how it has a nice line graph
here on the right
so matrixes a matrix refers to a
rectangular representation of an array
of numbers arranged in columns and rows
so we're talking M rows by n columns
here a11 is denotes the element of the
first row in the First Column similarly
a 12 and it's really pronounced a11 in
this particular setup so it's a row one
column one a 12 is a of Row one column
two first row and second column and so
on
and there's a lot of ways to denote this
I've seen these as like a capital letter
a smaller case a for the top row or I
mean you can see where they can go all
kinds of different directions as far as
the value
you just take a moment to realize there
needs to be some designation as far as
what row it's in and what column it's in
and we have our basic operations we have
addition so when you think about
addition you have uh two matrixes of two
by two and you just add each individual
number in that Matrix and then when you
get to the bottom you have in this case
the solution is 12 10 plus 2 is 12 5
plus 3 is 8 and so on and the same thing
with subtraction
now again your counting Matrix is you
want to check your dimensions of the
Matrix the shape you'll see shape come
up a lot in programming so we're talking
about Dimensions we're talking about the
shape if the two shapes are equal this
is what happens when you add them
together or subtract them
and we have multiplication when you look
at the multiplication you end up at the
very a slightly different setup going
now if we look at our last one where
we're like why this always gets to me
when we get to matrixes they don't
really say why you multiply matrixes
um you know my first thought is one
times two four times three but if you
look at this we get one times two plus
four times three one times three plus
four times five
uh six times two plus three times three
six times three plus three times five if
you're looking at these matrixes uh
think of this more as an equation and so
we have if you remember when we back up
here for our multiple line equations
let's just go back up a couple slides
where we were looking at uh two
variables so this is a two variable
equation ax plus b y equals c
um and this is a way to make it very
quick to solve these variables and
that's why you have the Matrix and
that's why you do
the multiplication the way they do and
this is the dot product of one times two
plus four times three
one times three plus four times five
six times two plus three times three
six times three plus three times five
and it gives us a nice little uh 14 23
21 and 33 over here which then can be
used and reduced down to a sample
formula as far as solving the variables
as you have enough inputs uh and then in
Matrix operations when you're dealing
with a lot of matrixes now keep in mind
multiplying matrixes is different than
finding the product of two matrixes okay
so we're talking about multiplication
we're talking about solving for
equations when you're finding the
product you are just finding one times
two keep that in mind because that does
come up I've had that come up a number
of times where I'm altering data and I
get confused as to what I'm doing with
it
transpose flipping the Matrix over its
diagonal comes up all the time where you
have you still have 12 but instead of it
being a 12 8 it's now 12 14 821 you're
just flipping the columns in the rows
and then of course you can do an inverse
changing the signs of the values of
across this main diagonal and you can
see here we have the inverse a to the
minus 1 and ends up with uh instead of
12 8 14 12 is now minus 22 minus 12.
vectors Vector just means we have
a value and a Direction
and we have down four numbers here on
our vector
in mathematics a one-dimensional matrix
is called a vector so if you have your X
plot and you have a single value that
value is along the x axis and it's a
single Dimension if you have two
Dimensions you can think about putting
them on a graph you might have X and you
might have y and each value denotes a
direction and then of course the actual
distance is going to be the hypothesis
so that triangle and you can do that
with three dimensionals X Y and Z and
you can do it all the way to nth
Dimensions so when they talk about the K
means for categorizing and how close
data is together they will compute that
based on the Pythagorean theorem so you
would take the square of each value add
them all together and find the square
root and that gives you a distance as
far as where that point is where that
Vector exists or an actual point value
and then you can compare that point
value to another one and it makes a very
easy comparison versus comparing 50 or
60 different numbers and that brings us
up to I Gene vectors and I Gene values
uh I Gene vectors the vectors that don't
change their span while transformation
and hygiene values the scalar values
that are associated to the vectors
conceptually you can think of the vector
as your picture you have a picture it's
two Dimensions X and Y
and so when you do those two dimensions
and those two values or whatever that
value is
um that is that point but the values
change when you skew it and so if we
take and we have a vector a
and that's a set value uh B is your is
your you have a and b which is your I
Gene Vector 2 is the I Gene value so
we're altering all the values by two
that means we're maybe we're stretching
it out One Direction making it tall if
you're doing picture editing
um that's one of the places this comes
in but you can see when you're
transforming uh your different
information how you transform it is then
your hygiene value and you can see here
a vector after line Transit transition
we have 3A a a is the I Gene Vector
three is the iodine value so a doesn't
change that's whatever we started with
that's your original picture and three
is skewing in One Direction and maybe uh
B is being skewed in another Direction
and so you have a nice tilted picture
because you've altered it by those by
the hygiene values
so let's go ahead and pull up a demo on
linear algebra and to do this I'm going
to go through my trusted Anaconda into
my Jupiter notebook
and we'll create a new uh notebook
called linear algebra
since we are working in Python we're
going to use our numpy I always import
that as NP or numpy array probably the
most popular
module for doing matrixes and things in
given that this is part of a series I'm
not going to go too much into numpy we
are going to go ahead and create two
different variables a for a numpy array
10 15 and b 29
. we'll go ahead and run this and you
can see there's our two arrays 10 15 29
and I went and added a space there in
between
so it's easier to read
and since it's the last line we don't
have to put the print statement on it
unless you want we can simply but we can
simply do a plus b so when I run this we
have 10 15 29 and we get 30 24 which is
what you expect 10 plus 20 15 plus 9 you
could almost look at this addition as
being
just adding up the columns on here
coming down and if we wanted to do it a
different way we could also do a DOT t
plus b dot T remember that t flips some
and so if we do that we now get them we
now have 30 24 going the other way we
could also do something kind of fun
there's a lot of different ways to do
this
uh as far as a plus b I can also do a
plus b dot t
and you're going to see that that will
come out the same the 3024 whether I
transpose A and B are transpose them
both at the end
and likewise we can very easily subtract
two vectors I can go a minus B
and we run that and we get minus 10 6.
now remember this is the last line in
this particular section and so I don't
have to put the print around it
and just like we did before
we can transpose either the individual
or we can transpose the main setup and
then we get a minus 10 6 going the other
way
now we didn't mention this in our notes
but you can also do a scalar
multiplication
and just put down the scalar so you can
remember that
and what we're talking about here is I
have this array here U and if I go a
times U
H we'll take the value 2 we'll multiply
it by every value in here so 2 times 30
is 60 2 times 15
and just like we did before
this happens a lot because when you're
doing matrixes you do need to flip them
you get 60 30 coming this way
so in numpy uh we have what they call
Dot product
and uh with this this is in a
two-dimensional vectors it is the
equivalent of two matrix multiplication
remember we were talking about matrix
multiplication uh where it is the
well let's walk through it
we'll go ahead and start by defining two
numpy arrays we'll have uh 10 20 25 6 or
our U and our V and then we're going to
go ahead and do if we take
the values and if you remember correctly
an array like this would be 10 times 25
plus 20 times 6. we'll go ahead and
print that
there we go
and then we'll go ahead and do the NP
dot dot of U comma
V
and we'll find when we do this we go and
run this
we're going to get
370
370. so this is a strain multiplication
where they use it to solve
linear algebra when you have multiple
numbers going across and so this could
be very complicated we could have a
whole string of different variables
going in here but for this we get a nice
value for our Dot multiplication
and we did addition earlier which is
just your basic addition and of course
the Matrix you can get very complicated
on these or in this case we'll go ahead
and do let's create two complex matrixes
this one is a matrix of
um you know 12 10 4 6 4 31. we'll just
print out a so you can see what that
looks like here's print a
we print a out you can see that we have
a
two by three layer Matrix for a and we
can also put together always kind of fun
when you're playing with print values we
can do something like this we could go
in here
there we go we could print a we have it
end with equals a run and this kind of
gives it a nice look here's your Matrix
that's all this is comma N means it just
tags it on the end that's all all that
is doing on there and then we can simply
add in what is a plus b and you should
already guess because this is the same
as what we did before there's no
difference we do a simple vector
addition we have 12 plus 2 is 14 10 plus
8 is 18 and so on
and just like we did The Matrix addition
we can also do a minus B into our Matrix
subtraction
and we look at this we have what 12
minus 2 is 10 10 minus eight uh where
are we
oh there we go eight minus ah
confusing what I'm looking at I should
have reprinted out the original numbers
but we can see here 12 minus 2 is of
course 10 10 minus 8 is 2 4 minus 46 is
minus 42 and so forth so same as its
attraction as before we just call it
Matrix subtraction it's identical
now if you remember up here we had a
scalar addition we're adding just one
number
to a matrix you can also do scalar
multiplication and so simply if you have
a single value a and you have B which is
your array we can also do a times B
when we run that you can see here we
have 2 times 4 is 8 5 times 4 is 20 and
so forth you're just multiplying the 4
across each one of these values
and this is an interesting one that
comes up a little bit of a brain teaser
is Matrix and Vector multiplication
and so when we're looking at this
we are let's do a regular arrays it
doesn't necessarily have to be a numpy
array we have a
which has our array of arrays and B
which is a single array and so we can
from here
do the DOT
a b
and this is going to return two values
and the verse value is that it's you
could say it's like uh we're doing the
this array B array first with a and then
with a second one and so it splits it up
so you have a matrix of vector
multiplication and you can mix and match
when you get into really complicated uh
back end stuff this becomes more common
because you're now you've got layers
upon layers of data and so you you'll
end up with a matrix and a set of Bolt
Vector matrices do you want to multiply
now keep in mind that if you're doing
data science a lot of times you're not
looking at this this is what's going on
behind the scenes so if you're in the
site kit looking at SK learn where
you're doing linear regression models
this is some of the math that's hidden
behind the scenes that's going on
other times you might find yourself
having to do part of this and manipulate
the data around so it fits right and
then you go back in and you run it
through the psy kit and if we can do
um
up here we did a matrix and Vector
multiplication we can also do Matrix to
matrix multiplication and if we run this
where we have the two matrixes you can
see we have very complicated array that
of course comes out on there for our DOT
and just to reiterate it we have our
transposer Matrix which is your dot t
and so if we create a matrix a and we do
transpose it you can see how it flips it
from 5 10 15 20 25 30 to 5 15 25 10 20
30. rows and columns
and certainly with the math this comes
up a lot it also comes up a lot with X Y
plotting when you put into Pi plot you
have one format where they're looking at
Pairs and numbers and then they want L
of x's and all y's so you know the
transpose is an important tool both for
your math and for plotting and all kinds
of things
another tool that we didn't discuss uh
is your identity Matrix
and this one is more definition
uh the identity Matrix we have here one
where we just did two so it comes down
as one zero zero one one zero zero zero
one zero it creates a diagonal of one
and what that is is when you're doing
your identities you could be comparing
all your different features to the
different features and how they
correlate and of course when you have a
feature one compared to feature one to
itself it is always one where usually
it's between zero one depending on how
well correlates so when we're talking
about identity Matrix that's what we're
talking about right here is that you
create this preset Matrix and then you
might adjust these numbers depending on
what you're working with and what the
domain is and then another thing we can
do to kind of wrap this up we'll hit you
with the most complicated uh piece of
this puzzle here is an inverse a matrix
and let's just go ahead and put the um
oh that's a lengthy description
let's go and put the description this is
straight out of the
the website for numpy so given a square
Matrix a here's our Square Matrix a
which is two one zero zero one zero one
two one keep in mind three by three it's
Square it's got to be equal it's going
to return the Matrix a inverse
satisfying dot a
um a inverse so here's our matrix
multiplication
um and then of course it equals the dot
uh yeah a inverse of a with an identity
shape of a DOT shape zero this is just
reshaping the identity
that's a little complicated there uh so
we're going to have our here's our array
we'll go ahead and run this and you can
see what we end up with is we end up
with an array 0.5 minus 0.5 and so forth
with our two one one going down two one
zero zero one zero one two one
um getting into a little deep on the
math understanding when you need this is
probably really is is what's really
important when you're doing data science
versus uh handwriting this out and
looking up the math and handwriting all
the pieces out you do need to know about
the linear algorithm inverse of a so if
it comes up you can easily pull it up or
at least remember where to look it up we
took a look at the algebra side of it
let's go ahead and take a look at the
calculus side of what's going on here
with the machine learning so calculus oh
my goodness and differential equations
you got to throw that in there because
that's all part of the bag of tricks
especially when you're doing large
neural networks but it also comes up in
many other areas the good news is most
of it's already done for you in the back
end so when it comes up you really do
need to understand from the data science
not data analytics data analytics means
you're digging deep into actually
solving these math equations in a neural
network is just a giant differential
equation
uh so we talk about calculus uh we're
going to go ahead and understand it by
talking about cars versus time and speed
uh so it helps to calculate the
spontaneous rate of change
uh so suppose we plot a graph with the
speed of a car with respect to time so
as you can see here going down the
highway probably merged into the highway
from an on-ramp so I had to accelerate
so my speed went way up
stuck in traffic merged into the traffic
traffic opens up and I accelerate again
up to the speed limit and maybe Peter's
off up there so you can look at this as
as
the speed versus time I'm getting faster
and faster because I'm continually
accelerating and if I hit the brakes it
go the other way
so the rate of change of speed with
respect to time is nothing but
acceleration how fast are we
accelerating the acceleration is the
area between the start point of X and
the endpoint of Delta X
so we can calculate a simple if you had
X and Delta X we could put a line there
and that slope of the line is our
acceleration
now that's pretty easy when you're doing
linear algebra but I don't want to know
it just for that line in those two
points I want to know it across the
whole of what I'm working with that's
where we get into calculus
so we talk about the distance between X
and Delta X it has to be the smallest
possible near to zero in order to
approximate the acceleration
so the idea is that instead of I mean if
you ever did took a basic calculus class
they would draw bars down here and you
would divide this area up let's go back
up a screen you divide this area up of
this time period up into maybe 10
sections and you'd use that and you
could calculate the acceleration between
each one of those 10 sections kind of
thing and then we just keep making that
space smaller and smaller until Delta X
is almost
infinitesimally small
and so we get a function of a equals a
limit as H goes to 0 of a function of a
plus h minus a function of a over H and
that is your Computing the slope of the
line
we're just Computing that slope under
smaller and smaller and smaller samples
uh and that's what calculus is calculus
is the integral you can see down here we
have our nice uh integral sign looks
like a giant s and that's what that
means is that we've taken this down to
as small as we can for that sampling
so we're talking about calculus we're
finding the area under the slope is the
main process in the integration similar
small intervals are made of the smallest
possible length of X Plus Delta X where
Delta X approaches almost an
infinitesimally small space
and then it helps to find the overall
acceleration by summing up all the links
together so we're summing up all the
accelerations from the beginning to the
end
and so here's our integral we sum of a
of x times D of x
equals a plus c and that is our basic
calculus here
so when we talk about multivariate
calculus
multivariate calculus deals with
functions that have multiple variables
and you can see here we start getting
into some very complicated equations
change in W over change of time equals
change of w over change of Z the
differential of Z to DX differential of
x to DT it gets pretty complicated and
it really translates into the
multivariate integration using double
integrals and so you have the the sum of
the sum of f of x of Y of D of a equals
the sum from C to D and A to B of f of x
o y d x d y equals the sum of a to B sum
of C to D of f x or y d y d x
understanding the very specifics of
everything going on in here and actually
doing the math is use the calculus one
calculus two and differential equations
so you're talking about three full
length courses to dig into and solve
these math equations what we want to
take from here is we're talking about
calculus we're talking about summing of
all these different slopes and so we're
still solving a linear uh expression
we're still solving y equals m x plus b
but we're doing this for infantismally
small x's and then we want to sum them
up that's what this integral sign means
the sum of a of x d of x equals a plus c
and when you see these very complicated
uh multivariate differentiation using
the chain rule when we come in here and
we have the change of w to the change of
T equals a change of w DZ uh and so
forth that's what's going on here that's
what these means we're basically looking
for the area under the curve which
really comes to how is the change
changing speeds going up how is that
changing and then you end up with a
multiple layer so if I have three layers
of neural networks how is the third
layer changing based on the second layer
changing which is based on the first
layer changing and you get the picture
here that now we have a very complicated
multivariate integration with integrals
the good news is we can solve this
mathematically and that's what we do
when you do neural networks and reverse
propagation so the nice thing is that
you don't have to solve this on paper
unless your data analysis and you're
working on the back end of integrating
these formulas and building the script
to actually build them so we talk about
applications of calculus it provides us
the tools to build an accurate
predictive model so it's really behind
the scenes we want to guess at what the
change of the change of the change is
that's a little goofy I I know I just
threw that out there's kind of a meta
term but if you can guess how things are
going to change then you can guess what
the new numbers are
multivariate calculus explains the
change in our Target variable in
relation to the rate of change in the
input variables
so there's our multiple variables going
in there if one variable is changing how
does it affect the other variable
and then in gradient descent calculus is
used to find the local and Global Maxima
and this is really big we're actually
going to have a whole section here on
gradient descent because it is really I
mean I talked about neural networks and
how you can see how the different layers
go in there but gradient descent is one
of the most key things for trying to
guess the best answer to something
so let's take a look at the code behind
gradient descent and before we open up
the code let's just do real quick
gradient descent
let's say we have a curve like this and
most common
is that this is going to represent your
error oops
error there we go error uh hard to read
there and I want to make the error as
low as possible
and so when I'm looking at it is I want
to find this line here
which is the minimum value so we're
looking for the minimum
and it does that by sampling there
and then it based on this it guesses it
might be someplace here and it goes hey
this is still going down it goes here
and then goes back over here and then
goes a little bit closer and it's just
playing a high low until it gets to that
spot that bottom spot
and so we want to minimize the error in
on the flip note you could also want to
be maximizing something you want to get
the best output of it that's simply
minus the value so if you're looking for
where the peak is this is the same as a
negative for where the valley is I'm
looking for that Valley that's all that
is and this is a way of finding it so
the cool thing is all the heavy
lifting's done I actually ended up
putting together one of these a while
back as uh when I didn't know about
sidekick and I was just starting boy
it's a long while back and uh is playing
high low how do you play high low not
get stuck in The Valleys figure out
these curves and things like that while
you do that and the back end is all the
calculus and differential equations to
calculate this out the good news is you
don't have to do those
uh so instead we're going to put
together the code and let's go ahead
and see what we can do with that
so uh guys in the back put together a
nice little piece of code here which is
kind of fun uh some things we're gonna
note and this is this is really
important stuff because when you start
doing your data science and digging into
your machine learning models uh you're
gonna find these things are stumbling
blocks the first one is current X where
do we start at keep in mind
your model that you're working with is
very generic so whatever you use to
minimize it the first question is where
do we start and we started at this
because the algorithm starts at x equals
three so we arbitrarily picked five
learning rate is how many bars to skip
going one way or the other I'm in fact
I'm going to separate that a little bit
because these two are really important
if we're dealing with something like
this where we're talking about well
here's our here's the function we're
going to use our gradient of our
function 2 times X plus five keep it
simple so that's a function we're going
to work with so if I'm dealing with
increments of a thousand point one is
going to be a very long time and if I'm
dealing with increments of 0.001
0.1 is going to skip over my answer so I
won't get a very good answer and then we
look at Precision this tells us when to
stop the algorithm so again very
specific to what you're working on if
you're working with money
and you don't convert it into a float
value you might be dealing with 0.01
which is a penny that might be your
Precision you're working with
and then of course the previous step
size Max iterations we want something to
cut out at a certain point usually
that's built into a lot of minimization
functions and then here's our actual
formula we're going to be working with
and then we come in we go while previous
step size is greater than precision and
itters is less than Max and Max itters
say that 10 times fast
um
we're just saying if it's uh if we're if
we're still greater than our Precision
level we still got to keep digging
deeper and then we also don't want to go
past a thou or whatever this is a
million or 10 000 running that's
actually pretty high I almost never do
Max iterations more than like a hundred
or two hundred rare occasions you make
up to four or five hundred if it's
depending on the problem you're working
with uh so we have our previous equals
our current that way we can track time
wise uh the current now equals the
current minus the rate times the formula
of our previous X
so now we've generated our new version
uh previous step size equals the
absolute current previous
so we're looking for the change in x
errors equals iterations plus one that's
how we know to stop if we get too far
and then we're just going to print the
local minimum occurs at axon here and if
we go ahead and run this
uh you can see right here it gets down
to this point and it says hey
um
local minimum is minus
3.322 for this particular series we
created and this is created off of our
formula here Lambda x 2 times X plus
five now
when I'm running this stuff you'll see
this come up a lot
in with the sklearn kit and one of the
nice reasons of breaking this down the
way we did is I could go over those top
pieces those top pieces are everything
when you start looking at these
minimization toolkits in built-in code
and so from
um we'll just do it's actually Docs
Dot
scipy.org and we're looking at
the scikit
there we go optimize minimize
you can only minimize one value you have
the function that's going in this
function can be very complicated so we
used a very simple function up here it
could be there's all kinds of things
that could be on there and there's a
number of methods to solve this as far
as how they shrink down and your X
naught there's your there's your start
value so your function your start value
there's all kinds of things that come in
here that you can look at which we're
not going to
optimization automatically creates
constraints bounds some of this it does
automatically but you really the big
thing I want to point out here is you
need to have a starting point you want
to start with something that you already
know is mostly the answer if you don't
then it's going to have a heck of a time
trying to calculate it out
or you can write your own little script
that does this and does a high low
guessing and tries to find the max value
that brings us to statistics what this
is kind of all about is figuring things
out a lot of vocabulary and statistics
ah so statistics well I guess it's all
relative it's definitely not an adult
class so a bunch of stuff going on
statistics statistics concerns with the
collection organization analysis
interpretation and presentation of data
that is a mouthful
um so we have from end to end where
where does it come from is it valid what
does it mean how do we organize it how
do we analyze it and then you got to
take those analysis and interpret it
into something that uh people can use
kind of reduce it to understandable and
nowadays you have to be able to present
it if you can't present it then no one
else is going to understand what the
heck you did
so we look at the terminologies uh there
is a lot of terminologies depending on
what domain you're working in
so clearly if you're working in a domain
that deals with
viruses and T cells and and how does you
know where does that come from and
you're studying the different people the
beginning of a population if you are
working with
um
mechanical gear you know a little bit
different if you're looking for the
wobbling statistics to know when to
replace a rotor on a machine or
something like that that can be a big
deal you know we have these huge fans
that turn
in our sewage processing systems and so
those fans they start to wobble and hum
and do different things that the sensors
pick up at one point do you replace them
instead of waiting for it to break in
which case it costs a lot of money
instead of replacing a bushing you're
replacing the whole fan unit
uh an interesting project that came up
for our city a while back
so population all objects are
measurements whose properties are being
observed uh so that's your population
all the objects it's easy to see it with
people because we have our population in
large but in the case of the sewer fans
we're talking about having the fan units
that's the population of fans that we're
working with
you have a parameter a matrix that is
used to represent a population or
characteristic
you have your sample a subset of the
population studied you don't want to do
them all because then you don't have a
if you come up with a conclusion for
everyone you don't have a way of testing
it so you take a sample sometimes you
don't have a choice you can only take a
sample of what's going on you can't
study the whole population and a
variable a metric of interest for each
person or object in a population
types of sampling we have probabilistic
approach selecting samples from a larger
population using a method based on the
theory of probability
and we'll go into a little bit more
deeper on these we have random
systematic stratified and then you have
non-probabilistic approach selecting
samples based on the subjective Judgment
of the researcher rather than random
selection it has to do with convenience
trying to reach a quota or snowball
uh and they're very biased that's one of
the reasons you'll see this big stamp on
it says biased uh so you gotta be very
careful on that
so probabilistic sampling uh when we
talk about a random sampling we select
random size samples from each group or
category so we it's as random as you can
get uh we talk about systematic sampling
we're selecting random size samples from
each group or category with a fixed
periodic interval
uh so we kind of split it up this would
be like a Time setup or a different
categories and you might ask your
question what is a category or a group
uh if you look at I'm going to go back a
window let's say we're studying
economics of different of an area
we know pretty much that based on their
culture where they came from they might
need to be separated and so uh and when
I say separated I don't mean separated
from their their place where they live I
mean as far as the analysis we want to
look at the different groups and make
sure they're all represented
so if we had like an 80 uh of a group
that is uh say Hispanic and or Indian
and also in that same area we have 20 20
who are let's call our expatriates they
left America and they're nice and uh
your Caucasian group we might want to
sample a group that is representative of
both uh so we're talking about
stratified sampling and we're talking
about groups those are the groups we're
talking about and that brings us to
stratified sampling selecting
approximately equal size samples from
each group or category
uh this way we can actually separate the
categories and give us an insight into
the different cultures and how that
might affect them in that area
so you can see these are very very
different kind of depends on what you're
working with as far as your data and
what you're studying and so we can see
here just a little bit more we have
selecting 25 employees from a company of
250 employees randomly don't care
anything about them what groups they're
in which office they're in nothing
uh and we might be selecting one
employee from every 50 unique employees
in a company of 250 employees and then
we have selecting one employee from
every branch in the company office so we
have all the different branches there's
our group or our categories by the
branch and the category could depend on
what you're studying so it has a lot of
variation on there you see this kind of
grouping and categorizing is also used
to generate a lot of misinformation
uh so if you only study one group and
you say this is what it is then
everybody assumes that's what it is for
everybody and so you've got to be very
careful of that and it's very unethical
thing to kind of do
so types of Statistics we talk about
statistics we're going to talk about
descriptive and inferential statistics
there are so many different terms and
statistics to break it up so we so we're
talking about a particular
setup
so we're talking about descriptive and
inferential uh statistics the base of
the word describe is pretty solid you're
describing the data what does it look
like with inferential statistics we're
going to take that from the small
population to a large population so if
you're working with a drug company you
might look at the data and say these
people were helped by this drug they did
80 better as far as their health or 80
percent better survival rate than the
people
who did not have the drug so we can
infer that that drug will work in the
greater populace and will help people so
that's where you get your inferential so
we are predicting how it's going to
affect the greater population
so descriptive statistics it is used to
describe the basic features of data and
form the basis of quantitative analysis
of data
so we have a measure of central
Tendencies we have your mean median and
mode
and then we have a measure of spread
like your range your interquartile range
your variance in your standard deviation
and we're going to look at all these a
little deeper here in a second but one
of them you can think of is
how the data difference
differences you know what's the max Min
range all that stuff is your spread and
anything that's just a single number is
usually your central Tendencies measure
of central tendencies
so we talk about the mean it is the
average of the set of values considered
what is the average outcome of
whatever's going on
and then your median separates the
higher half and the lower half of data
so where's the center point of all your
different data points so your mean might
have some a couple really big numbers
that skew it so that the average is much
higher than if you took those outliers
out where the median would by separating
the high from the low might give you a
much lower number you might look at and
say oh that's that's odd Y is the
average so much higher than the median
well it's because you have some outliers
or why is it so much lower
and then the mode is the most frequent
appearing value this is really
interesting if you're studying economics
and how people are doing you might find
that the most common
income like in the U.S was at 1.24 000 a
year
where the average was closer to 80 000
and it's like wow what a difference well
there's some people have a lot of money
and so that skews that way up so the
average person is not making that kind
of money and then you look at the median
income and you're like well the median
income is a little bit closer to the
average so it does create a very
interesting way of looking at the data
again these are all uh Central
Tendencies single numbers you can look
at for the whole spread of the data
and we look at the measure of central
Tendencies the mean is the average marks
of a student's in a classroom so here we
have the mean some of the marks of the
students total number of students and as
we talked about the median if we have 0
through 10 and we take half the numbers
and put them on one side of the line
half the numbers on the other side of
the line we end up with five in the
middle and then the mode what Mark was
scored by most of the students in a test
in a simple case where most people
scored like an 82 percent and got
certain problems wrong easy to figure
out uh not so easy when you have
different areas where like you have like
the um oh let's go back to economy a
little bit more difficult to calculate
if you have a large group of scores that
makes 30 000 and a slightly bigger group
that makes twenty six thousand so what
do you put down for the mode uh
certainly there's a number of ways to
calculate that and there's actually a
different variations depending on what
you're doing so now we're looking at a
measure of spread uh range what's the
difference between the highest and the
lowest value first thing you want to
look at you know it's uh we had
everybody in the test scored between 60
and 100 percent so we got 100 or maybe
60 to 90 percent it was so hard that a
lot of people could not get a hundred
percent
um you have your interquartile range
quartiles divide a rank ordered data set
into four equal parts
very common thing to do as part of all
the basic packages whether you're
working in uh data frames with pandas
whether you're working in Scala whether
you're working in R you'll see this come
up where they have range your Min your
Max and then it'll have your
interquartile range how does it look
like in each quarter of data variants
measures how far each number in the set
is from the mean and therefore from
every other number in the set
uh so you have like how much turbulence
is going on in this data
and then the standard deviation it is a
measure the variance or the dispersion
of a set of values from the mean
and you'll usually see if I'm doing a
graph I might have the value graphed and
then based on the the error I might grab
graph the standard deviation in the
error on the graph as a background so
you can see how far off it is
so standard deviation is used a lot
so measurement of spread marks of a
student out of 100 we have here from 50
to 63 or 50 to 90 so the range maximum
marks minimum marks we have 90 to 45 and
the spread of that is 45 90 minus 45 and
then we have the interquartile range
using the same marks over there you can
see here where the median is and then
there's a first quarter the second
quarter and the third quarter based on
splitting it apart by those values and
to understand the variance and standard
deviation we first need to find out the
mean so here's our you know calculating
the average there we end up
approximately 66 for the average and
then we look at that in the variance
once we know the means we can do equals
the marks minus the mean squared Y is a
squared because one you want to make
sure it's you don't have like if you if
you're putting all this stuff together
you end up with an error as far as one's
negative one's positive one's a little
higher one's a little lower uh so you
always see the squared value and over
the total observations and so the
standard deviation equals the square
root of the variance which is
approximately 16. and if you were
looking at a predictable model you would
be looking at the deviation based on the
error how much error does it have that's
again really important to know if you're
if your prediction is predicting
something what's a chance of it being
way off or just a little bit off
now that we've looked at the tools as
far as some of the basics for doing your
statistics what we're talking about
let's go ahead and pull up a little demo
and show you what that looks like in
Python code so you can get some little
Hands-On here for that let's go back
into our Jupiter notebook in Python now
almost all of this you can do in numpy
last time we worked in numpy this time
we're going to go ahead and use pandas
and if you remember from pandas on here
this is basically a data frame rows
columns let's just go ahead and do a
print df.head
and run that
and you can see we have the name Jane
Michael William Rosie Hannah sat on
their salaries on here and of course
instead of having to do all those hand
calculations and add everything together
and divide by the total we can do
something very simple on this uh like
use the command mean in pandas and so if
I go ahead and do this print DF pick our
column salary because we want to find
the means of that calorie
we want to find the means of that column
and we go and print this out and you can
see that the average income on here is
71
000.
and let's just go ahead and do this
we'll go ahead and put in means
and if we're going to do that we also
might want to find the median
and the median is very similar
except it actually is just median we're
used to means in average it's kind of
interesting that those are they use the
two different words
there can be in some computation slight
differences but for the most part the
means is the average and then the median
oops let's put a
median here do you have salary that way
it displays a little better we can see
the median is 54
000. so the halfway mark is
significantly below the average why
because we have somebody in here who
makes 189 000. darn you Rosie for
throwing off our numbers uh but that's
something you'd want to notice this is
this is the difference between these is
huge and so is what is the meaning
behind that when you're studying a
populist and looking at the different
data coming in and of course we also
want to find out hey what's the most
common
income that people make
in this little tiny sample and so we'll
go ahead and do the mode
and you can see here with the mode uh
it's at fifty thousand
so this is this is very telling that
most people are making 50 000 the middle
point is at fifty four thousand so half
the people are making more than that
what that tells me is that if the most
common income is way is below the median
then there's a few there's a you know
there's a lot of high salaries going up
but there's some really low salaries in
there and so this trend which is very
common in statistic and when you're in
analyzing the economy in different
people's income is pretty common and the
bigger difference between these is also
very important when we're studying
statistics and when you hear someone
just say hey the average income was you
might start asking questions at that
point why aren't you talking about the
median income why aren't you talking
about the mode the most common income
what are you hiding and if you're doing
these analysis you should be looking at
these saying hey why are this
discrepancies why are these so different
and of course with any analysis it's
important to find out the minimum
and the maximum so we'll go ahead it's
just simply
dot min
so pull up your minimum and then dot Max
pulls up the maximum pretty
straightforward on as far as
translating it and knowing what you're
you know put the your lowest value and
what your highest value is here
which you'll use to generate like a
spread later on and real quick on no
mode uh note that it puts mode zero like
I said there's a couple different ways
you can compute the mode
um although the standard one's pretty
good we can of course do the range which
is your max minus your Min so now we
have a range of 149 000 between the
upper end and the lower end and you
might want to be looking at the
individual values on all of these but it
turns out there is a describe
feature in pandas
and so in pandas we can actually do DF
salary describe and if we do this you
can see we have that there's seven
setups here's our mean our standard
deviation which we didn't compute yet
which would just be a DOT STD and you
gotta be a little careful because when
it computes it it looks for axes and
things like that we have our minimum
value and here's our quartiles
our maximum value and then of course the
name salary uh so these are these are
the basic statistics you can pull them
up and like just describe this is a
dictionary so I could actually do
something like um
and here I could actually go uh count
and run and now it just prints the count
so because this is a dictionary you can
pull any one of these values out of here
it's kind of a quick and dirty way to
pull all the different information and
then split it up depending on what you
need now if I just walked in and gave
you this information in a meeting
at some point you would just kind of
fall asleep that's what I would do
anyway
um so we want to go ahead and see about
graphing it here we'll go ahead and put
it into a histogram and plot that graph
on it
of the salaries and let's just go ahead
and put that in here so
we do our matplot inline remember that's
a Jupiter's notebook thing a lot of the
new version of the matplot library does
it automatically but just in case I
always put it in there import matplot
library pipelot is PLT that's my
plotting
and then we have our data frame I guess
I really don't need to respell the data
frame maybe we could just remind
ourselves what's in it so we'll go ahead
and just print
DF that way we still have it and then we
have our salary DF salary salary Dot
Plot history title salary distribution
color gray
uh plot axvline salary the mean value so
we're going to take the mean value
color violet line style Dash this is
just all making it pretty
what color dash line line width of two
that kind of thing and the median and
let's go ahead and run this just so you
can see what we're talking about
and so up here we are taking on our plot
so here's the data here's our our data
frame printed out so you can see it with
the salaries we'll look at the salary
distribution and just look at this the
way there the salary is distributed
um you have our in this case we did
Let's see we had red for the median
we have violet
for our average or mean and you can just
see how it really here's our outlier
here's our person who makes a lot of
money here's the average and here's the
median
um and so as you look at this you can
see wow based on the average it really
doesn't tell you much about what people
are really taking home all it does is
tell you how much money is in this you
know what the average salary is
so some of the things you want to take
away in addition to this is that it's
very easy to plot
um
an axv line these are these up and down
lines for your markers
um and as you just just play the data I
mean you can add all kinds of things to
this and get really complicated keeping
it simple is pretty straightforward I
look at this and I can see we have a
major outlier out here we can definitely
do a histogram and stuff like that but
you know picture's worth a thousand
words what you really want to make sure
you take away is that we can do a basic
describe which pulls all this
information out and we can print any of
the individual information from the
describe because this is a dictionary
and so if we want to go ahead and look
up
the mean value we can also do describe
mean so if you're doing a lot of
Statistics being able to
doesn't have the print on there so it's
only going to print the last one which
happens to be the mean you can very
easily reference any one of these and
then you can also if you're doing
something a little bit more complicated
and you don't need just the basics you
can come through and pull any one of the
individual
um
references from the from the pandas on
here so now we've had a chance to
describe our data let's get into
inferential statistics inferential
statistics allows you to make
predictions or inferences from data and
you can see here we have a nice little
picture movie ratings and
if we took this group of people and said
hey how many people like the movie
dislike it can't say and then you ask
just a random person who comes out of
the movie who hasn't been in the study
you can infer that 55 chance of saying
liked 35 chance of saying disliked or a
10 or 11 chance of can't say so that's
real basics of what we're talking about
is you're going to infer that the next
person is going to follow these
statistics
uh so let's look at Point estimation it
is a process of finding an approximate
value for a population's parameter like
mean or average from random samples of
the population let's take an example of
testing vaccines for covid-19 vaccines
and flu bugs all that it's a pretty big
thing of how do you test these out and
make sure they're going to work on the
populace
a group of people are chosen from the
population medical trials are performed
results are generalized for the whole
population so here's a protected here's
our small group up here where we've
selected them we run medical trials on
them and then the results work for the
population a nice diagram with the
arrows going back and forth in the very
scary coveted virus in the middle of one
and let's take a look at the
applications of inferential statistics
very Central is what they call
hypotheses testing and the confidence
interval which go with that and then as
we get into
probability we get into our binomial
theorem our normal distribution in
central limit theorem hypothesis testing
hypothesis testing is used to measure
the plausibility of a hypothesis
assumption by using sample data now when
we talk about theorems Theory
hypothesis keep in mind that if you are
in a philosophy class theory is the same
as hypothesis where theorem is a
scientific uh statement that is
something that has been proven although
it is always up for debate because in
science we always want to make sure
things are up to debate so a hypothesis
is the same as a philosophical class
calling a theory where theory in science
is not the same theory in science says
this has been well proven gravity is a
theory so if you want to debate the
theory of gravity try jumping up and
down if you want to have a theory about
why the economy is collapsing in your
area that is a philosophical debate very
important I've heard people mix those up
and it is a pet peeve of mine when we
talk about hypotheses testing the steps
involved in the hypotheses testing is
first we formulate a hypothesis we
figure out the right test to test our
hypothesis we execute the test and we
make a decision it and so when you're
talking about hypothesis you're usually
trying to disprove it if you can't
disprove it and it works for all the
facts then you might call that a theorem
at some point
so in a use case let's consider an
example we have four students we're
given a task to clean a room every day
sounds like working with my kids they
decided to distribute the job of
cleaning the room among themselves they
did so by making four chits which has
their names on it and the name that gets
picked up has to do the cleaning for
that day Rob took the opportunity to
make chits and wrote everyone's name on
it so here's our four people Nick Rob
imlia imlia and summer
now Rick Emilia and summer are asking us
to decide whether Rob has done some
Mischief in preparing the chits I.E
whether Rob has written his name on one
of the chit for that we will find out
the probability of Rob getting the
cleaning job on first day second day
third day and so on till 12 days the
probability of Rob getting the job
decreases every day I.E his turn never
comes up then definitely he has done
some Mischief while making the chits so
the probability of Rob not doing work on
day one is three out of four there's a
0.75 chance that he didn't do work uh
two days three fourths times
three-fourths equals 0.56
three days you have three fours three
fourths three-fourths which equals 0.42
uh when you get to day 12 it's 0.032
Which is less than 0.05 remember this
0.05 that comes up a lot when we're
talking about certain values when we're
looking at statistics Rob is cheating as
he wasn't chosen for 12 consecutive days
that's a very high probability when on
day 12 he still hasn't gotten the job
cleaning the room
so we come up to our important important
terminologies we have null hypothesis
a general statement that states that
there is no relationship between two
measured phenomenon or no association
among the groups
alternative hypothesis contrary to the
null hypothesis it states whenever
something is happening a new theory is
preferred instead of an old one and so
the two hypotheses go hand in hand uh so
your null this is always interesting in
in when talking about data science and
the math behind it it's about proving
that the things have no correlation null
hypothesis says these two have zero
relation to each other where the
alternative hypothesis says hey we found
a relation this is what it is
we have p-value the p-value is a
probability of finding the observed or
more extreme results when the null
hypotheses of a study question is true
and the T value it is simply the
calculated difference represented in
units of standard error the greater the
magnitude of T the greater the evidence
against the null hypothesis and you can
look at the T values being specific to
the test you're doing
where the p-value is derived from your T
value and you're looking for what they
call the five percent or the 0.05
showing that it has a high correlation
so digging in deeper let's assume that a
new drug is developed with the goal of
lowering the blood pressure more than
the existing drug and this is a good one
because the null value here isn't that
you don't have any drug then null value
here is it is better than existing drug
the new drug doesn't lower the blood
pressure more than the existing drug
now if we get that that says our null
hypothesis is correct there is no
correlation and the new drug is not
doing its job the alternative hypothesis
the new drug does significantly lower
the blood pressure more than an existing
drug uh yay we got a new drug out there
and that's our alternative hypothesis or
the H1 or h a
and we look at the p-value results from
the evidence like medical trials showing
positive results which will reject the
null hypothesis and again they're
looking for a 0.05 or 5 percent and the
T value comparing all the positive test
results and finding means of different
samples in order to test hypothesis so
this is specific to the test how what
percentage of increase did they have
and this leads us to the confidence
intervals a confidence interval is a
range of values we are sure our true
values of observations lie in
let's say you asked a dog owner around
you and asked them how many cans of food
do you buy for your per year for your
dog
through calculations you got to know
that the on an average around 95 percent
of the people bought around 200 to 300
cans of food hence we can say that we
have a confidence interval of 2 300
where 95 percent of our values lie in
that data spread and this the graph
really helps a lot so you can start
seeing what you're looking at here where
you have the 95 percent you have your
peak in this case it's a normal
distribution so you have the nice bell
curve equal on both sides it's not
asymmetrical and 95 percent of all the
values lie within a very small range and
then you have your outliers the 2.5
percent going each way
so we touched upon hypothesis and we're
going to move into probability so you
have your hypothesis once you've
generated your hypothesis we want to
know the probability of something
occurring probability is a measure of
the likelihood of an event to occur any
event can be predicted with total
certainty and can only be predicted as a
likelihood of its occurrence so any
event cannot be predicted with total
search and T can only be predicted as a
likelihood of its occurrence score
prediction how good you're going to do
in whatever sport you're in weather
prediction stock prediction if you've
studied physics and Chaos Theory even
the location of the chair you're sitting
on has a probability that it might move
three feet over
granted that probability is one in like
uh I think we calculated as under one in
trillions upon trillions so it's
the better the probability the more
likely it's going to happen there are
some things that have such a low
probability that we don't see them so we
talk about random variable a random
variable is a variable whose possible
values are numerical outcomes of a
random phenomena so we have the coin
toss how many heads will occur in the
series of 20 coin flips probably you
know the on average they're 10 but you
really can't know because it's very
random how many times a red ball is
picked from a bag of balls if there's
equal number of red balls and blue balls
and green balls in there how many times
the sum of digits on two dice results
are five each
so you know there's how often you're
going to roll two fives on your pair of
Dives
so in a use case let's consider the
example of rolling two dice we have a
random variable outcome equals y you can
take values two three four five six
seven eight nine ten eleven twelve
so we have a random variable and a
combination of dice and instead of
looking at how many times both dice for
roll five let's go ahead and look at
total sum of five and you have in as far
as your random variables you can have a
one four equals five four one two three
three two
so four of those roles can be four if
you look at all the different options
you have four of those random roles can
be a five
and if we look at the total number
which happens to be 36 different options
uh you can see that we have four out of
36 chance every time you roll the dice
that you're going to roll a total of
five you're gonna have an outcome of
five
and uh we'll look a little deeper as to
what that means but you could think of
that at what point if someone never
rolls a five or they always roll a five
can you say hey that person's probably
cheating we'll look a little closer at
the math behind that but let's just
consider this is one of the cases is
rolling two dice and gambling
there's also a binomial distribution it
is the probability of getting success or
failure as an outcome in an experiment
or trial that is repeated multiple times
and the key is is by meaning two
binomial so passing or filling an exam
winning or losing a game and getting
either head or tails so if you ever see
binomial distribution it's based on a
true false kind of setup you win or lose
let's consider a use case and let's
consider the game of football between
two clubs Barcelona and Dortmund the
teams will have to play a total of four
matches and we have to find out the
chances of Barcelona winning the series
so we look at the total games and we're
looking at five different games or
matches let's say that the winning
chance for Barcelona is 75 or 0.75 that
means that each game they have a 75
chance that they're going to win that
game and losing chances are 25 or 0.25
clearly 0.75 plus 0.25 equals one so
that accounts for 100 of the game
probability for getting K wins in in
matches is calculated
and we we're talking like so if you have
five games uh and you want to know if I
play
um how many wins in those five games
should I get what's the percentage on
those and the probability for getting K
wins and N matches is calculated by p x
equals k equals nck P to the k q to the
N minus K here p is a probability of
success and Q is the probability of
failure and so we can do total games of
n equals five where k equals zero one
two three four five P which is the
chance of winning is 0.75 Q the chance
of losing equals 1 minus P which equals
1 minus 0.075 which equals 0.25 the
probability that Barcelona will lose all
of the matches can then just plug in the
numbers and we end up with a point zero
zero zero nine seven six five six two
five so very small chance they're going
to lose all their matches
and we can plug in the value for two
matches probability of Barcelona will
win at least two matches is 0.0878 and
of course we can go on to the
probability of the Barcelona will win
three matches the 0.26 and of course
four matches and so on and it's always
nice to take this information
um and let's find the accumulated
discrete probabilities for each of the
outcomes where Barcelona has won three
or more matches x equals three x equals
four x equals five
and we end up with the p equals 0.264
plus 0.395 plus 237 which equals 0.89
in reality
the probability of Barcelona winning the
series is much higher than 0.75 and it's
always nice to uh
put out a nice graph so you can actually
see the number of wins to the
probability and how that pans out with
our binomial case
continuing in our important terminology
location the location of the center of
the graph depends on the mean value and
this is some very important things so
much of the data we look at and when you
start looking at probabilities almost
always has a normalized look like the
graph in the middle
uh but you do have left skewed where the
data is skewed off to the left and you
have more stuff happening out to the
left and you have right skewed data and
so when this comes up and these
probabilities come up where they're
skewed it's really important to take a
closer look at that mostly you end up
with a normalized set of data but you
got to also be aware that sometimes it's
a skewed data
and then the height height of the slope
inversely depends upon the standard
deviation
so you can see down here the standard
deviation is really large it kind of
squishes it out and if the standard
deviation is small then most of your
data is going to hit right there in the
middle you can have a nice Peak and so
being aware of this that you might have
a probability that fits certain data but
it has a lot of outliers so you're if
you have a really high standard
deviation if you're doing stock market
analysis this means your predictions are
probably not going to make you much
money where if you have a very small
deviation you might be right on Target
and set to become a millionaire which
leads us to the z-score z-score tells
you how far from the mean a data point
is it is measured in terms of standard
deviations from the mean around 68
percent of the results are found between
one standard deviation around 95 percent
of the results are found between two
standard deviations and you read the
symbols of course they love to throw
some Greek letters in there we have mu
minus two Sigma mu is just a quick ways
to kind of funky you it just means the
mean and then the sigma is the standard
deviation and that's the o with that
little arrow off to the right or the
little waggly Tail Going up the o with
it with a line on it some U minus 2
Sigma is your 95 of the results are
found between two standard deviations
Central limit theorem this goes back to
the skew if you remember we were looking
at the skew values on this previous
slide have left skewed normalized and
right skewed when we're talking about it
being skewed or not skewed the
distribution of the sample means will be
approximately normally distributed
evenly distributed and not skewed if you
take large random samples from the
population with the mean mu and the
standard deviation Sigma with
replacement
and you can see here of course we have
our mu minus two Sigma and the spread
down here the mean the median and the
mode and so you're talking about very
large populations
these numbers should come together and
you shouldn't have a skewed value if you
do that's a flag that something's wrong
that's why this is so important to be
aware of what's going on with your data
where your samples are coming from and
the math behind it and if you're going
to do all this we got to jump into
conditional probability
the conditional probability of an event
a is a probability that the event will
occur given the knowledge that an event
to be has already occurred and you'll
see this as Bayes theorem b-a-y-e-s Bays
and this is red
I mean you have these funky looking
little p brackets a b this is the
probability of a being true while B is
already true
and you have the probability of B being
true when a is already true so P B of A
probability of a being true divided by
the probability of B being true
and we talk about Bayes theorem which
occurred back in the 1800s when he
discovered this this is such an
important formula and it's really it's
not if you actually do the math you
could just kind of do
um
um x y equals J K and then you divide
them out and you're going to see the
same math but it works with
probabilities which makes it really nice
and so if you have a set you might have
uh eight or nine different studies going
on in different areas different people
have done the studies they brought them
together
if we look at today's covet virus the
virus spread certainly the studies done
in China versus the studies the way
they're done in the U.S that data is
different in each of those studies but
if you can find a place where it
overlaps where they're studying the same
thing together you can then compute the
changes that you need to make in one
study to make them equal
and this is also true if you have a
study of one group and you want to find
out more about it so this formula is
very powerful and it really has to do
with the data collection part of the
math and data science and understanding
where your data is coming from and how
you're going to combine different
studies in different groups
and we're going to go into a use case
let's find out the chance of a person
getting lung disease due to smoking and
this is kind of interesting the way they
word this let's say that according to
medical report provided by the hospital
states that around 10 percent of all
patients they treated suffered long lung
disease so we have kind of a generic
medical report they further found out by
a survey that 15 percent of the patients
that visit them smoke
so we have 10 percent that are lung
disease and 15 percent of the patients
smoke and finally five percent of the
people continued smoke even when they
had lung disease not the brightest
choice but you know it is an addiction
so it can be really difficult to kick
and so we can look at the probability of
a uh prior probability of 10 percent
people having lung disease
and then probability B probability that
a patient smokes is 15 percent
uh and the probability of B if B then a
the probability of a patient smokes even
though they have lung disease is five
percent
and probability of a is B probability
that the patient will have lung disease
if they smoke and then when you put the
formulas together you get a nice
solution here you get the probability of
a of B probability that the patient will
have lung disease if they smoke
and you can just plug the numbers right
in and we get a 3.33 percent chance
hence there is a 3.33 chance that a
person who smokes will get a lung
disease
so we're gonna pull up a little python
code and we're always my favorite roll
up the sleeves
keep in mind we're going to be doing
this
um kind of like the back end way
so that you can see what's going on and
then later on we're going to create
um we'll get into another demo which
shows you some of the tools that are
already pre-built for this
let's start by creating a set so we're
going to create a set with curly braces
this means that our set has only unique
values so you have a list you have your
tuples which can never change and then
you have in this case the the set so
four seven you can't create a four seven
comma four it'll delete the four out so
it's only unique values and if you use
dictionaries
quick reminder this should look familiar
because it is a dictionary where you
have a value and that value is assigned
to or that key is assigned to a value
uh so you could have a key value set up
as a dictionary so it's like a
dictionary without the value it's just
the keys and they all have to be unique
and if we run this we have a set of four
seven
we can also take a list of regular setup
and I'm going to go ahead and just throw
in another number in here four
and run it and you can see here if I
take my list one two three four four and
I convert it to a set and here it is my
set from list equals set my list
the result is one two three four so it
just deletes that last four right out of
there
and with the sets you can also go in
there and print here is my set my set
three is in the set and then if you do
three in my set
that's going to be a logic function and
one in my set 6 is not in the set and so
forth if we run this
we get three is in the set true one is
in the set false because three five
seven is another one six is in the set 6
is not in the set so not in my set
you can also use this with a list we
could have just used 357 and it would
have the same response on there is three
and usually you do if three is in but
three in my set is still works on just a
regular list
and we'll go ahead and do a little
iteration we're going to do kind of the
dice one remember
um uh one two three four five six and so
we're going to bring in the iteration
tool and import product as product
and I'll show you what that means in
just a second so we have our two dice we
have dice a
and it's going to be a set of values you
can only have one value for each one
that's why they put it in a set and if
you remember from range it is up to
seven so this is going to be one two
three four five six it will not include
the seven and the same thing for our
dice B
and then we're going to do is we're
going to create a list
which is the product
of A and B so what's a plus b
and if we go ahead and run this it'll
print that out and you'll see in this
case when they say product because it's
an iteration tool
we're talking about creating a tuple of
the two so we've now created a tuple of
all possible outcomes of the dice where
dice a is one two three one to six and
dice B is one to six and you can see one
to one one to two one to three and so
forth you remember we had a slide on
this earlier where we talked about
um the different all the different
outcomes of a dice
we can play around with this a little
bit uh we can do in dice equals two dice
faces one two three four five six
uh another way of doing what we did
before and then we can create an event
space where we have a set which is the
product of the dice faces repeat equals
in dice and we're going to just run this
and you can see here it just again puts
it through all the different possible
variables we can have
and then if we want to take the same set
on here and print them all out like we
had before we can just go through four
outcome and event space outcome and
equals
so the event space is creating
uh sequence and as you can see here when
we print it out it Stacks them versus
going through and putting them in a nice
line
and we'll go ahead and do something
let's go print
since we have the End Printing with a
comma that just means it's just gonna
it's not going to hit the return going
down to the next line uh and we'll go
ahead and do the links
of our event space that'll be an
important variable we're going to want
to know in a minute
and of course if I get carried away with
my typing of length I will print it
twice and it'll give me an error so we
have 36 different possible variations
here
and we might want to calculate something
like
um what about the multiple of three what
if we want to have
the probability of the multiple of three
in our setup
and so we can put together the code for
the outcome and event space of X Y
equals outcome if X Plus y
remainder three so we're going to divide
by 3 and look at the remainder and it
equals zero
then it's a favorable outcome we're
going to pop that outcome on the end
there
and we'll turn it into a set so the
favor outcome equals a set not necessary
because we know it's not going to be
repeating itself but just in case we'll
go ahead and do that
and if we want to print out the outcome
we can go ahead and see what that looks
like and you can see here these are all
multiples of three one plus two is three
five plus four is nine which divided by
three is three and so forth
and just like we looked up the length of
the one before let's go ahead and print
the length
of our F outcome so we can see what that
looks like
there we go
and of course I did forget to add the
print in the middle because We're
looping through and putting an end on
the on the setup on there so we're going
to put the print in there and if I run
this you can see uh
we end up with 12. so we have 36 total
options
we have 12 that are multiple that add up
to a multiple of three
and we can easily convert the
probability of this by simply taking the
length of our favorable outcome over the
length of the event space
and if we print it out let me put that
in there probability
last line so we just type it in we end
up with a 0.33333 Chance
and it's roughly a third
and we might want to make this look nice
so let's go ahead and put in another
line there the probability of getting
the sum which is a multiple of three is
0.33333
we can compute the same thing for five
dice
and if we do this for five dice and go
and run it you can see we just have a
huge amount of choices so just goes on
and on down here and we can look at the
length of the event space
and we have over
776 choices that's a lot of choices
if we want to ask the question like we
did above uh what is the sum where the
sum is a multiple of five but not a
multiple of three
we can go through all of these different
options and then you can see here D1 D2
D3 D4 D5 equals the outcome and if you
add these all together and the
division by five does not have a
remainder of zero but the remainder is
also of a division by three is not equal
to zero so the multiple of five is equal
to zero but the multiple three is not we
can just append that on here and then we
can look at that uh favorable outcome
we'll go ahead and set that and we'll
just take a look at this what's our
length
of our favorable outcome
it's always good to see what we're
working with and so we have 904 out of
770
6 and then of course we can just do a
simple division to get the probability
on here what's the probability that
we're going to roll a multiple of five
when you add them together
but not a multiple of three
and so we're just going to divide those
two numbers and you can see here we get
0.116255 or 11.62 percent
and so you can really have a nice visual
that this is not really complicated in
math right here on probabilities it's
just how many options do you have and
how many of those are you possibly going
to be able to come up with with the
solution you're looking for and this
leads us to a confusion Matrix
a confusion Matrix is a table which is
used to describe the performance of a
classification model on a set of test
data for which the True Values are known
and so you'll see on the left we have
the predicted and the actual and we have
a negative false negative positive true
positive
and then we have false positive and true
negative and you can think of this as
your predicted model what does that mean
that means if you divided your data and
you used two-third of us to create the
model
you might then test it against an actual
case for the last third and see how well
it comes out how many times was it true
positive versus uh false positive again
a false positive response and you can
imagine in medical situations this is a
pretty big deal you don't want to give a
false positive so you might adjust your
model accordingly so you don't have a
false positive say with a covet virus
test it'd be better to have a false
negative when they go back and get
retested then to have thirty percent
false positives where then the test is
pretty much invalid so in a use case
like cancer prediction let's consider an
example where a cancer prediction model
is put to the test for its accuracy and
precision actual result of a person's
medical report is compared with the
prediction made by the machine learning
model
and so you can see here here's our
actual predicted whether they have
cancer or not you know cancer a big one
you don't want to have a false positive
I mean a false negative in other words
you don't want to have it tell you that
you don't have cancer when you do so
that would be something you'd really be
looking for in this particular domain
you don't want a false negative
and this is again you know you've
created a model you have hundreds of
people or thousands of pieces of data
that come in there's a real famous case
study where they have the imagery and
all the measurements they take and
there's about 36 different measurements
they take and then if you run the a
basic model you want to know just how
accurate is how many negative results do
you have that are either telling people
they have cancer that don't or telling
people that don't have cancer that they
do and then we can take these numbers
and we can feed them into our accuracy
our precision and our recall
so accuracy precision and recall
accuracy metric to measure how
accurately the results are predicted
and this is your total true where you
got the right results you add them
together the true positive the true
negative over all the results so what
percentage of them were accurate versus
what were wrong
we're talking about Precision is a
metric to measure how many of the
correctly predicted cases are actually
turned out to be positive
uh so we have a Precision on true
positive again if you're talking about
like uh covid testing with the viruses
uh you really want this to be a high
number you want this true that to be the
center point where you might have the
opposite if you're dealing with cancer
where you want no false negatives
uh so this is your metric on here
Precision is your test positive true
positive plus false positive
and then your recall how many of the
actual positive cases we were able to
predict quickly with our model uh so
test positive is a test positive plus
the false negative on there Excel radio
in air and machine learning with a
comprehensive postgraduate program in Ai
and machine learning in expertise in
machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumni membership
and hackathons and ask me anything
sessions by IBM with three capstones and
25 plus industry projects using real
data sets from Twitter Uber and more you
will gain practical experience this
program covers statistics python
supervised and unsupervised learning NLP
neural networks computer vision gns KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below what are the
different types of machine learning
algorithms machine learning algorithms
are broadly classified into three types
the supervised learning unsupervised
learning and reinforcement learning
supervised learning in turn consists of
techniques like regression and
classification and unsupervised learning
we use techniques like Association and
clustering and reinforcement learning is
the recently developed technique and it
is very popular in gaming some of you
must have heard about alphago so this
was developed using reinforcement
learning primary difference between
supervised learning and unsupervised
learning supervised learning is used
when we have historical data and we have
labeled data which means that we know
how the data is classified so we know
the classes if we are doing
classification or we know the values
when we are doing regression so if we
have historical data with these values
which are known as labels then we use
supervised learning in case of
unsupervised learning we do not have
past labeled data historical labeled
data so we use techniques like
Association and clustering to maybe form
clusters or new classes maybe and then
we move from there in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain Target and it
is rewarded when it is moving towards
that Target and it is penalized if it is
moving in a direction which is not
achieving that Target so it's more like
a carrot and stick model so what is the
difference between these three types of
algorithms supervised algorithms or
supervised learning algorithms are used
when you have a specific Target value
that you would like to predict the
target could be categorical having two
or more possible outcomes or classes if
you will that is what is classification
or the target could be a value which can
be measured and that's where we use
regression like for example whether
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud or not a fraud or if it is email
spam whether it is Spam or not spam so
that is a classification example so if
you know or this is known as labeled
information if you have the labeled
information then you use supervised line
in case of unsupervised learning we have
input data but we don't have the labels
or what the output is supposed to be so
that is when we use unsupervised
learning techniques like clustering and
Association and we try to analyze the
data in case of reinforcement learning
it allows the agent to automatically
determine the ideal Behavior within a
specific context and it has to do this
to maximize the performance like for
example playing a game so the agent is
told that you need to score the maximum
score possible without losing lives so
that is a Target that is given to the
agent and it is allowed to learn from
scratch play the game itself multiple
times and slowly it will learn the
behavior which will increase the score
and keep the lives to the maximum that's
example of reinforcement learning Excel
radio Courier in air and machine
learning with our comprehensive
postgraduate program in Ai and machine
learning in expertise in machine
learning deep learning NLP computer
vision and reinforcement learning you
will receive a prestigious certificate
exclusive alumni membership and
hackathons and ask me anything sessions
by IBM with three capstones and 25 plus
industry projects using real data sets
from Twitter Uber and more you will gain
practical experience this program covers
statistics python supervised and
unsupervised learning NLP neural
networks computer vision gns KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below
different machine learning algorithms we
can divide them into three areas
supervised unsupervised reinforcement
we're only going to look at supervise
today unsupervised means we don't have
the answers we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information until after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polynomial linear regression now on
these three simple linear regression is
the examples we've looked at so far
where we have a lot of data and we draw
a straight line through it multiple
linear regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it'd be multiple
linear regression and finally we have
polynomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricket
fever to predict the number of runs
apply player with score in the coming
matches based on the previous
performance I'm sure you can figure out
other applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some euclidean geometry the simplest
form of a simple linear regression
equation with one dependent and one
independent variable as is represented
by y equals m times X plus C and if you
look at our model here we plotted two
points on here X1 and y1 X2 and Y2 y
being the dependent variable remember
that from before and X being the
independent variable so y depends on
whatever X is m in this case is the
slope of the line where m equals the
difference in the Y2 minus y1 and X2
minus X1 and finally we have C which is
the coefficient of the line or where it
happens to cross the zero axes let's go
back and look at an example we used
earlier of linear regression we're going
to go back to plotting the amount of
crop yield based on the amount of
rainfall and here we have our rainfall
remember we cannot change rainfall and
we have our crop yield which is
dependent on the rainfall so we have our
independent and our dependent variables
we're going to take this and draw a line
through it as best we can through the
middle of the data and then we look at
that we put the red point on the y-axis
is the amount of crop yield you can
expect for the amount of rainfall
represented by the Green Dot so if we
have an idea what the rainfall is for
this year and what's going on then we
can guess how good our crops are going
to be and we've created a nice line
right through the middle to give us a
nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes
I want you to note that when we get into
the case study and we actually apply
some python script that this math you're
going to see here is already done
automatically for you you don't have to
have it memorized it is however good to
have an idea what's going on so if
people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was one we got y equals 2
when X was 2 y was 4 and so on and so on
if we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is what the means is of each of the
data coming in the X and the Y the means
doesn't mean anything other than the
average so we add up all the numbers and
divide by the total so one plus two plus
three plus four plus five over five
equals three and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get three
comma four which draws a nice line down
the middle a good estimate here we're
going to dig deeper into the math behind
the regression line now remember before
I said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whiz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the X Y points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x times Y and
then it takes all of X and adds them up
all of Y adds them up all of x squared
adds them up and so on and so on and you
can see we have the sum of equal to 15
the sum is equal to 20. all the way up
to x times Y where the sum equals 66
this all comes from our formula for
calculating a straight line where y
equals the slope times X plus a
coefficient C so we go down below and
we're going to compute more like the
averages of these and we'll explain
exactly what that is in just a minute
and where that information comes from is
called the square means error but we'll
go into that in detail in a few minutes
all you need to do is look at the
formula and see how we've gone about
Computing it line by line instead of
trying to you have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and then the
top part if you read through the
brackets you have the number of data
points times the sum of x times Y which
we computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 times 20. and on
the bottom we have the number of lines
times the sum of x squared easily
computed as 86 for the sum minus I'll
take all that and subtract the sum of x
squared and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on the piece of paper or calculator
and you'll get a slope of 0.6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equals 0.6 and C equals 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y equals
0.6 times where x equals 1 plus 2.2
equals 2.8 and so on and so on and here
the Blue Points represent the actual y
values and the brown points represent
the predicted y values based on the
model we created the distance between
the actual and predicted values is known
as residuals or errors the best fit line
should have the least sum of squares of
these errors also known as e-square if
we put these into a nice chart we can
see X and you can see Y what the actual
values were and you can see why it
predicted you can easily see where we
take y minus y predicted and we get an
answer what is the difference between
those two and if we square that y minus
y prediction squared we can then sum
those squared values that's where we get
the 0.64 plus the 0.36 plus 1 all the
way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for each line and conclude
the best fit line having the least e
Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least squared distance between
the data points and the regression line
now we only looked at the most commonly
used formula for minimizing the distance
there are lots of ways to minimize the
distance between the line and the data
points like sum of squared errors sum of
absolute errors root mean square error
Etc which you want to take away from
this is whatever formula is being used
you can easily using a computer
programming and iterating through the
data calculate the different parts of it
that way these complicated formulas you
see with the different summations in
absolute values are easily computed one
piece at a time up until this point
we've only been looking at two values X
and Y well in the real world it's very
rare that you only have two values when
you're figuring out a solution so let's
move on to the next topic multiple
linear regression let's take a brief
look at what happens when you have
multiple inputs so in multiple linear
regression we have well we'll start with
the simple linear regression where we
had y equals M plus X plus C and we're
trying to find the value of y now with
multiple linear regression we have
multiple variables coming in so instead
of having just X we have X1 X2 X3 and
instead of having just one slope each
variable has its own slope attached to
it as you can see here we have M1 M2 M3
and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y equals
M1 times X1 plus M2 times X2 so on all
the way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R and D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict as profit but instead of
predicting it just on the R and D we're
going to look at other factors like
Administration cost marketing cost and
so on and from there we're going to see
if we can figure out what the profit of
that company is going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data to see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
and just see how valid our solution is
so let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy because it's such a
visual to look at it's so easy to use
just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries were
importing first we're going to import
numpy as NP and then I want you to skip
one line and look at import pandas as PD
these are very common tools that you
need with most of your linear regression
the numpy which stands for number python
is usually denoted as NP and you have to
almost have that for your SK learn
toolbox you always import that right off
the beginning pandas although you don't
have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so so we
usually like to use the pandas when we
can and I'll show you what that looks
like the other three lines are for us to
get a visual of this data and take a
look at it so we're going to import
matplotlibrary.pi plot as PLT and then
Seaborn as SNS Seabourn works with the
matplot library so you have to always
import matplot library and then Seaborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the Seaborn is so easy to use
it just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber signed matplot library
in line that is only because I'm doing
an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting so we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in this slide you'll
see companies equals pd.read CSV and it
has a long line there with the file at
the end one thousand companies.csv
you're going to have to change this to
fit whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals
pd.reader underscore CSV and I've
changed this path to match my computer C
colon slash simply learn slash 1000
underscore companies.csv and then below
there we're going to set the x equals to
companies under the I location and
because this is companies is a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except up
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a pandas data set but we can easily
extract the data from our pandas data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies dot head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the pandas sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
zero one two three four and then it
shows your different columns R and D
spend Administration marketing spend
State profit it even notes that the top
top are column names it was never told
that but pandas is able to recognize a
lot of things that they're not the same
as the data rows why don't we go ahead
and open this file up in a CSV so you
can actually see the raw data so here
I've opened it up as a text editor and
you can see at the top we have r d spend
comma Administration comma marketing
spin comma State comma profit carries
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
165
349.20 compared to the administration
cost of 136
897.80 so on so on helps to create the
the profit of 192 261 and 83 cents that
makes no sense to me whatsoever no pun
intended so let's flip back here and
take a look at our next set of code
where we're going to graph it so we can
get a better understanding of our data
and what it means so at this point we're
going to use a single line of code to
get a lot of information so we can see
where we're going with this let's go
ahead and paste that into our notebook
and see what we got going and so we have
the visualization and again we're using
SNS which is pandas as you can see we
imported the matplot Library dot Pi plot
is PLT which then the Seaborn uses and
we imported the Seaborn as SNS and then
that final line of code helps us show
this in our inline coding without this
it wouldn't display and you could
display it to a file in other means and
that's the matplot library in line with
the Amber sign at the beginning so here
we come down to the single line of code
Seaborn is great because it actually
recognizes the panda data frame so I can
just take the com companies dot core for
coordinates and I can put that right
into the Seaborn and when we run this we
get this beautiful plot and let's just
take a look at what this plot means if
you look at this plot on mine the colors
are probably a little bit more purplish
and blue than the original one we have
the columns and the rows we have R and D
spending we have Administration we have
marketing spending and profit and if you
cross index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so r d spending is going
to be the same as r d spending and the
same thing with Administration costs so
right down the middle you get this dark
row or dark diagonal row that shows that
this is the highest corresponding data
that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with r d spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
jupyter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder instead it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies dot
head and we printed the top five rows of
data we have our columns going across we
have column 0 which is R and D spending
column one which is Administration
column two which is marketing spending
and column three is State and you'll see
under State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a New York it has a 0 or 1 or a
two and then finally we need to do a one
hot encoder which equals one hot in
order categorical features equals three
and then we take the X and we go ahead
and do that equal to one hot encoder fit
transform X to array this final
transformation preps our data Force so
it's completely set the way we need it
is just a row of numbers even though
it's not in here let's go ahead and
print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers
next on setting up our data we have
avoiding dummy variable trap this is
very important why because the computers
automatically transformed our header
into the setup and it's automatically
transformed all these different
variables so when we did the encoder the
encoder created two columns and what we
need to do is just have the one because
it has both the variable and the name
that's what this piece of code does here
let's go ahead and paste this in here
and we have x equals x colon comma one
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label
encoding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first step is
going to be in splitting the data now
whenever we create a predictive model of
data we always want to split it up so we
have a training set and we have a
testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and I'll go ahead and shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our RND spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test split we take X and
we take y we've already created those X
has the columns with the data in it and
Y has a column with profit in it and
then we're going to set the test size
equals 0.2 that basically means 20
percent so twenty percent of the rows
are going to be tested we're going to
put them off to the side so since we're
using a thousand lines of data that
means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting so setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahead and
put that code in there and walk through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the sklearn dot linear
underscore model we're going to import
linear regression now I don't know if
you recall from earlier when we were
doing all the math let's go ahead and
flip back there and take a look at that
do you remember this or we had this long
formula on the bottom and we were doing
all this summarization and then we also
looked at setting it up with the
different lines and then we also looked
all the way down to multiple linear
regression where we're adding all those
formulas together all of that is wrapped
up in this one section so what's going
on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor.fet in this case we do X
train and Y train because we're using
the training data X being the data n and
y being profit what we're looking at and
this does all that math for us so within
one click and one line we've created the
whole linear regression model and we fit
the data to the linear regression model
and you can see that when I run the
regressor it gives an output linear
regression it says copy x equals True
Fit intercept equals true in jobs equal
one normalize equals false it's just
giving you some general information on
what's going on with that regressor
model now that we've created our linear
regression model let's go ahead and use
it and if you remember we kept a bunch
of data aside so we're going to do a why
predict variable and we're going to put
in the X test and let's see what that
looks like scroll up a little bit paste
that in here predicting the test set
results so here we have y predict equals
regressor dot predict X test going in
and this gives us y predict now because
I'm in Jupiter in line I can just put
the variable up there and when I hit the
Run button it'll print that array out I
could have just as easily done print y
predict so if you're in a different IDE
that's not an inline setup like the
Jupiter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side is going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us we can simply just
print regressor dot coefficient
underscore when I run this you'll see
our coefficients here and if we can do
the regressor coefficient we can also do
the regressor intercept and let's run
that and take a look at that this all
came from the multiple regression model
and we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y equals M1 times X1 plus M2
times X2 and so on and so on plus C the
coefficient so these variables fit right
into this formula y equals slope one
times column one variable plus slope two
times column two variable all the way to
the m into the n and x to the N plus C
the coefficient or in this case you have
minus 8.89 to the power of two etc etc
times the First Column and the second
column and the third column and then our
intercept is the minus one zero three
zero zero nine point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r squared value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from
sklearn.metrix we're going to import R2
score that's the r squared value we're
looking at the error so in the R2 score
we take our y test versus our y predict
y test is the actual values we're
testing that was the one that was given
to us so we know are true the Y predict
of those 200 values is what we think it
was true and when we go ahead and run
this we see we get a
0.9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93 percent correct but
you do want that in the upper 90s oh and
higher shows that this is a very valid
prediction based on the R2 score and if
r squared value of 0.91 or 92 as we got
on our model remember it does have a
random generation involved this proves
the model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model let's take a look at what we went
over today Excel radio career in air and
machine learning with our comprehensive
postgraduate program in Ai and machine
learning in expertise in machine
learning deep learning NLP computer
vision and reinforcement learning you
will receive a prestigious certificate
exclusive alumni membership and
hackathons and ask me anything sessions
by IBM with three capstones and 25 plus
industry projects using real data sets
from Twitter Uber and more you will gain
practical experience this program covers
statistics python supervised and
unsupervised learning NLP neural
networks computer vision gns KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below
largest security
as I mentioned
positive regression is an algorithm for
performing binary classification so
let's take an example and see how this
works let's say your car has not been
serviced for quite a few years and now
you want to find out if it is going to
break down in the near future so this is
like a classification problem find out
whether your car will break down or not
so how are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y axis X is the number of years since
the last service was performed and why
is the probability of your car breaking
down and let's say this information was
this data that was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along x and y axis so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from 0 to 1
as you all aware probability can be
between 0 and 1. and as we can imagine
it is intuitive as well as the number of
years are on the Lower Side maybe one
year two years or three years till after
the service the chances of your car
breaking down are very limited right so
for example chances of your car breaking
down or the probability of your car
breaking down within two years of your
last service are 0.1 probability
similarly three years is maybe 0.3 and
so on but as the number of years
increases let's say if it was six or
seven years there is almost a certainty
that your car is going to break down
that is what this graph shows so this is
an example of a application of the
classification algorithm and we will see
in little details how exactly logistic
regression is applied here one more
thing needs to be added here is that the
dependent variables outcome is discrete
so if we are talking about whether the
car is going to break down or not so
that is a discrete value the why that we
are talking about the dependent variable
that we are talking about what we are
looking at is whether the car is going
to break down or not yes or no that is
what we are talking about so here the
outcome is discrete and not a continuous
way so this is how the logistic
regression curve looks let me explain a
little bit what exactly how exactly we
are going to uh
determine the class or the outcome
rather so for a logistic regression
curve a threshold has to be set saying
that because this is a probability
calculation remember this is a
probability calculation and the
probability itself will not be 0 or 1
but based on the probability we need to
decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is considered to
be 0 and any value above 0.5 is
considered to be 1. so an output of
let's say 0.8 will mean that the car
will break down so that is considered as
an output of 1 and let's say an output
of 0.29 is considered as 0 which means
that the car will not break down so
that's the way logistic regression works
now let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove that
confusion so what is linear regression
linear regression is a process is once
again an algorithm for supervised
learning however here you are going to
find a continuous value you are going to
determine a continuous value it could be
the price of a real estate property it
could be your height how much height
you're going to get or it could be a
stock price these are all continuous
values these are not discrete compared
to a yes or no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say at the HR
team of a company tries to find out what
should be the salary hike of an employee
so they collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
us it is straight and it learns from
this labeled information so that when a
new employee's information is fed based
on the rating it will determine what
should be the high so this is a linear
regression problem and a linear
regression example now salary is a
continuous value you can get 5000
5500
5600 it is not discrete like a cat or a
dog or an apple or a banana these are
discrete or a yes or a no these are
discrete values right so this way you
are trying to find continuous values is
where we use linear regression so let's
say just to extend on this scenario we
now want to find out whether this
employee is going to get a promotion or
not so we want to find out that is a
discrete problem right a yes or no kind
of a problem in this case we actually
cannot use linear regression even though
we may have labeled data so this is the
label date So based on the employee
rating these are the ratings and then
some people got the promotion and this
is the ratings for which people did not
get promotion that is a no and this is
the rating for which people got
promotion we just plotted the data about
whether a person has got an employer's
got promotion or not yes no right so
there is nothing in between and what is
the employee's rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes no
okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see it
doesn't look very right because looks
like there will be lot of errors this
root mean square error if you remember
for linear regression would be very very
high and also the the values cannot go
beyond zero or Beyond one so the graph
should probably look somewhat like this
clicked at 0 and 1. but still this
straight line doesn't look right
therefore instead of using a linear
equation we need to come up with
something different and therefore the
logistic regression model looks somewhat
like this so we calculate the
probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what the depression is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and this is how the math behind logistic
regression looks so we are trying to
find the odds for a particular event
happening and this is the formula for
finding the odd so the probability of an
event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus p
this is how you measure the odds now the
values of the odds range from 0 to
Infinity so when this probability is 0
then the odds will the value of the odds
is equal to 0 and when the probability
becomes 1 then the value of the odds is
1 by 0 that will be Infinity but the
probability itself remains between 0 and
1. now this is how an equation of a
straight line love so Y is equal to
beta0 plus beta 1 x where beta0 is the
y-intercept and beta 1 is the slope of
the line if we take the odds equation
and take a log of both sides then this
would look somewhat like this and the
term logistic is actually derived from
the fact that we are doing this we take
a log of p x by 1 minus p x this is an
extension of the calculation of odds
that we have seen right and that is
equal to beta0 plus beta 1 x which is
the equation of the straight line and
now from here if you want to find out
the value of p x we will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of PX like this p x is
equal to 1 by 1 plus e to the power of
minus beta0 plus beta 1 x and recall
this is nothing but the equation of the
line which is equal to y y is equal to
beta0 plus beta 1 X so that this is the
equation also known as the sigmoid
function and this is the equation of the
logistic regression and all right and if
this is plotted this is how the sigmoid
curve is obtained so let's compare
linear and logistic regression how they
are different from each other let's go
back so linear regression is solved or
used to solve regression problems and
logistic regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical or
discrete in nature and linear regression
helps to estimate the dependent variable
when there is a change in the
independent variable table whereas here
in case of logistic regression it helps
to calculate the probability or the
possibility of a particular event
happening and linear regression as the
name suggests is a straight line that's
why it's called linear regression
whereas logistic regression is a sigmoid
function and the curve is the shape of
the curve is s it's an s-shaped curve
this is another example of application
of logistic regression in weather
prediction that it's going to rain or
not rain now keep in mind both are used
in weather prediction if we want to find
the discrete values like whether it's
going to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not is
going to be sunny or not it is going to
snow or not these are all logistic
regression examples a few more examples
classification of objects this is a
again another example of logistic
regression now here of course one
distinction is that these are
multi-class classification so logistic
regression is not used in its original
form but it is used in a slightly
different form so we say whether it is a
dog or not a dog I hope you understand
so instead of saying is it a dog or a
cat or a elephant we convert this into
saying so because we need to keep it to
Binary classification so we say is it a
dog or not a dog is it a cat or not a
cat so that's the way logistic
regression can be used for classifying
objects otherwise there are other
techniques which can be used for
performing multi-class classification
Healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
drama score and age and so on and so
forth and try to predict the rate of
survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into jupyter notebook and show the
code but before that let me take you
through a couple of slides to explain
what we are trying to do so let's say
you have an eight by eight image and
there the image has a number one two
three four and you need to train your
model to predict what this number is so
how do we do this so the first thing is
obviously in any machine learning
process you train your model so in this
case we are using logistic regression so
and then we provide a training set to
train the model and then we test how
accurate our model is with the test data
which means that like any machine
learning process we split our initial
data into two parts training set and
listen with the training set we train
our model and then with the test set we
test the model didn't we get good
accuracy and then we use it for for
inference right so that is typical
methodology of
training testing and then deploying of
machine learning models so let's uh take
a look at the code and see what we are
doing so I'll not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using matplotlib some of the images or a
sample of these images and then we split
the data into training and test as I
mentioned earlier and we can do some
exploratory analysis and then we build
our model we train our model with a
training set and then we test it with
our test set and find out how accurate
our model is using the confusion Matrix
the heat map and use Heat plan for
visualizing this and I will show you in
the code what exactly is the confusion
Matrix and how it can be used for
finding the accuracy in our example we
get an accuracy of about 0.94 which is
pretty good or 94 percent which is
pretty good all right so what is the
confusion Matrix this is an example of a
confusion Matrix and this is used for
identifying the accuracy of a
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs we are
expecting
so the the most important part here is
that the model will be most accurate
when we have the maximum numbers in its
diagram like in this case that's why it
has almost 93 94 percent because the
diagonals should have the maximum
numbers and the others other than
diagnose the cells other than the
diagonal should have very few numbers so
here that's what is happening so there
is a two here there are there's a one
here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and actual value are the same so
along the diagonals that is true which
means that let's let's take this
diagonal right if the maximum number is
here that means that like here in this
case it is 34 which means that 34 of the
images that have been fed are rather
actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two missed
classifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100 accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
hundred percent accurate model okay so
that's uh just of how to use this Matrix
how to use this confusion Matrix I know
the name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information on what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and the actual name that's all right
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and
predicted value and the actual value is
exactly the same whereas in this case
right it has there are I think 37 plus 5
yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and
um and then we will see how well it is
trained and whether it is able to
predict these numbers correct a or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and then the
last line in this block is to load the
digits so let's go ahead and run this
code then here we will visualize the
shape of these digits so we can see here
okay if we take a look this is how the
shape is 1797 by 64. these are like
eight by eight images so that's that's
what is reflected in this shape now from
here onwards we are basically once again
importing some of the libraries that are
required like numpy and matplot and we
will take a look at some of the sample
images that we have loaded so this one
for example creates a figure and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so so these are about five
images sample images that we are looking
at zero one two three four so this is
how the images this is how the data is
okay and based on this we will actually
train our logistic regression model and
then we will test it and see how well it
is able to recognize so the way it works
is the pixel information so as you can
see here this is an 8 by 8 pixel kind of
a image and the each pixel whether it is
activated or not activated that is the
information available for each pixel now
based on the pattern of this activation
and non-activation of the various pixels
this will be identified as a zero for
example right similarly as you can see
so overall each of these numbers
actually has a different pattern of the
pixel activation and that's pretty much
that our model needs to learn for which
number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and tests so that the training
data set is used to train the system so
we pass this probably multiple times and
then we test it with the test data set
and the split is usually in the form of
there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
20 or 0.23 that means 23 percent of that
entire data is used for testing and the
remaining 77 percent is used for
training so there is a readily available
function which is called train test
split so we don't have to write any
special code for the splitting it will
automatically split the data based on
the proportion that we give here which
is test size so we just give the test
size automatically training size will be
determined and we pass the data that we
want to split and the the results will
be stored in X underscore train and Y
underscore train for the training data
set and what is X underscore train these
are these are the features right which
is like the independent variable and Y
underscore train is the label right so
in this case what happens is we have the
input value which is or the features
value which is in X underscore train and
since this is a labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that that's this is what will
be used for compare comparison to find
out whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way X underscore train
y underscore train is for the training
data set X underscore test y underscore
test is for the test data set okay so
let me go ahead and execute this code as
well and then we can go and check
quickly what is the how many entries are
there and in each of this so X
underscore train the shape is
1383 by 64 and Y underscore train has
1383 because there is uh nothing like
the second part is not required here and
then X underscore test shape we see is
414 so actually there are 414
observations in test and 1383
observations in train so that's
basically what these four lines of code
are are saying okay then we import the
logistic regression library and which is
a part of scikit-learn so we we don't
have to implement the logistic
regression process itself we just call
these uh the function and let me go
ahead and execute that so that we have
the logistic regression Library imported
now we create an instance of logistic
regression right so logistic regr is is
an instance of logistic regression and
then we use that for training our model
so let me first execute this code so
these two lines so the first line
basically creates an instance of
logistic regression model and then the
second line way is where we are passing
our data the training data set right
this is our the the predictors and this
is our Target we are passing this data
set to train our model all right so once
we do this in this case the data is not
large but by and large and the training
is what takes usually a lot of time so
we spend in machine learning activities
in machine learning projects we spend a
lot of time for the training part of it
okay so here the data set is relatively
small so it was pretty quick so all
right so now our model has been trained
using the training data set and we want
to see how accurate this is so what
we'll do is we will test it out in
probably faces so let me first try out
how well this is working for one image
okay I will just try it out with one
image my the first entry in my test data
set and see whether it is uh correctly
predicting or not so and in order to
test it so for training purpose we use
the fit method there is a method called
fit which is for training the model and
once the training is done if you want to
test for a particular value new input
you use the predict method okay so let's
run the predict method and we pass this
particular image and we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 seems to be fine so let me just
go ahead and test the entire data set
okay that's basically what we will do so
now we want to find out how accurately
this has is performed so we use the
score method to find what is the
percentages of accuracy and we see here
that it has performed up to 94 percent
accurate okay so that's on this part now
what we can also do is we can
um also see this accuracy using what is
known as a confusion Matrix so let us go
ahead and try that as well so that we
can also visualize how well this model
has done so let me execute this piece of
code which will basically import some of
the libraries that are required and we
we basically create a confusion Matrix
an instance of confusion matrix by
running confusion Matrix and passing
these values so we have so this
confusion underscore Matrix method takes
two parameters one is the Y underscore
test and the other is the prediction so
what is the Y underscore test these are
the labeled values which we already know
for the test data set and predictions
are what the system has predicted for
the test data set okay so this is known
to us and this is what the system has
the model has generated so we kind of
create the confusion Matrix and we will
print it and this is how the confusion
Matrix looks as the name suggests it is
a matrix and the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
of them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers which means the accuracy is very
low the diagonal indicates a correct
prediction that so this means that the
actual value is same as the predicted
value here again actual value is same as
the predictive value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 plus 44 Plus 43 and so
on and divide that by the total number
of test observations that will give you
the percentage accuracy using a
confusion Matrix now let us visualize
this confusion Matrix tricks in a
slightly more sophisticated way using a
heat map so we will create a heat map
with some We'll add some colors as well
it's uh it's like a more visually
visually more appealing so that's the
whole idea so if we let me run this
piece of code and this is how the heat
map looks and as you can see here the
diagonals again are all the values are
here most of the values so which means
reasonably this seems to be reasonably
accurate and yeah basically the accuracy
score is 94 percent this is calculated
as I mentioned by adding all these
numbers divided by the total test value
so the total number of observations in
test data set okay so this is the
confusion Matrix for logistic regression
all right so now that we have seen the
confusion Matrix let's take a quick
sample and see how well the system has
classified and we will take a few
examples of the data so if we see here
we picked up randomly a few of them so
this is uh number four which is the
actual value and also the predicted
value both are four this is an image of
a zero so the predicted value is also
zero actual value is of course zero then
this is the image of 9. so this has also
been predicted correctly nine and actual
value is 9 and this is the image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images Excel
radio career in air and machine learning
with our comprehensive postgraduate
program in Ai and machine learning in
expert is in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and hackathons and ask
me anything sessions by IBM with three
capstones and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience this program covers
statistics python supervised and
unsupervised learning NLP neural
networks computer vision Gans KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below so first we
will import some major libraries of
python so here I will write import
ance as PD
and Report
numpy as NP
then
import
c bond
that's SNS
and import
SK learn
dot model selection
port
train underscore
test the score split
before that
I will import
matplotlib
dot Pi plot
as PLT
okay then
I will write here from SK learn
Dot
Matrix
import
accuracy
or
than from
Escalon
dot matrix
import
classification
to report
and port
Ari
then import
string
okay
then press enter
so it is saying
okay
here I have to write from
everything seems good
loading let's see
okay till then number is a python
Library used for working with arrays
which also has function for working with
domain of lineal algebra and matrices
it is an open source project and you can
use it freely
number stand for numerical python
pandas so panda is a software Library
written for Python programming language
for data manipulation and Analysis in
particular it offers data structure and
operation for manipulating numerical
tables and Time series
then
Seaborn
an open source python Library based on
matplotlib is called C bone it is
utilized for data exploration and data
visualization
with data frames and the pandas Library
c-bond functions with ease
than matplotlib for Python and its
numerical extension numpy
matplotlib is a cross platform for the
data visualization and graphical
charting package
as a result it presents a strong open
source suitable formatlab
the apis for matplotlib allow
programmers to incorporate graphs into
GUI applications then this train test
split we may build our training data and
the test data with the aid of SQL and
train test split function
this is so because the original data set
often serves as both the training data
and the test data starting with a single
data set we divide it into two data sets
to obtain the information needed to
create a model like hone and test
accuracy score the accuracy score is
used to Gorge the model's Effectiveness
by calculating the ratio of total true
positive to Total to negative across all
the model prediction
this re regular expression
the function in the model allow you to
determine whether a given text fits a
given regular expression or not which is
known as re
okay then string a collection of letters
words or other character is called a
string it is one of the basic data
structure that serves as the foundation
of manipulating data
the Str class is a built-in string class
in Python because python strings are
immutable they cannot be modified after
they have been formed
okay so now let's import the data set
we will be going to import two data set
one for the fake news and one for the
True News or you can say not fake news
okay
so I will write here
EF underscore
big
question
PD Dot
read underscore CSV
oh what can I say dear fake okay
hit underscore fake
okay
then
pick
dot CSV
you can download this data set from the
description box below
then data Dot
true
equals to PD dot read
underscore CSV
sorry CSC
plan
big news sorry true
dot CHP
then press enter
so these are the two data set
you can download these data set from the
description box below so let's see the
board data set okay
then I will write here data underscore
fake
dot head
so this is the fake data okay then
data underscore true
Dot
and this is the two data
okay this is not fake so if you want to
see your top five rows of the particular
data set you can use head
and if you want to see the last five
rows of the data set you can use tail
instead of head
okay
so let me give some space for the better
visual
so now we will insert column class as a
Target feature okay then I will write
here data
let's go fake
Plus
equals to zero
then
Theta underscore true
and
Plus
one
okay
then
I will write here data underscore fake
dot shape and data underscore true
dot ship
okay then press enter
so the shape method return the shape of
an array the shape is a tuple of
integers these number represent the
length of the corresponding array
dimension in other words a tuple
containing the quantities of entries on
each axis is an array shape dimension
so what's the meaning of shape
in the fake world
in this data set we have two three four
eight one rows and five columns
and in this data set true
we have two one four one seven rows and
five column okay so these are the rows
column rows column for the particular
data set
so now let's move
and let's remove the last 10 rows for
the manual testing okay
then I will write here data underscore
speak
let's go manual
testing
question
data underscore fake
dot tail
for the last 10 rows I have to write
here 10.
okay so for I
in range
2 3 4
8 1.
sorry zero
comma 2 3
4 7 0
comma minus 1.
okay
and
TF underscore not DF data
underscore fake
dot drop
one
instead of one I can write here I
comma
this equals to zero
place
equals to true
then
data
not here
data underscore
same I will write for I will copy from
here
and I will paste it here and I will make
the particular changes
so here I can write true
that I can write true
okay
then I have to change a number
two one
six
okay
that's right
2 1 4 0 6
-1
same
so press enter
X is equals to zero
alert since X maybe you mean double zero
or of this
okay we will put here double course
and I'm putting this
take dot drop i x is equal to zero okay
in place okay
we're also write equals to a question
yeah
so
okay axis is not defined
so now it's working so
let me see
now
did the
and that's cool
fake dot shape
okay
and data dot true
and data underscore true
dot shape
as you can see
10 rows are deleted from each data set
so I will write here data underscore
fake underscore manual
testing
yeah
class
equals to zero
and data underscore
true
underscore
manual underscore testing
class equals to
one
okay
just ignore this warning
and
let's see
data underscore
fake underscore
manual
testing dot head
as you can see we have this
and then data dot sorry underscore true
underscore
manual
testing
dot at
done
this is this is done uh true data set
so here I will merge data
let us go merge
pursue
PD Dot
concat
en concat is used for the concatenation
data underscore fake
data underscore
comma
axis
equals to zero
then data underscore merge
dot head
the top 10 rows
yeah
as you can see the data is merged
here
okay first it will come for the fake
news and then with that for the True
News
and let's merge true and fake data
frames okay
we did this and
let's merge the column then data dot
merge
Dot columns or let's see the columns
it is not defined whatever data
underscore merge
these are the column same title tag
subject date class okay
now
let's remove those columns which are not
required for the further process so here
I will write data underscore
or request to
data underscore merge
crop
title I don't need
that
object
we don't need then
so one
so let's check some null values
it's giving
because of this
that's good then data
dot is null
dot sum
Center
so no null values
okay
then
let's do the random shuffling of the
data frames okay
for that we have to write here data
equals to
data dot sample
one
then
data
okay
data
dot hat
okay now you can see here the random
shuffling is done
and one for the
true data reset and zero for the fake
news one okay
then
let me write here data Dot
reset
underscore index
place
because you
through
a dot dot drop
comma axis
equals to 1
then comma in place
question true
okay
then let me see columns now data Dot
columns
so here we have two columns only rest we
have deleted
okay
see data dot at
yeah
everything seems good
let's proceed further and let's create a
function to process the text okay
for that I will write here
but
okay
you can use any name
text
and text equal to
text Dot lower
okay
and text
equal to re dot for the substring
remove these things
from the
datas okay
so for that I'm writing here
comma
okay
then text equals to
re Dot substring
comma
comma text
okay
then I have to write text equals to
re Dot substring
to www
Dot
S Plus
comma
comma text
okay then text equals to
re Dot substring
then
oh
comma
okay
then
text equals to
re Dot substring
and
percentage
as
again percentage or
re dot SK function
right here string
dot punctuation
comma
and comma then text
right
then text equals to re Dot substring
and n
comma
x equals to
re Dot
substring
right here
and again d
then again
then comma
and again
texture
okay then at the end after right here
return text
so everything like this these type of
special character will be removed from
the data set
okay let's run this let's see
yeah so here I will add DF sorry not DF
data
data
then
next
pursue
data
dot apply
through the function name wordpod word
opt
okay
press enter
yeah
so now let's define the dependent and
independent variables okay x equals to
data
text
and Y equals to
data
class
okay
then splitting training and testing data
okay sorry
so here I will write X underscore train
comma X underscore test
then y underscore train
comma y underscore test
equals to
train underscore test underscore split
then X comma y
comma test
let's go size equals to
0.25
okay press enter
so now let's convert text to vectors
for that I have to write here
said it's X
so here I will write from sqlarn
Dot teacher
extraction
Dot text import
t
vectorizer
okay
then vectorization
plus 2 tfid
factorizer
okay
then
see
underscore
train
equals to
vectorization
or reset it t i 100 factorization dot
fit
that transform
X underscore train
okay
then
x v underscore test equals to
factorization
Dot transform
X underscore test
okay then press enter
[Music]
so now let's see our first model
logistic regression
so here I will write
from
sqlan Dot
linear underscore model
okay import
logistic
regression
yeah
then a lot goes to
logistic
regression
and
have to write here LR Dot
wait
then XV
Dot
dot dot dot train
comma
at 3 underscore test
okay
press enter
okay here I have to write y train
and press enter
will work so here I will write
prediction
underscore
linear regression
dot predict
three underscore test
okay let's see the accuracy score
for that I have to write LR DOT score
then XV underscore test
comma y underscore test
okay
let's see the accuracy so here as you
can see accuracy is quite good 98
percent
now let's print
the classification
code
I underscore test comma
prediction of linear regression
okay
so this is you can see Precision score
then F1 is code then support value
accuracy
okay
so now we will do this same for the
decision tree gradient boosting
classifier random Forest classifier okay
then we will do model testing then we
will predict this score
okay so now for the decision tree
classification so for that I have to
import from SK learn
dot tree
import
decision
free
classifier
okay
then at the short form I will write here
I will copy it from here
then
okay
then I have to write the same as this so
I will copy it from here
and
yeah
let's change linear regression
to
season 3 classifier
okay
then I will write here same
let's go DT
question
DT dot predict
V Let's Go
test
e
still loading it's it will take time
okay
till then let me write here for the
accuracy
DT DOT score
V underscore test
comma y
let's wait okay
let's run
the accuracy
so as you can see accuracies good than
this linear regression
okay logistic regression
okay so let me
show you deep let me predict
trend
so this is the accuracy score this is
the all the report
yeah
now let's move for the gradient boosting
classifier
okay for that I've read from
sqlan
dot ensemble
quote
gradient
boosting
classifier
pacifier
I will write here GB
equals to let me copy it from here
I will give here random
let's go state
equals to zero
wait wait wait wait so I will write here
GB Dot
fit
three underscore train
comma
y underscore chain okay then press enter
so here I will write predict
underscore GB
was who
GB Dot
wait sorry
Reddit
three
Dot test
dot dot underscore test
till then it's loading so I will write
here uh it's for this code then I will
add GB DOT score
then
three underscore test
comma
y underscore test
okay
so let's wait it is running this part
till then let me write for the printing
this
case taking time
taking time still taking that
if I will run this
it's not coming because of this
yeah it's done now so you can see the
accuracies
not good then
decision tree but yeah it is also good
99
0.4 something okay so
now let's check for the last one random
Forest
first I will do
for the random for us we have to write
from sqlarn Dot
example
import
random
Forest
classifier
okay
and here I will write RF
equals to
right I will copy it from here
then
random
date
question
zero
and
RF Dot
fit
three underscore train
comma y underscore train
okay then press enter
and predict
underscore
RC
or F
equals to
RF dot predict
C underscore test
okay
till then I will write it still loading
it will take time
so till then I will write for the score
score accuracy score
XV underscore test comma y underscore
test
okay
then I will write here till then print
classification
code
and Y underscore test
comma
it will take time little bit
so
it run the accuracy score is 99 it is
also good
so now I will write the code for the
model testing so I will get back to you
but after writing the code so
so I have made two functions one for the
output label and one for the manual
testing okay
so it will predict
the all the from the all models from the
repeat so it will predict
the the news is fake or not from all the
models okay
so for that
let me write
here news
also string
but
okay
then I will write a manual underscore
testing
so
here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want
so I'm just copying from the internet
okay from the Google
the news
which is not fake okay I'm adding which
is not fake because I already know I
searched on Google so I'm entering this
so just run it let's see what is showing
okay
string input object is not callable okay
let me check this first
okay I have to give here Str only
yeah let's check
okay I have to add here again the Escape
yeah
manual testing is not defined
let me see manual testing
okay I have to edit something
it is just GB and it is just RF
in GBC is not defined okay okay
so what I have to do
I have to remove this
this
everything seems sorted
now
as I said to you
I just copied this news from the
internet I already know the news is not
fake so it is showing not a fake news
okay so now what I will do I will copy
one fake news from the internet
and let's see it is detecting it or not
okay
so let me run this
and let me add the news for this
so
all the models are predicting right it
is a fake news
or you can add your own script like this
is the fake news okay
I hope you guys understand
till here
so I hope you guys must have understand
how to detect a fake news using machine
learning you can you can copy it any
news from the internet and you can check
Excel radio career in air and machine
learning with a comprehensive
postgraduate program in Ai and machine
learning the next part is in machine
learning deep learning NLP computer
vision and reinforcement learning you
will receive a prestigious certificate
exclusive alumni membership and
hackathons and ask me anything sessions
by IBM with three capstones and 25 plus
industry projects using real data sets
from Twitter Uber and more you will gain
practical experience this program covers
statistics python supervised and
unsupervised learning NLP neural
networks computer vision gns KRS
tensorflow and many more skills enroll
now and unlock exciting Ai and ml
opportunities the link is mentioned in
the description box below
opencv open source computer vision
library is an open source computer
vision and machine learning software
Library it is written in C plus plus but
has a binding for various programming
languages such as python Java Matlab
opencv was designed with the goal of
providing a common infrastructure for
computer vision applications and to
accelerate the use of machine learning
perception in commercial product opencv
is widely used in a variety of
Industries including robotics automotive
and Healthcare it's supported by a large
community of developers researchers and
users who contribute to its development
and provide supports to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide supports to its users so now
let's see what is object detection
object detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is a kernel
python kernel here we will change this
name
so here I will write object
detection
demo
okay
so
first we will import some major Library
like opencv so for that we will write
import CB2 and the next one is import
matplotlab
dot Pi plot
as PLT
so why we are writing PLT because we
can't write again and again matplotlib
dot pipelot okay it's a long one so we
can write a short form pld
so yeah
so let's run this
so what is opencv opencv is an open
source software library for computer
vision and machine learning the opencv
full form is open source computer vision
Library it was created to provide a
shared infrastructure for application
for computer vision and to speed up the
use of machine learning perception in
consumer products opencv has a PSD
license software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
life simple and opencv is one of them
second one is matte broadly matplotlib
is a easy to use and an amazing
visualized library in Python it is built
on numpy array and designed to work with
broader sci-fi stack and consists of
several plots like line bar scatter
histogram and many others okay so moving
forward we will import our file okay so
here I will write config
file
equals to this is our file name SSD
underscore
mobilenet
to V3
large
Coco
22
14 dot BB
okay
so you can find this file on the
description box below
Frozen
model
question
explain you
every single thing
inference
graph
dot PB okay
so let me run it first
mobilenet as a name applied the mobile
net model is designed to use in mobile
application and its tensorflow first
mobile computer vision model mobile net
used depth wise separable convolutions
it significantly reduces the numbers of
parameter when compared to the network
with regular convolutions with the same
depth in the heads this result in the
lightweight of the deep neural network
so mobilenet is a class of CNN that was
open source by Google and therefore this
gives us an excellent starting point for
training our classifiers that are
insanely small and insanely fast okay so
what is this large Coco this is a data
set Coco data set like with applications
such as object detection segmentation
and captioning the Coco data set is
widely understood by the state of the
art of neural network its versatility as
a multi-purpose scene variations of best
to train a computer vision model and
Benchmark its performance okay so what
is Coco the common object and context is
one of the most popular large scale
labeled images data available for public
use it represents a handful of objects
we encounter on a daily basis and
contains Imaging notations in 80
categories I will show you the
categories I have with over 1.5 million
object instances okay so modern day AI
driven solution are still not capable of
producing absolute accuracy and result
which comes down to the fact that Coco
dataset is a major Benchmark for CV to
train test and polish refine models for
faster scaling of The annotation
Pipeline on the top of that the Coco
data set is a supplement to transfer
learning where the data used for one
model serves as a starting point for the
another so what is frozen Insurance
graph like freezing is the process to
identify and save all the required
graphs like weights and many others in a
single file that you can usually use a
typical tensorflow model contains four
file and this com contains a complete
graph okay so forward let's create one
model here I will add modern
DNN
model
chosen
then
let's config file
so here I am giving the parameters two
parameters like frozen model and config
file
score here yeah you run it first
okay there is error
okay so either this cv2.dnn detection
model returned as result
with an exception set
the question comes what is the meaning
of detection model or DNN detection
model
so this class represents a high level
API for object detection networks
detection models allows to set
parameters for pre-processing input
image detection model creates net from
file and with drain weights and config
sets in processing input runs forward
pass and return the result deductions
okay moving forward let's set the class
levels
class
labels
file name
txt
I will put this file on the description
box below you can download from there
open
file name
pass labels
history
so here I created one array of name
class labels so this is the file name
what I am doing I am putting this label
file into this class label okay so here
if I will print
class labels
so these are the 80
categories in the Coco data set
okay this person bicycle car motorbike
airplane bus train these all are the 80
categories
I will put this file label.txt in the
description box below you can download
from them
okay fine
so let's print the length of the Coco
data set or you can see class labels
this is 80 as you can see I have already
told you
the length will be 80.
so here let's
set this some model input size scaling
mean and all so I will write here model
Dot
set
put
eyes
T20 comma 20.
set
input
scale
1.0
slash 127
0.5
okay I will explain you don't worry
model dot set
import
me
127.5
comma 127.5
comma 127.5
okay and then model
dot set
to web
B
will be
what is set input size
okay
so set input size is a size of new frame
the shape of the new blob Less Than Zero
okay so this is the size of the new
frame the second one is set input scale
so set input scale is a scale factor of
the value for the frame
or you can say the perimeter will be the
multiplier of the frame values or you
can say multiplier for the frame values
okay so at input mean so it set the mean
value for the frame the frame in which
the photo will come the video will come
or my webcam will come so it set the
mean value for the frame or the four
parameters mean scalar with the mean
values which are subtracted from the
channels you can say and the last one is
set input swap RB so it set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be swipe I'll be flag
which indicates the swipe first and the
last channels so moving forward we will
forward one image
M read
y dot jpg
Dot
I am sure
so this is the size of 320 by 320 okay
so first thing is
you can download this the random picture
from the Google I do from Google itself
so now what we will do
we will set the class index
the confidence value
box is the boundary box which I will
create for that particular person's
cycle motorbike and the car
okay
because to model
kept
the confidence threshold
threshold is used for
if my model will confirm it's the
particular image which is detecting is
correct it will print the name
okay
so let me print
pass
class index is coming one two three four
okay so one means person
two means bicycle
three means car and four means motorbike
this is the class index index for
particular level what I will do I will
print the boxes
font
here
equals to 3
and the font equals to CV2 Dot
font Hershey
again
four
class
index and then
confidence and the box is
and
x dot
pattern
confidence
pattern
box the boundary box
okay
and I will write here CV2 dot rectangle
make the rectangle
set the image
and boxes
I've
comma 0 comma zero
this is the color of the box and this
will be the thickness
okay
then I will write CV2 dot put
text
image then
first labels
I will write class
index minus 1
because always index start with 0.
that's fine and the boxes
zero
and
common boxes
fine
body
okay
I want
comma
scale
assume font
scale
okay color
question
this will be the text color
zero comma 255
comma 0
and the thickness
two three
let me run it
ah no Adder okay now PLT dot I am sure
then CV2 dot CBT
color
then
comma CV t two
dot color
mean
hello I'm BGR
to prg
that is why we wrote
swap RB equals to true because every
time we will convert pgr to brg
sorry
GB
RGB so we don't have to write again and
again it will convert all the files into
RGB okay run it
okay as you can see the motorbike is
coming
bicycle is coming the person is coming
the car will come the car is coming okay
so it's detecting the right
for the image now we will do this for
the video and for the webcam
we are done with this image one and then
now I will write here
okay
so this is we'll do for the video
for the video I will write here cap
equals to capture
you can write any name so CV2 Dot
video
capture
so you can take any random video I took
this pixels
George
you can comment down
share the link
for
app dot is open
so here I will add cap
equals to
be two
sorry CB2
dot video
capture
0.
and if not
cap
dot is open
then
and this
input output
error
and
open the video
and open the video
here everything will be the same font
scale
equals to 3
okay if you want
equals to CV2 Dot
font
I'll say
okay
so here I will write y true
from a frame
equals to cap dot sheet
this for the reading of the file
the same I will write class index
comma confidence
from a boundary box
to assume model
dot detect
game
and the confidence
threshold
0.55
okay everything is the same
we did before so here I will print
class
index
okay
so here I will write if
and
the class index
does not equals to zero
then what to perform is here I have to
add 4
class index
comma confidence
got my boxes
and zip
us
x dot
flatten
flatten is a layers
okay
confidence
flatten
e-box
and if
class
index is greater than equals to 80.
then
what to do
here
okay the same thing I have to write here
so here I will write CV2 Dot
I am sure
this will be the return in the frame
object detection
by simply learn
and frame
so if CV2 Dot
weight key
to
and 0
equal to f of x
equals to
Ord
Q okay
then
here I will write break
will you break when
and get into two the weight key will be
2.
okay I will tell you what is the weight
key
so here I will write cap dot release
and CV2 Dot
destroy
or Windows
okay
so now let me done
it's here there will be better okay
JV Python programming language modules
we run it again
[Music]
okay
so video is here
the video is here as you can see see
bicycle
the person the person the bus car
traffic light the person person so it
our object detection for the video is
coming right
okay person
okay person traffic light bus
this is how you can do for the video
okay
so now let's we will do for the webcam
live
so this is for the video so
if
we want to do for the webcam with
okay so we need to just change
one one thing only we have to change
instead of giving the file we have to
write one here
okay the rest will be the same
got it so I have to just shut down my
webcam
so
let me shut down the webcam and get back
to you
okay
so as you can see
this is a 320 by 320 box so
so this is coming right okay so I if I
will show this the mobile phone is
coming right now okay so this is how you
can do the correct object detection okay
with that we have reached the end of
this tutorial if you have any questions
please feel free to comment and we'll
have it answered for you as as soon as
possible
until next time thank you for watching
stay safe keep learning and get ahead
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign