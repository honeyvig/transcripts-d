welcome to the future of technology are
you ready to dive head first into the
world of artificial intelligence with
python in 2023 well you have come to the
right place in this exhilerating YouTube
video we are taking you on a thrilling
Journey Through The Cutting Edge python
AI course of 2023 prepare to witness
mindblowing breakthroughs unravel the
secrets of machine learning and explore
the endless possibilities of AI from
powerful algorithms to real world
applications we have got it all covered
whether you are a season coder or a
curious bner this course is designed for
you get ready to master Python and
concur AI like never before so without
further Ado let's jump between the
Incredible World of python AI in 2023
and before starting if you are one of
the aspiring python Enthusiast or a ml
Enthusiast looking for online training
and graduating from the best
universities or a professional who
elates to switch careers in python or a
ml by learning from the experts then try
giving a short to Simply learns
postgraduate program in Ai and machine
learning in collaboration with
University and IBM and for this course
you only need 50% in your curriculum and
you just have to be one plus years of
experience to be preferred and the
course link is mentioned in the
description box that will navigate you
to the course page where you can find a
complete overview of the program being
open picture this a machine that could
organize your cupboard just as you like
it or serve every member of the House a
customized cup of coffee makes your day
easier doesn't it these are the products
of artificial intelligence but why use
the term artificial intelligence well
these machines are artificially
Incorporated with humanlike intelligence
to perform tasks as we do this
intelligence is built using complex
algorithms and mathematical functions
but AI may not be as obvious as in the
previous examples in fact AI is used in
smartphones cars social media feeds
video games banking surveillance and
many other aspects of our daily life the
real question is what does an AI do at
its core here is a robot we built in our
lab which is now dropped onto a field in
spite of a variation in lighting
landscape and dimensions of the field
the AI robot must perform as expected
this ability to react appropriately to a
new situation is called generalized
learning the robot is now at a crossroad
one that is paved and the other Rocky
the robot must determine which path to
take based on the circumstances this
portrays the robot's reasoning ability
after a short stroll the robot now
encounters a stream that it cannot swim
across using the plank provided as an
input the robot is able to cross this
stream so our robot uses the given input
and finds the solution for a problem
this is problem solving these three cap
capabilities make the robot artificially
intelligent in short AI provides
machines with the capability to adapt
reason and provide
Solutions well now that we know what AI
is let's have a look at the two broad
categories and AIS classified into weak
AI also called narrow AI focuses solely
on one task for example alphago is a
maestro of the game go but you can't
expect it to be even remotely good at
Jess this makes alphago a weak AI you
might say Alexa is definitely not a weak
AI since it can perform multiple tasks
well that's not really true when you ask
Alexa to play despacito it picks up the
key words play and despacito and runs a
program it is trained to Alexa cannot
respond to a question it isn't trained
to answer for instance try asking Alexa
the status of traffic from work to home
Alexa cannot provide you this
information as she is not trained to and
that brings us to our second category of
AI strong AI now this is much like the
robots that only exist in fiction as of
now Ultron from Avengers is an ideal
example of a strong AI That's because
it's self-aware and eventually even
develops emotions this makes the ai's
response
unpredictable You Must Be Wondering well
how is artificial intelligence different
from machine learning and deep learning
we saw what AI is machine learning is a
technique to achieve Ai and deep
learning in turn is a subset of machine
learning machine learning provides a
machine with the capability to learn
from data and experience through
algorithms deep learning does this
learning through ways inspired by the
human brain brain this means through
deep learning data and patterns can be
better perceived Ray kheel a well-known
futurist predicts that by the year 2045
we would have robots as smart as humans
this is called the point of Singularity
well that's not all in fact Elon Musk
predicts that the human mind and body
will be enhanced by AI implants which
would make us partly cyborgs artificial
intelligence is reshaping Industries and
transforming the way we live and work it
encompasses a wide range of Technologies
including machine learning deep learning
natural language processing computer
vision and many more with its ability to
analyze vast amount of data and make
Intelligent Decisions AI has become a
game changer across various domains hi
everyone welcome to Simply learns
YouTube channel this AI road map guides
you in navigating the path towards
building a successful career in
artificial intelligence the ultimate
goal of artificial intelligence is is to
create intelligent missions that can
perform complex task exhibit humanik
intelligence and contribute positivity
to society AI presents exciting career
opportunity in various Industries and
sectors roles like AI engineer data
scientist NLP Engineers computer vision
Engineers AI research scientists robotic
engineers and many more offer exciting
prospects for working with cutting H
Technologies and making an impact
through artificial intelligence
Innovation according to glass door the
average reported salary of an engineer
in the United State is around
$105,000 per year however in India it is
10 lakh per anom leading companies
worldwide are fully aware of the immense
value of AI and are actively pursuing
skilled AI Engineers to contribute to
their research development and
implementations of AI technology among
this top companies are Google Microsoft
Amazon Goldman saat apple and JP Morgan
Chase therefore top companies hiring AI
engineer provide excellent opport
opportunity for aspiring professionals
on that note elevate your career with
our Ai and ml course developed in
partnership with P University and IBM
gain expertise in high demand skills
such as machine learning deep learning
NLP computer vision reinforcement
learning chat GPD generative Ai
explainable Ai and more Unleash Your
Potential and unlock exciting
opportunity in the world of AI and ml
this course cover tools like python
tensorflow K CH GPT open I gy matplot
lab and many more so hurry up and find
the course Link in the description box
for more details our Learners have
experienced huge success in their
careers listen to their experience find
the simply learn Course Review Link in
the description box without any further
delay let's dive into the topic now if
you decide to become an AI engineer here
are the steps you can follow to achieve
your goal firstly obtain a strong
foundation in mathematics and
programming gain a strong foundation in
creating mathematic Concepts like linear
algebra calculus and probability theory
in addition to that it is crucial to
Acure Mastery of a programming languages
like python commonly used in Ai and
become proficient in coding then earn a
degree in a relevant field that means
earn a bachelor's or master degree in
computer science data science AI or a
related field to gain a comprehensive
understanding of AI principles and
techniques next gain knowledge in
machine learning and deep learning to
become a AI engineer develop familiarity
with ML algorithms NE networks and deep
learning Frameworks example tensor Flow
by Tor to train and optimize models
using real world data set followed by
that work on practical projects gain
hands-on experience and showcase your
skills by working on AI projects
moreover build a portfolio of projects
demonstrating your ability to solve eii
problems impressing potential employers
then stay updated with the latest
advancements stay updated with the
latest trends and research in AI a
rapidly involving field and expanding
your knowledge by reading research
papers participating in online courses
and workshops and joining AI communities
next collaborate and network engage with
AI communities attend conference
participate in online forums to connect
with Professionals in this field
collaborating with others can enhance
your learning experience and open new
opportunities later seek internships or
entry-level positions gain practical
experience through AI internships or
entry-level roles in industry or
research ins utions this provides
valuable exposures and help develop your
skills finally continuously learn and
adapt in the rapidly involving field of
AI stay updated on new developments
explore specialized areas and embrace
emerging Technologies and tools to
pursue a career as an AI engineer now
that we have covered the essential steps
to become an AI engineer let's explore
the necessary programming languages and
algorithms for aspiring EI Engineers so
mastering programming languages like
python R Java and C++ is vital for
acquiring Proficiency in AI this
languages enables you to construct a
deploy models effectively additionally
it would help if you gain a thorough
understanding of machine learning
algorithms such as linear regressions K
near as neighbors na base and support
Vector missions this languages and
algorithms are fundamental tools in the
field of AI they will enable you to
develop and Implement effective
artificial intelligence models hello
everyone and Welcome to our video on a
skills required for an AI engineer with
the Advent of artificial intelligence
and its Superior abilities to improve
human life the need and demand for
expert AI professional is at an all-time
high this expanding demand has led to a
lot of people applying for AI jobs and
upskilling themselves in the field of AI
but doing this is just like playing a
game but not knowing its rules for
someone to become an AI professional and
learn a job like an AI engineer they
first have to understand the demand from
this role and what skills are expected
of an AI engineer so if you're someone
who wishes to become an AI engineer or
if you wish to upgrade your skills in Ai
and get promoted as an AI engineer then
make sure you watch this amazing video
till the very end in this video we will
be breaking down in complete detail each
and every skill that you would need in
order to crack an AI engineer job
interview but why should you consider a
career in ai ai is not just a passing
Trend it's a sesmic ship that is
reshaping our world and creating new
avenues for Innovation and discovery by
embracing a career in AI you become a
part of dynamic field that thrives on
solving complex problems pushing
boundaries and making a profound impact
or Society the demand for AI
professional is scating across
industries from healthare and finance to
entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but what skills does it take to
become an AI engineer how can you embark
on the thrilling Journey we have
answered to all of your questions also
accelerate your career in AI ml with our
comprehensive post-graduate program in
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive alumini membership
and ask me anything session by IIA with
three Capstone project and 25 plus
industry projects using real data set
from Twitter Uber and many more you will
gain practical experience master classes
by ctech faculty and IBM expert and
ensure topnotch education simply learn
job assist help you to get notice by
Leading companies this program covers
statistic python supervised and
unsupervised learning NLP neural network
computer vision G caras tensor flow and
many more skills so enroll now and
unlock exciting AI ml opportunities the
link is in the description box below so
without any further delay let's get
started some steps are crucial to master
in the field of AI and becoming and AI
engineer let's go through them real
quick first establish a strong
foundation in mathematics and
programming start by gaining a solid
understanding of critical Concepts like
linear algebra calculus and probability
Theory additionally it is crucial to
become proficient in programming
language like python which is commonly
used in Ai and developed coding skills
and the next one is pursue a degree in
relevant field earn a bachelor or master
degree in computer science data science
or AI or related discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensor flow
and pytorch to train and optimize models
using real world data sets afterward
engage in Practical projects gain
hands-on experience and demonstrate your
skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
communities attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is seek internship or entry-level
positions gain practical experience
through AI internship or entry-level
roles in Industries research institution
this will provide valuable exposure and
help you further develop your skill and
at the last continuously learning and
adapt in the fastpac world of AI it is
crucial to stay updated on new
developments explore specialized areas
and embrace emerging Technologies and
tools continuous learning and
adaptability are essential for pursuing
a successful career as a AI engineer so
now that you are familiar with the steps
involved in the Journey of AI engineer
let's have a look at the salary of an a
engineer so according to glass door the
average reported salary of an AI
engineer in the United States is $15,000
per year however in India it is 10 lakh
perom so these figures are way better
than the average sell figures of any job
road so let's discuss the skills you
need to become an AI engineer you should
have a strong background in data science
and machine learning here is a breakdown
of your skills number one strong
programming abilities the typically
refers to expertise in one or more
programming languages commonly used in
data science and machine learning such
as Python and R Proficiency in
programming allows you to write
efficient and scalable code for data
analysis modeling and algorithm implen
and the second one is knowledge of
machine learning algorithms this
involves understanding and familiarity
with the wide range of machine learning
algorithms including both supervised and
unsupervised learning you should be able
to select and apply appropriate
algorithm for specific problem as well
as evaluate and optimize their
performance and the third one is
Proficiency in statistic and Mathematics
sound knowledge of statistics and
Mathematics is fundamental for data
analysis and machine learning you should
be comfortable with statical Concept
hypothesis testing regression analysis
probability Theory lar algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popularity in recent years
and familiarity with deep learning
Frameworks like tensor flow P torch or
caras is valuable these Frameworks
provide tools and libraries for building
training and deploying deep neural
networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with big data
Technologies dealing with large scale
data set requires knowledge of Big Data
Technologies such as apachi Hado or
Apache spark or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed environment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be adapt
at critical thinking troubleshooting and
debugging to handle real world
challenges in the data science and
machine learning remember to stay
updated with the latest advancement in
the field and continuing learning to
stay at the Forefront of data science
and machine learning hello everyone and
Welcome to our video on skills required
for an AI engineer with the Advent of
artificial intelligence and its Superior
abilities to improve human life the need
and demand for expert AI professional is
at an all-time high this expanding
demand has led to a lot of people
applying for AI jobs and upskilling
themsel in the field of AI but doing
this is just like playing a game but not
knowing its rules for someone to become
an AI professional and landar a job like
an AI engineer they first have to
understand the demand from this role and
what skills are expected of an AI
engineer so if you're are someone who
wishes to become an AI engineer or if
you wish to upgrade your skills in Ai
and get promoted as an AI engineer then
make sure you watch this amazing video
till the very end in this video we will
be breaking down in complete detail each
and every skill that you would need in
order to crack an AI engineer job
interview but why should you consider a
career in ai ai is not just a pass Trend
it's a sesmic shift that is reshaping
our world and creating new avenues for
Innovation and Discovery by embracing a
career in AI you become a part of
dynamic field that thrives on solving
complex problems pushing boundaries and
making a profound impact on society the
demand for AI professional is discar
roating across industries from healthare
and finance to entertainment and
transportation organization are actively
seeking talented individuals who can
harness the power of AI to drive their
business business forward but what
skills does it take to become an AI
engineer how can you embark on the
thrilling Journey we have answered to
all of your questions also accelerate
your career in AI ml with a
comprehensive post-graduate program in
Ai and machine learning gain expertise
in machine learning deep learning NLP
computer vision and reinforcement
learning you will receive a prestigious
certificate exclusive aluminium
membership and ask me anything session
by ibia with three Capstone project and
25 plus industry projects using real
data set from Twitter Uber and many more
you will gain practical experience
master classes by ctech faculty and IBM
expert ensure topnotch education simply
learn job assist help you to get notice
by Leading companies this program covers
statistic python supervised and
unsupervised learning NLP neural network
computer vision Gan caras tensorflow and
many more skills so enroll now and
unlock exciting AI ml opportunities the
link is in the description box below
below so without any further delay let's
get started some steps are crucial to
master in the field of AI and becoming
an AI engineer let's go through them
real quick first establish a strong
foundation in mathematics and
programming start by gaining a solid
understanding of critical Concepts like
linear algebra calculus and probability
Theory additionally it is crucial to
become proficient in programming
language like python which is commonly
used in Ai and developed coding skills
and the next one is pursue a degree in
relevant field earn a bachelor or master
degree in computer science data science
or AI or related discipline to acquire
comprehensive understanding of AI
principles and techniques then acquire
knowledge in machine learning and deep
learning familiarize yourself with ML
algorithms neural networks and deep
learning Frameworks that is tensorflow
and P torch to train and optimize models
using real world data sets afterward
engage in practical iCal projects gain
hands-on experience and demonstrate your
skills by working on AI projects
building a portfolio of projects that
showcase your ability to solve AI
problems can make a strong impression on
potential employers and the fifth one is
collaborate and network engage with
communities attend conferences and
participate in the online forums to
connect with the Professionals in the
field collaborating with others can
enhance your learning experience and
open up new opportunities and the sixth
one is SE inter ship or entry-level
positions gain practical experience
through AI internship or entry level
roles in Industries or research
institution this will provide valuable
exposure and help you further develop
your skill and at the last continuously
learning and adapt in the fastpac world
of AI it is crucial to stay updated on
new developments explore specialized
areas and embrace emerging Technologies
and tools continuous learning and
adaptability are essential for pursuing
a successful career as a AI engineer so
now that you are familiar with the steps
involved in the Journey of AI engineer
let's have a look at the salary of an AI
engineer so according to glass door the
average reported salary of an AI
engineer in the United States is $15,000
per year however in India it is 10 lakh
perom so these figures are way better
than the average selling figures of any
job road so let's discuss the skills you
need to become an AI engineer you should
have strong background in data science
and machine learning here is a breakdown
of your skills number one strong
programming abilities the typically
refers to expertise in one or more
programming languages commonly used in
data science and machine learning such
as Python and R Proficiency in
programming allows you to write
efficient and scalable code for data
analysis modeling and algorithm
implementation and the second one is
knowledge of machine learning algorithms
this involves understanding and
familiarity with a wide range of machine
learning learning algorithm including
both supervised and unsupervised
learning you should be able to select
and apply appropriate algorithm for
specific problem as well as evaluate and
optimize their performance and the third
one is Proficiency in statistic and
Mathematics sound knowledge of
statistics and Mathematics is
fundamental for data analysis and
machine learning you should be
comfortable with statical Concept
hypothesis testing regression analysis
probability Theory L algebra and
calculus and the fourth one
is familiarity with deep learning
Frameworks deep learning has gained
significant popularity in recent years
and familiarity with deep learning
Frameworks like tensor flow P torch or
kasas is valuable these Frameworks
provide tools and libraries for building
training and deploying deep neural
networks for task such as image
recognition natural language processing
and time series analysis and the fifth
one is experience with Big Data
Technologies dealing with large scale
data set requires knowledge of Big Data
Technologies such as Apache Hadoop or
Apache spark or distributed computing
Frameworks understanding how to process
store and analyze data efficiently in
distributed environment and the sixth
one is excellent problem solving and
analytical skills these skills enable
you to break down complex problem
identify key factors and develop
effective Solutions you should be adapt
at critical thinking troubleshooting and
debug to handle real world challenges in
the data science and machine learning
remember to stay updated with the latest
advancement in the field and continuing
learning to stay at the Forefront of
data science and machine
learning machine learning is
transforming Industries and driving
Innovation by enabling systems to learn
from data and make Intelligent Decisions
this has resulted in a skyrocketing
demand for machine learning Engineers
these professionals possess the skills
skills to develop Advanced algorithms
build productive models and extract
valuable insights from vast amounts of
data the AA of Big Data necessites
professionals who can derive insights
from vast information and machine
learning Engineers possess the skills to
analyze complex data sets extract
valuable knowledge and build predictive
models for accurate forecast process
Automation and optimized decision making
Industries such as Healthcare Finance
e-commerce and autonomous vehicles
heavily rely on machine learning to
enhance processes and drive goals as
businesses increasingly recognize the
power of Data Insights the demand for
skilled machine learning Engineers
continues to search they are
instrumental in realizing the potential
of machine learning to optimize
operations improve efficiency and gain a
Competitive Edge with the market value
of machine learning expected to grow
exponentially now is the time to enter
into the field of machine learning and
become an ml engineer who are at the
Forefront of technological advancement
and bagging lucrative carer
opportunities in this Dynamic field that
opens doors to a world of endless
possibilities and promises a fulfilling
and impactful career on that note hello
everyone welcome to Simply learn today
we will explore a profession that has
witnessed a huge surge in demand in
recent years which is going to be the
next big thing in the future can you
guess which profession we talking about
well you got it right that's machine
learning engineer having said that if
you want to Embark your career as an ml
engineer then our postgraduate program
in& ml can be the right option for you
this amml postgraduate program by simply
learn in collaboration with PUO
University covers a wide range of topics
and provides hands-on experience in
developing Ai and machine learning
solutions for professionals who want to
upgrade their skills and prer to a
career in the field of AI and machine
learning this course features master
classes by Purdue faculty and IBM
experts exclusive hackathons to help you
master various skills like statistics
python supervised learning neural
networks NLP and much more by covering
tools and techniques like numpy pandas
python scipi along with amazing industry
projects so what is topic in you from
building your future in& ml enroll now
course link is added in the description
box below make sure to check that out
without any further Ado let's get
started meet John who has been working
as a software developer in the IT
industry for over 5 years now John has
been hearing a lot about machine
learning lately and how it has been
revolutionizing the tech industry he
wonders whether learning machine
learning would be worth it and that's
when his colleague Harry helps him out
hey harry I've been reading a lot about
machine learning engineering lately what
are your thoughts absolutely machine
learning is booming right right now
really quite intriguing but how can I
become an ml engineer well with the
right skill set you can my friend
possibilities are endless wow that's
impressive looks like machine learning
can be a rewarding career with great
opportunities and lucrative salaries
absolutely John so let me tell you more
about the working aspects of machine
learning engineer in detail we'll start
this tutorial with a quick introduction
to who ml engineer is and next we'll
understand what exactly does a machine
learning engineer do next we'll dive
into the roles and responsibilities of
of a machine learning engineer after
that we'll discuss some of the most
important skills that an ml engineer
should possess and finally we'll talk
about the salary and job prospects of a
machine learning engineer so firstly who
is a machine learning engineer a machine
learning engineer is a skilled
professional who combines expertise in
programming mathematics and machine
learning to develop and deploy
intelligent systems that can learn from
data and make accurate predictions or
decisions the role of a machine learning
engineer involves working with large
data sets pre-processing and cleaning
the data selecting appropriate machine
learning algorithms and training models
using this data they often collaborate
with data scientist and domain experts
like software Engineers to integrate
machine learning Solutions into
production environments for optimizing
and fine-tuning models for accuracy and
efficiency as well as evaluating their
performance and finally they perform a
thorough effective data exploration and
visualization techniques in order to
contribute for better understanding of
the data and identifying patterns making
informed decision throughout the machine
learning pipelines so what are the roles
and responsibilities of a machine
learning engineer firstly problem
understanding and solution design
machine learning Engineers work closely
with stakeholders domain experts to
understand the business problem or
objective that machine learning can
address they analyze requirements
identify suitable data sources and
propose machine learning based Solutions
this involves translating business
problems into technical specifications
defining measurable goals and designing
the overall architecture of the machine
learningstem system data preparation and
Analysis machine learning Engineers need
to have a deep understanding of the data
they're working with this involves
cleaning pre-processing and transforming
raw data into a format suitable for
training and evaluating ml models
exploratory data analysis techniques are
often used to gain insights and
understand the underlying patterns and
relationships within the data model
development and evaluation ml Engineers
are responsible for developing training
and fine-tuning machine learning models
this involves select ing the appropriate
algorithms architectures and techniques
based on the problems at hand you will
need to implement and experiment with
different models such as decision trees
neural networks support Vector machines
and much more and finally monitoring
maintenance and iterative Improvement
after deployment ml Engineers monitor
the performance of deployed models and
address any sort of issues that arise
they design and Implement monitoring
systems to track model performance
detect any kind of anomalies and
identify data drift or concept drift
regular model maintenance is necessary
to retrain and refine models using new
data to maintain accuracy and
adaptability so these are some of the
day to-day roles and responsibilities of
an ml engineer well let us not talk
about the skill set that a ml engineer
should possess firstly they should have
a strong knowledge on programming tools
now strong programming skills are
essential for machine learning Engineers
programming languages like R which is a
free software environment for for
statistical Computing and Graphics can
surely help but mainly they use python
python is widely used in the machine
Learning Community due to your
Simplicity extensive libraries and
strong ecosystem so you need to
familiarize yourself with python
fundamentals data manipulation libraries
like numai and pandas and machine
learning Frameworks such as tensorflow
pyo and psychic learn also familiarize
yourself with programming Concepts data
structures and algorithms and
additionally knowledge of other
languages like R and Java can be
beneficial secondly machine learning and
deep learning Concepts ml Engineers
should have a deep understanding of
various machine learning algorithms
including supervised learning
unsupervised learning and reinforcement
learning familiarize yourself with
Concepts like KNN linear regression
decision trees support Vector missiones
neural networks Ensemble methods and
much more not just that furthermore ml
Engineers need a solid foundation in
mathematics and statistics this includes
knowledge of linear algebra calculus
probability Theory and statistical
analysis understanding these Concepts is
crucial for Designing and implementing
machine learning algorithms now in
addition to these technical skills
machine learning Engineers should also
possess strong analytical and soft
skills to excel in their roles they need
to think critically and out of the box
to understand problems break them down
into manageable components and develop
appropriate machine learning solutions
they should be able to anal data
identify patterns and make informed
decisions based on the results and
finally communication skills now
effective communication is crucial for
machine learning Engineers they need to
expand complex technical Concepts to
non-technical stakeholders as well while
collaborating with cross functional
teams and present their findings and
recommendations strong written and
verbal communication skills will surely
help convey information clearly and
concisely which not only enhance their
professional growth but also contribute
to their over overall effectiveness as a
machine learning engineer with all that
having said let us now discuss some of
the salary and job prospects of a
machine learning engineer now the salary
for machine learning engineer can vary
significantly based on V various factors
in United States of America machine
learning Engineers are in high demand
particularly in technology hubs like
Silicon Valley in California and Tech
hubs like New York according to a report
by glass door the average salary of a
professional ml engineer is around
$119,000 per year now salaries in the
tend to be higher due to the cost of
living and fierce competition for talent
and entry-level machine learning
Engineers can expect salaries ranging
from $80,000 to $120,000 per year while
mid-level professionals with a few years
of experience can around $10,000 to
$180,000 per year and finally a senior
level or an experienced machine learning
engineer can command salaries exceeding
$200,000 per year as well especially in
Top tire companies or leadership
positions on the other hand in India the
Sal salary range for machine learning
Engineers is relatively lower compared
to the US but it's important to consider
the lower cost of living as well now the
average salary of a machine learning
engineer in India is around 8.5 lakhs
while entry-level machine learning
engineers in India can expect salaries
ranging from 4 lakhs to 8 lakhs per year
and mid-level professionals can ear on
on somewhere between 8 lakhs to 15 lakhs
per year and finally senior level
machine learning Engineers with
extensive experience and expertise can
earn salaries ranging from 15 lakh to 30
lakhs per year now additionally other
benefits and compensation components
such as bonuses stock options Healthcare
benefits and retirement plan should also
be considered but let's talk about
reality it's crucial to note that these
figures are approximate and can vary
based on various such factors so here
are some of the salary deciding factors
of an machine learning engineer firstly
company and location now the location of
the employing company can impact a
machine learning engineer salary large
technology companies or organizations
with a strong Focus on& ml tend to offer
competitive compensation packages to
attract top talent Additionally the cost
of living in different Geographic areas
can affect salary ranges with salaries
often being higher intake hubs or cities
with a higher cost of living work
experience experience plays a
significant role in determining the
salary of an ml engineer generally
Engineers with more years of experience
tend to command higher salaries this is
because experience indicates a deeper
understanding of machine learning
Concepts techniques and real world
applications which makes experienced
Engineers more valuable to the employers
out there skill set and expertise now
specific skills and expertise possessed
by a machine learning engineer can
impact their salaries as well ml
Engineers proficient in popular
programming languages like python R Java
and Frameworks like tensorflow and py to
and librar such as psyit learn are often
in high demand and can negotiate higher
salaries additionally specialized
knowledge in areas like deep learning
natural language processing computer
vision can also be valuable and
potentially result in higher
compensation and finally the industry
that you're working on and the current
demand Trends now the industry in which
an machine learning engineer Works can
influence their salary as well
industries that heavily rely on machine
learning and AI applications such as
Finance Healthcare e-commerce and
autonomous vehicles offer higher salary
packages to machine learning Engineers
due to the high demand for their
expertise also it's important to note
that these relative importance of these
factors may vary depending on the
specific circumstances and job market
conditions salaries can also be
influenced by economic factors market
demand and individual negotiation skills
overall machine learning engineers in
both us India and the rest of the world
can earn competitive salaries and it's
essential to assess the overall
compensation package growth
opportunities and Company culture when
evaluating job
opportunities artificial intelligence is
a vast topic and it is something which
will have to mark Master bit by bit
however there are many basic projects
that you can take up as a beginner in AI
there are plenty of resources online but
sometimes as a beginner in AI doesn't
know where to start this video will help
you to get interesting ideas on projects
in AI hey everyone welcome to Simply
learn before moving on please make sure
you subscribe to Simply learn's YouTube
channel and press the Bell icon to never
miss any updates let us Define
artificial intelligence First artificial
intelligence is a technique of turning a
computer based robot to work and act
like humans now let me list out
important AI projects for you pH
detection system it is a form of
biometric recognition a method for
identifying or confirming someone's
identity by glancing at their face is
called facial recognition people can be
identified by securing a mats on facial
ID using this technique realtime visuals
videos and photos can be the sources to
run face detection this technology is
mostly employed in security and law
enforcement open CV is the best
technology to create it a python package
called open CV is made specifically to
address computer vision jobs computer
vision is a process used in the
processing of images by computers it is
concerned with the in-depth
comprehension of digital ital photos or
films it is necessary to automate
operations that can be performed by
human visual systems therefore a
computer should be able to identify
items like a statue or a lamp poost or
even the face of a human being second
one is chatbot it is the best idea if
you have chosen chatbot as your project
topic this will make your resume more
attractive in case you're looking for a
job an oracle survey suggests that 80%
of the businesses uses chatbot you can
use Python Java Ruby C++ or PHP as a
programming language to develop a
chatbot designing and building NLP
chatbots that accept speech and Text
data is made easier by dialog flow you
can go through many projects to get the
rough idea there are many platforms
which will help you to build a chat bot
regardless of how excellent your chat is
there is always room for development the
finest chatboard developers constantly
enhance their Bots over the time using
Ai and machine learning social media
recommendation system the rise of web
services like Netflix Amazon and YouTube
has increased the use of recommended
systems in our daily lives they are
algorithms that insist users in finding
information that is pertinent to them
recommender systems are important in
some forms forms since they can generate
a lot of income or allow you to set
yourself apart from rivals in order to
provide recommendations it evaluates the
relationship between the user and the
object as well as the parallels between
users and positions coming to predicting
stock this application is widely
applicable everywhere as AI career
aspirants one will love to develop stock
prediction applications as it is full of
data this project would be ideal for
students who want to work in the finance
industry because it it can provide I
repeat because it can provide them a
better understanding of various aspects
of the field coming to medical diagnosis
this project is advantageous from a
medical standpoint AI projects can be
created to detect heart-based diseases
and also detect cancer it is intended to
offer patients with heart illness online
medical advice and guidance after
processing the data this system will
search the database for any illnesses
that might be connected to the given
details using data mining techniques
this intelligent system determines the
disease that the patient's information
most closely
resembles based on the system diagnosis
users can then speak with qualified
medical professionals users of the
system can also view information about
various doctors coming to an important
project that is search engine search
engines are utilized by all we look for
information on the greatest product to
buy a nice area to hang out or solutions
to any questions we have MLP is quite
significant in modern search engines
because a lot of language processing
takes place there python is the widely
used language to develop any search
engine search engine is mainly confined
with lots and lots of data it is helpful
for any AI career aspirant next next is
virtual assistants here the challenge is
to build a virtual assistant to assist
user why do you need virtual assistant
in your devices when you are building
your own it is also interesting for a ml
developer to build a virtual assistant
it involves NLP and data mining
voice-based virtual assistants are
popular today because they make life
easier for users NLP is utilized to
comprehend human language in in order to
construct this system when a voice
command is received the system will
translate it into machine language and
store the commands in its database hate
speech detection in social media
automated hate speech detection is a
crucial weapon in the fight against hate
speech propagation especially on social
media for the job many techniques have
been developed including a recent
explosion of deep learning based system
for the objective of detecting hate
speech a number of techniques have been
investigated including conventional
classifiers classifiers based on deep
learning and combination of both of them
on the other hand a number of data set
benchmarks including Twitter sentiment
analysis have been introduced and made
available for the evolution of the
performance of these algorithms and the
last one is predicting house price you
will need to estimate the sale price of
a brand brand new home in any place for
this assignment the data set for this
project includes information on the cost
of homes in various City neighborhoods
the UCI machine learning repository is
the place where you may find the data
set needed for This research you will
also receive other data set with
information on the age of the population
the city's crime rate and the location
of non- retail Enterprises in addition
to the pricing of various residences
it's an an excellent project to test
your knowledge if you are an up finding
a suitable job in the field of machine
learning is becoming increasingly
difficult the ideal way to display your
machine learning skill is in the form of
portfolio of data science and machine
learning projects a solid portfolio of
projects will illustrate that you can
utilize those machine learning skills in
your profile as well projects like movie
decementation system fake news detection
and many more are the best way to
improve your early programming skills
you may have the knowledge but putting
it to the use what is keep you
competitive here are 10 machine Lear
project that can increase your portfolio
and enable you to acquire a job as a
machine learning engineer at number 10
we have loan approval prediction system
in this machine learning project we will
analyze and make prediction about the
loan approval process of any person this
is a classification problem in which we
must determine whether or not the loan
will be approved a classification
problem is a predictive modeling problem
that predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news ml is the answer you will able to
tell the difference between real and
fake news by practicing this project of
detecting fake news this ml project for
detecting fake news is concerned with
the fake news and the true news on our
data set we create a tfid vectorizer
with escalan the model is is then fitted
using a passive aggressive classifier
that has been initialized finally the
accuracy score and the confusion matrics
indicate how well our model performs the
link for the project is in the
description box below at number eight we
have personality prediction system the
idea is based on determining an
individual personality using machine
learning techniques a person personality
influences both his personal and
professional life nowadays many company
are shortlisting applicant based on
their personality which increases job
efficiency because the person is working
on what is good at rather than what is
compelled to do in our study we
attempted to combine personality
prediction system using machine learning
techniques such as SVD na base and
logistic regression to predict a
personal personality and talent
prediction using phrase frequency method
this model or method allows users to
recognize their personality and
Technical abilities easily to learn
about more this project check the link
in the description box below at number
seven we have Parkinson disease system
Parkinson disease is a progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and
affects more than 1 million worldwide
each other in this machine learning
project we will develop an svm model
using python modules psychic learn numpy
and pandas and svm we will import the
data extract the features and label and
scale the features split the data set
design an an model and calculate the
model accuracy and at the end we will
check the Parkinson disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text to speech apis are
available for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3
before API various pre-rain models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below at number five we have speech
recognition system speech recognition
often known as spe to text is the
capacity of a machine or program to
recognize and transfer word spoken
allowed into readable text MLS spe
recognition uses algorithm that model
speech in terms of both language and
sound to extract the more important
parts of the the speech such as words
sentences and acostic modeling is used
to identify the phenomes and the
phonetics on the speech for this project
we will utilize pyttsx3 pyttsx3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and Python 3 to learn more about this
project check the link in the
description box below at number four we
have sentiment analysis sentiment
analysis also known as opinion mining is
a straightforward process of determining
the author's feeling about text what was
the user intention when he or she wrote
something to determine what could be
personal information we employ a variety
of natural language processing and text
analysis technology we must detect
extract and quantify such information
from the text to enable classification
and data manipulation in this project we
will use the Amazon customer review data
set for the sentiment analysis check the
link in the description box below at
number three we have image
classification using CNN deep learning
is a booving field currently most
projects and problem statement used deep
learning is an any sort of work many of
you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
classification in this project to learn
more about this project check the link
in the description box below at number
two we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use open CV and phas
recognition libraries to create a phase
detection system opencv provides a
real-time computer vision tool library
and Hardware we can create amazing
real-time projects using open CV to
learn how to create face recognition
system for you check the link in the
description box below and last but not
the least we have movie recommendations
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendation are often made based on a
viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens data set
generated by the more than 6,000 users
to learn how to create movie
recommendation system for yourself or
for your loved ones check the project in
the description box below hello everyone
welcome to this video on what is mlops
by simply learn and let me tell you guys
that we have regular updates on multiple
Technologies if you are a tech geek in a
continuous earn for the latest
technological Trends then consider
getting subscribed to a YouTube channel
and press that Bell icon to never miss
any update from Simply learn in this
video first we will see what mlops is
after that we will cover the use of
mlops after that we will see some real
world application of mlops after that we
will see components of mlops at the end
we will see difference between mlops and
devops by the end of this video I can
assure you that all your questions and
doubts related to what is mlops will
have been clear on that note if you are
an aspiring AIML engineer then there is
no better time to train yourself in the
exciting field of machine learning if
you're are looking for course that
covers everything from the fundamental
to Advanced Techniques like machine
learning algorithm development and
unsupervised learning look no further
than our ktech program in partnership
with I
this AIML program boot camp is
collaboration with keltech will help you
advance your career as an AIML
specialist the AIML boot camp includes
live classes delivered by industry
expert Hands-On legs industry relevant
projects and master classes by celtech
professors key feature of this amazing
course includes earn up to 22 cus from
CCH ctme online convocation by CCH ctm
program director simply learn Career
Services help you get noticed by top
hiring companies earn a boot camp ctech
certificate industry relevant Capstone
project in three domains ctech ctm
Circle membership 25 plus Hands-On
project across Industries verticals with
integrated Labs so why wait join now
seats are feeling fast find the course
link from the description box below so
moving forward let's see what is mlops
mlops short for machine learning
operation is a set of practices tools
and principles that aim to streamline
and enhance the deployment management
and monitoring of machine learning
models in production of environments it
draws inspiration from devops
development and operation practices and
applies them specifically to the
challenges of deploying and maintaining
ml model effectively moving forward
let's see what is the use of mlops mlop
serves crucial purposes in the context
of deploying and managing ml models in
production environment the first one is
efficient development mlops helps
automate the deployment process of
machine learning models making it faster
repeatable and less error Pro own this
enables the organization to quickly
transition from development to
production reducing the time it takes to
make model available to end users the
second one is collaboration mlops
encourages collaboration between
different teams such as data scientist
machine learning engineer operation and
business stakeholders by frosting better
communication and Alignment mlops helps
deliver solution that meets Technical
and business requirement the third one
is monitoring and maintenance mlops
provides tools for monitoring the
performance of of deployed models in
real world condition this allows teams
to detect anomalies track key
performance metrics and address issue
prompty ensuring that the models
continue to deliver accurate result over
time the fourth one is cost efficiency
by optimizing resource utilization and
automatic process mlops can lead to cost
saving efficiently manage deployment
reduce operational overhead and resource
vted making machine learning project
more economically viable the fifth one
is Security in many Industries
compliance and security are Paramount
concern mlops provides practices for
managing access controls data privacy
and regular environment ensuring
deployed model adhere to necessary
standard so in summary mlops plays a
critical role in maximizing the value of
machine learning by ensuring that models
are efficiently deployed monitored
maintained and improved in production
environment it enables organization to
drive consistent reliable and scalable
benefits from their machine learning
investment more moving forward let's see
some real world application of mlops so
mlops find application in various areas
where machine learning models are
deployed in real world production
environment some typical application of
mlops include the first one is
Predictive Analytics mlops helps deploy
Predictive Analytics that analyze
historical data to predict future events
these models are used in finance market
sales and other Industries to forecast
Trends and optimize decision making the
second one is image video analysis
mlops is used to application like image
classification object detection and
facial recognition these Technologies
are employed in security system
autonomous vacus and Medical Imaging the
third one is Healthcare and medical
diagnostic mlops AIDS in development and
deploying models for medical imaging
analysis disease diagnosis drug
Discovery and patient risk assessment
the fourth one is autonomous system
mlops is crucial for autonomous systems
like wle drones and Robotics in ening
that AI model function reliable and
adapt to changing condition so these are
some just a few example of mlop and the
explication of mlops are continuously
expanding as organization explore new
ways to leverage machine learning in
their operation so accelerate your
career in AIM with Comprehensive
postgraduate program in AIML machine
learning boost your career with this
AIML course delivered in collaboration
with part University and IBM learn in
demand skills such as machine learning
deep learning NLP computer vision
reinforcement learning generative by
prompt engineering chat gity and many
more you will receive a prestigious
certificate and ask me anything session
by IBM with five Capstone in different
domains using real data set you will
gain practical experience master classes
by P University and perdu faculty and
IBM experts ensure Top Notch education
simply and job assist help you get
noticed by Leading companies so why wait
join now seats are filling fast course
link is in the description box below and
in the pin comment moving forward let's
see component of mlops the extent of
mlops in machine learning projects can
vary based on the project needs
sometimes mlops covers everything from
preparing the data to making the final
model other times it might just involve
putting the model into action most
companies use mlops for first one is
explored data analysis looking into the
data to understand it the second one is
getting the data ready and making
valuable features from it data prepation
and feature engineering the third one is
model training and the fine tuning the
fourth one is model review and
governance fifth one is model inference
and serving model sixth one is model
deployment and monitoring the last one
is teaching the model again
automatically when needed moving forward
let's see some difference between mlops
and devops the first one is mlops
specifically focus on the deployment
monitoring and management of machine
learning models while devops devops Prim
focus on the collaboration and
integration between development and
operation the second one is it is
centered around machine learning AI
application addressing the unique
challenges of deploying and maintaining
models while in devops it applies to
software development in in general
covering all aspects of software
engineering and deployment the third one
is mlops involves automating the
endtoend machine learning life cycle
from data pre-processing to data model
monitoring and retraining ha while
devops aims to steam line the deployment
life cycle automate deployment pipelines
improve collaboration and ensure faster
and more Reliable Software releases and
the the fourth one is mlop employs tools
for model versioning model packaging
model monitoring automated retraining
and model explanation while develops
employes tools like Version Control
Systems like G continuous integration
continuous deployment cicd pipelines
configuration management and
infrastructure as scod to achieve its
goal the last one is efficient model
deployment continuous model performance
monitoring reproductivity collaboration
between data scientists and engineers
and adapting to changing data and
requirements here the faster development
Cycles continuous integration quick and
automated deployment improve team
collaboration and increase software
reliability in summary while devops and
mlops share concept of automation
collaboration and continuous Improvement
devops has a broader focus on software
development and deployment while mlops
is tailors to change the challenges
specific to deployment and managing
machine learning models in production
setting both practices aims to enhance
efficiency collaboration and reliability
in their respective domain and we also
have a boot camp for you guys and if you
are one of the aspiring AI ml Enthusiast
looking for online training and
graduating from the best universities or
a professional who elicits to switch
careers in Ai and ml by learning from
the experts then try giving a short to
Simply learns keltech Ai and machine
learning boot camp the course link or
the boot camp link is mentioned in the
description box that will never get you
to the course page where you can find a
complete overview of the program being
offered today we are going to discuss
the top 10 AI tools for business in 2023
but before we begin if you haven't
already then consider getting subscribed
to our YouTube channel and hit the Bell
to get notified and also if you are an
aspiring artificial intelligence
engineer looking for online training and
certifications from the prestigious
universities and collaborating with
leading experts to have the most robust
Foundation skills knowledge and what it
takes to be the best then search no more
simply launch postgraduate program in
artificial intelligence from from her
University in collaboration with IBM and
simply learns artificial intelligence
and machine learning boot camp from CTIC
University should be your right choice
for more details use the links in the
description box below with that in mind
let's get started with the first one
that is clickup clickup is an AI powered
assistant that is integrated to the
clickup productivity platform it can
help you with a variety of tasks
including generating text translating
languages answering questions
brainstorming ideas summarizing
documents and much more when it comes to
generating text clickup AI can generate
text for a wide variety of purposes such
as writing blog post creating marketing
copy or summarizing documents and when
it comes to translating languages
clickup AI can translate text between a
variety of languages moving to answering
questions clickup AI can answer your
questions about a variety of topics such
as project management productivity or
business and next we have brainstorming
ideas clickup AI can help you brainstorm
ideas for projects products and
marketing campaigns and lastly
summarizing documentations clickup AI
can summarize documents into a concise
and easy to understand format clickup AI
is still under development but it is
already a powerful tool that can help
you save time and improve your
productivity here are some key features
of clickup it is accessible via web
desktop and mobile apps 100 plus
role-based AI tools with ready toade
prompts slri command quickly to
brainstorm your ideas access the AI
command from the text tool bar to edit
text summarize long form content with
click of a button it can integrate with
click a productivity platform and can
help you with a wide variety of tasks
such as generating text translating
languages answering questions
brainstorming ideas and summarizing
documentations now moving ahead into the
secondary tool for today's discussion
which is about chat GPT chat GPT is an
AI powered language model developed by
open AI it is capable of generating
humanlike text based on context and past
conversations it is still under
development but it has already been used
for a variety of purposes such as
chatting with humans generating text
translating languages answering
questions let's discuss each one of
these chatting with humans chat GPT can
hold conversations with humans elit in a
limited capacity it can answer questions
follow instructions and even tell
stories when it comes to generating text
chat gbt can generate text for a wide
variety of purposes such as writing blog
posts creating marketing copy or
summarizing documents it can also
translate languages and answer all of
your questions some of the key features
of chat GPT are as follows it has a
clean and minimalistic UI which makes it
extremely easy to use it has natural
language processing it has previous
conversation details about an
interactive which is two-way dialogs and
then it supports 50 plus languages that
is spoken languages and it has a vast
knowledge of Base spanning multiple
Industries and also has vast knowledge
based subject areas now we will move
into the next tool for today's
discussion that is doll e or doll E2 is
a text to image Fusion model created by
open AI it can generate realistic images
from text descriptions for example you
can give it a prompt like a cat riding a
unicycle on a rainbow which will
generate an image of exact that dolly is
still considered to be under development
but it has already been used to create
some amazing images it has the potential
to be used for a variety of purposes
such as creating art designing products
generating Market materials educ
educating people when it comes to
creating art Dolly 2 can be used to
create original artwork such as
paintings drawings and sculptures when
it comes to designing products Dolly to
can be used to design new products such
as clothing furniture or toys when it
comes to generating marketing materials
Dolly 2 can be used to generate
marketing materials such as posters
flyers and advertisements and lastly
when it comes to educating people Dolly
to can be used to create educational
material such as illustrations and and
diagrams some of the most important key
features of dolly2 are it is very
intuitive interface that makes it easy
to use it can generate unique and high
quality images in seconds the native
editing tool lets you generate text
prompts for replacing different elements
in a picture and it has an out painting
feature that lets you expand the canvas
now let's proceed into the fourth tool
for today's discussion that is bricka
bra bricka bra AI is an AI powered app
generator that allows you to create web
applications games and tools without
coding it uses chat GPT 4 natural
language processing technology to
convert text descriptions into fully
functional web apps with a responsive
user interface breaker bra offers a free
plan that allows users to create up to
six apps per month and paid plans that
start at $68 per month here are some key
Fe features of Bricker bra it is a
time-saving and costeffective AI tool
for hiring professional and experienced
developers the buil-in functional
applications from scratch in minutes
just using text an intuitive drag and
drop editor can let you create
customized AI tool to suit your needs
and requirements you can export your app
in file formats such as HTML CSS and
JavaScript it has unlimited free web app
hosting capabilities
now we will be proceeding into the next
tool for today's discussion which is to
Tom is an AI powered storytelling tool
that helps you create and share
immersive narratives quickly and easily
it uses a combination of machine
learning and natural language processing
to generate presentations outlines and
stories with text images and other media
to AI is easy to use just type a
description of your desired presentation
or story into a text box and Tom AI will
generate it for you you can then
customize the content and style for your
liking to AI is a powerful tool that can
be used for a variety of purposes
including creating presentations for
your work or school writing posts or
articles developing marketing materials
pitching ideas to investors sharing
personal stories and much more to AI is
still under development but it is
already a valuable tool for anyone who
wants to create and share their ideas in
a more engaging way some of the key
features of Tom AI are as follows it can
create a complete presentation with text
prompt only adds specific slides pages
to a presentation with natural language
prompts converts and upload a document
into a presentation with a click of a
button it can integrate with the dolly
to to produce unique AI images for your
presentation embedded live Pages for any
website or external apps and lastly it
has sharing and commenting features for
collaborating with your team now
proceeding into the next tool which is
second brain second brain AI is an AI
powered writing companion that helps you
to write better articles emails tweets
messages and much more it uses the d003
text model from open AI to generate text
and rephrase existing text it also
offers text generation based on a prompt
and creating custom AI a tasks second AI
is still under development but it has a
wide potential to be a powerful tool for
anyone who wants to improve their
writing skills here are some key
features of second brain it can be
helpful to training your new chatboard
and embedding it on your side page the
chatbot can be trained with data from 90
different languages it quickly and
accurately response to your customer
queries you can also create multiple
Bots depending on your paid plan and
lastly it is great for entrepreneurs if
you are looking for a way to improve
your writing skills second Brin AI is a
great option to consider it is easy to
use and affordable and it has potential
to help you write better articles email
and other content now let us proceed
into the next tool that is Jasper AI
Jasper AI is an AI writing assistant
that helps you create high quality
content quickly and easily it uses a
variety of AI models to generate text
translate languages write different
kinds of creative content and answer
your queries in an informative way
Jasper AI is a powerful tool that can be
used for a variety of purposes including
writing blog posts and articles creating
marketing copies translating languages
answering customer questions generating
creative content such as poems code
scripts musical pieces emails letters
Etc some of the key features of Jasper
AI are it has 50 plus templates to speed
up the content creat process it can
generate content in 30 plus languages it
lets you repurpose content for multiple
platforms it has built-in collaboration
tools for working with your team it can
find un content output to match your
Brand's tone and style and lastly it can
integrate with grammarly copy space and
Surfer SEO Jasper AI is still under
development but it has already been used
for businesses of all sizes to improve
their content marketing customer service
and much more if you are looking for a
way to improve your content creation
Jasper AI is a great option to consider
it is easy to use and affordable and it
has the potential to help you create
better content that will engage your
audience and help you achieve your
business goals next is plus AI plus AI
is an AI tool that helps businesses
automate their reporting and analytics
it uses a wide variety of AI models to
extract insights from data generate
reports and create dashboards plus AI is
a powerful tool that can be used for a
variety of purposes including automating
reporting and analytics tasks extracting
insights from data generating reports
and dashboards improving decision making
reducing costs the key features of plus
AI are as follows it is part by the
latest in generative AI it has
integration between Google slice and
PowerPoint which is seamless it can
create a presentation that needs only
minor editing it has the ability to
rewrite content on slides which is a
biggest game changer plus AI is still
under development but it has already
been used by businesses of old sizes to
improve their reporting and analytics
capabilities now let's proceed into the
ninth one which is about fireflies
fireflies is an AI powered note maker
that can transcribe summarize and
analyze meetings it uses a combination
of machine learning and natural language
processing to automatically capture and
transcribe meetings and then generate
summaries and insights that you can use
to stay on top of your work here are
some things that you can do with
fireflies AI stay on top of your work by
automatically transcribing and
summarizing your meetings get insights
into your team discussions by analyzing
meeting transcripts share meeting
transcripts and insights with your team
members use meeting transcripts to
create presentations or reports and some
of the key features of fireflies are as
follows it it can record and transcribes
calls instantly it has Chrome extension
to capture all your meetings it has
simple to use search that allows you to
review your calls an easy to use meeting
P to invite fir flight pods to a meeting
transcribe existing audio files
instantly inside the dashboard offer
native integration to dialers zapier and
other apis and lastly it eliminates the
task of note taken if you're looking for
AI tool that can help you improve your
productivity firefli AI is a great
option to consider it is easy to use and
affordable and it has the potential to
save time and stay on top of your work
now the last tool for today's discussion
which is speechify speechify is an AI
powered text to speech DTS tool that
helps you read and listen to text more
efficiently it can read allow any text
you provide including PDFs articles
websites and social media posts spe Fey
also offers a variety of features to
customize your listening experience such
as adjusting the reading speed voice and
background noise some of the key
features of speechify are it has web
based with chrome and Safari extensions
more than 15 languages are supported by
spech IFI it has over 30 voices to
select from and it can scan and convert
printed text to speech speechify is an
excellent tool for anyone who wants to
improve their reading comprehension ion
Focus or productivity it can also be
helpful for people with dyslexia ADHD or
other learning disabilities and AI tools
for business in 2023 but before we begin
if you haven't already then consider
getting subscribed to our YouTube
channel and hit the Bell to get notified
and also if you are an aspiring
artificial intelligence engineer looking
for online training and certifications
from the prestigious universities and
collaborating with leading experts to
have the most robust Foundation skills
knowledge and what it takes to be the
best then search no more simply launch
postgraduate program in artificial
intelligence from PD University in
collaboration with IBM and simply learns
artificial intelligence and machine
learning boot camp from calch University
should be your right choice for more
details use the links in the description
box below with that in mind let's get
started with the first one that is
clickup clickup is an AI powered
assistant that is integrated to the
clickup productivity platform it can
help you with a variety of tasks
including generating text translating
languages answering questions
brainstorming ideas summarizing
documents and much more when it comes to
generating text clickup AI can generate
text for a wide variety of purposes such
as writing blog post creating marketing
copy or summarizing documents and when
it comes to translating languages
clickup AI can translate text between a
variety of languages moving to answering
questions clickup AI can answer your
questions about a variety of topics such
as project management productivity or
business and next we have brainstorming
ideas clickup AI can help you brainstorm
ideas for projects products and
marketing campaigns and lastly
summarizing documentations clickup AI
can summarize documents into a concise
and easy to understand format clickup AI
is still under development but it is
already a powerful tool that can help
you to save time and improve your
productivity here are some key features
of clickup it is accessible via web
desktop and mobile apps 100 plus role
based AI tools with ready toade prompts
SLR command quickly to brainstorm your
ideas access the AI command from the
text tool bar to edit text summarize
long form content with click of a button
it can integrate with click a
productivity platform and can help you
with a wide variety of tasks such as
generating text transl languages
answering questions brainstorming ideas
and summarizing documentations now
moving ahead into the secondary tool for
today's discussion which is about chat
GPT Chad GPT is an AI powered language
model developed by open AI it is capable
of generating humanlike text based on
context and past conversations it is
still under development but it has
already been used for a variety of
purposes such as chatting with humans
generating text trans ating languages
answering questions let's discuss each
one of these chatting with humans chat
GPT can hold conversations with humans
Alit in a limited capacity it can answer
questions follow instructions and even
tell stories when it comes to generating
text chat gbt can generate text for a
wide variety of purposes such as writing
blog posts creating marketing copy or
summarizing documents it can also
translate languages and answer all of
your questions some of the key features
of chat GPT are as follows it has a
clean and minimalistic UI which makes it
extremely easy to use it has natural
language processing it has previous
conversation details about an
interactive which is two-way dialogues
and then it supports 50 plus languages
that is spoken languages and it has a
vast knowledge of Base spanning multiple
Industries and also has vast knowledge
based subject areas now we will move
into the next tool for today's
discussion that is doll e or doll E2 is
a text to image diffusion model created
by open AI it can generate realistic
images from text descriptions for
example you can give it a prompt like a
cat riding a unicycle on a rainbow which
will generate an image of exact that
dolly is still considered to be under
development but it has already been used
to create some amazing images it has the
potential to be used for a variety of
purposes such as creating art designing
products generating Market materials
educating people when it comes to
creating art Dolly 2 can be used to
create original artwork such as
paintings drawings and sculptures when
it comes to designing products Dolly 2
can be used to design new products such
as clothing furniture or toys when it
comes to generating marketing materials
Dolly 2 can be used to generate
marketing materials such as posters
flyers and advertisements and lastly
when it comes to educating people Dolly
2 can be used to create educational
material such as illustrations and
diagrams some of the most important key
features of dolly2 are it is very
intuitive interface that makes it easy
to use it can generate unique and high
quality images in seconds the native
editing tool lets you generate text
prompts for replacing different elements
in a picture and it has an out painting
feature that lets you expand the canvas
now let's proceed into the fourth tool
for today's discussion that is brecka
bra breaka bra AI is an AI powered app
generator that allows you to create web
applications games and tools without
coding it uses chat GPT 4 natural
language processing technology to
convert text descriptions into fully
functional web apps with a responsive
user interface breakup braack offers a
free plan that allows users to to create
up to six apps per month and paid plans
that start at $68 per month here are
some key features of Bricker Brack it is
a time-saving and cost-effective AI tool
for hiring professional and experienced
developers the built-in functional
applications from scratch in minutes
just using text an intuitive drag and
drop editor can let you create
customized AI tool to suit your needs
and requirements you can export your app
in file formats such as HTML CSS and
JavaScript it has unlimited free web app
posting capabilities now we will be
proceeding into the next tool for
today's discussion which is Tom Tom is
an AI powered storytelling tool that
helps you create and share immersive
narratives quickly and easily it uses a
combination of machine learning and
natural language processing to generate
presentations outlines and stories with
text images and other media to AI is
easy to use just type a description of
your desired presentation or story into
a text box and Tom AI will generate it
for you you can then customize the
content and style for your liking to AI
is a powerful tool that can be used for
a variety of purposes including creating
presentations for your work or school
writing posts or articles developing
marketing materials pitching ideas to
investors sharing personal stories and
much more to AI is still under
development but it is already a valuable
tool for anyone who wants to create and
share their ideas in a more engaging way
some of the key features of Tom AI are
as follows it can create a complete
presentation with text prompt only adds
specific slides pages to a presentation
with natural language prompts converts
and upload a document into a
presentation with a click of a button it
can integrate with the dolly2 to produce
unique AI images for your presentation
embedded live Pages for any website or
external apps and lastly it has sharing
and commenting features for
collaborating with your team now
proceeding into the next tool which is
second brain second brain AI is an AI
powered writing companion that helps you
to write better articles emails tweets
messages and much more it uses the DAV
003 text model from open AI to generate
text and rephrase existing text it also
offers text generation based on a prompt
and creating custom AI tasks second AI
is still under development but it has a
wide potential to be a powerful tool for
anyone who wants to improve their
writing skills here are some key
features of second brain it can be
helpful to training your new chatboard
and embedding it on your side page the
chatbot can be trained with data from 90
different languages
it quickly and accurately responds to
your customer queries you can also
create multiple Bots depending on your
paid plan and lastly it is great for
entrepreneurs so why machine learning
why do we even care about having these
computers come up and be able to do all
these new things for us well because
machines can now drive your car for you
still very in the infant stage but it's
just exploding as we see with uh
Google's whmo and then Uber had their
program which unfortunately crashed they
know that this is huge this is going to
be the huge industry to change our whole
Transportation infrastructure machine
learning is now used to detect over 50
eye diseases do you know how amazing
that is to have a computer that double
checks for the doctor for things they
might miss that's just huge in the
health industry pretty soon they
actually do already have that in some
areas where maybe not for eyes but for
other diseases where they're using the
camera on your phone to help
pre-diagnosed before you go and see the
doctor and because the machine can now
unlock your phone with your face I mean
that's just cool having it being able to
identify your face or your voice and be
able to turn stuff on and off for you
depending on where you're at and what
you need talking about an ultimate
automation our world we live in and as
we dig in deeper we have a nice example
of Facebook as you can see here they
have the Facebook post with Halloween
comment yes if you want it order here
nobody likes spam post on Facebook that
annoy them into interacting with likes
shares comments and other actions I
remember the original On's raw if you
don't click on here you will have bad
luck or some kind of Fear Factor well
this is a huge thing in a social media
when people are getting spammed and so
this tactic known as engagement bait
takes advantage of Facebook's Newsfeed
algorithm by choosing engagement in
order to get the greater reach to
eliminate engagement bait the company
reviewed and categorized hundreds of
thousands of posts to train a machine
learning model that detects different
types of Engagement bait so in this case
we have we're using Facebook but this is
of course across all the different
social media they have different tools
are building and the Facebook scroll GIF
will be replaced kind of like a virus
coming in there it notices that there's
a certain setup with Facebook and it's
able to replace it and they have like
vote baiting react baiting share baiting
they have all these different these are
kind of General titles but there
certainly are a lot of way of baiting
you to go in there and click on
something so they fed all this this data
was fed into the machine and then they
have the new post the new post comes up
that Takes Over part of the Facebook
setup and that's what you're looking at
you're looking at this new post that's
replaced like a virus has replaced that
so what Facebook do to eliminate this is
they start scanning for keywords and
phrases like this and checks the
clickthrough rate so it starts looking
for people who are clicking through it
without even looking at it or clicking
through it and it's not something that
normally would be clicked through once
Facebook has scanned for these keywords
and phrases it is now able to identify
the spam coming in and this makes your
life easier you're not getting spammed
it's not like walking through an airport
and in a lot of countries you have like
hundreds of people trying to sell you
time share come join us sign up for this
eliminates that annoyingness so now you
can just enjoy your Facebook and your
cat pictures or maybe it's your family
pictures mine is family certainly people
like their cat pictures too another good
example is Google's Deep Mind project
Alpha go a computer program that plays a
board game go has defeated the world's
number one go player and I hope I say
his name right kyg the ultimate go
challenge came a three of three was on
May 27th 2017 so that's just last year
that this happened and what makes this
so important is that you know go is just
is a game so it's not like you're
driving a car or something in our real
world but they are using games to learn
how to get the machine learning program
to learn they want it to learn how to
learn and that is a huge step a lot of
this is still in its infant stage as far
as development as we saw with happened
with the as I referred to earlier the
Uber cars they lost their whole division
because they jumped ahead too fast so
it's still an infant stage but boy is
this like the beginning of just an
amazing world that is automated in ways
we can't even imagine what tomorrow's
going to look like we've looked at a lot
of examples of machine learning so let's
see if we can give a little bit more of
a concrete definition what is machine
learning machine learning is the science
of making computers learn and act like
humans by feeding data and information
without being explicitly programmed we
see here we have a nice little diagram
where we have our ordinary system and
your computer nowadays you can even run
a lot of this stuff on a cell phone
because cell phones Advanced so much and
then with artificial intelligence and
machine learning it now takes the data
and it learns from what happened before
and then it predicts what's going to
come next and then really the biggest
part right now in machine learning
that's going on is it improves on that
how do we find a new solution so we go
from descriptive where it's learning
about stuff and understanding how it
fits together to predicting what it's
going to do to postcript coming up with
a new solution and when we're working on
machine learning there's a number of
different diagrams that people have
posted for what steps to go through a
lot of it might be very domain specific
so if you're working on Photo
identification versus language versus
medical or physics some of these are
switched around a little bit or new new
things are put in they're very specific
to The Domain this is kind of a very
general diagram first you want to Define
your objective very important to know
what it is you're wanting to predict
then you're going to be collecting the
data so once you've defined an objective
you need to collect the data that
matches you spend a lot of time in data
science collecting data and the next
step preparing the data you got to make
sure that your data is clean going in
there's the old saying bad data in bad
answer out or bad data out and then one
once you've gone through and we've
cleaned all this stuff coming in then
you're going to select the algorithm
which algorithm are you going to use
you're going to train that algorithm in
this case I think we're going to be
working with svm the support Vector
machine then you have to test the model
does this model work is this a valid
model for what we're doing and then once
you've tested it you want to run your
prediction you want to run your
prediction or your choice or whatever
output is going to come up with and then
once everything is set and you've done
lots of testing then you want to go go
ahead and deploy the model and remember
I said domain specific this is very
general as far as the scope of doing
something a lot of models you get
halfway through and you realize that
your data is missing something and you
have to go collect new data because
you've run a test in here someplace
along the line you're saying hey I'm not
really getting the answers I need so
there's a lot of things that are domain
specific that become part of this model
this is a very general model but it's a
very good model to start with and we do
have some basic divisions of what
machine learning does that's important
to know for instance do you want to
predict a category well if you're
categorizing thing that's classification
for instance whether the stock price
will increase or decrease so in other
words I'm looking for a yes no answer is
it going up or is it going down and in
that case we'd actually say is it going
up true if it's not going up it's false
meaning it's going down this way it's a
yes no 01 do you want to predict a
quantity that's regression so remember
we just did classification now we're
looking at regression these are the two
major division and what data is doing
for instance predicting the age of a
person based on the height weight health
and other factors So based on these
different factors you might guess how
old a person is and then there are a lot
of domain specific things like do you
want to detect an anomaly that's anomaly
detection this is actually very popular
right now for instance you want to
detect money withdrawal anomalies you
want to know when someone's making a
withdrawal that might not be their own
account we've actually brought this up
because this is really big right now if
you're predicting the stock whether to
buy stock or not you want to be able to
know if what's going on in the stock
market is an anomaly use a different
prediction model because something else
is going on you got to pull out new
information in there or is this just the
norm I'm going to get my normal return
on my money invested so being able
detect anomalies is very big in data
science these days another question that
comes up which is on what we call
untrained data is do you want to
discover structure in unexplored data
and that's called clustering for for
instance finding groups of customers
with similar Behavior given a large
database of customer data containing
their demographics and past buying
records and in this case we might notice
that anybody who's wearing certain set
of shoes goes shopping at certain stores
or whatever it is they're going to make
certain purchases by having that
information it helps us to Market or
group people together so that we can now
explore that group and find out what it
is we want to Market to them if you're
in the marketing world and that might
also work in just about any Arena you
might want to group people together
whether they're based on their different
areas and Investments and financial
background whether you're going to give
them a loan or not before you even start
looking at whether they're valid
customer for the bank you might want to
look at all these different areas and
group them together based on unknown
data so you're not you don't know what
the data is going to tell you but you
want to Cluster people together that
come together let's take a quick DeTour
for quiz time oh my favorite so we're
we're going to have a couple questions
here under our quiz time and um we'll be
posting the answers in the part two of
this tutorial so let's go ahead and take
a look at these quiz times questions and
hopefully you'll get them all right and
it'll get you thinking about how to
process data and what's going on can you
tell what's happening in the following
cases of course you're sitting there
with your cup of coffee and you have
your check box and your pen trying to
figure out what's your next step in your
data science analysis so the first one
is grouping documents in to different
categories based on the topic and
content of each document very big these
days you know you have legal documents
you have uh maybe it's a Sports Group
documents maybe you're analyzing
newspaper postings but certainly having
that automated is a huge thing in
today's world B identifying handwritten
digits and images correctly so we want
to know whether uh they're writing an A
or capital a BC what are they writing
out in their hand digit their
handwriting writing C behavior of a
website indicating that the site is not
working as designed D predicting salary
of an individual based on his or her
years of experience HR hiring uh setup
there so stay tuned for part two we'll
go ahead and answer these questions when
we get to the part two of this tutorial
or you can just simply write at the
bottom and send a note to Simply learn
and they'll follow up with you on it
back to our regular content and these
last last few bring us into the next
topic which is another way of dividing
our types of machine learning and that
is with supervised
unsupervised and reinforcement learning
supervised learning is a method used to
enable machines to classify predict
objects problems or situations based on
labeled data fed to the machine and in
here you see we have a jumble of data
with circles triangles and squares and
we label them we have what's a circle
what's a triangle what's a square we
have our model training and it trains it
so we know the answer very important
when you're doing supervised learning
you already know the answer to a lot of
your information coming in so you have a
huge group of data coming in and then
you have new data coming in so we've
trained our model the model now knows
the difference between a circle a square
a triangle and now that we've trained it
we can send in in this case a square and
a circle goes in and it predicts that
the top one's a square and the next
one's a circle and you can see that this
is uh being able to predict whether
someone is going to default on a loan
because I was talking about Banks
earlier supervised learning on stock
market whether you're going to make
money or not that's always important and
if you are looking to make a fortune on
the stock market keep in mind it is very
difficult to get all the data correct on
the stock market it is very it
fluctuates in ways you really hard to
predict so it's quite a roller coaster
ride if you're running machine learning
on the stock market you start realizing
you really have to dig for new data so
we have supervised learning and if you
have supervised we should need
unsupervised learning in unsupervised
learning machine learning model finds
the hidden pattern in an unlabeled data
so in this case instead of telling it
what the circle is and what a triangle
is and what a square is it goes in there
looks at him and says for whatever
reason it groups them together maybe
it'll group it by the number of corners
and it notices that a number of them all
have three corners a number of them all
have four corners and a number of them
all have no corners and it's able to
filter those through and group them
together we talked about that earlier
with looking at a group of people who
are out shopping we want to group them
together to find out what they have in
common and of course once you understand
what people have in common maybe you
have one of them who's a customer at
your store or you have five of them are
customer at your store and they have a
lot in common with five others who are
not customers at your store how do you
Market to those five who aren't
customers at your store yet they fit the
demograph of who's going to shop there
and you'd like them to shop at your
store not the one next door of course
this is a simplified version you can see
very easily the difference between a
triangle and a circle which is might not
be so easy in marketing reinforcement
learning reinforcement learning is an
important type of machine learning where
an agent learns how to behave in an
environment by performing actions and
seeing the result and we have here where
the in this case a baby it's actually
great that they used an infant for this
slide because the reinforcement learning
is very much in its infant stages but
it's also probably the biggest machine
learning demand out there right now or
in the future it's going to be coming up
over the next few years is reinforcement
learning and how to make that work for
us and you can see here where we have
our action in the action in this one it
goes into the fire hopefully the baby
didn't it was just a little candle not a
giant fire pit like it looks like here
when the baby comes out and the new
state is the baby is sad and crying
because they got burned on the fire and
then maybe they take another action the
baby's called the agent because it's the
one taking the actions and in this case
they didn't go into the fire they went a
different direction and now the baby's
happy and laughing and playing rein
enforcement learning is very easy to
understand because that's how as humans
that's one of the ways we learn we learn
whether it is you know you burn yourself
on the stove don't do that anymore don't
touch the stove in the big picture being
able to have machine learning program or
an AI be able to do this is huge because
now we're starting to learn how to learn
that's a big jump in the world of
computer and machine learning and we're
going to go back and just kind of go
back over supervised versus unsupervised
learning understanding this is H huge
because this is going to come up in any
project you're working on we have in
supervised learning we have labeled data
we have direct feedback so someone's
already gone in there and said yes
that's a triangle no that's not a
triangle and then you predict an outcome
so you have a nice prediction this is
this this new set of data is coming in
and we know what it's going to be and
then with unsupervised trading it's not
labeled so we really don't know what it
is there's no feedback so we're not
telling it whether it's right or wrong
we're not telling it whether it's a
triangle or a square we're not telling
it to go left or right all we do is
we're finding hidden structure in the
data grouping the data together to find
out what connects to each other and then
you can use these together so imagine
you have an image and you're not sure
what you're looking for so you go in and
you have the unstructured data find all
these things that are connected together
and then somebody looks at those and
labels them now you can take that label
data and program something to predict
what's in the picture so you can see how
they go back and forth and you can start
connecting all these different tools
together to make a bigger picture there
are many interesting machine learning
algorithms let's have a look at a few of
them hopefully this give you a little
flavor of what's out there and these are
some of the most important ones that are
currently being used we'll take a look
at linear regression decision tree and
the support Vector machine let's start
with a closer look at linear regression
linear regression is perhaps one of the
most well-known and well understood
algorithms in statistics and machine
learning linear regression is a linear
model for example a model that assumes a
linear relationship between the input
variables X and the single output
variable Y and you'll see this if you
remember from your algebra classes y
equal mx + C imagine we are predicting
distance traveled y from speed X our
linear regression model representation
for this problem would be y = m * x + C
or distance = M * speed plus C where m
is the coefficient and C is the Y
intercept and we're going to look at two
different variations of this first we're
going to start with time is constant and
you can see we have a bicyclist he's got
a safety gear on thank goodness speed
equals 10 m/ second and so over a
certain amount of time his distance
equals 36 kilm we have a second
bicyclist who's going twice the speed or
20 m/ second and you can guess if he's
going twice the speed and time is a
constant then he's going to go twice the
distance and that's easily to compute 36
* 2 you get 72 kilm and so if you had
the question of how fast would
somebody's going three times that speed
or 30 m/ second is you can easily
compute the distance in our head we can
do that without needing a computer but
we want to do this for more complicated
data so it's kind of nice to compare the
two but let's just take a look at that
and what that looks like in a graph so
in a linear regression model we have our
distance to the speed and we have our m
equals the ve slope of the line and
we'll notice that the line has a plus
slope and as speed increases distance
also increases hence the variables have
a positive relationship and so your
speed of the person which equals yal MX
plus C distance traveled in a fixed
interval of time and we could very
easily compute either following the line
or just knowing it's three times 10 m/s
that this is roughly 102 km distance
that this third bicep has traveled one
of the key definition conditions on here
is positive relationship so the slope of
the line is positive as distance
increase so does speed increase let's
take a look at our second example where
we put distance is a constant so we have
speed equals 10 m/ second they have a
certain distance to go and it takes him
100 seconds to travel that distance and
we have our second bicyclist who's still
doing 20 m/ second since he's going
twice the speed we can guess he'll cover
the distance in about half the time 50
seconds and of course you could probably
guess on the third one 100 ided 30 since
he's going three times the speed you can
easily guess that this is
33333 seconds time when we put that into
a linear regression model or a graph if
the distance is assumed to be constant
let's see the relationship between speed
and time and as time goes up the amount
of speed to go that same distance goes
down so now your m equals a minus ve
slope of the line as the speed increases
time decreases hence the variable has a
negative relation relationship again
there's our definition positive
relationship and negative relationship
dependent on the slope of the line and
with a simple formula like this um and
even a significant amount of data Let's
uh see with the mathematical
implementation of linear regression and
we'll take this data so suppose we have
this data set where we have xyx = 1 2 3
4 5 standard series and the yvalue is 3
22 43 when we take that and we go ahead
and plot these points on a graph you can
see there's kind of a nice scattering
and you could probably eyeball a line
through the middle of it but we're going
to calculate that exact line for linear
regression and the first thing we do is
we come up here and we have the mean of
XI and remember mean is basically the
average so we added five + 4 plus 3+ 2 +
1 and divide by five and that simply
comes out as three and then we'll do the
same for y we'll go ahead and add up all
those numbers and divide by five and we
end up with the mean value of y I equals
2.8 where the XI references it's an
average or means value and the Yi also
equals a means value of y and when we
plot that you'll see that we can put in
the yal 2.8 and the xal 3 in there on
our graph we kind of gave it a little
different color so you can sort it out
with the dash lines on it and it's
important to note that when we do the
linear regression the linear regression
model should go through that dot now
let's find our regression equation to
find the best fit line remember we go
ahead and take our y = mx plus C so
we're looking for M and C so to find
this equation for our data we need to
find our slope of M and our coefficient
of c and we have y = mx + C where m
equals the sum of x - x average time y -
y aage or y means and X means over the
sum of x - x means squared that's how we
get the slope of the value of the line
and we can easily do that by creating
some columns here we have XY computers
are really good about iterating through
data and so we can easily compute this
and fill in a graph of data and in our
graph you can easily see that if we have
our x value of one and if you remember
the XI or the means value is 3 1 - 3 = a
-2 and 2 - 3 = a -1 so on and so forth
and we can easily fill in the column of
x - x i y - Yi and then from no we can
compute x - x i^ 2 and x - x i * y - Yi
and you can guess it that the next step
is to go ahead and sum the different
columns for the answers we need so we
get a total of 10 for our x - x i^ 2 and
a total of two for x - x i * y - Yi and
we plug those in we get 210 which equals
.2 so now we know the slope of our line
equals 0 2 so we can calculate the value
of c that'd be the next step is we need
to know where crosses the Y AIS and if
you remember I mentioned earlier that
the linear regression line has to pass
through the means value the one that we
showed earlier we can just flip back up
there to that graph and you can see
right here there's our means value which
is 3 x = 3 and Y = 2.8 and since we know
that value we can simply plug that into
our formula y = 2x + C so we plug that
in we get 2.8 = 2 * 3 + C and you can
just solve for C so now we know that our
coefficient equals 2.2 and once we have
all that we can go ahead and plot our
regression line Y = 2 * x + 2.2 and then
from this equation we can compute new
values so let's predict the values of Y
using x = 1 2 3 4 5 and plot the points
remember the 1 2 3 4 5 was our original
X values so now we're going to see what
y thinks they are are not what they
actually are and when we plug those in
we get y of designated with Y of P you
can see that x = 1 = 2.4 x = 2 = 2.6 and
so on and so on so we have our y
predicted values of what we think it's
going to be when we plug those numbers
in and when we plot the predicted values
along with the actual values we can see
the difference and this is one of the
things is very important with linear
aggression in any of these models is to
understand the error and so we can
calculate the error on all of our
different Val values and you can see
over here we plotted um X and Y and Y
predict and we drawn a little line so
you can sort of see what the error looks
like there between the different points
so our goal is to reduce this error we
want to minimize that error value on our
linear regression model minimizing the
distance there are lots of ways to
minimize the distance between the line
and the data points like sum of squared
errors sum of absolute errors root mean
square error Etc we keep moving this
line through the data points to make
sure the best fit line has the least
Square distance between the data points
and the regression line so to recap with
a very simple linear regression model we
first figure out the formula of our line
through the middle and then we slowly
adjust the line to minimize the error
keep in mind this is a very simple
formula the math gets even though the
math is very much the same it gets much
more complex as we add in different
dimensions so this is only two
Dimensions y = mx plus C but you can
take that out to X
Z JQ all the different features in there
and they can plot a linear regression
model on all of those using the
different formulas to minimize the error
let's go ahead and take a look at
decision trees a very different way to
solve problems in the linear regression
model decision tree is a tree-shaped
algorithm used to determine a course of
action each branch of a tree represents
a possible decision occurrence or
reaction we have data which tells us if
it is a good day to play golf and if we
were to open this data up in a general
spreadsheet you can see we have the
Outlook whether it's a rainy overcast
Sunny temperature hot mild cool humidity
windy and did I like to play golf that
day yes or no so we're taking a census
and certainly I wouldn't want a computer
telling me when I should go play golf or
not but you could imagine if you got up
in the night before you're trying to
plan your day and it comes up and says
tomorrow would be a good day for golf
for you in the morning and not a good
day in the afternoon or something like
that this becomes very beneficial and we
see this in a lot of applications coming
out now where it gives you suggestions
and lets you know what would uh fit the
match for you for the next day or the
next purchase or the next uh whatever
you know next mail out in this case is
tomorrow a good day for playing golf
based on the weather coming in and so we
come up and let's uh determine if you
should play golf when the day is sunny
and windy so we found out the forecast
tomorrow is going to be sunny and windy
and suppose we draw our tree like this
we're going to have our hum humidity and
then we have our normal which is if it's
if you have a normal humidity you're
going to go play golf and if the
humidity is really high then we look at
the Outlook and if the Outlook is sunny
overcast or rainy it's going to change
what you choose to do so if you know
that it's a very high humidity and it's
sunny you're probably not going to play
golf because you're going to be out
there miserable fighting off the
mosquitoes that are out joining you to
play golf with you maybe if it's rainy
you probably don't want to play in the
rain but if it's slightly overcast and
you get just the right
Shadow that's a good day to play golf
and be outside out on the green now in
this example you can probably make your
own tree pretty easily because it's a
very simple set of data going in but the
question is how do you know what to
split where do you split your data what
if this is much more complicated data
where it's not something that you would
particularly understand like studying
cancer they take about 36 measurements
of the cancerous cells and then each one
of those measurements represents how
bulbous it is how extended it is how
sharp the edges are something that as a
human we would have no understanding of
so how do we decide how to split that
data up and is that the right decision
tree but so that's the question is going
to come up is this the right decision
tree for that we should calculate
entropy and Information Gain two
important vocabulary words there are the
entropy and the Information Gain entropy
entropy is a measure of Randomness or
impurity in the data set entropy should
be low so we want the chaos to be as low
as possible we don't want to look at it
and be confused by the images or what's
going on there with mixed data and the
Information Gain it is the measure of
decrease in entropy after the data set
is split also known as entropy reduction
Information Gain should be high so we
want our information that we get out of
the split to be as high as possible
let's take a look at entropy from the
mathematical side in this case we're
going to denote entropy as I of P of and
N where p is the probability that you're
going to play a game of golf and N is
the probability where you're not going
to play the game of golf now you don't
really have to memorize these formulas
there's a few of them out there
depending on what you're working with
but it's important to note that this is
where this formula is coming from so
when you see it you're not lost when
you're running your programming unless
you're building your own decision tree
code in the back and we simply have a
log 2 of P Over p+ N minus n / P plus n
* the log square of n of p plus n but
let's break that down and see what
actually looks like when we're Computing
that from the computer script side
entropy of a target class of the data
set is the whole entropy so we have
entropy play golf and we look at this if
we go back to the data you can simply
count how many yeses and no in our
complete data set for playing golf days
in our complete set we find we have five
days we did play golf and nine days we
did not play golf and so our I equals if
you add those together 99 + 5 is 14 and
so our I equals 5 over 14 and 9 over 14
that's our p and N values that we plug
into that formula and you can go 5 over
14 = 36 9 over 14 = 64 and when you do
the whole equation you get the minus. 36
logun 2ar of 36 -64 log s < TK of 64 and
we get a set value we get
.94 so we now have a full entropy value
for the whole set of data that we're
working with and we want to make that
entropy go down and just like we
calculated the entropy out for the whole
set we can also calculate entropy for
playing golf in the Outlook is it going
to be overcast or rainy or sunny and so
we look at the entropy we have P of
Sunny time e of 3 of two and that just
comes out how many sunny days yes and
how many sunny days no over the total
which is five don't forget to put the
will divide that five out later on uh
equals P overcast = 4 comma 0 plus rainy
= 2A 3 and then when you do the whole
setup we have 5 over 14 remember I said
there was a total of five 5 over 14 *
the I of 3 of 2 + 4 over 14 * the 4
comma 0 and 514 over I of 23 and so we
can now compute the entropy of just the
part it has to do with the forecast and
we get 6 6 93 similarly we can calculate
the entropy of other predictors like
temperature humidity and wind and so we
look at the gain Outlook how much are we
going to gain from this entropy play
golf minus entropy play golf Outlook and
we can take the original 0.94 for the
whole set minus the entropy of just the
um rainy day in temperature and we end
up with a gain of 247 so this is our
Information Gain remember we Define
entropy and we Define Information Gain
the higher the information gain the
lower the entropy the better the
information gain of the other three
attributes can be calculated in the same
way so we have our gain for temperature
equals
0.029 we have our gain for humidity
equals 0.152 and our gain for a windy
day equals
048 and if you do a quick comparison
you'll see the. 247 is the greatest gain
of information so that's the split we
want now let's build the decision tree
so we have the Outlook as a going to be
sunny overcast or rainy that's our first
split because that gives us the most
Information Gain and we can continue to
go down the tree using the different
information gains with the largest
information we can continue down the
nodes of the tree where we choose the
attribute with the largest Information
Gain as the root node and then continue
to split each sub node with the largest
Information Gain that we can compute and
although it's a little bit of a tongue
twister to say all that you can see that
it's a very easy to view visual model we
have our Outlook we split it three
different directions if the Outlook is
overcast we're going to play and then we
can split those further down if we want
so if the over Outlook is sunny but then
it's also windy if it's uh windy we're
not going to play if it's not windy
we'll play so we can easily build a nice
decision treat to guess what we would
like to do tomorrow and give us a nice
recommendation for the day so we want to
know if it's a good day to play golf
when it's sunny and windy remember the
original question that came out
tomorrow's weather report is sunny and
windy you can see by going down the tree
we go outl sunny out look windy we're
not going to play golf tomorrow so our
little Smartwatch pops up and says I'm
sorry tomorrow's not a good day for golf
it's going to be sunny and windy and if
you're a huge golf fan you might go uhoh
it's not a good day to play golf we can
go in and watch a golf game at home so
we'll sit in front of the TV instead of
being out playing golf in the wind now
that we looked at our decision tree
let's look at the third one of our
algorithms we're investigating support
Vector machine support vector Vector
machine is a widely used classification
algorithm the idea of support Vector
machine is simple the algorithm creates
a separation line which divides the
classes in the best possible manner for
example dog or cat disease or no disease
suppose we have a labeled sample data
which tells height and weight of males
and females a new data point arrives and
we want to know whether it's going to be
a male or a female so we start by
drawing a line we draw decision lines
but if we consider decision line one
then we will CL classify the individual
as a male and if we consider decision
line two then it will be a female so you
can see this person kind of lies in the
middle of the two groups so it's a
little confusing trying to figure out
which line they should be under we need
to know which line divides the classes
correctly but how the goal is to choose
a hyperplane and that is one of the key
words they use when we talk about
support Vector machines choose a
hyperplane with the greatest possible
margin between the decision line and the
nearest Point within the training set so
so you can see here we have our support
Vector we have the two nearest points to
it and we draw a line between those two
points and the distance margin is the
distance between the hyperplane and the
nearest data point from either set so we
actually have a value and it should be
equally distant between the two um
points that we're comparing it to when
we draw the hyperplanes we observe that
line one has a maximum distance so we
observe that line one has a maximum
distance margin so we'll classify the
new data point correctly
and our result on this one is going to
be that the new data point is Mel one of
the reasons we call it a hyper plane
versus a line is that a lot of times
we're not looking at just weight and
height we might be looking at 36
different features or dimensions and so
when we cut it with a hyper plane it's
more of a three-dimensional cut in the
data multi-dimensional it cuts the data
certain way and each plane continues to
cut it down until we get the best fit or
match let's s understand this with the
help of an example problem statement I
always start with a problem statement
when you're going to put some code
together we're going to do some coding
now classifying muffin and cupcake
recipes using support Vector machines so
the cupcake versus the muffin let's have
a look at our data set and we have the
different recipes here we have a muffin
recipe that has so much flour I'm not
sure what measurement 55 is in but it
has 55 maybe it's
ounces but uh has a certain amount of
flour certain amount of milk sugar
butter egg baking powder vanilla and
salt and So based on these measurements
we want to guess whether we're making a
muffin or a cupcake and you can see in
this one we don't have just two features
we don't just have height and weight as
we did before between the male and
female in here we have a number of
features in fact in this we're looking
at eight different features to guess
whether it's a muffin or a cupcake
what's the difference between a muffin
and a cupcake turns out muffins have
more flour while cupcakes have more more
butter and sugar so basically the
cupcakes a little bit more of a dessert
where the muffins a little bit more of a
fancy bread but how do we do that in
Python how do we code that to go through
recipes and figure out what the recipe
is and I really just want to say
cupcakes versus muffins like some big
professional wrestling thing before we
start in our cupcakes versus muffins we
are going to be working in Python
there's many versions of python many
different editors that is one of the
strengths and weaknesses of python is it
just has so much stuff attached to it
and it's one of the more popular data
science programming packages you can use
in this case we're going to go ahead and
use anaconda and Jupiter notebook the
Anaconda Navigator has all kinds of fun
tools once you're into the anacon
Navigator you can change environments I
actually have a number of environments
on here we'll be using python 36
environment so this is in Python version
36 although it does doesn't matter too
much which version you use I usually try
to stay with the 3x because they're
current unless you have a project that's
very specifically in version 2x 27 I
think is usually what most people use in
the version two and then once we're in
our um Jupiter notebook editor I can go
up and create a new file and we'll just
jump in here in this case we're doing
SPM muffin versus Cupcake and then let's
start with our packages for data
analysis and we almost always use a
couple there's a few very standard
packages we use we use import oops
import
import
numpy that's for number python they
usually denote it as NP that's very
comma that's very common and then we're
going to import pandas as
PD and numpy deals with number arrays
there's a lot of cool things you can do
with the numpy uh setup as far as
multiplying all the values in an array
in a numpy array data array pandas I
can't remember if we're using it
actually in this data set I think we do
as an import it makes a nice data frame
and the difference between a data frame
and a nump array is that a data frame is
more like your Excel spreadsheet you
have columns you have indexes you have
different ways of referencing it easily
viewing it and there's additional
features you can run on a data frame and
pandas kind of sits on numpy so they you
need them both in there
and then finally we're working with the
support Vector machine so from sklearn
we're going to use the sklearn model
import svm support Vector
machine and then as a data scientist you
should always try to visualize your data
some data obviously is too complicated
or doesn't make any sense to the human
but if it's possible it's going to take
a second look at it so you can actually
see what you're doing now for that we're
going to use two packages we're going to
import map plot library. pyplot as PLT
again very common and we're going to
import caborn as SNS and we'll go ahead
and set the font scale in the SNS right
in our import line that's with this um
semicolon followed by a line of data
we're going to set the SNS and these are
great because the the caborn sits on top
of map plot Library just like pandas
hits on numpy so it adds a lot more
features and uses and control we're
obviously not going to get into matplot
library and caborn it' be its own
tutorial we're really just focusing on
the svm the support Vector machine from
sklearn and since we're in Jupiter
notebook uh we have to add a special
line in here for our map plot library
and that's your percentage sign or Amber
sign mat plot library in line now if
you're doing this in just a straight
code Project A lot of times I use like
notepad++
and I'll run it from there you don't
have to have that line in there because
it'll just pop up as its own window on
your computer depending on how your
computer set up because we're running
this in the Jupiter notebook as a
browser setup this tells it to display
all of our Graphics right below on the
page so that's what that line is for
remember the first time I ran this I
didn't know that and I had to go look
that up years ago it's quite a headache
so M plot library in line is just
because we're running this on the web
setup and we can go ahead and run this
make sure all our modules are in they're
all imported which is great if you don't
have them import you'll need to go ahead
and pip use the PIP or however you do it
there's a lot of other install packages
out there although pip is the most
common and you have to make sure these
are all installed on your python setup
the next step of course is we got to
look at the data you can't run a model
for predicting data if you don't have
actual data so to do that let me go
ahead and open this up and take a look
and we have our uh cupcakes versus
muffins
and it's a CSV file or CSV meaning that
it's comma separated
variable and it's going to open it up in
a nice uh spreadsheet for me and you can
see up here we have the type we have
muffin muffin muffin cupcake cupcake
cupcake and then it's broken up into
flour milk sugar butter egg baking
powder vanilla and salt so we can do is
we can go ahead and look at this data
also in our
python let us create a variable recipes
equals we're going to use our pandas
module. read CSV remember is a comma
separated
variable and the file name happened to
be cupcakes versus muffins oops I got
double brackets
there do it this
way there we go cupcakes versus
muffins because the program I loaded or
the the place I saved this particular
particular Python program is in the same
folder we can get by with just the file
name but remember if you're storing it
in a different location you have to also
put down the full path on
there and then because we're in pandas
we're going to go ahead and you can
actually in line you can do this but let
me do the full print you can just type
in recipes. head in the Jupiter notebook
but if you're running in code in a
different script you need to go ahead
and type out the whole print recipes.
head
and pandas knows that that's going to do
the first five lines of data and if we
flip back on over to the spreadsheet
where we opened up our CSV
file uh you can see where it starts on
line two this one calls it zero and then
2 3 4 5 six is going to match go and
close that out because we don't need
that anymore and it always starts at
zero and these are it automatically
indexes it since we didn't tell it to
use an index in here so that's the index
number for the left left hand side and
it automatically took the top row as
labels so Panda's using it to read a CSV
is just really slick and fast one of the
reasons we love our pandas not just
because they're cute and cuddly teddy
bears and let's go ahead and plot our
data and I'm not going to plot all of it
I'm just going to plot the uh sugar and
flour now obviously you can see where
they get really complicated if we have
tons of different features and so you'll
break them up and maybe look at just two
of them at a time to see how they
connect and to plot them we're going to
go ahead and use
caborn so that's our SNS and the command
for that is SNS LM plot and then the two
different variables I'm going to plot is
flour and
sugar data equals recipes the Hue equals
type and this is a lot of fun because it
knows that this is is pandas coming in
so this is one of the powerful things
about pandas mixed with Seaborn and
doing
graphing and then we're going to use a
pallet set one there's a lot of
different sets in there you can go look
them up for Seaborn or do a regular a
fit regular equals false so we're not
really trying to fit anything and it's a
scatter
kws a lot of these settings you can look
up in Seaborn half of these you could
probably leave off when you run them
somebody played with this and found out
that these were the best settings for
doing a Seaborn plot let's go ahead and
run that and because it does it in line
it just puts it right on the
page and you can see right here that
just based on sugar and flour alone
there's a definite split and we use
these models because you can actually
look at it and say hey if I drew a line
right between the middle of the blue
dots and the red dots we'd be able to do
an svm and and a hyperplane right there
in the
middle then the next St is to format or
pre process our
data and we're going to break that up
into two
parts we need to type label and remember
we're going to decide whether it's a
muffin or a cupcake well a computer
doesn't know muffin or cupcake it knows
zero and one so what we're going to do
is we're going to create a type label
and from this we'll create a nump array
andp where and this is where we can do
some logic we take our recipes from our
Panda and wherever type equals muffin
it's going to be zero and then if it
doesn't equal muffin which is cupcakes
it's going to be one so we create our
type label this is the answer so when
we're doing our training model remember
we have to have a a training data this
is what we're going to train it with is
that it's zero or one it's a muffin or
it's
not and then we're going to create our
recipe
features and if you remember correctly
from right up here the First Column is
type so we really don't need the type
column because that's our muffin or
cupcake and in pandas we can easily sort
that
out we take our value
recipes dot columns that's a pandas
function built into
pandas do values converting them to
values so it's just the column titles
going across the top and we don't want
the first one so what we do is since it
always starts at zero we want
one colon till the
end and then we want to go ahead and
make this a list and this converts it to
a list of
strings and then we can go ahead and
just take a look and see what we're
looking at for the features make sure it
looks right me go ahead and run
that and I forgot the S on recipes so
we'll go ahead and add the s in there
and then run that and we can see we have
flour milk sugar butter egg baking
powder vanilla and salt and that matches
what we have up here right where we
printed out everything but the type so
we have our features and we have our
label Now the recipe features is just
the titles of the columns and we
actually need the ingredients
and at this point we have a couple
options one we could rent it over all
the
ingredients and when you're doing this
usually you do but for our example we
want to limit it so you can easily see
what's going on because if we did all
the ingredients we have you know that's
what um seven eight different
hyperplanes that would be built into it
we only want to look at one so you can
see what the svm is
doing and so we'll take our recipes and
we'll do just flour and sugar again you
can replace that with your recipe
features and do all of them but we're
going to do just flour and sugar and
we're going to convert that to values we
don't need to make a list out of it
because it's not string values these are
actual values on there and we can go
ahead and just
print ingredients you can see what that
looks
like uh and so we have just the am of
flour and sugar just the two sets of
plots and just for fun let's go ahead
and take this over here and take our
recipe
features and so if we decided to use all
the recipe features you'll see that it
makes a nice column of different data so
it just strips out all the labels and
everything we just have just the values
but because we want to be able to view
this easily in a plot later on we'll go
ahead and take that and just do flow and
sugar and we'll run that you'll see it's
just the two columns
so the next step is to go ahead and fit
our
model we'll go a and just call it model
and it's a svm we're using a package
called
SVC in this case we're going to go ahead
and set the kernel equals linear so it's
using a specific setup on there and if
we go to the reference on their website
for the
svm you'll see that there's about
there's eight of them here three of them
are for regression three are for
classification the SVC support Vector
classification is probably one of the
most commonly
used and then there's also one for
detecting outliers and another one that
has to do with something a little bit
more specific on the model but SBC and
SV are the two most commonly used
standing for support vector classifier
and support Vector regression remember
regression is an actual value a float
value or whatever you're trying to work
on and SB is a classifier so it's a yes
no true
false but for this we want to know 01
muffin cupcake we go ahead and create
our model and once we have our model
created we're going to do model. fit and
this is very common especially in the
sklearn all their models are followed
with the fit
command and what we put into the fit
what we're training with it is we're
putting in the ingredients which in this
case we limited to just flour and sugar
and the type label is it a muffin or
cupcake now in more complicated data
science series you'd want to split into
we won't get into that today we split it
into uh training data and test data and
they even do something where they split
it into thirds where a third is used for
where you switch between which one's
training and test there's all kinds of
things go into that and gets very
complicated when you get to the higher
end not overly complicated just an extra
step which we're not going to do today
because this is a very simple set of
data and let's let's go ahead and run
this and now we have our model fit and I
got a error here so let me fix that real
quick it's Capital SBC it turns
out I did it
lowercase support
Vector classifier there we go let's go
ahead and run that and you'll see it
comes up with all this information that
it prints out automatically these are
the defaults of the model you notice
that we changed the kernel to linear and
there's our kernel linear on the
printout and there's other different
settings you can mess
with we're going to just leave that
alone for right now for this we don't
really need to mess with any of
those so next we're going to dig a
little bit into our newly trained model
and we're going to do this so we can
show you on a
graph and let's go ahead and get the
separating and we're going to say we're
going to use a W for our variable on
here
we're going to do
model.
coefficient 0 so what the heck is that
again we're digging into the model so
we've already got a prediction and a
train this is a math behind it that
we're looking at right now and
so the W is going to represent two
different coefficients and if you
remember we had y equals MX plus C so
these coefficients are connected to that
but in two-dimensional it's a
plane we don't want to spend too much
time on this because you can get lost in
the confusion of the math so if you're a
math wiiz this is great you can go
through here and you'll see that we have
AAL minus W 0 over W of 1 remember
there's two different values there and
that's basically the slope that we're
generating and then we're going to build
an XX what is XX we're going to set it
up to a numpy array there's our NP do
linespace so we're creating a
line of values between 30 and 60 so it
just creates a set of numbers for
x and then if you remember correctly we
have our formula y equals the slope time
X Plus The Intercept well to make this
work we can do this as y y equals the
slope times each value in that array
that's the neat thing about numpy so
when I do a * XX which is a whole numpy
array of values it multiplies a across
all of them and then it takes those same
values and we subtract the model
intercept that's your uh we had MX plus
C so that'd be the C from the formula
yal MX plus
C and that's where all these numbers
come from a little bit confusing because
it's digging out of these different
arrays and then we want to do is we're
going to take this and we're going to go
ahead and plot it so plot the parallels
to separating hyper plane that pass
through the support vectors and so we're
going to create b equals a model support
vectors pulling our support vectors out
there here's our YY which we now know is
a set of data and we have we're going to
create YY down equal a * XX + B1 - A * B
0 and then model support Vector B is
going to be set that to a new value the
minus one setup y y up equals a * XX +
B1 - A * B 0 and we can go ahead and
just run this to load these variables up
if you wanted to know understand a
little bit more of what's going on you
can see if we
print y y me just run that you can see
it's an array it's this is a line it's
going to have in this case between 30
and 60 so it's going to be 30 variables
in here and the same thing with y y up y
y down and we'll we'll plot those in
just a minute on a graph so you can see
what those look
like just go ahead and delete that out
of here and run that so it loads up the
variables nice clean slate I'm just
going to copy this from before remember
this our SNS or Seaborn plot LM plot
flour
sugar and I'll just go and run that real
quick so you can see what remember what
that looks like it's just a straight
graph on there and then one of the new
things is because Seaborn sits on on top
of Pi plot we can do the P plot for the
line going through and that is simply
PLT do
plot and that's our xx and y y our two
corresponding values XY and then
somebody played with this to figure out
that the line width equals two and the
color black would look nice so let's go
ahead and run this whole thing with the
PIP plot on there and you can see when
we do this it's just doing flour and
sugar on here
corresponding line between the sugar and
the flour and the muffin versus
Cupcake um and then we generated the um
support vectors the y y down and y y up
so let's take a look and see what that
looks
like so we'll do our PL
plot and again this is all against XX
the our x value but this time we have
YY
down and let's do something a little fun
with this we can put in a k Das Dash
that just tells it to make it a dotted
line and if we're going to do the down
one we also want to do the up one so
here's our
YY up and when we run that it adds both
sets
aligned and so here's our support and
this is what you expect you expect these
two lines to go through the nearest data
point so the dash lines go through the
nearest muffin and the nearest cupcake
when it's plotting it and then your svm
goes right down the middle so it gives
it a nice split in our data and you can
see how easy it is to see based just on
sugar and flour which one's a muffin or
a
cupcake let's go ahead and create a
function to
predict muffin or
cupcake I've got my um recipes I pulled
off the um internet and I want to see
the difference between a muffin or a
cupcake and so we need a function to
push that through and create a function
with DEA and let's call it muffin or
cupcake and remember we're just doing
flour and sugar today not doing all the
ingredients and that actually is a
pretty good split you really don't need
all the ingredients to know it's flour
and
sugar and let's go ahead and do an F
statement so if model
predict is of flour and sugar equals
zero so we take our model and we do run
a predict it's very common in sklearn
where you have a DOT predict you put the
data in and it's going to return a value
in this case if it equals zero then
print you're looking at a muffin recipe
else if it's not zero that means it's
one then you're looking at a cupcake
recipe that's pretty straightforward
for function or def for definition DF is
how you do that in Python and of course
you're going to create a function you
should run something in it and so let's
run a cupcake and we're going to send it
values 50 and 20 a muffin or a cupcake I
don't know what it is and let's run this
and just see what it gives us and it
says oh it's a muffin you're looking at
a muffin recipe so it very easily
predicts whether we're looking at a
muffin or a cupcake recipe let's plot
this there we go plot this on the graph
so we can see what that actually looks
like and I'm just going to copy and
paste it from below where we plotting
all the points in there so this is
nothing different than what we did
before if I run it you'll see it has all
the points and the lines on there and
what we want to do is we want to add
another point and we'll do PLT
plot and if you remember correctly we
did for our test we did 50 and
20 and then somebody went in here and
decided we'll do uh yo for yellow or
it's kind of a orange is yellow color is
going to come out marker size nine those
are settings can play with somebody else
played with them to come up with the
right setup so it looks good and you can
see there it is graft um clearly a
muffin in this case in cupcakes versus
muffins the muffin has won and if you'd
like to do your own muffin cupcake
Contender series you certainly can send
a note down below and the team at simply
learn will send you over the data they
use for the muffin and cupcake and
that's true of any of the data um we
didn't actually run a plot on it earlier
we had men versus women you can also
request that information to run it on
your data setup so you can test that
out so to go back over our setup we went
ahead for our support Vector machine
code we did a predict 40 Parts flour 20
Parts sugar I think it was different
than the one we did whether it's a
muffin or a cupcake hence we have built
a classifier using SPM which is able to
classify if a recipe is of a cupcake or
a muff
which wraps up our cupcake versus muffin
today in our second tutorial we're going
to cover K means and linear regression
along with going over the quiz questions
we had during our first tutorial what's
in it for you we're going to cover
clustering what is clustering K means
clustering which is one of the most
common used clustering tools out there
including a flowchart to understand K
means clustering and how it functions
and then we'll do an actual python live
demo on clustering of card cars based on
Brands then we're going to cover
logistic regression what is logistic
regression logistic regression curve and
sigmoid function and then we'll do
another python code demo to classify a
tumor as malignant or benign based on
features and let's start with clustering
suppose we have a pile of books of
different genres now we divide them into
different groups like fiction horror
education and as we can see from this
young lady she definitely is into heavy
you can just tell by those eyes in the
maple Canadian leaf on our shirt but we
have fiction horror and education and we
want to go ahead and divide our books up
well organizing objects into groups
based on similarity is clustering and in
this case as we're looking at the books
we're talking about clustering things
with known categories but you can also
use it to explore data so you might not
know the categories you just know that
you need to divide it up in some way to
conquer the data and to organize it
better but in this case we're going to
be looking at cluster ing in specific
categories and let's just take a deeper
look at that we're going to use K means
clustering K means clustering is
probably the most commonly used
clustering tool in the machine learning
library K means clustering is an example
of unsupervised learning if you remember
from our previous thing it is used when
you have unlabeled data so we don't know
the answer yet we have a bunch of data
that we want to Cluster into different
groups Define clusters in the data based
on feature similarity so we've uced a
couple terms here we've already talked
about unsupervised learning and
unlabeled data so we don't know the
answer yet we're just going to group
stuff together and see if we can find an
answer of how things connect we've also
introduced feature similarity features
being different features of the data now
with books we can easily see fiction and
horror and history books but a lot of
times with data some of that information
isn't so easy to see right when we first
look at it and so K means is one of
those tools tools where we can start
finding things that connect that match
with each other suppose we have these
data points and want to assign them into
a cluster now when I look at these data
points I would probably group them into
two clusters just by looking at them I'd
say two of these group of data kind of
come together but in K means we pick K
clusters and assign random centroids to
clusters where the K clusters represents
two different clusters we pick K
clusters inside random cids to the
Clusters then we compute distance from
objects to the centroids now we form new
clusters based on minimum distances and
calculate the centroids so we figure out
what the best distance is for the
centroid then we move the centroid and
recalculate those distances repeat
previous two steps iteratively till the
cluster centroid stop changing their
positions and become Static repeat
previous two steps iteratively till the
cluster centroid stop changing and the
positions become Static once the
Clusters become Static then K means
clustering algorithm is said to be
converged and there's another term we
see throughout machine learning is
converged that means whatever math we're
using to figure out the answer has come
to a solution or it's converged on an
answer shall we see the flowchart to
understand make a little bit more sense
by putting it into a nice easy step by
step so we start we choose K we'll look
at the elbow method in just a moment we
assign random centroids two clusters and
sometimes you pick the centroids because
you might look at the data in in a graph
and say oh these are probably the
central points then we compute the
distance from the objects to the
centroids we take that and we form new
clusters based on minimum distance and
calculate their centroids then we
compute the distance from objects to the
new centroids and then we go back and
repeat those last two steps we calculate
the distances so as we're doing it it
brings into the new centroid and then we
move the centroid around and we figure
out what the best which objects are
closest to each centroid so the objects
can switch from one C to the other as
the centroids are moved around and we
continue that until it is converged
let's see an example of this suppose we
have this data set of seven individuals
and their score on two topics A and B so
here's our subject in this case
referring to the person taking the uh
test and then we have subject a where we
see what they've scored on their first
subject and we have subject B and we can
see what they score on the second
subject now let's take two farthest
apart points as initi cluster centroids
now remember we talked about selecting
them randomly or we can also just put
them in different points and pick the
furthest one apart so they move together
either one works okay depending on what
kind of data you're working on and what
you know about it so we took the two
furthest points one and one and five and
seven and now let's take the two
farthest apart points as initial cluster
centroids each point is then assigned to
the closest cluster with respect to the
distance from the centroids so we take
each one of these points in there we
measure that distance and you can see
that if we measured each of those
distances and you use the Pythagorean
theorem for a triangle in this case
because you know the X and the Y and you
can figure out the diagonal line from
that or you just take a ruler and put it
on your monitor that'd be kind of silly
but it would work if you're just
eyeballing it you can see how they
naturally come together in certain areas
now we again calculate the centroids of
each cluster so cluster one and then
cluster two and we look at each
individual dot there's one two three
we're in one cluster uh the centroid
then moves over it becomes 1.8 comma 2.3
so remember it was at one and one well
the very center of the data we're
looking at would put it at the one point
roughly 22 but 1.8 and 2.3 and the
second one if we wanted to make the
overall mean Vector the average Vector
of all the different distances to that
centroid we come up with four comma 1
and 54 so we've now moved the centroids
We compare each individual's distance to
its own own cluster mean and to that of
the opposite cluster and we find can
build a nice chart on here that the as
we move that centr around we now have a
new different kind of clustering of
groups and using ukian distance between
the points and the mean we get the same
formula you see new formulas coming up
so we have our individual dots distance
to the mean centr of the cluster and
distance to the mean Central of the
cluster only individual three is nearer
to the mean of the opposite cluster
cluster two than its own cluster one and
you you can see here in the diagram
where we've kind of circled that one in
the middle so when we've moved the clust
the centroids of the Clusters over one
of the points shifted to the other
cluster because it's closer to that
group of individuals thus individual 3
is relocated to Cluster 2 resulting in a
new Partition and we regenerate all
those numbers of how close they are to
the different clusters for the new
clusters we will find the actual cluster
centroids so now we move the centroids
over and you can see that we've now
formed two very distinct clusters on
here on comparing the distance of each
individual's distance to its own cluster
mean and to that of the opposite cluster
we find that the data points are stable
hence we have our final clusters now if
you remember I brought up a concept
earlier K me on the K means algorithm
choosing the right value of K will help
in less number of iterations and to find
the appropriate number of clusters in a
data set we use the elbow method and
within sum of squares WSS is defined as
the sum of the squared distance between
each member of the cluster and its
centroid and so you see what we've done
here is we have the number of clusters
and as you do the same K means algorithm
over the different clusters and you
calculate what that centroid looks like
and you find the optimal you can
actually find the optimal number of
clusters using the elbow the graph is
called as the elbow method and on this
we guessed at two just by looking at the
data but as you can see the slope you
actually just look for right there where
the elbow is in the slope and you have a
clear CLE answer that we want two
different to start with k means equals 2
A lot of times people end up Computing K
means equals 2 3 4 or five until they
find the value which fits on the elbow
joint sometimes you can just look at the
data and if you're really good with that
specific domain remember domain I
mentioned that last time you'll know
that that where to pick those numbers or
where to start guessing at what that K
value is so let's take this and we're
going to use a use case using K means
clustering to Cluster cars into Brands
using parameters such as horsepower
cubic inches make year Etc so we're
going to use the data set cars data
having information about three brands of
cars Toyota Honda and Nissan we'll go
back to my favorite tool the Anaconda
Navigator with the Jupiter notebook and
let's go ahead and flip over to our
Jupiter notebook and in our Jupiter
notebook I'm going to go ahead and just
paste the uh basic code that we usually
start a lot of these off with we're not
I'm not going to go too much into this
code because we've already discussed
numpy we've already discussed matplot
library and pandas numpy being the
number array pandas being the panda data
frame and Matt plot for the graphing and
don't forget uh since if you're using
the Jupiter notebook you do need the
matplot library in line so that it plots
everything on the screen if you're using
a different python editor then you
probably don't need that because it'll
have a pop-up window on your computer
and we'll go ahead and run this just to
load our libraries and our setup into
here here the next step is of course to
look at our data which I've already
opened up in a spreadsheet and you can
see here we have the miles per gallon
cylinders cubic inches horsepower weight
pounds how you how heavy it is time it
takes to get to 60 my card is probably
on this one at about 80 or 90 what year
it is so this is you can actually see
this is kind of older cars and then the
brand Toyota Honda Nissan so the
different cars are coming from all the
way from 1971 if we scroll scroll down
to uh the 80s we have between the 70s
and 80s a number of cars that they've
put out and let's uh when we come back
here we're going to do importing the
data so we'll go ahead and do data set
equals and we'll use pandas to read this
in and it's uh from a CSV file remember
you can always post this in the comments
and request the data files for these
either in the comments here on the
YouTube video or go to Simply learn.com
and request that the cars CSV I put it
in the same folder as the code that I've
stored so my python code is stored in
the same folder so I don't have to put
the full path if you store them in
different folders you do have to change
this and double check your name
variables and we'll go ahead and run
this and uh We've chosen data set
arbitrarily because you know it's a data
set we're importing and we've now
imported our car CSV into the data set
as you know you have to prep the data so
we're going to create the X data this is
the one that we're going to try to
figure out what's going on with and then
there is a number of ways to do this but
we'll do it in a simple Loop so you can
actually see what's going on so we'll do
for i n x. columns so we're going to go
through each of the columns and a lot of
times it's important I I'll make lists
of the columns and do this because I
might remove certain columns or there
might be columns that I want to be
processed differently but for this we
can go ahead and take X of I and we want
to go fill Na and that's a panda's
command but the question is when are we
going to fill the missing dat dat with
we definitely don't want to just put in
a number that doesn't actually mean
something and so one of the tricks you
can do with this is we can take X of I
and in addition to that we want to go
ahead and turn this into an integer
because a lot of these are integers so
we'll go ahead and keep it integers and
me add the bracket here and a lot of
editors will do this they'll think that
you're closing one bracket make sure you
get that second bracket in there if it's
a double bracket that's always something
that happens regularly so once we have
our integer of X of this is going to
fill in any missing data with the
average and I was so busy closing one
set of brackets I forgot that the mean
is also has brackets in there for the
pandas so we can see here we're going to
fill in all the data with the average
value for that column so if there's
missing data it's in the average of the
data it does have then once we've done
that we'll go ahead and loop through it
again and just check and see to make
sure everything is filled in correctly
and we'll print and then we take X is
null and this returns a set of the null
value or the how many lines are null and
we'll just sum that up to see what that
looks like and so when I run this and so
with the X what we want to do is we want
to remove the last column because that
had the models that's what we're trying
to see if we can cluster these things
and figure out the models there is so
many different ways to sort the X out
for one we could take the X and we could
go data set our variable we're using and
use the iocation one of the features
that's in in
pandas and we could take that and then
take all the rows and all but the last
column of the data set and at this time
we could do values we just convert it to
values so that's one way to do this and
if I let me just put this down here and
print X it's a capital x we chose and I
run this you can see it's just the
values we could also take out the values
and it's not going to return anything
because there's no values connected to
it what I like to do with this
is instead of doing the iocation which
does integers more common is to come in
here and we have our data set and we're
going to do data set dot or data set.
columns and remember that list all the
columns so if I come in here let me just
Mark that as red and I print data set.
columns you can see that I have my index
here I have my MPG cylinders everything
including the brand which we don't want
so the way to get rid of the brand would
be to do data Columns of Everything But
the last one minus one so now if I print
this you'll see the brand disappears and
so I can actually just take data set
columns minus one and I'll put it right
in here for the columns we're going to
look
at and let's unmark this and unmark
this and now if I do an x. head I now
have a new data frame and you can see
right here we have all the different
columns except for the brand at the end
of the year and it turns out when you
start playing with the data set you're
going to get an error later on and it'll
say cannot convert string to float value
and that's because for some reason these
things the way they recorded them must
have been recorded as strings so we have
a neat feature in here on pandas to
convert and it is simply convert
objects and for this we're going to do
convert oops convert
underscore numeric numeric equals true
and yes I did have to go look that up I
don't have it memorized the convert
numeric in there if I'm working with a
lot of these things I remember them but
um depending on where I'm at what I'm
doing I usually have to look it up and
we run that oops I must have missed
something in here let me double check my
spelling and when I double check my
spilling you'll see I missed the first
underscore in the convert objects and
when I run this it now has everything
converted into a numeric value because
that's what we're going to be working
with is numeric values down
here and the next part is that we need
to go through the data and eliminate
null values most people when they're
doing small amounts working with small
data pools discover afterwards that they
have a null value and they have to go
back and do this so you know be aware
whenever ever we're formatting this data
things are going to pop up and sometimes
you go backwards to fix it and that's
fine that's just part of exploring the
data and understanding what you
have and I should have done this earlier
but let me go ahead and increase the
size of my window one
notch there we go easier to
see so we'll do 4 I in working with x.
columns we'll page through all the
columns and we want to take X of I we're
going to change that we're going to
alter it and so with this we want to go
ahead and fill in X of I pandis Has The
Fill
Na and that just fills in any
non-existent missing data I we'll put my
brackets up and there's a lot of
different ways to fill this data if you
have a really large data set some people
just void out that data because if and
then look at it later in a separate
exploration of data one of the tricks we
can do is we can take our column and we
can find the
means and the means is in or quotation
marks so when we take the columns we're
going to fill in the the non-existing
one with the means the problem is that
returns a decimal float so some of these
aren't decimals certainly we need to be
a little careful of doing this but for
this example we're just going to fill it
in with the integer version of this
keeps it on par with the other data that
isn't a decimal
point and then what we also want to do
is we want to double check A lot of
times you do this first part first to
double check then you do the fill and
then you do it again just to make sure
you did it right so we're going to go
through and test for missing data and
one of the re ways you can do that is
simply go in here and take our X of I
column so it's going to go through the X
ofi column it says is null so it's going
to return any any place there's a null
value it actually goes through all the
rows of each column is null and then we
want to go ahead and sum that so we take
that we add the sum value and these are
all pandas so is null is a panda command
and so is sum and if we go through that
and we go ahead and run
it and we go ahead and take and run that
you'll see that all the columns have
zero null values so we've now tested and
double checked and our data is nice and
clean we have no null values everything
is now a number Val value we turned it
into numeric and we've removed the last
column in our data and at this point
we're actually going to start using the
elbow method to find the optimal number
of clusters so we're now actually
getting into the SK learn part uh the K
means clustering on here I guess we'll
go ahead and zoom it up one more notot
so you can see what I'm typing in
here and then from sklearn going to or
sklearn
cluster we're going to import K
means I always forget to capitalize the
K and the M when I do this so capital K
capital M K
means and we'll go and create a um array
wcss equals let make it an empty array
if you remember from the elbow method
from our
slide within the sums of squares WSS is
defined as the sum of squ Square
distance between each member of the
cluster and its centroid so we're
looking at that change in differences as
far as a squar distance and we're going
to run this over a number of K mean
values in fact let's go for I in range
we'll do 11 of
them range zero of
11 and the first thing we're going to do
is we're going to create the actual
we'll do it all lower
case
and so we're going to create this
object from the K means that we just
imported and the variable that we want
to put into this is in clusters and
we're going to set that equals to I
that's the most important one because
we're looking at how increasing the
number of clusters changes our answer
there are a lot of settings to the K
means our guys in the back did a great
job just kind of playing with some of
them the most common ones that you see
in a lot of stuff is how you init your K
means so we have K means plus plus plus
this is just a tool to let the model
itself be smart how it picks it
centroids to start with its initial
centroids we only want to iterate no
more than 300 times we have a Max
iteration we put in there we have a the
inth the knit the random State equals z
you really don't need to worry too much
about these when you're first learning
this as you start digging in deep you
start finding that these are shortcuts
that will speed up the process as far as
a setup but the big one that we're
working with is the in clusters equals I
so we're going to literally train our K
means 11 times we're going to do this
process 11 times and if you're working
with uh Big Data you know the first
thing you do is you run a small sample
of the data so you can test all your
stuff on it and you can already see the
problem that if I'm going to iterate
through a terabyte of data 11 times and
then the K means itself is iterating
through the data multiple times that's a
heck of a process so you got to be a
little careful with this a lot of times
though you can find your elbow using the
elbow method find your optimal number on
a sample of data especially if you're
working with larger data sources so we
want to go ahead and take our K means
and we're just going to fit it if you're
looking at any of the sklearn very
common you fit your model and if you
remember correctly our variable we're
using is the capital x and once we fit
fit this value we go back to the um
array we made and we want to go and just
toin that value on the
end and it's not the actual fitware
pending in there it's when it generates
it it generates the value you're looking
for is inertia so K means. inera we'll
pull that specific value out that we
need and let's get a visual on this
we'll do our PLT plot and what we're
plotting
here is first the xaxis which is range
01 so that will generate a nice little
plot there and the wcss for our Y
axis it's always nice to give our plot a
title and let's see we'll just give it
the elbow method for the title and let's
get some labels so let's go ahead and do
PLT X
label and what we'll do we'll do number
of clusters for that and PLT y label and
for that we can do oops there we go wcss
since that's what we're doing on the
plot on there and finally we want to go
ahead and display our graph which is
simply PLT do
oops. show there we go and because we
have it set to inline it'll appear in
line hopefully I didn't make a type
error on
there and you can see we get a very nice
graph you can see a very nice elbow
joint there at uh two and again right
around three and four and then after
that there's not very much now as a data
scientist if I was looking at this I
would do either three or four and I'd
actually try both of them to see what
the um output look like and they've
already tried this in the back so we're
just going to use three as a setup on
here and let's go ahead and see what
that looks like when we actually use
this to show the different kinds of
cars and so let's go ahead and apply the
K means to the car's data set and
basically we're going to copy the code
that we looped through up above where K
means equals K means number of clusters
and we're just going to set that number
of clusters to three since that's what
we're going to look for and you could do
three and four on this and graph them
just to see how they come up differently
be kind of curious to look at that but
for this we're just going to set it to
three go ahead and create our own
variable y k means for our answers and
we're going to set that equal to whoops
I double equal there to K means but
we're not going to do a fit we're going
to do a fit predict is the setup you
want to use and when you're using
untrained models you'll see um a
slightly different usually you see fit
and then you see just the predict but we
want to both fit and predict the K means
on this and that's fitcore predict and
then our capital x is the data we're
working
with and before we plot this data we're
going to do a little pandas trick we're
going to take our x value and we're
going to set X as Matrix so we're
converting this into a nice rows and
columns kind of set up but we want the
we're going to have columns equals none
so it's just going to be a matrix of
data in here and let's go ahead and run
that a little warning you'll see this
warnings pop up because things are
always being updated so there's like
minor changes in the versions and future
versions instead of Matrix now the it's
more common to set it do values instead
of doing as Matrix but M Matrix works
just fine for right now and you'll want
to update that later on but let's go
ahead and dive in and plot this and see
what that looks like and before we dive
into plotting this data I always like to
take a look and see what I am plotting
so let's take a look at why K means I'm
just going to print that out down here
and we see we have an array of answers
we have 2 1 0 2 one two so it's
clustering these different rows of data
based on the three different spaces it
thinks it's going to be
B and then let's go ahead and print X
and see what we have for x and we'll see
that X is an array it's a matrix so we
have our different values in the array
and what we're going to do it's very
hard to plot all the different values in
the array so we're only going to be
looking at the first two or positions
zero and
one and if you were doing a full
presentation in front of the board
meeting you might actually do a little
different in in a little deeper into the
different aspects because this is all
the different columns we looked at but
we only look at columns one and two for
this to make it easy so let's go ahead
and clear this data out of here and
let's bring up our plot and we're going
to do a scatter plot here so PLT
scatter
and this looks a little complicated so
let's explain what's going on with this
we're going to take the X
values and we're only interested in y of
K means equals z zero the first cluster
okay and then we're going to take value
zero for the x-axis and then we're going
to do the same thing here we're only
interested in K means equals zero but
we're going to take the second column so
we're only looking at the first two
columns in our answer or in the data and
then the guys in the back played with
this a little bit to make it
pretty and they discovered that it looks
good with has a size equals 100 that's
the size of the dots we're going to use
red for this one and when they were
looking at the data and what came out it
was definitely the Toyota on this so
we're just going to go ahead and label
it Toyota again that's something you
really have to explore in here as far as
playing with those numbers and see what
looks good we'll go ahead and hit enter
in there and I'm just going to paste in
the next two lines which is the next two
cars and this is our Nissa and Honda and
you'll see with our scatter plot we're
now looking at where Yore K means equals
1
and we want the zero column and y k
means equals 2 again we're looking at
just the first two columns zero and one
and each of these rows then corresponds
to Nissan and
Honda and I'll go ahead and hit enter on
there and uh finally let's take a look
and put the centroids on there again
we're going to do a scatter
plot and on the centroids you can just
pull that from our K means the model we
created dot cluster centers and we're
going to just do
um all of them in the first number and
all of them in the second number which
is 01 because you always start with zero
and
one and then they were playing with the
size and everything to make it look good
we'll do a size of 300 we're going to
make the color yellow and we'll label
them it's always good to have some good
labels
centroids and then we do want to do a
title PLT
title and pop up there PL title CU you
always make want to make your graphs
look pretty we'll call it clusters of
car
make and one of the features of the plot
library is you can add a legend it'll
automatically bring in it since we've
already labeled the different aspects of
the legend with Toyota Nissan and
Honda and finally we want to go ahead
and show so we can actually see it and
remember it's in line uh so if you're
using a different editor that's not the
Jupiter notebook you'll get a popup of
this
and you should have a nice set of
clusters here so we can look at this and
we have a clusters of Honda and green
Toyota and red Nissan and purple and you
can see where they put the centroids to
separate
them now when we're looking at this we
can also plot a lot of other different
data on here as far because we only
looked at the first two columns this is
just column one and two or 01 as as you
label them in computer scripting but you
can see here we have a nice clusters of
Carm make and we were able to pull out
the data
and you can see how just these two
columns form very distinct clusters of
data so if you were exploring new data
you might take a look and say well what
makes these different almost going in
reverse you start looking at the data
and pulling apart the columns to find
out why is the first group set up the
way it is maybe you're doing loans and
you want to go well why is this group
not defaulting on their loans and why is
the last group defaulting on their loans
and why is the middle group 50%
defaulting on their bank loans and you
start finding ways to manipulate the
data and pull out the answers you
want so now that you've seen how to use
K mean for clustering let's move on to
the next topic now let's look into
logistic regression the logistic
regression algorithm is the simplest
classification algorithm used for binary
or multi-classification problems and we
can see we have our little girl from
Canada who's into horror books is back
that's actually really scary when you
think about that with those big eyes in
the previous tutorial we learned about
linear regression dependent and
independent variables so to brush up y =
mx + C very basic algebraic function of
Y and X the dependent variable is the
target class variable we are going to
predict the independent variables X1 all
the way up to xn are the features or
attributes we're going to use to predict
the target class we know a linear
regression looks like but using the
graph we cannot divide the outcome into
categories it's really hard to
categorize 1.5 3.6 9.8 uh for example a
linear regression graph can tell us that
with increase in number of hours studied
the marks of a student will increase but
it will not tell us whether the student
will pass or not in such cases where we
need the output as categorical value we
will use logistic regression and for
that we're going to use the sigmoid
function so you can see here we have our
marks 0 to 100 number of hours studied
that's going to be what they're
comparing it to in this example and we
usually form a line that says y = mx + C
and when we use the sigmoid function we
have P = 1 over 1 + e minus y it
generates a sigmoid curve and so you can
see right here when you take the Ln
which is the natural logarithm I always
thought it should be n l not Ln that's
just the inverse of uh e your e to the
minus y and so we do this we get Ln of p
over 1us p = m * X plus C that's the
sigmoid curve function we're looking for
and we can zoom in on the function and
you'll see that the function as it
derives goes to one or to zero depending
on what your x value is and the
probability if it's greater than 0.5 the
value is automatically rounded off to
one indicating that the student will
pass so if they're doing a certain
amount of studying they will probably
pass then you have a threshold value at
the0 five it automatically puts that
right in the middle usually and your
probability if it's less than 0.5 the
value run it off to zero indicating the
student will fail so if they're not
studying very hard they're probably
going to fail this of course is ignoring
the outliers of that one student who's
just a natural genius and doesn't need
any studying to memorize everything
that's not me unfortunately have to
study hard to learn new stuff problem
statement to classify whether a tumor is
malignant or B9 and this is actually one
of my favorite data sets to play with
because it has so many features and when
you look at them you really are hard to
understand you can't just look at them
and know the answer so it gives you a
chance to kind of dive into what data
looks like when you aren't able to
understand the specific domain of the
data but I also want you to remind you
that in the domain of medicine if I told
you that my probability was really good
it classified things at say 90% or9 95%
and I'm classifying whether you're going
to have a malignant or a B9 tumor I'm
guessing that you're going to go get it
tested anyways so you got to remember
the domain we're working with so why
would you want to do that if you know
you're just going to go get a biopsy
because you know it's that serious this
is like an all or nothing just
referencing the domain it's important it
might help the doctor know where to look
just by understanding what kind of tumor
it is so it might help them or Aid them
in something they miss from before so
let's go ahead and dive into the code
and I'll come back to the domain part of
it in just a minute so use case and
we're going to do our noral Imports here
where we're importing numpy Panda
Seaborn the matplot library and we're
going to do matplot library in line
since I'm going to switch over to
Anaconda so let's go ahead and flip over
there and get this started so I've
opened up a new window in my anaconda
Jupiter
notebook by the way Jupiter notebook uh
you don't have to use Anaconda for the
Jupiter notebook I just just love the
interface and all the tools that
Anaconda brings so we got our import
numpy as in P for our numpy number array
we have our pandas PD we're going to
bring in caborn to help us with our
graphs as SNS so many really nice Tools
in both caborn and matplot library and
we'll do our matplot library. pyplot as
PLT and then of course we want to let it
know to do it in line and let's go and
just run that so it's all set up and
we're just going to call our data data
not creative today okay uh equals PD and
this happens to be in a CSV file so
we'll use a pd. read uncore CSV and I
happen to name the file I renamed it
data for
p2.png so when I pop it open in a local
spreadsheet and this is just a CSV file
comma separate variables we have an ID
so I guess they um categorizes for
reference of what id which test was done
the diagnosis M for malignant B for B9
so there's two different options on
there and that's what we're going to try
to predict is the m andb and test it and
then we have like the radius mean or
average the texture average perimeter
mean area mean smoothness I don't know
about you but unless you're a doctor in
the field most of the stuff I mean you
can guess what concave means just by the
term concave but I really wouldn't know
what that means in the measurements
they're taking so they have all kinds of
stuff like how smooth it is uh the
Symmetry and these are all float values
we just page through them real quick and
you'll see there's I believe 36 if I
remember correctly in this
one so there's a lot of different values
they take and all these measurements
they take when they go in there and they
take a look at the different growth the
two Rous growth so back in our data and
I put this in the same folder as a code
so I saved this code in that folder
obviously if you have it in a different
location you want to put the full path
in there and we'll just do U
Panda's first five lines of data with
the data. head and we run that we can
see that we have pretty much what we
just looked at we have an ID we have a
diagnosis if we go all the way across
you'll see all the different columns
coming across cross displayed nicely for
our
data and while we're exploring the data
our caborn which we referenced as
SNS makes it very easy to go in here and
do a joint plot you'll notice the very
similar to because it is sitting on top
of the um plot Library so the joint plot
does a lot of work for us and we're just
going to look at the first two columns
that we're interested in the radius mean
and the texture mean we'll just look at
those two columns
and data equals data so that tells it
which two columns we're plotting and
that we're going to use the data that we
pulled in let's just run that and it
generates a really nice graph on here
and there's all kinds of cool things on
this graph to look at I mean we have the
texture mean and the radius mean
obviously the axes you can also
see and one of the cool things on here
is you can also see the histogram they
show that for the radius mean where is
the most commonest radius mean come up
and where the most common texture is so
we're looking at the tech the on each
growth its average texture and on each
radius its average uh radius on there
gets a little confusing because we're
talking about the individual objects
average and then we can also look over
here and see the the histogram showing
us the median or how common each
measurement is and that's only two
columns so let's dig a little deeper
into Seaborn they also have a heat map
and if you're not familiar with heat
Maps a heat map just means it's in color
that's all that means heat map I guess
the original ones were plotting heat
density on something and so ever since
it's just called a heat map and we're
going to take our data and get our
corresponding numbers to put that into
the heat map and that's simply data. C
RR for that that's a panda expression
remember we're working in a pandas data
frame so that's one of the Cool Tools
and pandas for our data and this is pull
that information into a heat map and see
what that looks like
and you'll see that we're now looking at
all the different features we have our
ID we have our texture we have our area
our compactness concave points and if
you look down the middle of this chart
diagonal going from the upper left to
bottom right it's all white that's
because when you compare texture to
texture they're identical so they're
100% or in this case perfect one in
their
correspondence and you'll see that when
you look at say area or right below it
it has almost a black on there when you
compare it to texture so these have
almost no corresponding data They Don't
Really form a linear graph or something
that you can look at and say how
connected they are they're very
scattered data this is really just a
really nice graph to get a quick look at
your data doesn't so much change what
you do but it changes verifying so when
you get an answer or something like that
or you start looking at some of these
individual pieces you might go hey that
doesn't match according to showing our
heat map this should not correlate with
each other and if it is you're going to
have to start asking well why what's
going on what else is coming in there
but it does show some really cool
information on here mean we can see from
the ID there's no real one feature that
just says if you go across the top line
that lights up there's no one feature
that says hey if the area is a certain
size then it's going to be B9 or
malignant it says there's some that sort
of add up and that's a big hint in the
data that we're trying to ID this
whether it's malignant or B9 that's a
big hint to us as data scientists to go
okay we can't solve this with any one
feature it's going to be something that
includes all the features or many of the
different features to come up with the
solution for it and while we're
exploring the data let's explore one
more area and let's look at data. isnull
we want to check for null values in our
data if you remember from earlier in
this tutorial we did it a little
differently where we added stuff up and
summed them up you can actually with
pandas do it really quickly data. is
null and Summit and it's going to go
across all the columns so when I run
this you're going to see all the columns
come up with no null
data so we've just just to reash these
last few steps we've done a lot of
exploration we have looked at the first
two columns and seen how they plot with
the caborn with a joint plot which shows
both the histogram and the data plotted
on the X Y coordinates and obviously you
can do that more in detail with
different columns and see how they plot
together and then we took and did the
Seaborn heat map the SNS do heat map of
the data and you can see right here
where it did a nice job showing us some
bright spots where stuff correlates with
each other and forms a very nice
combination or points of scattering
points and you can also see areas it
don't and then finally we went ahead and
checked the data is the data null value
do we have any missing data in there
very important step because it'll crash
later on if you forget to do this step
it will remind you when you get that
nice error code that says null values
okay so not a big deal if you miss it
but it it's no fun having to go back
when you're you're in a huge process and
you've missed this step and now you're
10 steps later and you got to go
remember where you were pulling the data
in
so we need to go ahead and pull out our
X and our y so we just put that down
here and we'll set the x equal to and
there's a lot of different options here
certainly we could do x equals all the
columns except for the first two because
if you remember the first two is the ID
and the diagnosis so that certainly
would be an option but what we're going
to do is we're actually going to focus
on the worst the worst radius the worst
texture parameter area smoothness comp
pness and so on one of the reasons to
start dividing your data up when you're
looking at this information is sometimes
the data will be the same data coming in
so if I have two measurements coming
into my model it might overweigh them it
might overpower the other measurements
because it's measuring it's basically
taking that information in twice that's
a little bit past the scope of this
tutorial I want you to take away from
this though is that we are dividing the
data up into pieces and our team in the
back went ah head and said hey let's
just look at the worst so I'm going to
create a an array and you'll see this
array radius worst texture worst
perimeter worst we've just taken the
worst of the worst and I'm just going to
put that in my X so this x is still a
pandas data frame but it's just those
columns and our y if you remember
correctly is going to be oops hold on
one second it's not X it's data there we
go so x equals data and then it's a list
of the different columns the worst of
the worst
and if we're going to take that then we
have to have our answer for our Y for
the stuff we know and if you remember
correctly we're just going to be looking
at the diagnosis that's all we care
about is what is it diagnosed is it Bine
or malignant and since it's a single
column we can just do diagnosis oh I
forgot to put the brackets the there we
go okay so it's just diagnosis on there
and we can also real quickly do like x.
head if you want to see what that looks
like and Y y do head and run this and
you'll see um it only does the last one
I forgot about that if you don't do
print you can see that the the Y do head
is just Mmm because the first ones are
all malignant and if I run this the x.
head is just the first five values of
radius worst texture worst parameter
worst area worst and so on I'll go ahead
and take that out so moving down to the
next step we've built our two data sets
our answer and then the features we want
to look
at in data science it's very important
to test your
model so we do that by splitting the
data and from sklearn model selection
we're going to import train test split
so we're going to split it into two
groups there are so many ways to do this
I noticed in one of the more modern ways
they actually split it into three groups
and then you model each group and tested
against the other groups so you have all
kinds of and there's reasons for that
which is past the scope of this and for
this particular example isn't necessary
for this we're just going to split it
into two groups one to train our data
and one to test our data and the sklearn
uh do model selection we have train test
split you could write your own quick
code to do this where you just randomly
divide the data up into two groups but
they do it for us
nicely and we actually can almost we can
actually do it in one statement with
this where we're going to generate four
variables capital x train capital X test
so we have our training data we're going
to use to fit the model and then we need
something to test it and then we have
our y train so we're going to train the
answer and then we have our test so this
is the stuff we want to see how good it
did on our model and we'll go ahead and
take our train test split that we just
imported and we're going to do X and our
y our two different data that's going in
for our split and then the guys in the
back came up and wanted us to go ahead
and use a test size equals. 3 that's
testore size random State it's always
nice to kind of switch your random State
around but not that important what this
means is that the test size is we're
going to take 30% of the data and we're
going to put that into our test
variables our y test and our X test and
we're going to do 70% into the X train
and the Y train so we're going to use
70% of the data to train our model and
30% to test it let's go ahead and run
that and load those up so now we have
all our stuff split up and all our data
ready to go and now we get to the actual
Logistics Park we're actually going to
do our create our model so let's go
ahead and bring that in from sklearn
we're going to bring in our linear model
and we're going to import logistic
regression that's the actual model we're
using and this we'll call it log
models there we go model and let's just
set this equal to our logistic
regression that we just imported it so
now we have a variable log model set to
that class for us to use and with most
the uh models in the SK learn we just
need to go ahead and fix it fit do a fit
on there and we use our X train that we
separated out with our y train and let's
go ahead and run this so once we've run
this we'll have a model that fits this
data that 70% of our training
data uh and of course it prints this out
that tells us all the different
variables that you can set on there
there's a lot of different choices you
can make but for word do we're just
going to let all the defaults sit we
don't really need to mess with those on
this particular example and there's
nothing in here that really stands out
as super important until you start
fine-tuning it but for what we're doing
the basics will work just fine and then
let's we need to go ahead and test out
our model is it working so let's create
a variable y predict and this is going
to be equal to our log model and we want
to do a predict again very standard uh
format for the sklearn library is taking
your model and doing a predict on it and
we're going to test y predict against
the Y test so we want to know what the
model thinks it's going to be that's
what our y predict is and with that we
want the capital x x test so we have our
train set and our test set and now we're
going to do our y predict and let's go
ahead and run
that and if we uh
print
y predict let me go ahead and run that
you'll see it comes up and it preds a
prints a nice array of uh B and M for B9
and
malignant for all the different test
data we put in there so it does pretty
good we're not sure exactly how good it
does but we can see that it actually
works and it's functional was very easy
to create you'll always discover with
our data science that as you explore
this you spend a significant amount of
time time prepping your data and making
sure your data coming in is good uh
there's a saying good data in good
answers out bad data in bad answers out
that's only half the thing that's only
half of it selecting your models becomes
the next part as far as how good your
models are and then of course
fine-tuning it depending on what model
you're using so we come in here we want
to know how good this came out so we
have our y predict here log model.
predict X test
so for deciding how good our model is
we're going to go from the SK learn.
metrics we're going to import
classification report and that just
reports how good our model is doing and
then we're going to feed it the uh model
data and let's just print this out and
we'll take our classification
report and we're going to put into
there our test our actual data so this
is what we actually know is true and our
prediction what our model predicted for
that data on the test side and let's run
that and see what that
does so we pull that up you'll see that
we have um a Precision for B9 and
malignant b& M and we have a Precision
of 93 and 91 a total of 92 so it's kind
of the average between these two of 92
there's all kinds of different
information on here your F1
score your Rec call your support coming
through on this and for this I'll go
ahead and just flip back to our slides
that they put together for describing it
and so here we're going to look at the
Precision using the classification
report and you see this is the same
print out I had up above some of the
numbers might be different because it
does randomly pick out which data we're
using so this model is able to predict
the type of tumor with
91% accuracy so when we look back here
that's you will see where we have uh B9
andant it actually is 92 coming up here
but we're looking about a 92 91%
precision and remember I reminded you
about domain so when we're talking about
the domain of a medical domain with a
very catastrophic outcome you know at 91
or 92% Precision you're still going to
go in there and have somebody do a
biopsy on it very different than if
you're investing money and there's a 92%
chance you're going to earn 10% and 8%
chance you're going to lose 8% you're
probably going to bet the money cuz at
that odds it's pretty good that you'll
make some money and in the long run if
you do that enough you definitely will
make money and also with this domain
I've actually seen them use this to
identify different forms of cancer
that's one of the things that they're
starting to use these models for because
then it helps the doctor know what to
investigate so that wraps up this
section we're finally we're going to go
in there and let's discuss the answer to
the quiz asked in machine learning
tutorial part one can you tell what's
happening in the following cases
grouping documents into different
categories based on the topic and
content of each document this is an
example of clustering where K means
clustering can be used to group the
documents by topics using bag of words
approach so if You' gotten in there that
you're looking for clustering and
hopefully you had at least one or two
examples like K means that are used for
clustering different things then give
yourself a two thumbs up B identifying
handwritten digits in images correctly
this is an example of classification the
traditional approach to solving this
would be to extract digit dependent
features like curvature of different
digits Etc and then use a classifier
like svm to distinguish between images
again if you got the fact that it's a
classification example give yourself a
thumb up and if you're able to go hey
let's use svm or another model for this
give yourself those two thumbs up on it
C behavior of a website indicating that
the site is not working as designed this
is an example of anomaly detection and
in this case the algorithm learns what
is normal and what is not normal usually
by observing the logs of the website
give yourself a thumbs up if you got
that one and just for a bonus can you
think of another example of anomaly
detection one of the ones I use for my
own business is detecting anomalies in
stock markets stock markets are very
ficked and they behave very ertical so
finding those erratic areas and then
finding ways to track down why they're
erratic was something released in social
media was something released you can see
where knowing where that anomaly is can
help you to figure out what the answer
is to it in another area D predicting
salary of an individual based on his or
her years of experience this is an
example of regression this problem can
be mathematically defined as a function
between independent years of experience
and dependent variables salary of an
individual and if you guess that this
was a regression model give yourself a
thumbs up and if you're able to remember
that it was between independent and
dependent variables and that terms give
yourself two thumbs up summary so to
wrap it up we went over what is K means
and we went through also the chart of
choosing your elbow method and assigning
a random centroid to the Clusters
Computing the distance and then going in
there and figuring out what the minimum
centroids is and Computing the distance
and going through that Loop until it
gets the perfect centroid and we looked
into the elbow method to choose K based
on running our clusters across the
number of variables and finding the best
location for that we did a nice example
of clustering cars with K means even
though we only looked at the first two
columns to make it simple and easy to
graph can easily extrapolate that and
look at all the different columns and
see how they all fit together and we
looked at what is logistic regression we
discussed the sigmoid function what is
logistic regression and then we went
into an example of classifying tumors
with
Logistics I hope you enjoyed part two of
machine learning thank you for joining
us today for more information visit
www.s simply learn.com again my name is
Richard kersner a member of the
simplylearn team get certified get ahead
if you have any questions or comments
feel free to write those down below the
YouTube video or visit us at simply
learn.com we'll be happy to supply you
with the data sets or other information
as
[Music]
requested hi there if you like this
video subscribe to the simply learn
YouTube channel and click cck here to
watch similar videos to nerd up and get
certified click
here today we're going to cover the K
nearest neighbors L referred to as knnn
and KNN is really a fundamental place to
start in the machine learning it's a
basis of a lot of other things and just
the logic behind it is easy to
understand and Incorporated in other
forms of machine learning so today
what's in it for you why do we need KNN
what is KNN how do we choose the factor
K when do we use
knnn how does knnn algorithm work and
then we'll dive in to my favorite part
the use case predict whether a person
will have diabetes or not that is a very
common and popular Ed data set as far as
testing out models and learning how to
use the different models in machine
learning by now we all know machine
learning models make predictions by
learning from the past data available so
we have our input value values our
machine learning model Builds on those
inputs of what we already know and then
we use that to create a predicted output
is that a dog little kid looking over
there watching the black cat cross their
path no dear you can differentiate
between a cat and a dog based on their
characteristics cats cats have sharp
claws uses to climb smaller length of
ears meows and purs doesn't love to play
around dogs they have dle claws bigger
length of ears barks loves to run around
you usually don't see a cat running
around people although I do have a cat
that does that where dogs do and we can
look at these we can say uh we can
evaluate the sharpness of the claws how
sharp are their claws and we can
evaluate the length of the ears and we
can usually sort out cats from dogs
based on even those two characteristics
now tell me if it is a cat or a dog not
question usually little kids know cats
and dogs by now unless they live a place
where there's not many cats or dogs so
if we look at the sharpness of the claws
the length of the ears and we can see
that the cat has smaller ears and
sharper claws than the other animals its
features are more like cats it must be a
cat sharp claws length of ears and it
goes in the cat group because KNN is
based on feature similarity we can do
classification using KNN classifier so
we have our input value the picture of
the black cat it goes into our trained
model and it predicts that this is a cat
coming out so what is KNN what is the
KNN algorithm K nearest neighbors is
what that stands for is one of the
simplest supervised machine learning
algorithms mostly used for
classification so we want to know is
this a dog or it's not a dog is it a cat
or not a cat it classifies a data point
based on how its neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we gone
from cats and dogs right into wine
another favorite of mine KNN stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different wines they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride K and KNN is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put uh k equal
5 we'll talk about K in just a minute a
data point is classified by the majority
of votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose K
how do we know k equals five I mean
that's was the value we put in there so
we're going to talk about it how do we
choose the factor K KN andn algorithm is
based on feature similarity choosing the
right value of K is a process called
parameter tuning and is important for
better accuracy so at k equal 3 we can
classify we have a question mark in the
middle as either a as a square or not is
it a square or is it in this case a
triangle and so if we set k equals to
three we're going to look at the three
nearest neighbors we're going to say
this is a square and if we put k equals
to 7 we classify as a triangle depending
on what the other data is around and you
can see as the K changes depending on
where that point is that drastically
changes your answer and uh we jump here
we go how do we choose the factor of K
you'll find this in all machine learning
choosing these factors that's the face
you get it's like oh my gosh did I
choose the right K did I set it right my
values in whatever machine learning tool
you're looking at so that you don't have
a huge bias in One Direction or the
other and in terms of knnn the number of
K if you choose it too low the bias is
based on it's just too noisy it's it's
right next to a couple things and it's
going to pick those things and you might
get a skewed answer and if your K is too
big then it's going to take forever to
process so you're going to run into
processing issues and resource issues so
what we do the most common use and
there's other options for choosing K is
to use the square root of n so N is a
total number of values you have and you
take the square root of it in most cases
you also if it's an even number so if
you're using uh like in this case
squares and triangles if it's even you
want to make your K value odd that helps
it select better so in other words
you're not going to have a balance
between two different factors that are
equal so usually take the square root of
N and if it's even you add one to it or
subtract one from it and that's where
you get the K value from that is the
most common use and it's pretty solid it
works very well when do we use KNN we
can use KNN when data is labeled so you
need a label on it we know we have a
group of pictures with dogs dogs cats
cats data is Noise free and so you can
see here when we have a class and we
have like underweight 140 23 Hello Kitty
normal that's pretty confusing we have a
high variety of data coming in so it's
very noisy and that would cause an issue
data set is small so we're usually
working with smaller data sets where I
you might get into gig of data if it's
really clean doesn't have a lot of noise
because KNN is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the KNN and also just
using for smaller data sets KNN works
really good how does a KNN algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false or either
normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal underweight using KNN so if we
have new data coming in that says 57
kilg and 177 cm is that going to be
normal or underweight to find the
nearest neighbors we'll calculate the
ukan distance according to the ukian
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the Square t of x - A
2 + y - b^ 2 and you can remember that
from the two edges of a triangle we're
Computing the third Edge since we know
the X side and the yide let's calculate
it to understand clearly so we have our
unknown point and we placed it there in
red and we have our other points where
the data is scattered around the
distance D1 is a square OT of 170 - 167
2ar + 57 - 51 SAR which is about 6.7 and
distance two is about 13 and distance
three is about 13 .4 similarly we will
calculate the ukian distance of unknown
data point from all the points in the
data set and because we're dealing with
small amount of data that's not that
hard to do and it's actually pretty
quick for a computer and it's not a
really complicated Mass you can just see
how close is the data based on the ukian
distance hence we have calculated the
ukian distance of unknown data point
from all the points as showing where X1
and y1 equal 57 and 170 whose class we
have to classify so now we're looking at
that we're saying well here's the ukian
distance who's going to be their closest
neighbors now let's calculate the
nearest neighbor at k equals 3 and we
can see the three closest neighbors puts
them at normal and that's pretty
self-evident when you look at this graph
it's pretty easy to say okay what you
know we're just voting normal normal
normal three votes for normal this is
going to be a normal weight so majority
of neighbors are pointing towards normal
hence as per K&N algorithm the class of
57170 should be normal so recap of KNN
positive integer K is specified along
with a new sample we select the K
entries in our database which are
closest to the new sample we find the
most common classification of these
entries this is the classification we
give to the new sample so as you can see
it's pretty straightforward we're just
looking for the closest things that
match what we got so let's take a look
and see what that looks like in a use
case in Python so let's dive into the
predict diabetes use case so use case
predict diabetes the objective predict
whether a person will be diagnosed with
diabetes or not we have a data set of
768 people who were or were not
diagnosed with diabetes and let's go
ahead and open that file and just take a
look at that data and this is in a
simple spreadsheet format the data
itself is comma separated very common
set of data and it's also a very common
way to get the data and you can see here
we have columns a through I that's what
one two 3 four five six seven eight um
eight columns with a particular
attribute and then the ninth column
which is the outcome is whether they
have diabetes as a data scientist the
first thing you should be looking at is
insulin well you know if someone has
insulin they have diabetes because
that's why they're taking it and that
could cause issue on some of the machine
learning packages but for very basic
setup this works fine for uh doing the
KNN and the next thing you notice is it
it didn't take very much to open it up
um I can scroll down to the bottom of
the data there's
768 it's pretty much a small data set
you know it's 769 I can easily fit this
into my ram on my computer I can look at
it I can manipulate it and it's not
going to really tax just a regular
desktop computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python 36 I have a couple
different uh versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where I can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
uh the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see you
have a number of windows open up at the
top the one we're working in and uh
since we're working on the KNN predict
whether a person will have diabetes or
not let's go and put that title in there
and I'm also going to go up here and
click on sell actually we want to go
ahead and first insert a cell below and
then I'm going to go back up to the top
cell and I'm going to change the cell
type to markdown that means this is not
going to run as python it's a markdown
language so if I run this first one it
comes up in nice big letters which is
kind of nice remind us what we're
working on and by now you should be
familiar with doing all of our Imports
we're going to import the pandas as PD
import numpy is NP pandas is the pandas
data frame and numpy is a number array
very powerful tools to use in here so we
have our Imports so we've brought in our
pandas our numpy our two general python
tools and then you can see over here we
have our train test split by now youed
should be familiar with splitting the
data we want to split part of it for
training our thing and then training our
particular model and then we want to go
ahead and test the remaining data just
see how good it is pre-processing a
standard scaler pre-processor so we
don't have a bias of really large
numbers remember in the data we had like
number preg gencies isn't going to get
very large where the amount of insulin
they take and get up to 256 so 256
versus 6 that will skew results so we
want to go ahead and change that so that
they're all uniform between minus one
and one and then the actual tool this is
the K neighbors classifier we're going
to use and finally the last three are
three tools to test all about testing
our model how good is it me just put
down test on there and we have our
confusion Matrix our F1 score and our
accuracy so we have our two general
python modules we're importing and then
we have our six modules specific from
the sklearn setup and then we do need to
go ahead and run this so these are
actually imported there we go and then
move on to the next step and so in this
set we're going to go ahead and load the
database we're going to use pandas
remember pandas is PD and we'll take a
look at the data in Python we looked at
it in a simple spreadsheet but usually I
like to also pull it up so that we can
see what we're doing so here's our data
set equals pd. read CSV that's a pandas
command and the diabetes folder I just
put in the same folder where my I python
script is if you put in a different
folder you'd need the full length on
there we can also do a quick length of
uh the data set that is a simple python
command Len for length we might even
let's go ahead and print that we'll go
print and if you do it on its own line
link. dat set in the jupyter notebook
it'll automatically print it but when
you're in most of your different setup
so you want to do the print in front of
there and then we want to take a look at
the actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I usually always
like to keep the print statement in
there but because most projects only use
one data frame P his data frame doing it
this way doesn't really matter the other
way works just fine and you can see when
we hit the Run button we have the 768
lines which we knew and we have our
pregnancies it's automatically given a
label on the left remember the head only
shows the first five lines so we have
zero through four and just a quick look
at the data you can see it matches what
we looked at before we have pregnancy
glucose blood pressure all the way to
age and then the outcome on the end and
we're going to do a couple things in
this next step we're going to create a
list of columns where we can't have zero
there's no such thing as zero skin
thickness or zero blood PR pressure zero
glucose uh any of those you'd be dead so
not a really good Factor if they don't
if they have a zero in there because
they didn't have the data and we'll take
a look at that because we're going to
start replacing that information with a
couple of different things and let's see
what that looks like so first we create
a nice list as you can see we have the
values we talked about glucose blood
pressure skin thickness uh and this is a
nice way when you're working with
columns is to list the columns you need
to do some kind of transformation on uh
very common thing to do and then for
this particular setup we certainly could
use the there's some Panda tools that
will do a lot of this where we can
replace the na but we're going to go
ahead and do it as a data set column
equals data set column. replace this is
this is still pandas you can do a direct
there's also one that that you look for
your nan a lot of different options in
here but the N nump Nan is what that
stands for is is non doesn't exist so
the first thing we're doing here is
we're repl replacing the zero with a
numpy none there's no data there that's
what that says that's what this is
saying right here so put the zero in and
we're going to replace zeros with no
data so if it's a zero that means the
person's well hopefully not dead
hopefully they just didn't get the data
the next thing we want to do is we're
going to create the mean which is the in
integer from the data set from the
column do mean where we skip Nas we can
do that that is a panda's command there
the skip na so we're going to figure out
the mean of that data set and then we're
going to take that data set column and
we're going to replace all the
npn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replaced zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you can see that we're switching this
to a non-existent value then we're going
to create the mean well this is the
average person so if we don't know what
it is if they did did not get the data
and the data is missing one of the
tricks is you replace it with the
average what is the most common data for
that this way you can still use the rest
of those values to do your computation
and it kind of just brings that
particular value or those missing values
out of the equation let's go ahead and
take this and we'll go ahead and run it
doesn't actually do anything so we're
still preparing our data if you want to
see what that looks like we don't have
anything in the first few lines so it's
not going to show up but we certainly
could look at a row let's do that let's
go into our data set with printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones it shows you can see it skipped
a bunch in the middle because that's
what it does if you have too many lines
in Jupiter notebook it'll skip a few and
and go on to the next in a data set let
me go and remove this and we'll just
zero out that and of course before we do
any processing before proceeding any
further we need to split the data set
into our train and testing data that way
we have something to train it with and
something to test it on and you're going
to notice we did a little something here
with the Panda's database code there we
go my drawing tool we've added in this
right here off the data set and what
this says is that the first one in
pandas this is from the PD pandas it's
going to say within the data set we want
to look at the iocation and it is all
rows that's what that says so we're
going to keep all the rows but we're
only looking at 0er column 0 to 8
remember column 9 here it is right up
here we printed in here is outcome well
that's not part of the training data
that's part of the answer yes it's
column 9 but it's listed as eight number
eight so 0 to8 is nine columns so uh
eight is the value and when you see it
in here zero this is actually 0 to 7even
it doesn't include the last one and then
we go down here to Y which is our answer
and we want just the last one just
column 8 and you can do it this way with
this particular notation and then if you
remember we imported the train test
split that's part of the SK learn right
there and we simply put in our X and our
y we're going to do random State equals
zero you don't have to necessarily seed
it that's a seed number I think the
default is one when you seed it I'd have
to look that up and then the test size
test size is 02 that simply means we're
going to take 20% of the data and put it
aside so that we can test it later
that's all that is and again we're going
to run it not very exciting so far we
haven't had any print out other than to
look at the data but that is a lot of
this is prepping this data once you prep
it the actual lines of code are quick
and easy and we're almost there with the
actual writing of our KNN we need to go
ahead and do a scale the data if you
remember correctly we're fitting the
data in a standard scaler which means
instead of the data being from you know
5 to 303 in one column and the next
column is 1 to six we're going to set
that all so that all the data is between
minus1 one and one that's what that
standard scaler does keeps it
standardized and we only want to fit the
scaler with the training set but we want
to make sure the testing set is the X
test going in is also transformed so
it's processing it the same so here we
go with our standard scaler we're going
to call it scor X for the scaler and
we're going to import the standard
scaler into this variable and then our X
train equals score x. fit transform so
creating the scaler on the X train
variable and then our X test we're also
going to transform it so we've trained
and transformed the X train and then the
X test isn't part of that training it
isn't part of that of training the
Transformer it just gets transformed
that's all it does and again we're going
to go and run this if you look at this
we've now gone through these steps all
three of them we've taken care of
replacing our zeros for key columns that
shouldn't be zero and we replace that
with the means of those columns that way
that they fit right in with our data
models we've come down here and we split
the data so now we have our test data
and our training data and then we've
taken and we' scaled the data so all of
our data going in now no we don't tra we
don't train the Y part the Y train and Y
test that never has to be trained it's
only the data going in that's what we
want to train in there then Define the
model using K neighbors classifier and
fit the train data in the model so we do
all that data prep and you can see down
here we're only going to have a couple
lines of code where we're actually
building our model and training it
that's one of the cool things about
Python and how far we've come it's such
an exciting time to be in machine
learning because there's so many
automated tools let's see before we do
this let's do a quick length of and
let's do y we want let's just do length
of Y and we get 768 and if we import
math we do math do square root let's do
y train there we go it's actually
supposed to be X train before we do this
let's go ahead and do import math and do
math square root length of Y test and
when I run that we get
12.49 I want to see show you where this
number comes from we're about to use 12
is an even number so if you know if
you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take one away we'll make it 11 let
me delete this out of here that's one of
the reasons I love Jupiter notebook cuz
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
K neighbors classifier n neighbors equal
11 remember we did 12 minus 1 for 11 so
we have an odd number of neighbors P
equals 2 because we're looking for is it
are they diabetic or not and we're using
the ukian metric there are other means
of measuring the distance you could do
like square square means value there's
all kinds of measure this but the ukian
is the most common one and it works
quite well it's important to evaluate
the model let's use the confusion Matrix
to do that and we're going to use the
confusion Matrix wonderful tool and then
we'll jump into the F1 score and finally
accuracy score which is probably the
most commonly used quoted number when
you go into a meeting or something like
that so let's go ahead and paste that in
there and we'll set the cm equal to
confusion Matrix y test y predict so
those are the two values we're going to
put in there and let me go ahe and run
that and print it out and the way you
interpret this is you have the Y
predicted which would be your title up
here we could do uh let's just do p r d
predicted across the top and actual
going down actual it's always hard to to
write in here actual that means that
this column here down the middle that's
the important column and it means that
our prediction said 94 and prediction
and the actual agreed on 94 and 32 this
number here the 13 and the 15 those are
what was wrong so you could have like
three different if you're looking at
this across three different variables
instead of just two you'd end up with
the third row down here and the column
going down the middle so in the first
case we have the the and I believe the
zero is a 94 people who don't have
diabetes the prediction said that 13 of
those people did have diabetes and were
at high risk and the 32 that had
diabetes it had correct but our
prediction said another 15 out of that
15 it classified as incorrect so you can
see where that classification comes in
and how that works on the confusion
Matrix then we're going to go ahead and
print the F1 score let me just run that
and you see we get a 69 in our F1 score
the F1 takes into account both sides of
the balance of false positives where if
we go ahead and just do the accuracy
account and that's what most people
think of is it looks at just how many we
got right out of how many we got wrong
so a lot of people when you're a data
scientist and you're talking to other
data scientists they're going to ask you
what the F1 score the F score is if
you're talking to the general public or
the U decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82% not too bad
for a quick flash look at people's
different statistics and running an
sklearn and running the KNN the K
nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accurac score we got the 0
818 was pretty close to what we got and
we can pretty much round that off and
just say we have an accuracy of 80%
tells us that is a pretty fair fit in
the model the principal component uh
analysis we're going to cover
dimensionality reduction principal
component analysis what is it important
PCA terminologies and you'll see it
abbreviated uh normally as PCA principal
component analysis PCA properties PCA
example and then we'll pull up some
python code in our jupyter notebook and
have some Hands-On demo on the PCA and
how it's used dimensionality reduction
dimensionality reduction refers to the
technique that reduces the number of
input variables in a data set and so you
can see on the table on the right shows
the orders made at an automobile parts
retailer the retailer sells different
automobile parts from different
companies and you can see we have
company uh bpex isomax and they have the
item the tire the axle an order ID a
price number and a quantity in in order
to predict the future cells we find out
that using correlation analysis that we
just need three attributes therefore we
have reduced the number of attributes
from five to three and clearly we don't
really care about the part number I
don't think the part number would have
an effect on how many tires are bought
um and even the store who's buying them
probably does not have an effect on that
in this case that's what they've
actually done is removed those and we
just have the item the tire the price
and the quantity one of the things you
should be taking away from this is in
the scheme of things we are in the
descriptive phase we're describing the
data and we're pre-processing the data
what can we do to clean it up why
dimensionality reduction well number one
less dimensions for a given data set
means less computation or training time
that can be really important if you're
trying a number of different models uh
and you're rerunning them over and over
again and even if you have 7 gabt of
data that can start taking days to go
through all those different models so
this is huge this is probably the hugest
part as far as reducing um our data set
redundancy is removed after removing
similar entries from the data
set again pre-processing some of our
models like a neural network if you put
into the same data it might give them a
higher weight than they would if it was
just once we want to get rid of that
redundancy it also increases the process
processing time if you have multiple uh
data coming in space required to store
the data is
reduced so if we're committing this into
a big data uh pool we might not send the
company that bought it why would we want
to store two whole extra columns when we
added into that pool of data makes the
data easy for plotting in 2D and 3D
plots this is my favorite part very
important you're in your shareholder
meeting you want to be able to give them
a really good um clear and simplified
version you want to reduce it down to
something people can take in it helps to
find out the most significant features
and skip the rest which also comes in in
post scribing uh leads to better human
interpretation that kind of goes with
number four what makes data easy for
plotting you have a better
interpretation when we're looking at it
principal component analysis so what is
it uh principal component analysis is a
technique for reducing the
dimensionality of data sets increas
ining interpretability but at the same
time minimizing information loss so we
take some very complex data set with
lots of variables we run it through the
PCA we reduce the variables we end up
with a reduced variable
setup this is very confusing to look at
because if you look at the end result we
have the different colors all lined up
so what we're going to take a look at is
let's say we have a picture here uh
let's say you are asked to take a
picture of some Toddlers and you are
deciding which angle would be the best
to take the picture from so if we come
up here we look at this we say okay this
is you know one angle uh we get the back
of a lot of heads not many faces uh so
we'll do it from here we might get the
one person up front smiling a lot of the
people in the class are missing so we
have a huge amount off to the right of
blank space Maybe from up here again we
have the back of someone's head and uh
it turns out that the best angle to
click the picture from might be this
bottom left angle you look at it and you
say hey that makes sense it's a a good
um configuration of all the people in
the picture now when we're talking about
data it's not you really can't do it by
what you think is going to be the best
we need to have some kind of
mathematical formula so it's consistent
and so it makes sense in the back end
one of the projects I worked on many
years ago uh had something similar to
the iris and if you've ever done the
iris data set is probably one of the
most common ones out there where they
have the flower and they're measuring
the stammen uh in the petals and they
have width and they have length of the
petal instead of putting through the
width and the length of the petal we
could just as easily do the um width to
length ratio we can divide the width by
the length and you get a single number
where you had two that's the kind of
idea that's going on into this in
pre-processing and looking at what we
can do to bring the data down the very
simplified example on my uh I
pedal example when we look at the
similarity in PCA we find the best
picture or projection of the data points
and so we look down at from one angle
we've drawn a line down there uh we can
see these data points based on in this
case just two variables now keep in mind
we're usually talking about 36 40
variables um almost all of your business
models usually have about 26 to 27
different variables they're looking at
at uh same thing with like a bank loan
model we're talking 26 to 36 different
variables they're looking at that are
going in so what we want to do is we
want to find the best view in this case
we're just looking at the
XY we look down at it and we have our
second um idea PC2 and again we're
looking at the XI this XY this time from
a different direction here for our ease
we can consider that we get two
principal components namely pc1 and
PC2
comparing both the principal components
we find the data points are sufficiently
spaced in
pc1 so if you look at what we got here
we have uh pc1 you can see along the
line how the data points are spaced
versus the spacing in PC2 and that's
what they're coming up with what is
going to give us the best look for these
data points when we combine them and
we're looking at them from just a single
angle whereas in PC2 they are less
spaced which makes the observation and
further calculations much more more
difficult therefore we accept the pc1
and not the PC2 as the data points are
more spaced now obviously the backend
calculations are a little bit more
complicated when we get into the math of
how they decide what is more
valuable this gives you an idea though
that when we're talking about this we're
talking about the perspective uh which
would help in understanding how PCA
analysis
works we want to go ahead and do is dive
into the important termin terminologies
under
PCA and important terminologies are
views the perspective through which data
points are
observed and so you'll hear that if
someone's talking about a PCA
presentation and they're not taking the
time to reduce it to something that the
average person shareholders can
understand you might hear and refer to
it as the different views what view are
we taking Dimension number of columns in
a data set are called the dimensions of
that data set and we talked about you'll
hear features Dimensions um I was
talking about features there's usually
when you're running a business you're
talking 25 26 27 different features
minimal and then you have the principle
component new variables that are
constructed as linear combinations or
mixtures of the initial
variables principal component is very
important it's a combination if you
remember my flower example it would be
the width over the length of the petal
as opposed to putting both width and
length in you just put in the um ratio
instead which is a single number versus
two separate numbers
projections the perpendicular distance
between the principal component and the
data
points and that goes to that line we had
earlier it's that right angle line of
where those Point how those points fall
onto the line important properties
important properties number of principal
components is always less than or equal
to the number of
attributes that just makes Common Sense
uh you're not going to do 10 principal
properties with with only three features
you're trying to reduce them so that's
just kind of goofy but it is important
to remember that people will throw weird
code out there and just randomly do
stuff instead of really thinking it
through principal components are
orthagonal
and this is what we're talking about
that right angle from the line we when
we do pc1 we're looking at how those
points fall onto that line uh same thing
with PC2 we want to make sure that pc1
does not equal PC2 we don't want to have
the same two principal points uh when we
do two
points the priority of principal
components decreases as their numbers
increase this is important to understand
if you're going to
create uh one
principal component everything is
summarized into that one component as we
go to two components the priority um how
much it holds value decreases as we go
down so if you have five different
points each one of those points is going
to have less value than just the one
point which has everything summarized in
it how PCA works I said there was more
in the back end we talk about the math
this is what we're talking about is how
does it actually work so now we have
understanding that you you're looking at
a perspective uh now we want to see how
that math side Works PCA performs the
following operations in order to
evaluate the principal components for a
given data
set first we start with the
standardization then we have a
covariance matrix
computation and we use that to generate
our igene vectors and igene
values which is the feature vector and
if you remember the igene vector is like
a translation if we're um moving the
data from xal 1 to xal 2 or whatever
altering it and the igene value is the
final value that we
generate when we talk about
standardization the main aim of this
step is to standardize the range of the
attributes so that each one of them lie
within similar boundaries this process
involves removal of the mean from the
variable values and scaling the data
with respect to the standard
deviation and you can see here we have Z
equals the variable values minus the
mean over the standard deviation The
co-variance Matrix computation
co-variance Matrix is used to express
the correlation between any two or more
attributes in multi-dimensional data set
the co-variance Matrix has the entries
as the variance and the co-variance of
the attribute vales the variance is
denoted by VAR and the co-variance is
denoted by Cove on the right we can see
the co-variance Matrix for two
attributes and their
values when we do a Hands-On look at the
code we'll do a display of this so you
can see what we're talking about and
what that looks like for now you can
just notice that this is a matrix that
we're generating with the variance and
then the co-variance of x to Y on the
right side we can see the co-variance
table for more than two attributes in a
multi-dimensional data
set this is what I was talking about we
usually are looking at uh not just one
feature or two features we're usually
looking at 25 30 features going
on and so if we
setup like this we should see all those
different features as the different
variables covariance Matrix tells us how
the two or more variables are related
positive covariant indicate that the
value of one variable is directly
proportional to the other variable
negative covariant indicate that the
value of one variable is inversely
proportional to the other variable that
is always important to note whenever
we're doing any of these matrixes that
we're going to be looking at that
positive and negative whether it's
inverted or not and then we have the
igene values and the igene vectors igene
values and igene vectors are the
mathematical value that are extracted
from the co-variance
table they are responsible for the
generation of a new set of variables
from the old set of variables which
further lead to the construction of the
principal components igene vectors do
not change directions after linear
transformation ige values are the
scalers or the magnitude of the IG
vectors and again again this is just
change transforming that data so we're
going to change uh the vector B to the B
Prime as denoted on the chart and so
when we have like multiple variables how
do we calculate that new variable and
then we have feature vectors feature
vectors is simply a matrix that has IG
vectors of the components that we decide
to keep as the columns here we decide
whether we must keep or discard the less
significant principal components that we
have generated in the above steps this
becomes really important as we start
looking at U the back end of this and
we'll do this in the demo uh but one of
the more important steps to understand
and so we have the PCA example consider
Matrix x with n rows or observations and
K columns or variables now for this
Matrix we would construct a variable
space with as many dimensions as the
variable but for our Simplicity let's
consider three dimensions for
now now each observation row of the
Matrix X is placed in the K dimensional
variable space such that the rows in the
data table form a swarm of points in
this
space now we find the mean of all the
observations and then place it along the
data points on the
plot the first principal component is a
line that best accounts for the shape of
the point swarm it represents a maximum
variance Direction in the
data each observation may be projected
onto this line in order to get a
coordinate value along the pc1 this
value is known as a
score usually only one principal
component is insufficient to model the
systematic variation for a data set thus
a second principal axis is
created the second principal component
is oriented such that it reflects the
second largest source of variation in
the data while being orthagonal to pc1
PC2 also passes through the average
Point let's go ahead and pull this up
and just see what that means uh inside
our python scripting I'm going to use
the Anaconda Navigator and I will be in
Python
3.6 for this example I believe there's
even like a 3.9 out I tend to stay in
3.6 because a lot of the models I use
especially with the neural networks are
stable in 36
and then we open up our uh Jupiter I'm
in Chrome and we go ahead and create a
new Python
3 and for ease of use uh our team in the
back was nice enough to put this
together for me and we'll go and start
with the libraries the first thing I
like to do whenever I'm looking at any
new setup uh well you know what Let's do
let's do the libraries first we're going
to do our basic libraries which is
matplot Library uh the PLT from the
matplot library pandas are data frame uh
PD numpy or numbers array NP caborn for
graphing SNS that goes with the plot
that actually sits on Matt plot Library
so the Seaborn sits on there and then we
have our Amber sign because we're in
Jupiter notebook map plot library in
line the newer version actually doesn't
require that um but I put it in there
either anyway just because I'm so used
to it
and then we want to go ahead and take a
look at the
data and in this case we're going to
pull in uh certainly you can have lots
of fun with different data but we're
going to use the cancer data set u one
of the reasons a cancer data set is is
it has like 36 35 different features so
it's kind of fun to use that as our base
for this and we'll go ahead and run this
and look at our
keys and the first thing we notice in
our keys for the cancer data set
uh we have our data we have our Target
our frame Target names description
feature names and file
name so what we're looking for in all
this is um well let's take a look at the
description let's go in here and pull up
the description on
here I'm not going to spend a huge
amount of time on the description um
because this is we don't want to get
into a medical domain we want to focus
on our PCA setup
uh what's important is you start looking
at what the different attributes are
what they mean um if you were in the
medical field you'd want to note all
these different things whether what
they're measuring where it's coming from
you can actually see the actual
different um measurements they're
taking no missing
attributes we page all the way to the
bottom and you're going to have your
data in this case our
Target and if you dig deep enough to the
Target uh let's actually do this let's
go ahead and print Target
names real quick here I always like to
to just take a look and see what's on
the other end of this uh
Target
names run
that and so the target name is is it
malignant or is it B9 um so in other
words is this uh dangerous growth or is
it something we don't have to worry
about that's the bottom line with the
cancer in this
case and then we can go ahead and load
our data and uh you know what let me go
up a just a notch here for easy of
reading it's hard to get that just right
that's all we have to do uh so let's go
ahead and look at our data uh our our
we're going to use our pandas and we're
going to go ahead and uh do our data
frame it's going to equal cancer data
columns equals cancer feature equals
feature names so remember up here we
already loaded the the U names up of our
of the features in there what is going
to come out of this let me just see if I
can get to
that it's at the top of Target names um
that's just this list of names here in
the
setup and we can go ahead and run this
code and it'll print the head and you
can see here we have the mean radius the
mean texture mean perimeter I I don't
know about you this is a wonderful data
set if you're playing with it because
like many of the data that most of the
data that comes in half the time we
don't even know we're looking at uh
we're just handed a bunch of stuff as a
data scientist going what the heck is
this and so this is a good place to
start because this has a number of
different features in there and we have
no idea what these feature means or
where they come from we want to just
look at the data and figure that
out and now we actually are getting into
the PCA side of it as we've noticed for
is difficult to visualize High
dimensional data we can use PCA to find
the first two principal components and
visualize the data this new
two-dimensional space with a single
scatter plot uh before we do this we
need to go ahead and scale our
data now I haven't run this to see if
you really have to scale the data on
this um but as just a
general runtime I almost do that as the
first step of any modeling even if it's
pre-modeling as we're doing here um in
neural networks that is so important
with PCA visualization it's already
going to scale it when we do the means
and deviation inside the PCA uh but just
in case it's always good to scale
it and then we're going to take our PCA
with the pyit learn uses very similar
process to other pre-processing
functions that come with pyit learn we
instantiate a PCA object find the
princial components using the fit method
then apply the rotation and
dimensionality reduction by calling
transform we can also specify how many
components we want to keep when creating
the PCA
object and so the code for
this oops getting a little bit
ahead let me go and run this code uh so
the code for this is from sklearn
decomposition import PCA PCA equals PCA
in components equals 2 and that's really
important to note that um because we're
only going to want to look at two
components I would never go over four
components especially if you're going to
demo this with somebody else if you're
showing this to the
shareholders the whole idea is to reduce
it to something people can
see uh and then the PCA fit we're going
to uh is going to take the scale data
that we generated up here and then you
can see we created our PCA model with in
components equals
2 now whenever I use a new tool I like
to go in there and actually see what I'm
using so let's go to the S kit uh web
page for the
PCA and you can see in here here's our
call statement it describes what all the
different uh setups you have on
there probably the biggest one to look
at would be um well the biggest one is
your components how many components do
you want uh which you have to put in
there pretty much and then you also
might look at the SVD solver it's on
auto right now but you can override that
and do different things with it it does
a pretty good job as it
is and if we go down all the way down
to
um here we go to our methods if you
notice we have fit we have hit
transform nowhere in here is predict
because this is not used for prediction
uh it's used to look at the data again
we're in the describ setup we're fitting
the data we're taking a look at it uh
we've already looked at our minimum
maximum we've already looked at what's
in each quarter we've done a full
description of the data this is part of
describing the data um that's the
biggest thing I take away when I come
zooming in here and of course they have
examples of it down here if you
forget um and the biggest one of course
is the number of components and then uh
I mean the rest you can play with um the
actual solver whether you're doing a
full randomize there's different things
it does pretty good on the
auto and now we can transform this data
to its first two principal
components and so we have our um X
PCA we're going to set that equal to PCA
transform scaled
data uh so there we go there's our first
transformation and let's just go ahead
and print the scaled data shape and the
xpc data
shape and the reason we want to do this
is just to show us um what's going on
here we've taken 30 features I think I
said 36 or something like that but it's
30 and we've compressed it down to two
features and we decided we wanted two
features and that's where where this
comes from uh we still have 569 data
sets I mean data rows not data sets we
still have 569 rows of data but instead
of computing 30 features we're now only
doing our model on two
features so let's go ahead and uh uh
plot these and take a look and see
what's going
on and
uh we're just going to use our p LT
figure we're going to set the figure
size on here here's our scatter plot um
X
PCA xcore PCA of uh of one these are two
different perceptions we're using uh and
then you'll see right here C for color
cancer equals Target and so remember we
have zero we have one and if I remember
correctly zero was malignant one was
B9 uh so every everything in the zero
column is going to be one color and the
other color is going to be one and then
we're going to use the plasma map just
kind of telling you what color it is add
some labels first principal component
second principal component we'll go and
run this and you can see here instead of
having a chart one of those heat maps
with 30 different columns in it we can
look at this and say hey this one
actually did a pretty good
job of separating the
data and a couple things when I'm
looking at this that I notice is first
we have a very clear area where it's
clumped
together um where it's going to be
benign and we have a huge area it's
still clumped together more spread out
where it's going to be uh malignant or I
think I had that backwards and then in
the middle because we're dealing with
something in this particular case cancer
we would try to separate I would be
exploring how to separate this middle
group out
in other words there's an area where
everything overlaps and we're not going
to have a clear result on it uh just
because those are the people you want to
go in there and have extra tests or
treat it differently versus going in and
saying just cutting into the can into
the cancer so the body absorbs it and it
dissipates versus uh actively going in
there removing it testing it going
through chemo and all the different
things that's a big difference you know
as far as what's going to happen here in
that middle line where they overlap is
going to be huge that's domain specific
uh going back to the data we can see
here uh clearly by using these two
components we can easily separate these
two classes so the next step is what
does that mean interpreting the
components unfortunately with this great
power of dimensionality reduction comes
the cost of not being able to easily
understand what these components
represent I don't know what principal
component one looks represents or second
principle the components correspond to
combinations of original features the
components themselves are stored as an
attribute of the filtered PCA object and
so when we t look at that we can go
ahead and do uh look at the PCA
components this is in our model we built
we've trained it we can run that and you
can see here's the actual components uh
it's the two components each have their
own
array and within the array you can see
the um what the scores they're using and
these actually give weight to what
features are doing
what so in this numpy Matrix array each
row represents a principal component and
each column relates back to the original
features what's really neat about this
is we can now go in
reverse and drop this onto a heat map
and start seeing uh what this means and
so let me go ahead and just put this
down up I already got it down
here uh we go ahead and put this in here
we're going to use our um DF comp data
frame and we do our PCA
components and I want you to notice how
easy this is uh we're going to set our
columns equal to cancer feature names
that just makes it really easy and we're
dumping it into a data frame what's neat
about a data frame is when we get to
Seaborn it will pull that data frame
apart and U and set it up for us where
we want and so we're just going to do
the CN the Seaborn heat map of our data
frame uh composition and we'll use the
plasma
coloring and it creates a nice little
color graph here you can see we have the
mean radius and all the different
features along the bottom on the right
uh we have a scale so we can see we have
the dark colors all the way to the
really light colors which are what's
really shining there this is like the
primary stuff we want to look
at so this heat map in the color bar
basically represent the correlation
between the various features and the
principal component
itself so you know very powerful map to
look at and then you can go in here and
we might notice that the mean radius
look how how on the bottom of the map it
is um on some of this uh so you have
some interesting correlations here that
change the variations on that and what
means what this is more when you get to
uh post cribe you can also use this to
try to guess as what these things mean
and what you want to change to get a
better result regularization in machine
learning so our agenda on this one is
fitting the data understanding linear
regression bias and variance what is
overfitting what is underfitting and
those are like the biggest things right
now in data science is overfitting and
underfitting what does that mean and
what is regularization and then we'll do
a quick Hands-On demo to take a look at
this so fitting the data let's start
with fitting the data and we talk about
uh what is data fitting it's a process
of plotting a series of data points and
drawing the best fit line to understand
the relationship between the variables
and this is what we called Data fitting
and you can see here we have a couple of
lines we've drawn on this graph we're
going to go in a little deeper on there
uh so we might have in this case just a
two dimension we have an efficiency of
the car and we have the distance
traveled in 1,000
km and so what is data fitting well it's
a linear relationship and a linear
relationship very specifically linear
means line uh the line used to represent
the relationship is a straight line that
passes through the data points and the
variables have linear
relationship linear
regression so let's start with uh how
linear regression Works a linear
regression ression uh finds the line
that best fits the data point and gives
a relationship between the two
variables and so you can see here we
have the efficiency of the car um versus
the distance traveled and you can see a
nice straight line drawn through there
and when you talk about multiple
variables all you're doing is putting
this instead of a line it now becomes a
plane it gets a little more complicated
with multiple variables but they all
come down to this linear kind of uh
drawing a line through your data and
finding what hits the the data the
best and so we can consider an example
uh let's say that we want to find the
relationship between the temperature
outside versus the sales of ice cream
and so we start looking at that we're
looking at the how many ice cream cones
we're selling or how much money we sold
in ice cream and we're looking at how
warm it is outside which would hopefully
draw a lot of people into the ice cream
store and suppose we have two lines
we're going to draw L1 and L2 and we're
going to kind of guess which one we
think is the best fit and which claim to
describe the relationship between the
variables and so first we find the
square of the distance between the line
L1 and each data point and add them all
and find the mean
distance and I want you to think about
that when we um uh Square something if
it's a negative or positive number it no
longer matters because a minus 2^ 2ar is
4 2^ 2 is 4 so we're removing what's of
the line it's on and we're just looking
for the error in this case the mean
distance of each of the little dotted
lines you see
here this way of calculating the square
of the distance adding them and then
taking the mean is called mean square
error or loss function and we talk about
loss how far off are we that's what
we're really talking about what did we
miss uh and we have a positive distance
and a negative distance and of course
when we Square it it is neither just
becomes a positive error
and so we take the mean square error and
a lot of times you'll see it referred to
as MSE uh if I look in a code and I'm
going through my python code and I see
MSE I know that's a mean squared error
and we take all the dotted lines and we
calculate this eror we add them all
together and then we average it or find
the
means and in this case they ran a demo
on this and it was uh 1,1
127.23 uh for our L1 line
now we find the loss function for line
L2 in a similar fashion and we get the
mean square error to be
6,397 and it's computed the same way so
maybe you put this line just way outside
the data range and this is the error you
get by analyzing our results we find
that the loss function or the mean
square error is less for L1 than L2
hence L1 is the best fit
line uh this process describes a lot of
machine learning
processes we're going to keep guessing
and get as close as we can to find the
right answer we have to have some way to
invi to um calculate this and figure out
which one's the best and the mean square
error is one of the better fits to doing
for doing this and most commonly
used we really want to talk about bias
and variance very important terms to
know in machine learning and with linear
regression so bias bias occurs when an
algorithm has limited flexibility to
learn from
data variance defines the algorithm's
sensitivity to specific sets of data
let's start with bias and variance you
can see here we have um the two
different setups uh bias you can think
is very generalized where variance is
very
specific uh and so we talk about bias
such models pay very little attention to
the training data and oversimplify the
model therefore the validation error or
prediction error and training error
follow similar
Trends and uh with bias if you over
simplify it so much you're going to miss
your um local uh if if you have like a
really good fit you're going to miss it
you're you're going to just kind of
guess what the average is and that's
what your answer is going to be
with variance a model with a high
variance pays a lot of attention to
training data and does not generalize
therefore the validation error or
prediction error are far apart from each
other such models always lead to a high
error on training and test data as a
bias does where variance such models
usually perform very well on training
data but have high error rates on test
data and I want you to think about this
um when we're talking about a bias uh
the error is going to be High both when
you're training it and you're testing it
why because we're just kind of getting
an average we're not really fitting it
close with variance we're fitting it so
close that the test data does really
good it's going toell it every time uh
if you're doing categorical testing
that's a car that's a truck that's a
bicycle um but with variance suddenly a
truck has to have certain features and
it might have to be red uh because you
had so many red pictures so if it has if
it's an 18-wheeler it has to be red if
it's uh blue then it has to be a bicycle
that's the kind of variance we're
talking about where it picks up on
something and it it cannot get the right
answer unless it gets a very specific
data and we see that so that as you're
testing it your models and you
programmed it you got to look for how I
trained it what is coming out and if
it's not if it's not looking good on
either bias or or if it's not looking
good on the training
or on the test data than your bias uh
than your bias in your data if it really
looks good on the training data then
that's going to be your variance you've
overfitted the data and those are very
important things to know when you are
building your models uh in regression of
any kind or any kind of uh setup for
predicting so in dark games if all the
data fall on a particular pointer this
can be considered as a biased throw and
the player aims for the particular score
for variance if all the darts fall on
different pointers and no two darts fall
on the same pointer then this can be S
considered as a varied throw and the
player aims for various scores uh again
the bias sums everything up in one point
kind of averages it together where the
variance really looks for the individual
uh predictions coming
out so let's go ahead and talk about
overfitting uh when we talk about
overfitting it's a scenario where the
machine learning model tries to learn
from the details along with the noise
and the data tries to fit each data
point on the curve uh you can see that
um if you plug in your coordinates
you're just going to get the whatever
it's fited every point on the data
stream there's no average there's no two
points that might have the me know y
might have two different answers because
uh if the wind blows a certain way um in
the efficiency of your car maybe you
have a headwind so your car might alter
how efficient it is as it goes and so
there's going to be this variance on
here and this says no you can't have any
variance what's you know that this is
it's going to be exactly this it can't
be any you can't be the same speed or
the same car and have a slightly
different
efficiency so as the model has very less
flexibility it fails to predict new data
points and thus the model rejects every
new data point during the
prediction uh so you'll get get like a
really high error on
here and so uh reasons for overfitting
uh data used for training is not cleaned
and contains noise garbage values in it
you can spend so much time cleaning your
data and it's so important it's so
important that if you have if you have
some kind of something wrong with the
data coming in it needs to be addressed
whether it's a source of the data maybe
they use in medical different measuring
tools uh so you now have to adjust for
data that came in from hospital a versus
Hospital b or even off of machine a and
machine B that's testing something and
those those numbers are coming in
wrong the model has a high variance uh
again wind is a good example I was
talking about that with the car you may
have a 100 tests but because the wind's
blowing it's all over the
place uh size of training data used is
not enough so a small amount of data is
going to also cause this problem you
only have a few points and you try to
plot everything the model is too complex
uh this comes up a lot we put too many
pieces together and how they interact
can even be tracked um and so you have
to go back break it up and find out
actually what correlates and what
doesn't so what is underfitting a
scenario where machine learning models
can neither
learn the relationship between the data
points nor predict or classify a new
data point and you can see here we have
our efficiency of our car and our line
drawn and it's just going to be way off
for both the training and the predicting
data as the model doesn't fully learn
the patterns it accepts every new data
point during the prediction so instead
of looking for a general pattern uh we
just kind of accept
everything data used for training is not
cleaned and contains noise garbage and
values again underfitting and
overfitting same issue you got to clean
your
data the model has a high
bias uh we've seen this in all kinds of
things
from uh the mo the most common is the
driving cars to facial identification or
whatever it is the model itself when
they build it might have a bias towards
one thing and this would be an
underfitted model would have that bias
because it's averaged it out so if you
have um five people from India and 10
people from um Africa and 20 people from
the US you've created a bias uh because
it's looking at the 20 people and you
only have a small amount of data to work
with size of training data used is not
enough uh that goes with the size I was
just talking about so we have a model
with a high bias we have size of
training data used is not enough the
model is too
simple again this is one straight line
through through all the data when it
needs has a slight shift to it for other
reasons so what is a good fit uh a
linear curve that best fits the data is
neither overfitting or underfitting
models but is just right and of course
we have the nice examples here where we
have overfitting lines going up and down
every Point's starting to be include
glued
underfitting um the line really is off
from where the data is and then a good
fit is got to get rid of that minimize
that um error coming through
regularization is taking the guesswork
out you're looking at this graph you're
going oh which one is that really
overfit or is that underfit that pretty
hard to
tell so we talk about
regularization regularization techniques
are used to calibrate the linear
regression models and to minimize the
adjusted loss function and prevent
overfitting or
underfitting so what that means uh in
this case we're going to go ahead and
take a look at a couple different things
we're going to look at regularization
which we'll start with a linear model
we'll look at the Ridge regularization
and the lasso
regularization and these models are just
like uh just like we did the um MLP the
multi-layered positron in the sklearn
module you could bring in the ridge
module and you can bring in the lassel
module so when we talk about um Ridge
regression it modifies the overfitted or
underfitted models by adding the penalty
equivalent to the sum of the squares of
the magnitude of the
coefficients and so we have a cost
function equals loss equals uh Lambda
time the sum of W squared or the
absolute value of w depending on how
you're doing it now remember we talked
about error where we either Square it or
we absolute value it uh because that
removes a plus or minus sign on there
and there's reasons to do it either way
but it is more common to square the
value
and then we have our uh um in this case
the Lambda is going to be the penalty
for the errors we've thrown in a Greek
character for you just to confuse
everybody and W is the slope of the
curve of the
line so uh we're going to look at this
and we're going to draw a line this is
going to be like a linear regression
model so if you had in sklearn you could
import just a standard linear regression
model uh it would plot this line across
whatever data we're working
on and we look at this and of course
we're just extrapolating this um I know
they use some specific data but you
don't want to get into the actual domain
and so for a linear regression line
let's consider two points that are on
the
line and we'll go ahead and have a loss
equal zero uh considering the two points
on the line we'll go and do Lambda
equals 1 we'll set our W is going to be
1.4 then the cost function equal 0 + 1 *
1.42 which = 1.96
uh so really don't get caught up too
much in the math on this other than
understanding um that this is something
that's very easy for a computer to
calculate and if you ever see the loss
plus the plus the Lambda time W the
sumus
w^2 and then let's say we have a ridge
uh regression line and it does this we
go ahead and plot it and we do the
calculations on the
data and for the rid regression let's
assume a loss equals 3^2 +22 = .13 so
when they put all the calculations
through of the two points we end up with
the 0.0
62 uh so we now had a linear regression
model we now had a ridge regression
model and the ridge regression model
plots a little differently than the
standard linear regression
model and comparing the two models with
all the data points we can see that the
ridge regression line fits the model
more accurately than the linear
regression line
and I find this true on a lot of data I
work with I'll end up using either the
ridge regression model or the lasso uh
Mars regression model for fitting um
especially dealing with a lot of like uh
stock markets daily um setup they come
out slightly better uh you get a
slightly better um
fit and so we have our lasso we just
talked about lasso coming in
here and the cost function equals uh um
instead of doing a squared we're just
going to do the absolute value and so if
you remember this is where Ridge
regression changes where's my Ridge
regression model uh we're squaring the
value
here and if you look at this we're not
squaring the value we're just finding
the absolute value on here and so the
loss of the of the squared
individuals um and here is
our Lambda symbol again penalty for
errors and W equals the slope of the
Curve and comparing the two models with
all the data points we can see that the
lasso regression line fits the model
more accurately than the linear
regression
line and this is like I said I I use
these two models a lot uh the Ridge and
this is important this is this is kind
of the meat of the matter how do you
know which one to use some of it is you
just do it a bunch of times and then you
figure it out uh Ridge regularization is
useful when we have many variables with
relatively smaller data samples the
bottle does not encourage convergence
towards zero but is likely to make them
closer to zero and prevent
overfitting the Lasser regularization
model is preferred when we are fitting a
linear model with fewer
variables so in the LA in the iris thing
we had four or five variables as they
measured the different Leaf pieces uh
you might be doing the measurements on
the cancer project which has 36
different variables um so as we get down
to the iris with four variables lassar
will probably work pretty good where you
might use the ridge regularization with
more model with if you have something
significantly larger and it encourages
the coefficients of the variables to go
toward zero because of the shape of the
constraint which is an absolute
value and with any of this we want to go
ahead and uh do a demo and lasso and
ridge regression so let's take a look
and see what that looks like in our code
and bring up our put notebook we'll
start with our Imports pandas as PD
import numpy as NP import map plot
Library as PLT um sklearn we're going to
import our data sets it's kind of more
generic um we usually just import one
data set instead of all of them but you
know quick and dirty when you're putting
some of these together uh we have our
sklearn model selection we're going to
import our train test split for
splitting our data up and then we'll
bring in our linear regression model
and we'll go ahead and run these just to
load them up and then load our data set
we're just talking about that uh you
could just um um have imported the load
Boston and Boston data set in there
instead of loading all the data sets
um and then once we've loaded our data
set we want to go ahead and take a look
at that data and see what we got here
let me just go and pop that down there
and go and run it and so we've got a and
taken our uh Boston uh data we're going
to look we put it into our pandas data
frame um the Boston data set and then
the Boston columns so we want to see
what's going on with them um we have our
Target we have the house price uh Etc
and so our x equals Boston um
iocation now remember in pandas the new
updates to pandas they want iocation if
you're going to pull data we used to be
able to leave this off but it does
something different it creates a slice
versus a direct um setup so make sure
you're using that ey location and the I
the output so this is all just bringing
our data
together and we can see here if we do uh
we print the Boston um Panda's head we
can see here all of our different um
aspects we're looking
for and if you're following the X and
the Y the x is um everything except for
the last column where Y is uh all the
it's that's what this means all the rows
except for the last column and then Y is
all the rows but just the last column so
Y is our house price and the x is the
Crim xan industry Chaz Knox and all
these other different um statistics
they've collected for house sales in
Boston there we go oops control ah
so we'll go ahead and split our data uh
X train and our x uh test y train y test
equals the train test split which we
imported and we have our Boston U you
could have easily used the X and Y on
here as opposed to Boston um
iocation and we'll create our test size
we're going to take 25% of the data and
put it in as a
test uh and then we'll go ahead and run
this uh
need an extra drink there uh so we have
our train and test and then of course
the print the train data
shape I love doing this kind of thing
whenever I'm working with this data
print out the shape make sure everything
looks correct uh so that we have 127 by
13 and 127 by 1 379 by 13 they should
match and if there if the the data sets
are not quite matching then you know
know something's wrong and you're going
to get all those errors I don't know how
many times I've gone through here and
it's dropped a row on one of them and
not on the other or something weird has
happen when I'm cleaning the data this
is pretty straightforward and simple
because the data comes in a nice
prepackage is all clean for
you so let's go ahead and apply apply
the multiple linear regression
model and uh we'll call this l r lreg
linear regression and we're going to go
ahead and fit that linear regression
model to X train and Y
train then we'll generate the prediction
on the test Set uh so here's our lre y
predict with our X test going into the
prediction and let's calculate that mean
square error msse I told you you'll see
MSE used a lot um people use it in
variables and things like that it's
pretty common and we get our mean
squared error equals uh this is just the
basic formula we've already been talking
about what's the difference
squared uh then we look for the average
of that we'll go ahead and just run
this and you can see when we get through
the end of this we have our mean square
error on
test we have our total and then we have
each column coming
down and at this point unless you really
know the data you're working with it's
not going to mean a whole lot so if it's
in your domain you might be know what
you're looking at when you see see these
kinds of numbers coming up uh but if
it's not it's just a bunch of numbers
and that's okay at least that's okay for
this demo uh and then we're going to go
ahead and plot these so we can see
what's going on and this is always kind
of fun it's always nice to have a nice
visual of what you're looking at and you
can see here um when we plot the
coefficient scores on here and we the
guys in the back did a great job putting
some pretty colors together making it
look nice and setting up the
columns you you can see here uh your nox
has like just a huge coefficient um when
I look at a table like this I look for
what has very
little um different coefficients they're
not using a huge change and what has
huge changes and that Flags you for all
kinds of things as you're working with
the data but it depends so much on the
domain you're working
with these are great things though as uh
just a quick look to see what's going on
with your data and what you're looking
for and of course one we look at this
now our motive is to reduce the
coefficient score so now we want to take
these and and uh bring them down as much
as we can and for that we're going to
work with the ridge regression on here
so let's start by going ahead and we're
going to import our Ridge model the
ridge regression from the sklearn
library or the
pyit and we're going to go ahead and
train the model so here's our Ridge r
equal Alpha equal 1 and I mentioned that
earlier
um when I work with the ridge model
you'll see Alpha equals 1 if you set
Alpha equal to zero that's a standard
linear regression model so you have
Alpha equals 1 2 3 4 and you usually use
1 2 or 3 4 and a standard integer on
there and we'll go ahead and fit the
ridge model on there with our X train
and our YT train data generate a
prediction for that um for our X test
and we'll calculate the mean square
error uh just like we did before this
should all look familiar here and we'll
go ahead and print that out and we'll
look at the uh Ridge coefficients for
our data and see what that looks
like now if I jump up and down between
these
two you'll get a headache
uh you'll still see the nox value let's
just look at the nox because that was
the biggest value with a minus 9 here
and if we go back up here the nox value
is a
minus8 so right off the bat I'm seeing a
huge change um in the biggest
coefficient
there uh so if we're going to do that
nice setup we want to go ahead and just
print it and see what that looks
like here we go and we've uh set up our
um plot subplots and again the team put
together some nice colors so it makes it
look good we're doing an xar based on
the columns and our um L regress
coefficients color equals color X spine
bottom and so forth uh so just puts
together a nice little
graph and you're starting to see uh one
this when you compare this if you put on
the same graph as this one up here this
is up here at minus 18 this is at Min -
9 and so this graph is half the size of
the graph above uh the same thing with
these values here they might look the
same but they're actually all
almost half the value on
here and then finally you can do the
same thing for the lasso
regression this will all look uh very
similar as far as what we worked on
before and I'm just going to ahead and
print that on here and run it and again
let's go up to uh nox look where KNX is
it's all the way down to zero and if we
look at our next biggest coefficient is
minus8 and really here's our
22.73 we go up
here um
16.7 and we go up here and we look at
the same number uh
16.69% 16. 78 69 is better than 78 uh so
from the very beginning we might start
looking at this first model for
overall
predicting but there's other factors
involved uh we might know that uh the
kned value is Central and the other ones
aren't quite as good and so we might
start looking at just certain setups
like what is our what is this particular
coefficient because it might have a
certain meaning to us and so forth and
so you got to look at all those
different thing um items in there again
but the bottom dollar is our first model
did better than our other two models our
mean square error on the test set um
continues to come down on this we're
going to cover reinforcement learning
today and what's in it for you we'll
start with why reinforcement learning
we'll look at what is reinforcement
learning we'll see what the different
kinds of learning strategies are that
are being used today in computer models
under supervised versus unsupervised
versus
reinforcement we'll cover important
terms specific to reinforcement learning
we'll talk about markov's decision
process and we'll take a look at a
reinforcement learning example well
we'll teach a tic-tac-toe how to play
why reinforcement learning training a
machine learning model requires a lot of
data which might not always be available
to us further the data provided might
not be reliable learning from a small
subset of actions will not help expand
the vast realm of solutions that may
work for a particular problem and you
can see here we have the robot learning
to walk um very complicated setup when
you're learning how to walk and you'll
start asking questions like if I'm
taking one step forward and left what
happens if I pick up a 50 pound object
how does that change how a robot would
walk these things are very difficult to
program because there's no actual
information on it until it's actually
tried out learning from a small subset
of actions will not help expect the vast
realm of solutions that may work for a
particular
problem and we'll see here it learned
how to walk this is going to slow the
growth that technology is capable of
machines need to learn to perform
actions by themselves and not just learn
off
humans and you see the objective climb a
mountain real interesting point here is
that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it most of the models the
non-reinforcement models in computer uh
machine learning aren't able to do that
very well uh there's a couple of them
that can be used or integrated see how
it goes is what we're talking about with
reinforcement learning so what is
reinforcement learning reinforcement
learning is a subbranch of machine
learning that trains a model to return
an Optimum solution for a problem by
taking a sequence of decision Decisions
by itself consider a robot learning to
go from one place to another the robot
is given a scenario must arrive at a
solution by itself the robot can take
different paths to reach the
destination it will know the best path
by the time taken on each path it might
even come up with a unique solution all
by itself and that's really important is
we're looking for Unique Solutions uh we
want the best solution but you can't
find it unless you try it so we're
looking at uh our different systems or
different model we have supervised
versus unsupervised versus reinforcement
learning and with the supervised
learning that is probably the most
controlled environment uh we have a lot
of different supervised learning models
whether it's linear regression neural
networks um there's all kinds of things
in between decision trees the data
provided is labeled data with output
values specified and this is important
because when we talk about supervised
learning you already know the answer for
all this information you already know
know the picture has a motorcycle in it
so you're supervised learning you
already know that um the outcome for
tomorrow for you know going back a week
you're looking at stock you can already
have like the graph of what the next day
looks like so you have an answer for
it and you have labeled data which is
used you have an external supervision
and solves Problems by mapping labeled
input to know output so very
controlled unsupervised learning and
unsupervised learning is really
interesting because it's now taking part
in many other models they start with an
you can actually insert an unsupervised
learning model um in almost either
supervised or reinforcement learning as
part of the system which is really cool
uh data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering
output uh so you can look at this and
you can think um some of these things go
with each other they belong together so
it's looking for what connects in
different ways and there's a lot of
different algorithms that look at this
um when you start getting into those are
some really cool images that come up of
what unsupervised learning is how it can
pick out say uh the area of a donut one
model will see the area of the donnut
and the other one will divide it into
three sections based on this location
versus what's next to it so there's a
lot of stuff that goes in with
unsupervised learning and then we're
looking at reinforcement learning
probably the biggest industry in today's
market uh in machine learning or growing
Market it's very in its very infant
stage uh as far as how it works and what
it's going to be capable of the machine
learns from its environment using
rewards and errors used to solve
reward-based problems no predefined data
is used no supervision follows Trail and
error problem solving approach uh so
again we have a random at first you
start with a random I try this it works
and this is my reward doesn't work very
well maybe or maybe doesn't even get you
where you're trying to get it to do and
you get your reward back and then it
looks at that and says well let's try
something else and it starts to play
with these different things finding the
best route so let's take a look at
important terms in today's reinforcement
model and this has become pretty
standardized over the last uh few years
so these are really good to know we have
the agent uh agent is the model that is
being trained via reinforcement learning
so this is your actual U entity that has
however you're doing it whether you're
using a neural network or a a q table or
whatever combination thereof this is the
actual agent that you're using this is
the
model and you have your environment uh
the training situation that the model
must optimize to is called its
environment uh and you can see here I
guess we have a robot who trying to get
a chest full of gems or whatever and
that's the output and then you have your
action this is all possible steps that
can be taken by the model and it picks
one action and you can see here it's
picked three different uh routes to get
to the chest of diamonds and
gems we have a state the current
position condition returned by the
model and you could look at this uh if
you're playing like a video game this is
the screen you're looking at uh so when
you go back here the environment is a
whole game board so if you're playing
one of those mobus games you might have
the whole game board going on uh but
then you have your current position
where are you on that game board what's
around that what's around you um if you
were talking about a robot the
environment might be moving around the
yard where it is in the yard and what it
can see what input it has in that
location that would be the current
position condition returned by the model
and then the reward uh to help the model
move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yeah you
did good or if uh didn't do as good
trying to maximize the reward and have
the best reward
possible and then policy policy
determines how an agent will behave at
any time it acts as a mapping between
action and present State this is part of
the model what what what is your action
that you're you're going to take what's
the policy you're using to have an
output from your agent one of the
reasons they separate uh policy as its
own entity is that you usually have a
prediction um of a different options and
then the policy well how am I going to
pick the best based on those predictions
I'm going to guess at different options
and we'll actually weigh those options
in and find the best option we think
will work uh so it's a little tricky but
the policy thing is actually pretty cool
how it works let's go ahead and take out
look at a reinforc learning example and
just in looking at this we're going to
take a look uh consider what a dog um
that we want to train uh so the dog
would be like the agent so you have your
your puppy or whatever uh and then your
environment is going to be the whole
house or whatever it is where you're
training them and then you have an
action we want to teach the dog to
fetch so action equals
fetching uh and then we have a little
biscuit so we can get the dog to perform
various actions by offering incentives
such as a dog biscuit as a
reward the dog will follow a policy to
maximize this reward and hence will
follow every command and might even
learn new actions like begging by itself
uh so you have B you know so we start
off with fetching it goes oh I get a
biscuit for that it tries something else
and you get a handshake or begging or
something like that and it goes oh this
is also reward-based and so it kind of
explores things to find out what will
bring it as biscuit and that's very much
like how a reinforced model goes is it
uh looks for different rewards how do I
find can I try different things and find
a reward that
works the dog also will want you run
around and play and explore its
environment uh this quality of model is
called exploration so there's a little
Randomness going on an
exploration and explores new parts of
the house climbing on the sofa doesn't
get a reward in fact it usually gets
kicked off the
sofa so let's talk a little bit about
markov's decision process uh markov's
decision process is a reinforcement
learning policy used to map a current
state to an action where the agent
continuously interacts with the
environment to produce new Solutions and
receive rewards and you'll see here's
all of our different uh uh vocabulary we
just went over we have a reward our
state our agent our environment and our
action and so even though the
environment kind of contains everything
um that you you really when you writing
the program your environment's going to
put out a reward in state that goes into
the agent uh the agent then looks at
this uh state or it looks at the reward
usually um first and it says okay I got
rewarded for whatever I just did or it
didn't get rewarded and then it looks at
the state and then it comes back and if
you remember from policy the policy
comes in um and then we have a reward
the policy is that part that's connected
at the bottom and so it looks at that
policy and it says hey what's a good
action that will probably be similar to
what I did or um uh sometimes they're
completely random but what's a good
action that's going to bring me a
different
reward so taking the time to just
understand these different pieces as
they go is pretty important in most of
the models today um and so a lot of them
actually have templates based on this
you can pull in and start using um
pretty straightforward as far as once
you start seeing how it works uh you can
see your environment send says hey this
is the agent did this if you're a
character in a game this happened and it
shoots out a reward in a state the agent
looks at the reward looks at the new
state and then takes a little guess and
says I'm going to try this action and
then that action goes back into the
environment it affects the environment
the environment then changes depending
on what the action was and then it has a
new state and a new reward that goes
back to the agent so in the diagram
shown we need to find the shortest path
between Noe A and D each path has a
reward associated with it and the path
with the maximum reward is what we want
to choose the nodes AB b c d denote the
nodes to travel from node uh A to B is
an action reward is the cost of each
path and policy is each path
taken and you can see here a can go uh
to b or a can go to C right off the bat
or you can go right to D and if you
explored all three of these uh you would
find that a going to D was a zero reward
um a going to C and D would generate a
different reward or you could go AC c b
d there's a lot of options here um and
so when we start looking at this diagram
you start to
realize that even though uh today's
reinforced learning models do really
good at um finding an answer they end up
trying almost all the different
directions you see
and so they take up a lot of work uh or
a lot of processing time for
reinforcement learning they're right now
in their infant stage and they're really
good at solving simple problems and
we'll take a look at one of those in
just a minute in a tic tac toe game uh
but you can see here uh once it's gone
through these and it's explored it's
going to find the
ACD is the best reward it gets a full 30
points for it so let's go ahead and take
a look at a reinforcement learning demo
uh in this demo we're going to use
reinforcement learning to make a tic tac
toe game you'll be playing this game
Against the Machine learning
model and we'll go ahead we're doing it
in Python so let's go ahead and go
through I always uh not always actually
have a lot of python tools let's go
through um Anaconda which will open up a
Jupiter notebook seems like a lot of
steps but it's worth it to keep all my
stuff separate and it's also has a nice
display when you're in the jupyter
notebook for doing python
so here's our Anaconda Navigator I open
up the notebook which is going to take
me to a web page and I've gone in here
and created a new uh python folder in
this case I've already done it and
enabled it to change the name to
tic-tac-toe uh and then for this example
uh we're going to go ahead
and import a couple things we're going
to um import numpy as NP we'll go ahead
and import pickle numpy of course is our
number array and then uh pickle is just
a nice way sometimes for storing uh
different information
uh different states that we're going to
go through on
here uh and so we're going to create a
class called State we're going to start
with
that and there's a lot of lines of code
to this uh class that we're going to put
in here don't let that scare you too
much there's not as much here um it
looks like there's going to be a lot
here but there really is just a lot of
setup going on in the in our class date
and so we have up here we're going to
initialize it um we have our board um
it's a tic teac toe board so we're only
dealing with nine spots on the board uh
we have player one player
two uh is in we're going to create a
board hash uh we'll look at that just a
minute we're just going to stir some
information in there symbol of player
equals one um so there's a few things
going on as far as the
initialization uh then something simple
we're just going to get the hash um of
the board we're going to get the
information from the board on there
which is columns and rows we want to
know when a winner occurs uh so if you
get three in a row that's what this
whole section here is for uh me go ahead
and scroll up a little bit and you can
get a copy of this code if you send a
note over to Simply learn we'll send you
over um this particular file and you can
play with it yourself and see how it's
put together I don't want to spend a
huge amount of time on this uh because
this is just some real General python
coding uh but you can see here we're
just going through um all the rows and
you add them together and if it equals
three three in a row same thing with
columns um diagonal so you got to check
the diagonal that's what all this stuff
does here is it just goes through the
different areas actually let me go ahead
and
put there we
go um and then it comes down here and we
do our sum and it says true uh minus
three just says did somebody win or is
it a tie
so you got to add up all the numbers on
there anyway just in case they're all
filled up and next we also need to know
available positions um these are ones
that don't no one's ever used before
this way when you try something or the
computer tries something uh it's not
going to give it an illegal move that's
what the available positions is doing uh
then we want to update our state and so
you have your position going in we're
just sending in the position that you
just chose and you'll see there's a
little user interface we put in there
you pick
pick the row and column in
there and again I mean this is a lot of
code uh so really it's kind of a thing
you'd want to go through and play with a
little bit and just read through it get
a copy of it uh great way to understand
how this works and here is a given
reward um so we're going to give a
reward result equals self winner this is
one of the hearts of what's going on
here uh is we have a result self. winner
so if there's a winner then we have a
result if the result equals one here's
our
feedback uh if it doesn't equal one then
it gets a zero so it only gets a reward
in this particular case if it
wins and that's important to know
because different uh systems of
reinforced learning do rewarding a lot
differently depending on what you're
trying to do this is a very simple
example with a a 3X3 board imagine if
you're playing a video game uh certainly
you only have so many actions but your
environment is huge you have a lot going
on in the environment and suddenly a
reward system like this is going to be
just um it's going to have to change a
little bit it's going to have to have
different rewards and different setup
and there's all kinds of advanced ways
to do that as far as weighing you add
weights to it and so they can add the
weights up depending on where the reward
comes in so it might be that you
actually you get a reward in this case
you get the reward at the end of the
game and I'm spending just a little bit
of time on this because this is an
important thing to note but there's
different ways to add up those rewards
it might have like if you take a certain
path um the first reward is going to be
weighed a little bit less than the last
reward because the last reward is
actually winning the game or scoring or
whatever it is so this reward system
gets really complicated on some of the
more advanced uh
setups um in this case
though you can see right here that they
give a um a 0.1 and a 0. five
reward um just for getting a picking the
right value and something that's
actually valid instead of picking an
invalid value so rewards again that's
like key it's huge how do you feed the
rewards back in uh then we have a board
reset that's pretty straightforward it
just goes back and resets the board to
the beginning cuz it's going to try out
all these different things while it's
learning it's going to do it by trial
and error so you have to keep resetting
it and then of course there's the play
we want to go ahead and play uh rounds
equals 100 depends on what you want to
do on here um you can set this different
you obviously set that to higher level
but this is just going to go through and
you'll see in here uh that we have
player one and player two this is this
is a computer playing itself uh one of
the more powerful ways to learn to play
a game or even learn something that
isn't a game is to have two of these
models that are basically trying to beat
each other and so they they keep finding
explore new things this one works for
this one so this one tries new things it
beats this we've seen this in um chess I
think was a big one where they had the
two players in chess with reinforcement
learning uh was one of the ways they
train one of the top um computer chess
playing
algorithms uh so this is just what this
is it's going to choose an action it's
going to try something and the more it
try stuff um the more we're going to
record the hash we actually have a board
hash where they self get the hash setup
on here where it stores all the
information and then once you get to a
win one of them wins it gets the reward
uh then we go back and reset and try
again and then kind of the fun part we
actually get down here is uh we're going
to play with a human so we'll get a
chance to come in here and see what that
looks like when you put your own
information in and then it just comes in
here does the same thing it did above it
gives it a reward for its things um or
sees if it wins or ties um looks at
available positions all that kind of fun
stuff and then finally we want to show
the board uh so it's going to print the
board out each
time really um as an integration is not
that exciting what's exciting uh in here
is one looking at this reward system
whoops Play One More up the reward
system is really the heart of this how
do you reward the different uh setup and
the other one is when it's playing it's
got to take an action and so what it
chooses for an action is also the heart
of reinforcement learning how do we
choose that action and those are really
key to right now where reinforcement
learning is um in today's uh technology
is uh figuring this out how do we reward
it and how do we guess the next best
action so we have our uh environment and
you can see the environment is we're
going to be or the state uh which is
kind of like what's going on we're going
to return the state depending on what
happens and we want to go ahead and
create our agent uh in this CL our
player so each one is let me go and grab
that and so we look at a class player um
this is where a lot of the magic is
really going on is what how is this
player figuring out how to maneuver
around the board and then the board of
course returns a state uh that it can
look at and a reward uh so we want to
take a look at this we have a name uh
self State this is class player and when
you say class player we're not talking
about a human player we're talking about
um just a uh the computer players and
this is kind of interesting so remember
I told you depending on what you're
doing there's going to be a Decay gamma
um explore rate uh these are what I'm
talking about is how do we train it
um as you try different moves it gets to
the end the first move is important but
it's not as important as the last one
and so you could say that the last one
has the heaviest weight and then as you
as you get there the first one let's see
the first move gives you a five reward
the second gives you a two reward and
the third one gives you a 10 reward
because that's the final ending you got
it the 10's going to count more than the
first step uh and here's our uh we're
going to you know get the board
information coming in and then choose an
action this was the second part that I
was talking about that was so important
uh so once you have your training going
on we have to do a little Randomness and
you can see right here is our NP random
uh uniform so it's picking out a random
number take a random action this is
going to just pick which row and which
column it is um and so choosing the
action this one you can see we're just
doing random States um Choice length of
positions action position and then it
skips in there and takes a look at the
board uh for p and positions you it's
actually storing the different boards
each time you go through so it has a
record of what it did so it can properly
weigh the values and this simply just
depins a hash date what's the last state
Pinn it to the uh um to our states on
here here's our
feedback reward so the reward comes in
and it's going to take a look at this
and say is it none uh what is the reward
and here is that formula remember I was
telling you about up here um that was
important because it has Decay gamma
times the reward this is where as it
goes through each step and this is
really important this is this is kind of
the heart of this of what I was talking
about earlier uh you have step
one and this might have a a reward of
two you have step two I should probably
should have done BC this has a step
three uh step
four so on till you get to step in and
this might have a reward of
10 uh so reward of
10 we're going to add that but we're not
adding uh let's say this one right here
uh let's say this reward here right
before 10 was um let's say it's also 10
that just makes the the math easy so we
had 10 and 10 we had 10 this is 10 and
10 n whatever it is but it's time it's
0. n uh so instead of putting a full 10
here we only do nine that's a 0.9
times
10 and so this
formula um as far as the Decay times the
reward minus the cell State value uh it
basically adds in it says here's one or
here's two I'm sorry I should have done
this ABC it would have been easier uh so
the first move goes in here and it puts
two in
here uh then we have our self uh setup
on here you can see how this gets pretty
complicated in the math but this is
really the key is how do we train our
states and we want the the final State
the win to get the most points if you
win you get most points uh and the first
step gets the least amount of points so
you're really training this almost in
Reverse you're training you're training
it from the last place where you have
like it says okay this is now where I
need to sum up my rewards and I want to
sum them up going in reverse and I want
to find the answer in Reverse kind of an
interesting uh uh play on the mind when
you're trying to figure this stuff
out and of course we want to go ahead
and reset the board down here uh and
save the policy load
policy these are the different things
that are going in between the agent and
the state to figure out what's going on
let's go ahead and load that up and then
finally we want to go ahead and create a
human
player and the human player is going to
be a little different uh in that uh you
choose an action row and column here's
your action uh if action is if action in
positions meaning positions that are
available uh you return the action if
not it just keeps asking you until you
get an action that actually works
and then we're going to go ahead and
append to the hash state which uh we
don't need to worry about because it
Returns the action up
here and feed forward uh again this is
because it's a
human um at the end of the game bat
propagate and update State values this
part isn't being done because it's not
programming uh the model uh the model is
getting its own rewards so we've gone
ahead and loaded this in here uh so
here's all our pieces and the first
thing we want to
do is set up uh P1 player one uh P2
player two and then we're going to send
our players to our state so now it has
P1 P2 and it's going to play and it's
going to play 50,000 rounds now we can
probably do a lot less than this and
it's not going to get the full results
in fact you know what uh let's go ahead
and just do five um just to play with it
because I want to show you something
here oops somewhere in there forgot to
load
something there we go I must have start
forgot to run this
run oops forgot a reference there for
the board rows and columns
3x3 um there is actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning uh so now I've only set this
up with um see where are we going here
I've only set this up to
train five times and the reason I did
that is we're going to uh come in and
actually play it and then I'm going to
change that and we can see how it
differs on
there there we go and I didn't even make
it through a run and we're going to go
ahead and save the
policy um so now we have our player one
and our player two policy uh the way we
set it up it has two separate policies
loaded up in
there and then we're going to come in
here and we're going to do uh player one
is going to be the computer experience
rate zero load policy one human player
human and we're going to go ahead and
play this and I remember I only went
through it um uh just one round of
training in fact minimal training and so
it puts an X there and I'm going to go
ahead and do row zero column one you can
see this is very uh basic on here and so
I put in my zero and then I'm going to
go zero blocket Z zero and you can see
right here it let me win uh just like
that I was able to win
zero two and woo human winds so I only
trained at five times we're going to run
this again and this time uh instead of
five let's do 5,000 or 50,000 I think
that's what the guys in the back had and
this takes a while to train it
this is where reinforcement learning
really falls apart look how simple this
game is we're talking about uh 3x3 set
of
columns and so for me to train it on
this um I could do a q table which would
take which would go much quicker um you
could build a quick Q table with almost
all the different options on there and
uh you would probably get a the same
result much quicker we're just using
this as an example so when we look at
reinforcement learning you need to be
very careful what you apply it to it
sounds like a good deal until you do
like a large neural network where you're
doing um you set the neural network to a
learning increment of one so every time
it goes through it
learns and then you do your action so
you pick from the learning uh setup and
you actually try actions on the learning
setup until you get the what you think
is going to be the best action so you
actually feed what you think is right
back through the neural network there's
a whole layer there which is really fun
to play
with and then it has an output well
think of all those processes I mean that
is just a huge amount of work it's going
to do uh let's go ahead and Skip ahead
here give it a moment it's going to take
a a minute or two to go ahead and
run now to train it uh we went ahead and
let it run and it took a while this this
took um I got a pretty powerful
processor and it took
about five minutes plus to run it and
we'll go ahead and
uh run our player setup on here oops it
brought in the last whoops I brought in
the last round so give me just a moment
to redo the policy save there we go I
forgot to save the policy back in
there and then go ahead and run our
player again so we we've saved the
policy and then we want to go ahead and
load the policy for P1 as a computer we
can see the computer's gone in the
bottom right corner I'm going to go
ahead and go uh one one which is the
center and it's gone right up the top
and if you have ever played tic-tac-toe
you know the computer has me uh but
we'll go ahead and play it out row zero
column
two there it is and then it's gone here
and so I'm going to go ahead and go row
01 2 no 01 there we go and column
zero that's where I wanted oh and it
says I okay you your action there we go
boom uh so you can see here we've got a
didn't catch the win on this it said Tai
um kind of funny they didn't catch the
win on
there but if we play this a bunch of
times you'll find it's going to win more
and more the more we train it the more
the reinforcement
happens this lengthy training process uh
is really the stopper on reinforcement
learning as this changes reinforce
enforcement learning will be one of the
more powerful uh packages evolving over
the next decade or two in fact I would
even go as far as to say it is the most
important uh machine learning tool and
artificial intelligence tool out there
as it learns not only a simple tick Tech
toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
it's in what's the environments it's in
and so being able to attach environment
and context and all those things
together is going to require
reinforcement learning to
do so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with uh run it you can test it
out you can do U you know test it for
different uh uh values you can switch
from P1
computer uh where we loaded the policy
one to load the policy 2 and just see
how it varies there's all kinds of
things you can do on there so what is Q
learning Q learning is reinforcement
learning policy which will fill the next
best action given a current state it
chooses this action at random and aims
to maximize the reward and so you can
see here's our standard reinforcement
learning graph um by now if you're doing
any reinforcement learning you should be
familiar with this where you have your
agent your agent takes an action the
action affects the environment and then
the environment sends back the reward or
the feedback and the state the new state
the a in where is it at on the
chessboard where is it at in the video
game um if your robots out there picking
trash up off the side of the road where
is it at on the road consider an ad
recommendation system usually when you
look up a product
online you get ads which will suggest
the same product over and over
again using Q learning we can make an ad
recommendation system which will suggest
related products to our previous
purchase the reward will be if user
clicks on the suggested
product and again you can see um you
might have a lot of products on uh your
web advertisement or your pages but it's
still not a float number it's still a
set number and that's something to be
aware of when you're using Q learning
and you can see here that if you have a
100 people clicking on ads and you click
on one of the ads it might go in there
and say okay this person clicked on this
ad what is the best set of ads based on
clicking on this at or these two ads
afterwards based on where they are
browsing so let's go a and take a look
at some important terms when we talk
about Q learning uh we have States the
state s represents the current position
of an agent in an
environment um the action the action a
is the step taken by the agent when it
is particular State rewards for every
action the agent will get a positive or
negative
[Music]
reward and again uh when we talk about
States we're usually not with when
you're using a q table you're not
usually talking about float variables
you're talking about true false um and
we'll take a closer look at that in a
second an episodes when an agent ends up
in a terminating State and can't take a
new
action uh this might be if you're
playing a video game your character
stepped in and is now dead or whatever
uh Q values used to determine how good
an action a taken at a particular state
s is q a of s and temporal difference a
formula used to find the Q value by
using the value of the current state and
action and previous state and action and
very I mean there's bellman's equation
which basically is the equation that
kind of uh covers what we just looked at
in all those different terms the Bellman
equation is used to determine the values
of a particular State and deduce how
good it is to be in take that state the
optimal the optimal state will give us
the highest optimal value Factor
influencing Q values the current state
and action that's your essay so your
current state in your
action uh then you have your previous
date in action which is your s um I
guess Prime I'm not sure how they how
they reference that S Prime a prime so
this is what happened before uh then you
have a reward for Action so you have
your R reward and you have your maximum
expected future
reward and you can see there's also a
learning rate put in there and a
discount rate uh so we're looking at
these just like any other model we don't
want to have an absolute um final value
on here we don't want it to if you do
absolute values instead of taking
smaller steps you don't really have that
approach to the solution you just have
it jump and then pretty soon if you jump
one solution out that's what's going to
be the new solution whichever one jumps
up really high first um kind of ruining
the whole idea of doing a random
selection and I'll go into the random
selection in just a second steps in Q
learning step one create an initial Q
table with all values initialized to
zero again we're looking at
01 uh so are you you know here's our
action we start we're an idol we took a
wrong action we took a correct action
and int and then we have our um actions
fetching sitting and running of course
we're just using the dog example and
choose an action and perform it update
values in the table and of course when
we're choosing an action we're going to
kind of do something random and just
randomly pick one so you start out and
you sit and you have then a um then
depending on that um um action you took
you can now update the value for sitting
after you start from start to
sitting get the value of the reward and
calculate the Val the value Q value
using the Bellman equation and so now we
attach a reward to
sitting and when we attach all those
rewards we continue the same until the
table's filled with or an episode
ends and and IM I was going to come back
to the random side of this and there's a
few different formulas they use for the
random um setup to pick it I usually let
whatever Q model I'm using do their
standard one because someone's usually
gone in and done the math uh for the
optimal uh spread uh but you can look at
this if I have running has a reward of
10 sitting has a reward of seven
fetching has a reward of five um just
kind of without doing like a a a means
you know using the bell curve for the
means value and like I said there's some
math you can put in there to pick um so
that you're more like so that running
has even a higher chance um but even if
you were just going to do an average on
this you could do an average a random
number by adding them all together uh so
you get 10 plus 7 + 5 is 22 you could do
0 to 22 and or 0 to 21 but 1 to 22 one
to five would be fetching uh and so
forth you know the last 10 so you can
just look at this as what percentage are
you going to go for that particular
option um and then that gets your random
setup in there and then as you slowly
increment these up uh you see that uh um
if you're idle um where's one here we go
sitting at the end if you're at the end
of wherever you're at sitting gets a
reward of one um where's the good one on
here oh wrong action running for a wrong
action gets almost no reward so that
becomes very very less likely to happen
but it still might happen it still might
have a percentage of coming up and
that's where the random programming and
Q learning comes in the below table
gives us an idea of how many times an
action has been taken and how positively
correct action or negatively wrong
action it is going to affect the next
state
so let's go ahead and dive in and pull
up a little piece of code and see what
this looks like um in
Python uh in this demo we'll use
q-learning to find the shortest path
between two given points if getting your
learning started is half the battle what
if you could do that for free visit
scaleup by simply learn click on the
link in the description to know more if
you've seen my videos before um I like
to do it in the uh an Ona Jupiter
notebook um setup just because it's
really easy to see and it's a nice demo
uh and so here's my anaconda this one
I'm actually using uh python 36
environment that I set up in here and
we'll go ahead and launch the Jupiter
Notebook on this and once we're in our
Jupiter notebook uh which has the kernel
loaded with Python 3 we'll go ahead and
create a new Python 3 uh folder in
here and we'll call this uh Q
learning and to start this
demo let's go ahead and import our numpy
array we'll just run that so it's
imported and like a lot of these uh
model programs when you're building them
you spend a lot of time putting it all
together um and then you end up with
this really short answer at the
end uh and we'll we'll take a look at
that as we come into it so we we go
ahead and start with our location to
State uh so we have um L1 L2 these are
our nine locations 1 to nine and then of
course the state is going to be 0 1 2 3
4 it's just a mapping of our location to
a integer on there and then we have our
actions our actions are simply uh moving
from um One
location to another so I can go to I can
go to location zero I can go to location
one 2 3 4 five six 7 8 uh so these are
my actions I can choose these are the
locations of our state
and if you remember earlier I mentioned
uh um that the limitation is that you
you don't want to put in um a
continually growing table because you
can actually create a dynamic Q table
where you continually add in new values
as they
arise because um if you have float
values this just becomes infinite and
then your memory on your computer's gone
or you know does it's not going to work
at the same time you might think well
that kind of limits the the Q uh T
learning setup but there are ways to use
it in conjunction with other systems and
so you might look
at uh well I do um been doing some work
in stock um and one of the questions
that comes out is to buy or sell the
stock and the state coming in might be
um you might take it create what called
buckets um where any that you predict is
going to return more than a certain
amount of money um the error for that
stock that you've had in the past you
put those in buckets and suddenly as you
start putting the creating these buckets
you realize you do have a limited amount
of information coming in you no longer
have a float number you now have um
bucket one two three and four and then
you can take those buckets put them
through a a q learning table and come up
with the best action which stock should
I buy it's like gambling stock is pretty
much gambling if you're doing day
trading you're not doing long-term um
Investments and so you can start looking
at it like that a lot of the um current
feeds say that the best algorithms used
for day Traders you're doing it on your
own is really to ask the question do I
want to trade the stock yes or no and
now you have it in a q learning table
and now you can take it to that next
level and you can see where that can be
a really powerful tool at the end of
doing a basic linear regression model
mod or something um what is the best
investment and you start getting the
best reward on
there uh and so if we're going to have
rewards these rewards we just create um
it says uh if basically if you're um
this should match our Q table because
it's going to be uh you have your state
and you have your action across the top
if you remember from the dog and so we
have whatever state we're in going down
and then the next action and what the
reward is for it um and of course if you
actually doing a um something more
connected your reward would be based on
um the actual environment it's in and
then we want to go ahead and create a
state to location uh so we can map the
indexes so just like we defined our
rewards uh we're going to go and do
state to location um and you can see
here it's a a dictionary setup for
location state in location to state with
items and we also need to
um Define what we want for learning
rates uh you remember we had our two
different rates um as far as like
learning from the past and learning from
the current so we'll go ahead and set
those to uh 75 and the alpha set to 0.9
and we'll see that when we do the
formula and of course any of this code
uh send a note to our simply learn team
they'll get you a copy of this code on
here let's go ahead and
pull
there we
go well the new next two
sections um since we're going to keep it
short and
sweet here we go so let's go ahead and
create our agent um so our agent is
going to have our initialization where
we sended all the information uh we'll
Define our s gamma equals gamma we could
have just set the gamma rate down here
instead of uh submitting it it's kind of
nice to keep them separate because you
can play with these numbers our self
Alpha um then we have our location State
we'll set that in here U we have our
choice of actions um we're going to go
ahead and just embed the rewards right
into the agent so obviously this would
be coming from somewhere else uh instead
of from uh self-generated and then a
self state to location equals our state
to location uh dictionary and we go
ahead and create a q learning table and
I went ahead and just set the Q learning
table up to um uh 0 to zero what what
what the setup is uh location to State
how many of them are there uh this just
creates an array of zero to zero setup
on
there and then the big part is the
training we have our rewards new equals
a copy of self.
rewards ending State equals the self
location state in location so this is
whatever we end up at rewards new equals
ending State plus ending State equals
999 just kind of goes to a dead end and
we start going through
iterations and we'll go ahead um let's
do this uh so this we're going to come
back and we're going to call call it on
here uh let me just erase that switch it
to an
arrow there we go uh so what we're doing
is we're going to send in here to train
it we're going to say hey um I want to
iterate through this a thousand times
and see what happens now this part would
actually be instead of iterating you
might have your external environment and
they're going back and forth and you
iterate through outside of here uh but
just for ease of use our agent going to
come in here and iterate through this
sometimes I'll put this iteration in
here and I'll have it call the
environment and say hey this is what I
did what's the next state and the
environment does this thing right in
here as I iterate through
it uh and then we want to go ahead and
pick a random state to start with that's
what's going on here you have to start
somewhere um and then you have your
playable actions we're going to start
with just an empty thing for playable
actions and we'll fill that up so that's
what choices I have and so we're going
to iterate through the rewards Matrix to
get the states uh directly reachable
from the randomly chosen current state
assign those states to a list named
playable
actions and so you can see here we have
uh range nine I usually use length of
whatever I'm looking at uh which is our
locations or States as they are uh we
have a reward so we want to look at the
current the rewards uh the new reward is
our uh is in our chart here of rewards
uncore new uh current state um plus J uh
J being what is the next date we want to
try and so we go ah and do our playable
actions and we pend
J and so we're doing is we're randomly
trying different things in here to see
what's going to generate a better
reward and then of course we go ahead
and choose our next State uh so we have
our random Choice playable actions and
if you remember I mentioned on this let
me just go ahead and uh whoops let's do
a free form when we were talking about
the next State uh this right here just
does a random selection instead of a
Rand random uh selection you might do
something where uh whatever the best
selection is which might be option three
here and then so you can see that it
might use a bell curve and then option
two over here might have a bell curve
like this oops and we start looking at
these averages and these spreads um or
we can just add them all together and
pick the one that kind of goes in all of
those uh so those are some of the
options we have in here we just go with
a random Choice uh that's usually where
you start play with it um and then we
have our reward section down here and so
we want to go ahead and find well in
this case a temporal difference uh so
you have your rewards new plus the self
gamma and this is the formula we were
looking at this is bellman's equation
here uh so we have our current value our
learning rate our discount rate involved
in there the reward system coming in for
that um and we can add it all together
this is of course our uh uh maximum
expected future setup in here uh so this
is all of our our bellman's equation
that we're looking at here and then we
come up in here and we update our Q
table that's all this is on this one uh
that's right here we have um self Q
current state next state and we add in
our um Alpha because we don't want to we
don't want to train all of it at once in
case there's slight differ inchas coming
in there we want to slowly approach the
answer uh and then we have our route
equals a start
location and next location equals start
location so we're just incrementing we
took a step forward and then finally
remember I was telling you how uh we're
going to do all this and just have some
simple thing at the end or it just
generates a simple path we're going to
go ahead and and get the optimal route
we want to find the best route in here
and so we've created a definition for
the optimal route down here scroll down
for that
and we get the optimal route we go ahead
and put the information in including the
Q table self uh start location in
location next location route q and it
says while next location is not equal to
in location so while we can still go our
start location equals self location to
State start location so we already have
our best value for the start
location uh the next state looks at the
Q table and says hey what's uh the next
one with the best value and then the
next location we go ahead and pull that
in and we just append it that's what's
going on down
here and then our start location equals
the next location and we just go through
all the steps and we'll go ahead and run
this and now that we have our Q table
our um Q agent loaded we're going to go
ahead and uh take our Q agent load them
up with our Alpha Gamma that we set up
above um along with the location step
action rewards state to
location and uh our goal is to plot a
course between L9 and
L1 and we're going to go through 100 a
thousand iterations on here and so when
I run that it runs pretty quick uh why
is this so fast um if youve been running
neural networks and you've been doing
all these other models you sit here and
wait a long time well we're a very small
amount of data these are all integers
these aren't float values there's not a
the math is not heavy on the on the
processing end and this is where Q table
are so powerful if you have a small
amount of information coming in you very
quickly uh get an answer off of this
even though we went through it a
thousand times to train it and you'll
see here we have l985 2 and one and
that's based on our reward table we had
set up on there and this is the shortest
path going between these different uh
setups in here and if you remember on
our reward table uh you can see that if
you start here you can go to here
there's places you can't go that's how
this reward table was set up so I can
only go to certain
places uh so kind of a little maze setup
in there and you can play with it this
is really fun uh setup to play with uh
and you can see how you can take this
whole code and you can like I was saying
earlier you can embed it into another
setup in model and predictions where you
put things into buckets and you're
trying to guess the best investment the
best course of action long as you can
take that course of action and and uh uh
reduce it down to a yes no um or if
you're using text you can use a one hot
encoder which word is next there's all
kinds of things you can do with a Q
table uh depending on just how much
information you're putting in there so
that wraps up our demo in this demo
we've uh found the shortest distance
between two paths based on whatever
rules or state rewards we have to get
from point A to point B and what
available actions there are hello and
welcome to this tutorial on deep
learning my name is moan and in the next
about 1 one and a half hours I will take
you through what is deep learning and
into tensorflow environment to show you
an example of deep learning now there
are several applications of deep
learning really very interesting and
Innovative applications and one of them
is identifying the geographic location
based on a picture and how does this
work the way it works is pretty much we
train an artificial neural network with
millions of images which are tagged
their geolocation is tagged and then
when we feed a new picture it will be
able to identify the geolocation of this
new image for example you have all these
images especially with maybe some
significant monuments or or U
significant locations and you train with
millions of such images and then when
you feed another image it need not be
exactly one of those that you have
trained it can be completely different
that is the whole idea of training it
will be able to recognize for example
that this is a picture from Paris
because it is able to recognize the iil
D so the way it works internally if we
have to look a little bit under the H is
these images are nothing but this is
digital information in the form of
pixels so each image could be a certain
size it can be 256 by 25 56 pixel kind
of a resolution and then each pixel is
either having a certain grade of color
and all that is fed into the neural
network and it then gets trained in and
it's able to based on these pixels pixel
information it is able to get trained
and able to recognize the features and
extract the features and thereby it is
able to identify these images and the
location of these images and then when
you Creed a new image it kind of based
on the training it will be able to
figure out where this imag is from so
that's the way a little bit under the
hood how it works so what are we going
to do in this tutorial we will see what
is deep learning and what do we need for
deep learning and one of the main
components of deep learning is neural
network so we will see what is neural
network what is a perceptron and how to
implement logic gates like and or nor
and so on using perceptrons the
different types of neural networks and
then applications of deep learning and
we will also see how neural
networks works so how do we do the
training of neural networks and at the
end we will end up with a small demo
code which will take you through
intensive flow now in order to implement
deep learning code there are multiple
libraries or development environments
are available and tensor flow is one of
them so the focus at the end of this
would be on how to use tensor flow to
write a piece of code using python as a
programming language and we will take up
a an example which is a very common one
which is like the hollow world of deep
learning the handwriting number
recognition which is a mnist commonly
known as mnist database so we will take
a look at amness database and how we can
train a neural network to recognize
handwritten numbers so that's what you
will see in this particular video so
let's get started what is deep learning
deep learning is like a subset of what
is known as a highlevel concept called
artificial intelligence you must be
already familiar must have heard about
this term artificial intelligence so
artificial intelligence is like the high
level concept if you will and in order
to implement artificial intelligence
applications we use what is known as
machine learning and within machine
learning a subset of machine learning is
deep learning machine learning is a
little bit more generic concept and deep
learning is one type of machine learning
if you will and we will see a little
later in maybe the following slides a
little bit more in detail how deep
learning is different from traditional
machine learning but to start with we
can mention here that deep learning
users one of the differentiators between
deep learning and traditional machine
learning is that deep learning uses
neural networks and we will talk about
what are neural networks and how we can
Implement neural networks and so on and
so forth as a part of this tutorial so a
little deeper into deep learning deep
learning primarily involves working with
complicated unstructured data compared
to traditional machine learning with
where we normally use structured data in
deep learning the data would be
primarily images or Voice or maybe text
file so and it is large amount of data
as well and deep learning can handle
complex operations it involves complex
operations and the other difference
between traditional machine learning and
deep learning is that the feature
extraction happens pretty much
automatically in traditional machine
learning feature engineering is done
manually the data scientists we data
scientists have to do feature
engineering feature extraction but in
deep learning that happens automatically
and of course deep learning for large
amounts of data complicated unstructured
data deep learning gives very good
performance now as I mentioned one of
the secret sources of deep learning is
neural networks let's see what neural
networks is neural networks is based on
our biological neurons the whole concept
of deep learning and artificial
intelligence is based based on human
brain and human brain consists of
billions of tiny stuff called neurons
and this is how a biological neuron
looks and this is how an artificial
neuron looks so neural networks is like
a simulation of our human brain human
brain has billions of biological neurons
and we are trying to simulate the human
brain using artificial NE neurons this
is how a biological neuron looks it has
dendrites and the corresponding
component with an artificial neural
network is or an artificial neuron are
the inputs they receive the inputs
through ddes and then there is the cell
nucleus which is basically the
processing unit in a way so in
artificial neuron also there is a piece
which is an equivalent of this cell
nucleus and based on the weights and
biases we will see what exactly weights
and biases are as we move the input gets
processed and that results in an output
in a biological neuron the output is
sent through a synapse and in an
artificial neuron there is an equivalent
of that in the form of an output and
biological neurons are also
interconnected so there are billions of
neurons which are interconnected in the
same way artificial neurons are also
interconnected so this output of this
neuron will be fed as an input to
another neuron and so on now in neural
network one of the very basic units is a
perceptron so what is a perceptron A
perceptron can be considered as one of
the fundamental units of neural networks
it can consist at least one neuron but
sometimes it can be more than one neuron
but you can create a perceptron with a
single neuron and it can be used to
perform certain functions it can be used
as a basic binary classifier it can be
trained to do some basic binary
classification and this is how a basic
perceptron looks like and this is
nothing but a neuron you have inputs X1
X2 X to xn and there is a summation
function and then there is what is known
as an activation function and based on
this input what is known as the weighted
sum the activation function either gets
gives an output like a z zero or a one
so we say the neuron is either activated
or not so that's the way it works so you
get the inputs these inputs are each of
the inputs are multiplied by a weight
and there is a bias that gets added and
that whole thing is fed to an activation
function and then that results in an
output and if the output is correct it
is accepted if it is wrong if there is
an error then that error is fed back and
the neuron then adjust the weights and
biases to give a new outut
and so on and so forth so that's what is
known as the training process of a
neuron or a neural network there's a
concept called perceptron learning so
perceptron learning is again one of the
very basic learning processes the way it
works is somewhat like this so you have
all these inputs like X1 to xn and each
of these inputs is multiplied by a
weight and then that sum this is the
formula or the equation so that sum wi I
XI
Sigma of that which is the sum of all
these product of X and W is added up and
then a bias is added to that the bias is
not dependent on the input but or the
input values but the bias is common for
one neuron however the bias value keeps
changing during the training process
once the training is completed the
values of these weights W1 W2 and so on
and the value of the bias gets fixed so
that is basically the whole training
process and that is what is known as the
perceptron training so the weights and
biases keep changing till you get the
accurate output and the summation is of
course passed through the activation
function as you see here this wixi
summation plus b is passed through
activation function and then the neuron
gets either fired or not and based on
that there will be an output that output
is compared paed with the actual or
expected value which is also known as
labeled information so this is the
process of supervised learning so the
output is already known and um that is
compared and thereby we know if there is
an error or not and if there is an error
the error is fed back and the weights
and biases are updated accordingly till
the error is reduced to the minimum so
this iterative process is known as
perceptron learning or perceptron
learning Rule and this error needs to be
minimized so till the error is minimized
this iteratively the weights and biases
keep changing and that is what is the
training process so the whole idea is to
update the weights and the bias of the
perceptron till the error is minimized
the error need not be zero the error may
not ever reach zero but the idea is to
keep changing these weights and bias so
that the error is minimum the minimum
possible that it can have so this whole
process is an iterative process and this
is the iteration continues till either
the error is zero which is uh unlikely
situation or it is the minimum possible
Within These given conditions now in
1943 two scientists Warren mik and
Walter pits came up with an experiment
where they were able to implement the
logical functions like and or and nor
using neurons and that was a significant
breakthrough in a sense so they were
able to come up with the most common
logical Gates they were able to
implement some of the most common
logical Gates which could take two
inputs Like A and B and then give a
corresponding result so for examp
example in case of an and gate A and B
and then the output is a in case of an R
gate it is a plus b and so on and so
forth and they were able to do this
using a single layer perceptron now most
of these Gates it was possible to use
single layer perceptron except for XR
and we will see why that is in a little
bit so this is how an endgate works the
inputs A and B the output should be
fired or the the neuron should be fired
only when both the inputs are one so if
you have 0 0 the output should be zero
for 01 it is again 0 1 0 again 0 and 1
one the output should be one so how do
we implement this with a neuron so it
was found that by changing the values of
Weights it is possible to achieve this
logic so for example if we have equal
weights like 7 7 and then if we take the
sum of the weighted product so for
example 7 into 0 and then 7 into 0 will
give you 0 and so on and so forth and in
the last case when both the inputs are
one you get a value which is greater
than one which is the threshold so only
in this case the neuron gets activated
and the output is there is an output in
all the other cases there is no output
because the threshold value is one so
this is implementation of an and gate
using a single perceptron or a single
neuron similarly an orgate in order to
implement an orgate in case of an orgate
the output will be one if either of
these inputs is one so for example 01
will result in one or other in all the
cases it is one except for 0 0 so how do
we implement this using a perceptron
once again if you have a perceptron with
weights for example 1.2 now if you see
here if in the first case when both are
0 the output is zero in the second case
when it is 0 and 1 1.2 into 0 is 0 and
then 1.2 into 1 is 1 and in the second
case similarly the output is 1.2 in the
last case when both the inputs are one
the output is 2.4 so during the training
process these weights will keep changing
and then at one point where the weights
are equal to W1 is equal to 1.2 and W2
is equal to 1.2 the system learns that
it gives the correct output so that is
implementation of orgate using a single
neuron or single layer perceptron now
exr gate this was one of the challenging
ones they tried to implement an exr gate
with a single level perceptron but it
was not possible and therefore in order
to implement an XR so this was like a a
roadblock in the progress of U neural
network however subsequently they
realize that this can be implemented and
xrate can be implemented using a
multi-level perceptron or MLP so in this
case there are two layers instead of a
single layer and this is how you can
Implement an XR gate so you will see
that X1 and X2 are the inputs and there
is a hidden layer and that's why it is
denoted as H3 and H4 and then you take
the output of that and feed it to the
output at 05 and provide a threshold
here so we will see here that this is
the numerical calculation so the weights
are in this case for X1 it is 20 and
minus 20 and once again 20 and minus 20
so these inputs are fed into H3 and H4
so you'll see here for H3 the input is
01 1 1 and for H4 it is 1 1 1 and if you
now look at the output final output
where the threshold is taken as one if
you use a sigmoid with the threshold one
you will see that in these two cases it
is zero and in the last two cases it is
one so this is a implementation of XR in
case of XR only when one of the inputs
is one you will get an output so that is
what we are seeing here if we have
either both the inputs are one or both
the inputs are zero then the output
should be zero so that is what is an
exclusive or gate so it is exclusive
because only one of the inputs should be
one and then only you'll get an output
of one which is Satisfied by this
condition so this is a special
implementation sgate is a special
implementation of perceptron now that we
got a good idea about perceptron let's
take a look at what is the neural
network so we have seen what is a
perceptron we have seen what is a neuron
so we will see what exactly is a neural
network so neural network is nothing but
a network of these neurons and there are
different types of neural networks there
are about five of them these are
artificial neural network convolutional
neural network then recursive neural
network or recurrent neural network deep
neural network and deep belief Network
so and each of these types of neural
networks have a special you know they
can solve special kind of problems for
example convolutional neural networks
are very good at performing image
processing and image recognition and so
on whereas RNN are very good for speech
recognition and also text analysis and
so on so each type has some special char
istics and they can they good at
performing certain special kind of tasks
what are some of the applications of
deep learning deep learning is today
used extensively in gaming you must have
heard about alphao which is a game
created by a startup called Deep Mind
which got acquired by Google and alphao
is an AI which defeated the human world
champion lead all in the this game of Go
so gaming is an area where deep learning
is being extensively used and a lot of
research happens in the area of gaming
as well in addition to that nowadays
there are neural networks or special
type called generative adversarial
networks which can be used for
synthesizing either images or music or
text and so on and they can be used to
compose music so the neural network can
be trained to compose a certain kind of
music and autonomous cars you must be
familiar with Google Google's
self-driving car and today a lot of
Automotive companies are investing in
this space and uh deep learning is a
core component of this autonomous Cars
the cars are trained to recognize for
example the road the the lane markings
on the road signals any objects that are
in front any obstruction and so on and
so forth so all this involves deep
learning so that's another major
application and robots we have seen
several robots including Sofia you may
be familiar with sopia who was given a
citizenship by Saudi Arabia and there
are several such robots which are very
humanlike and the underlying technology
in many of these robots is deep learning
medical Diagnostics and Health Care is
another major area where deep learning
is being used and within Healthcare
Diagnostics again there are multiple
areas where deep learning and image
recognition image processing can be used
for example for cancer detection as you
may be aware if cancer is detected early
on it can be cured and one of the
challenges is in the availability of
Specialists who can diagnose cancer
using these diagnostic images and
various scans and and so on and so forth
so the idea is to train neural network
to perform some of these activities so
that the load on the can specialist
doctors or oncologists comes down and
there is a lot of research happening
here and there are already quite a few
applications that are claimed to be
performing better than human beings in
this space can be lung cancer it can be
breast cancer and so on and so forth so
Healthcare is a major area where deep
learning is being applied let's take a
look at the inner working of a neural
network so how does an artificial neural
network let's say identify can we train
a neural network to identify the shapes
like squares and circles and triangles
when these images are fed so this is how
it works any image is nothing but it is
a digital information of the pixels so
in this particular case let's say this
is an image of 28x 28 pixel and this is
an image of a square there's a certain
way in which the pixels are lit up and
so these pixels have a certain value
maybe from 0 to 256 and 0 indicates that
it is black or it is dark and 256
indicates it is completely it is white
or lit up so that is like an indication
or a measure of the how the pixels are
lit up and so this is an image is let's
say consisting of information of 784
pixels so all the information what is
inside this image can be kind of
compressed into the 784 pixels the way
each of these pixels is lit up provides
information about what exactly is the
image so we can train neural networks to
use that information and identify the
images so let's take a look how this
works so each neuron the value if it is
close to one that means it is white
whereas if it is close to zero that
means it is black now this is a an
animation of how this whole thing works
so these pixels one of the ways of doing
it is we can flatten this image and take
this complete 784 pixels and feed that
as input to our neural network the
neural Network can consist of probably
several layers there can be a few hidden
layers and then there is an input layer
and an output layer now the input layer
take the 784 pixels as input the values
of each of these pixels and then you get
an output which can be of three types or
three classes one can be a square a
circle or a triangle now during the
training process there will be initially
obviously you feed this image and it
will probably say it's a circle or it
will say it's a triangle angle so as a
part of the training process we then
send that error back and the weights and
the biases of these neurons are adjusted
till it correctly identifies that this
is a square that is the whole training
mechanism that happens out
here now let's take a look at a circle
same way so you feed these 784 pixels
there is a certain pattern in which the
pixels are are lit up and the neural
network is trained to identify that
pattern and during the training process
once again it would probably initially
identify it incorrectly saying this is a
square or a triangle and then that error
is fed back and the weights and biases
are adjusted finally till it finally
gets the image correct so that is the
training process so now we will take
take a look at same way a
triangle so now if you feed another
image which is consisting of triangle so
this is the training process now we have
trained our neural network to classify
these images into a triangle or a circle
and a square so now this neural network
can identify these three types of
objects now if you feed another image
and it will be able to identify whether
it's a square or a triangle or a circle
now what is important to be observed is
that when you feed a new image it is not
necessary that the image or the the
triangle is exactly in this position now
the neural network actually identifies
the patterns so even if the triangle is
let's say positioned here not exactly in
the middle but maybe at the corner or in
the side it would still identify that it
is a triangle and that is the whole idea
behind pattern recognition so how does
this training process work this is a
quick view of how the training process
works so we have seen that a neuron
consists of inputs it receives inputs
and then there is a weighted sum which
is nothing but this XI wi summation of
that plus the bias and this is then fed
to the activation function and that in
turn gives us a output now during the
training process initially obviously
when you feed these images when you send
maybe a square it will identify it as a
triangle and when you maybe feed a
triangle it will identify as a square
and so on so that error information is
fed back and initially these weights can
be random maybe all of them have zero
values and then it will slowly keep
changing so the as a part of the
training process the values of these
weights W1 W2 up to WN keep changing in
such a way that towards the the end of
the training process it should be able
to identify these images correctly so
till then the weights are adjusted and
that is known as the training process so
and these weights are numeric values
could be
0.525 35 and so on it could be positive
or it could be negative and the value
that is coming here is the pixel value
as we have seen it can be anything
between 0 to 1 you can scale it between
0 to 1 or 0 to 256 whichever way Zer
being black and 256 being white and then
all the other colors in between so that
is the input so these are numerical
values this multiplication or the
product W ixi is a numerical value and
the bias is also a numerical value we
need to keep in mind that the bias is
fixed for a neuron it doesn't change
with the inputs whereas the weights are
one per input so that is one important
point to be noted so but the bias also
keeps changing initially it will again
have a random value but as a part of the
training process the weights the values
of the weights W1 W2 WN and the value of
B will change and ultimately once the
training process is complete these
values are fixed for this particular
neuron W1 W2 up to WN and plus the value
of the B is also fixed for this
particular neuron and in this way there
will be multiple neurons and each there
may be multiple levels of neurons here
and that's the way the training process
works so this is another example of
multi-layer so there are two hidden
layers in between and then you have the
input layer values coming from the input
layer then it goes through multiple
layers hidden layers and then there is
an output layer and as you can see there
are weights and biases for each of these
neurons in each layer and all of them
gets keeps changing during the training
process and at the end of the training
process all these weights have a certain
value and that is a trained model and
those values will be fixed once the
training is completed all right then
there is something known as activation
function neural networks consists of one
of the components in neural networks is
activation function and every neuron has
an activation function and there are
different types of activation functions
that are used it could be a relu it
could be sigmoid and so on and so forth
and the activation function is what
decides whether a neuron should be fired
or or not so whether the output should
be zero or one is decided by the
activation function and the activation
function in turn takes the input which
is the weighted sum remember we talked
about wixi + B that weighted sum is fed
as a input to the activation function
and then the output can be either a zero
or a one and there are different types
of activation functions which are
covered in an earlier video you might
want to watch all right so as a part of
the training process we feed the inputs
the labeled data or the training data
and then it gives an output which is the
predicted output by the network which we
indicate as y hat and then there is a
labeled data because we for supervised
learning we already know what should be
the output so that is the actual output
and in the initial process before the
training is complete obviously there
will be error so that is measured by
what is known as the cost function so
the difference between the predicted
output and the actual output is the
error and U the cost function can be
defined in different ways there are
different types of cost functions so in
this case it is like the average of the
squares of the error so and then all the
errors are added which can sometimes be
called as sum of squares sum of square
errors or ssse and that is then fed as a
feedback in what is known as backward
propagation or back propagation and that
helps in the the network adjusting the
weights and biases and so the weights
and biases get updated till this value
the error value or the cost function is
minimum now there is a optimization
technique which is used here called
gradient descent optimization and this
algorithm Works in a way that the error
which is the cost function needs to be
minimized so there's a lot of
mathematics that goes behind the for
example they find the local Minima the
global Minima using the differentiation
and so on and so forth but the idea is
this so as a training process as the as
a part of training the whole idea is to
bring down the error which is like let's
say this is the function the cost
function at certain levels it is very
high the cost value of the cost function
the output of the cost function is very
high so the weights have have to be
adjusted in such a way and also the bias
of course that the cost function is
minimized so there is this optimization
technique called gradient descent that
is used and this is known as the
learning rate now gradient descent you
need to specify what should be the
learning rate and the learning rate
should be optimal because if you have a
very high learning rate then the
optimization will not converge because
at some point it will cross over to the
side on the other hand if you have very
low learning rate then it might take
forever to convert so you need to come
up with the optimum value of the
learning rate and once that is done
using the gradient descent optimization
the error function is reduced and that's
like the end of the training process all
right so this is another view of
gradient descent so this is how it looks
this is your cost function the output of
the cost function and that has to be
minimized using gradient descent
algorithm and these are like the
parameters and weight could be one of
them so initially we start with certain
random values so cost will be high and
then the weights keep changing and in
such a way that the cost function needs
to come down and at some point it may
reach the minimum value and then it may
increase so that is where the gradient
descent algorithm decides that okay it
has reached minimum value and it will
kind of try to stay here this is known
as the global Minima now sometimes these
curves may not be just for explanation
purpose this has been drawn in a nice
way but sometimes these curves can be
pretty eratic there can be some local
Minima here and then there is a peak and
then and so on so the whole idea of
gradient desent optimization is to
identify the global Minima and to find
the weights and the bi
at that particular point so that's what
is gradient descent and then this is
another example so you can have these
multiple local Minima so as you can see
at this point when it is coming down it
may appear like this is a minimum value
but then it is not this is actually the
global minimum value and the gradient
desent algorithm will make an effort to
reach this level and not get stuck at
this point so the algorithm is already
there and it knows how to identify
This Global minimum and that's what it
does during the training process now in
order to implement deep learning there
are multiple platforms and languages
that are available but the most common
platform nowadays is tensor flow and so
that's the reason we have uh this
tutorial we have created this tutorial
for tensor flow so we will take you
through a quick demo of how to write a
tensorflow code using Python and
tensorflow is uh an open source platform
form created by Google so let's just
take a look at the details of tens ofl
and so this is a a library a python
Library so you can use python or any
other languages it's also supported in
other languages like Java and R and so
on but python is the most common
language that is used so it is a library
for developing deep learning
applications especially using neural
networks and it consists of primarily
two parts if you will so one is the
tensors and then the other is the graphs
or the flow that's the way the name
that's the reason for this kind of a
name called tensorflow so what are
tensors tensors are like
multi-dimensional arrays if you will
that's one way of looking at it so
usually you have a one-dimensional array
so first of all you can have what is
known as a scalar which means a number
and then you have a onedimensional array
something like this which means this is
like a set of numbers so that is a
one-dimension array then you can have a
two-dimensional array which is like a
matrix and beyond that sometimes it gets
difficult so this is a three-dimensional
array but tens of flow can handle many
more Dimensions so it can have
multi-dimensional arrays that is the
strength of tensor flow and which makes
computation deep learning computation
much faster and that's the reason why
tensor flow is used for developing deep
learning applications so tensor flow is
a deep learning tool and this is the way
it works so the data basically flows in
the form of tensors and the way the
programming works as well is that you
first create a graph of how to execute
it and then you actually execute that
particular graph in the form of what is
known as a session we will see this in
the tensorflow code as we move forward
so all the data is managed or
manipulated in tensors and then the
processing happen happens using these
graphs there are certain terms called
like for example ranks of a tensor the
rank of a tensor is like a dimensional
dimensionality in a way so for example
if it is scalar so there is just a
number just one number the rank is
supposed to be zero and then it can be a
one-dimensional vector in which case the
rank is supposed to be one and then you
can have a
two-dimensional Vector typically like a
matrix then in that case we say the rank
is two and then if it is a
three-dimensional array then it rank is
three and so on so it can have more than
three as well so it is possible that you
can store multi-dimensional arrays in
the form of tensors so what are some of
the properties of tensor flow I think
today it is one of the most popular
platform tsor flow is the most popular
deep learning platform or Library it is
open source it's developed by Google
developed and maintained by Google but
it is open source one of the most
important things about tensorflow is
that it can run on CPUs as well as gpus
GPU is a graphical Processing Unit just
like CPU is central processing unit now
in earlier days GPU was used for
primarily for graphics and that's how
the name has come and one of the reasons
is that it cannot perform generic
activities very efficiently like CPU
but it can perform iterative actions or
computations extremely fast and much
faster than a CPU so they are really
good for computational activities and in
deep learning there is a lot of
iterative computation that happens so in
the form of matrix multiplication and so
on so gpus are very well suited for this
kind of computation and tensive flow
supports both GPU as well as CPU and
there's a certain way of writing code in
tensor flow we will see as we go into
the code and of course tensorflow can be
used for traditional machine learning as
well but then that would be an Overkill
but just for understanding it may be a
good idea to start writing code for a
normal machine learning use case so that
you get a hang of how tensorflow code
works and then you can move into neural
networks so that is um just a suggestion
but if you're already familiar with how
tensor flow works then probably yeah you
can go straight into into the neural
networks part so in this tutorial we
will take the use case of recognizing
handwritten digits this is like a hollow
world of deep learning and this is a
nice little amness database is a nice
little database that has images of
handwritten digits nicely formatted
because very often in deep learning and
neural networks we end up spending a lot
of time in preparing the data for
training and with amness database we can
avoid that you already have the data in
the right format which can be directly
used for training and amnest also offers
a bunch of built-in utility functions
that we can straight away use and call
those functions without worrying about
writing our own functions and that's one
of the reasons why amness database is
very popular for training purposes
initially when people want to learn
about deep learning and tensor flow this
is the database that is used and it has
a collection of 70,000 handwritten
digits and a large part of them are for
training then you have test just like in
any machine learning process and then
you have validation and all of them are
labeled so you have the images and
they're label and these images they look
somewhat like this so they are
handwritten images collected from a lot
of individuals people have these are
samples written by human beings they
have handwritten these numbers these
numbers going from 0 to 9 so people have
written these numbers and then the
images of those have been taken and
formatted in such a way that it is very
easy to handle so that is mes database
and the way we are going to implement
this in our tens oflow is we will feed
this data especially the training data
along with the label information and uh
the data is basically these images are
stored in the form of the pixel
information as we have seen in one of
the previous slides all the images are
nothing but these are pixels so an image
is nothing but an arrangement of pixels
and the value of the pixel either it is
lit up or it is not or in somewhere in
between that's how the images are stored
and that is how they are fed into the
neural network and for training once the
network is trained when you provide a
new image it will be able to identify
within a certain error of course and for
this we will use one of the simpler
neural network configurations called
softmax and for Simplicity what we will
do is we will flatten these pixels so
instead of taking them in a
two-dimensional arrangement we just
flatten them out so for example it
starts from here it is at 28x 28 so
there are
7484 pixels so pixel number one starts
here it goes all the way up to 28 then
29 starts here and goes up to 56 and so
on and the pixel number 784 is here so
we take all these pixels flatten them
out and feed them like one single line
into our neural network and this is a
what is known as a softmax layer what it
does is once it is trained it will be
able to identify what digit this is so
there are in this output layer there are
10 neurons each signifying a digit and
at any given point of time when you feed
an image only one of these 10 neurons
gets activated so for example if this is
strained properly and if you feed a
number nine like this then this part
particular neuron gets activated so you
get an output from this neuron let me
just use uh a pen or a laser to show you
here okay so you're feeding a number
nine let's say this has been trained and
now if you're feeding a number nine this
will get activated now let's say you
feed one to the trained Network then
this neuron will get activated if you
feed two this neuron will get activated
and so on I hope you get the idea so
this is one type of a neural network or
an activation function known as softmax
layer so that's what we will be using
here this one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and uh
we will show you there directly but very
quickly this is how the code looks and
uh let me run you through briefly here
and then we will go into the Jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using python here
and that's why the syntax of the
language is Python and the first step is
to import the tensorflow library so and
we do this by using this line of code
saying import tensor flow as TF DF is
just for convenience so you can name
give any name and once you do this TF is
tens flow is available as an object in
the name of TF and then you can run run
its uh methods and accesses its
attributes and so on and so forth and
mless database is actually an integral
part of tensor flow and that's again
another reason why we as a first step we
always use this example mnist database
example so you just simply import mnist
database as well using this line of code
and you slightly modify this so that the
labels are in this format what is known
as one hot true which means that
the label information is stored like an
array and uh let me just uh use the pen
to show what exactly it is so when you
do this one hot true what happens is
each label is stored in the form of an
array of 10 digits and let's say the
number is uh 8 okay so in this case all
the remaining values there will be a
bunch of zeros so this is like array at
position zero this is at position one
position two and so on and so forth
let's say this is position 7 then this
is position 8 that will be one because
our input is eight and again position 9
will be zero okay so one hot encoding
this one hot encoding true will kind of
load the data in such a way that the
labels are in such a way that only one
of the digits has a value of one and
that indic So based on which digit is
one we know what is the label so in this
case the eighth position is one
therefore we know this sample data the
value is eight similarly if you have a
two here let's say then the labeled
information will be somewhat like this
so you have your labels so you have this
as zero the zeroth position the first
position is also zero the second
position is one because this indicates
number two and then you have third as
zero and so on okay so that is the
significance of this one hot true all
right and then we can check how the data
is uh looking by displaying the the data
and as I mentioned earlier this is
pretty much in the form of digital form
like numbers so all these are like pixel
values so you will not really see an
image in this format but there is a way
to visualize that image I will show you
in a bit and uh this tells you how how
many images are there in each set so the
training there are 55,000 images in
training and in the test set there are
10,000 and then validation there are
5,000 so altogether there are 70,000
images all right so let's uh move on and
we can view the actual image by uh using
the matplot clip library and this is how
you can view this is the code for
viewing the images and you can view view
them in color or you can view them in
Gray scale so the cmap is what tells in
what way we want to view it and what are
the maximum values and the minimum
values of the pixel values so these are
the Max and minimum values so of the
pixel values so maximum is one because
this is a scaled value so one means it
is uh White and zero means it is black
and in between is it can be anywhere in
between between black and white and the
way to train the model there is a
certain way in which you write your
tensorflow code and um the first step is
to create some placeholders and then you
create a model in this case we will use
the softmax model one of the simplest
ones and um placeholders are primarily
to get the data from outside into the
neural network so this is a very common
mechanism that is used and uh then of
course you will have variables which are
your remember these are your weights and
biases so for in our case there are 10
neurons and each neuron actually has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs right this is the first neuron it
has it receives all the 784 this is the
second neuron this also receives all the
78 so each of these inputs needs to be m
multiplied with the weight and that's
what we are talking about here so these
are this is a a matrix of
784 values for each of the neurons and
uh so it is like a 10 by 784 Matrix
because there are 10 neurons and uh
similarly there are biases now remember
I mentioned bias is only one per neuron
so it is not one per input unlike the
weights so therefore there are only 10
biases because there are only 10 neurons
in this case so that is what we are
creating a variable for biases so this
is uh something little new in tensor
flow you will see unlike our regular
programming languages where everything
is a variable here the variables can be
of three different types you have
placeholders which are primarily used
for feeding data you have variables
which can change during the course of
computation and then a third type which
is not shown here are constants so these
are like fixed numbers all right so in a
regular programming language you may
have everything as variables or at the
most variables and constants but in tens
oflow you have three different types
placeholders variables and constants and
then you create what is known as a graph
so tensorflow programming consists of
graphs and tensors as I mentioned
earlier so this can be considered
ultimately as a tensor and then the
graph tells how to execute Ute the whole
implementation so that the execution is
stored in the form of a graph and in
this case what we are doing is we are
doing a multiplication TF you remember
this TF was created as a tensorflow
object here one more level one more so
TF is available here now tens oflow has
what is known as a matrix multiplication
or matal function so that is what is
being used here in this case so we are
using the matrix multiplication of of
tensor flow so that you multiply your
input values x with W right this is what
we were doing x w plus b you're just
adding B and this is in very similar to
one of the earlier slides where we saw
Sigma XI wi so that's what we are doing
here matrix multiplication is
multiplying all the input values with
the corresponding weights and then
adding the bias so that is the graph we
created and then we need to Define what
is our loss function and what is our
Optimizer so in this case we again use
the tensor flows apis so tf. NN softmax
cross entropy with logits is the uh API
that we will use and reduce mean is what
is like the mechanism whereby which says
that you reduce the error and Optimizer
for doing deduction of the error what
Optimizer are we using so we are using
gradient descent Optimizer we discussed
about this in couple of slides uh
earlier and for that you need to specify
the learning rate you remember we saw
that there was a a slide somewhat like
this and then you define what should be
the learning rate how fast you need to
come down that is the learning rate and
this again needs to be tested and tried
and to find out the optimum level of
this learning rate it shouldn't be very
high in which case it will not converge
or shouldn't be very low because it will
in that case it will take very long so
you define the optimizer and then you
call the method minimize for that
Optimizer and that will Kickstart the
training process and so far we've been
creating the graph and in order to
actually execute that graph we create
what is known as a session and then we
run that session and once the training
is completed we specify how many times
how many iterations we want it to run so
for example in this case we are saying
Thousand Steps so that is a exit
strategy in a way so you specify the
exit condition so training will run for
thousand iterations and once that is
done we can then evaluate the model
using some of the techniques shown here
so let us get into the code quickly and
see how it works so this is our Cloud
environment now you can install tensor
flow on your local machine as well I'm
showing this demo on our existing Cloud
but you can also install denslow on your
local machine and uh there is a separate
video on how to set up your tsor flow
environment you can watch that if you
want to install your local environment
or you can go for other any cloud
service like for example Google Cloud
Amazon or Cloud Labs any of these you
can use and U run and try the code okay
so it is got
started we will log
in all right so this is our deep
learning tutorial uh code and uh this is
our tensorflow
environment and uh so let's get started
the first we have seen a little bit of a
code walk through uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensor flow and then we will
import the data and we need to adjust
the data in such a way that the one hot
is encoding is set to True one hot
encoding right as I explained earlier so
in this case the label values will be
shown appropriately and if we just check
what is the type of the data so you can
see that this is a uh data sets python
data sets and if we check the number of
images the way it looks so this is how
it looks it is an array of type float 32
similarly the number if you want to see
what is the number
of training images there are 55,000 then
there are test images 10,000 and then
validation images 5,000 now let's take a
quick look at the data itself
visualization so we will use um mat plot
clip for this and um if we take a look
at the shape now shape gives us like the
dimension of the tensors or or or the
arrays if you will so in this case the
training data set if we seees the size
of the training data set using the
method shape it says there are 55,000
and 55,000 by 784 so remember the 784 is
nothing but the 28 by 28 28 into 28 so
that is equal to 784 so that's what it
is uh showing now we can take just uh
one image and just see what is the the
first image and see what is the shape so
again size obviously it is only 784
similarly you can look at the image
itself the data of the first image
itself so this is how it it shows so
large part of it will probably be zeros
because as you can imagine in the image
only certain areas are written rest is
uh black so that's why you will mostly
Zer either it is black or white but then
there are these values are so the values
are actually they are scaled so the
values are between zero and one okay so
this is what you're seeing so certain
locations there are some values and then
other locations there are zeros so that
is how the data is stored and loaded if
we want to actually see what is the
value of the handwritten image if you
want to view it this is how you view it
so you create like do this reshape and
um matplot lib has this um feature to
show you these images so we will
actually use the function called um IM
am show and then if you pass this
parameters appropriately you will be
able to see the different images now I
can change the values in this position
so which image we are looking at right
so we can say if I want to see what what
is there in maybe
5,000 right
so 5,000 has three similarly you can
just say five what is in five five as
eight what is in
50 again eight so basically by the way
if you're wondering uh how I'm executing
this code shift enter in case you're not
familiar with Jupiter notebooks shift
enter is how you execute each cell
individual cell and if you want to
execute the entire program you can go
here and say run all so that is
how this code gets executed and um here
again we can check what is the maximum
value and what is the minimum value of
this pixel values as I mentioned this is
it is scaled so therefore it is between
the values lie between 1 and zero now
this is where we create our
model the first thing is to create the
required placeholders and variables and
that's what we are doing here as we have
seen in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by 10
values okay so one for this 10 is for
each neuron there are 10 neurons and 784
is for
the pixel values inputs that are given
which is 28 into 28 and the biases as I
mentioned one for each neuron so there
will be 10 biases they are stored in a
variable by the name b and this is the
graph which is basically the
multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute
I think this code is executed then we
Define what is our the Y value is
basically the label value so this is
another placeholder we had X as one
placeholder and Yore true as a second
placeholder and this will have values in
the form of uh 10 digigit 10 digit uh
arrays and uh since we said one hot
encoded the position which has a one
value indicate what is the label for
that particular number all right then we
have cross entropy which is nothing but
the loss loss function and we have the
optimizer we have chosen gradient
descent as our Optimizer then the
training process itself so the training
process is nothing but to minimize the
cross entropy which is again nothing but
the loss function so we Define all of
this in the the form of a graph so the
up to here remember what we have done is
we have not exactly executed any
tensorflow code till now we are just
preparing the graph the execution plan
that's how the tensorflow code works so
the whole structure and format of this
code will be completely different from
how we normally do programming so even
with people with programming experience
may find this a little difficult to
understand it and it needs quite a bit
of practice so you may want to view this
uh video also maybe a couple of times to
understand this flow because the way
tensor flow programming is done is
slightly different from the normal
programming some of you who let's say
have done uh maybe spark programming to
some extent will be able to easily
understand this U but even in spark the
the programming the code itself is
pretty straightforward behind the scenes
the execution happens slightly
differently but in tens oflow even the
code has to be written in a completely
different way so the code doesn't get
executed uh in the same way as you have
written so that that's something you
need to understand and little bit of
practice is needed for this so so far
what we have done up to here is creating
the variables and feeding the variables
and um or rather not feeding but setting
up the variables and uh the graph that's
all defining maybe the uh what kind of a
network you want to use for example we
want to use softmax and so on so you
have created the variables have to load
the data loaded the data viewed the data
and prepared everything but you have not
yet executed anything in tens of flow
now the next step is the execution in
tens of flow so the first step for doing
any execution in tensor flow is to
initialize the variables so anytime you
have any variables defined in your code
you have to run this piece of code
always so you need to basically create
what is known as a a node for
initializing so this is a node you still
are not yet executing anything here you
just created a node for the
initialization so let us go ahead and
create that and here onwards is where
you will actually execute your code uh
intensive flow and in order to to
execute the code what you will need is a
session tensor flow session so tf.
session will give you a session and
there are a couple of different ways in
which you can do this but one of the
most common methods of doing this is
with what is known as a width Loop so
you have a withth tf. session as SS and
with a uh colon here and this is like a
block starting of the block and these
indentations tell how far this block
goes and this session is valid till this
block gets executed so that is the
purpose of creating this width block
this is known as a width block so with
tf. session as sess you say cs. run in
it now cs. run will execute a node that
is specified here so for example here we
are saying says do run sess is basically
an instance of the session right so here
we are saying tf. session so an instance
of the session gets created and we are
calling that cess and then we run a node
within that one of the nodes in the
graph so one of the nodes here is in it
so we say run that particular node and
that is when the initialization of the
variables happens now what this does is
if you have any
variables in your code in our case we
have W is a variable and B is a variable
so any variables that we created it you
have to run this code you have to run
the initialization of these variables
otherwise you will get an error okay so
that is the that's what this is doing
then we within this width block we
specify a for Loop and we are saying we
want the system to iterate for thousand
steps and per perform the training
that's what this for Loop does run
training for thousand
iterations and what it is doing
basically is it is fetching the data or
these images remember there are about
50,000 images but it cannot get all the
images in one shot because it will take
up a lot of memory and performance
issues will be there so this is a very
common way of Performing deep learning
training you always always do in batches
so we have maybe 50,000 images but you
always do it in batches of 100 or maybe
500 depending on the size of your system
and so on and so forth so in this case
we are saying okay get me 100 uh images
at a time and get me only the training
images remember we use only the training
data for training purpose and then we
use test data for test purpose you must
be familiar with machine learning so you
must be aware of this but in case you or
not in machine learning also not this is
not specific to deep learning but in
machine learning in general you have
what is known as training data set and
test data set your available data
typically you will be splitting into two
parts and using the training data set
for training purpose and then to see how
well the model has been trained you use
the test data set to check or test the
validity or the accuracy of the model so
that's what we are doing here and You
observe here that we are actually
calling an mnist function here so we are
saying mnist train. nextt batch right so
this is the advantage of using mes
database because they have provided some
very nice helper functions which are
readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a a lengthy
exercise so we can avoid all that if we
are using amness database and that's why
use this for the initial learning phase
okay so when we say fetch what it will
do is it will fetch the images into X
and the labels into Y and then you use
this batch of 100 images and you run the
training so cs. run basically what we
are doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously the
initially it will be wrong so all that
feedback is given back to the neural
network and thereby all the W's and Bs
get updated till it reaches th000
iterations in this case the exit
criteria is th000 but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neuron and that is run for
thousand iterations and typically by the
end of this thousand iterations the
model would have learned to recognize
these handwritten images obviously it
will not be 100% accurate okay so once
that is
done after so this happens for thousand
iterations once that is done you then
test the accuracy of these models by
using the test data set right so this is
what we are trying to do here the code
may appear a little complicated because
if you're seeing this for the first time
you need to understand uh the various
methods of tens of flow and so on but it
is basically comparing the output with
what has been what is actually there
that's all it is doing so you have your
test data and uh you're trying to find
out what is the actual value and what is
the predicted value and seeing whether
they are equal or not TF do equal right
and how many of them are correct and so
on and so forth and based on that the
accuracy is uh calculated as well so
this is the accuracy and uh that is what
we are trying to see how accurate the
model is in predicting these uh numbers
or these digits okay so let us run this
this entire thing is in one cell so we
will have to just run it in one shot it
may take a little while let us see and
uh not bad so it has finished the
thousand iterations and what we see here
as an output is the accuracy so we see
that the accuracy of this model is
around
91% okay now which is pretty good for
such a short exercise within such a
short time we got 90% accuracy however
in real life this is probably not
sufficient so there are other ways in to
increase the accuracy we will see
probably in some of the later tutorials
how to improve this accuracy how to
change maybe the hyper parameters like
number of neurons or number of layers
and so on and so forth and uh so that
this accuracy can be increased Beyond
90% hello and welcome to the tensorflow
object detection API tutorial in this
video I will walk you through the
tensorflow code to perform form object
detection in a video so let's get
started this part is basically you're
importing all the libraries we need a
lot of these libraries for example lpai
we need image IO datetime and pill and
so on and so forth and of course mat
plot lib so we import all these
libraries and then there are a bunch of
variables which have some parts for the
files and folders so this is regular
stuff let's keep moving then we import
the ma plot lib and make it in line and
uh a few more Imports all right and then
these are some warnings we can just
ignore them so if I run this code once
again it will go away all right and then
here onwards we do the model preparation
and what we're going to do is we're
going to use an existing neural network
model so we are not going to train a new
one because that really will uh take a
long time and uh it needs a lot of uh
computation resources and so on and it
is really not required there are already
models that have been trained and in
this case it is the SSD with mobile net
that's the model that we are going to
use and uh this model is trained to
detect objects and uh it is readily
available as open source so we can
actually use this and if you want to use
other models there are a few more models
available so you can click on this link
here and uh let me just take you there
there are a few more models but we have
chosen this particular one because this
is faster it may not be very accurate
but that is one of the faster models but
on this link you will see a lot of other
models that are readily available these
are trained models some of them would
take a little longer but they may be
more accurate and so on so you can
probably play around with these other
models okay so we will be using that
model so this piece of code this line is
basically importing that model and this
is also known as uh Frozen model the
term we use is frozen model so we import
download and import that and then we
will actually use that model in our code
all right so these two cells we have
downloaded and import the model and then
once it is available locally we will
then load this into our program all
right so we are loading this into memory
and uh the unit to perform a couple of
additional steps which is basically we
need to to map the numbers to text as
you may be aware when we actually build
the model and when we run predictions
the model will not give a text the
output of the model is usually a number
so we need to map that to a text so for
example if the network predicts that the
output is five we know that five means
it is an airplane things like that so
this mapping is done in this next cell
all right so let's keep moving and then
we have a helper code which will
basically load the data or load the
images and transform into numpy arrays
this is also used in doing object
detection in images so we are actually
going to reuse because video is nothing
but it consists of frames which in turn
are images so we are going to pretty
much use reuse the same code which we
used for doing object detection in
images so this is where the actual
detection starts so here this is the
path for where the images are stored so
this is here once again we are reusing
the code which we wrote for detecting
objects in an image so this is the path
where the images were stored and this is
the extension and this was done for
about two or three images so we will
continue to use this and uh we go down
I'll skip this section so this is the
cell where we are actually loading the
video and converting it into frames and
then using frame by frame we are
detecting the objects in the image so in
this code what we are doing basically is
there a few lines of code what they do
is basically once they find an object a
box will be drawn around those uh each
of those objects and the input file the
name of the input video file is uh
traffic it is the extension is MP4 and
uh we have this video reader excellent
object which is basically part of this
class called image iio so we can read
and write videos using that and uh the
video that we are going to use is
traffic. MP4 you can use any mp4 file
but in our case I picked up video which
has uh like car so let me just show you
so this is in this object detection
folder I have this mp4 file I'll just
quickly play this video it's a little
slow yeah okay so here we go this is the
video it's a short one relatively small
video so that for this particular demo
and what it will do is once we run our
code it will detect each of these cars
and it will annotate them as cars so in
this particular video we only have cars
we can later on see with another video I
think I have cat here so we can also try
with that but let's first check with
this uh traffic video so let me go back
so we will be reading this frame by
frame frame and um no actually we will
be reading the video file but then we
will be analyzing it frame by frame and
we will be reading them at 10 frames per
second that is the rate we are
mentioning here and analyzing it and
then annotating and then writing it back
so you will see that we will have a
video file named something like this
traffic annotated and um we will see the
annotated video so let's go back and run
through this piece of code and then we
will come back and see the annotated uh
video this might take a little while so
I will pause the video after running
this particular cell and then come back
to show you the results all right so
let's go ahead and run it so it is
running now and it is also important
that at the end you close the video
writer so that it is similar to a file
pointer when you open a file you should
also make sure you close it so that it
doesn't hog the resources so it's very
similar at the end of it the last piece
or last line of code should be video
writer. close all right so I'll pause
and then I'll come back okay so I will
see you in a little bit all right so now
as you can see here the processing is
done the r Glass has disappeared that
means the video has been processed so
let's go back and check the annotated
video we go back to my file manager so
this was the original traffic. MP4 and
now you have here traffic score
annotated MP4 so let's go and run this
and see how it
looks you see here it just got each of
these cars are getting detected let me
pause and show you so we pause here it
says car 70% let us allow it to go a
little further it detects something on
top what is that truck okay so I think
because of the board on top it somehow
thinks there is a truck let's lay some
more and see if it detects anything else
so this is again a car looks like so let
us yeah so this is a car and it has
confidence level of 69% okay this is
again a car all right so basically till
the end it goes and detects each and
every car that is passing by now we can
quickly repeat this process for another
video let me just show you the other
video which is a cat again there is uh
this cat is not really moving or
anything but it is just standing there
staring and moving a little slowly and
uh our application will our network will
detect that this is a cat and uh even
when the cat moves a little bit in the
other direction it'll continue to detect
and show that it is a cat Okay so yeah
so this is how the original video is
let's go ahead and change our code to
analyze this one and see if it detects
our Network detects this cat close this
here we go and I'll go back to my code
all we need to do is change this traffic
to cat the extension it will
automatically pick up because it is
given here and then it will run through
so very quickly once again what it is
doing is this video reader video uncore
reader has a a neat little feature or
interface whereby you can say for frame
in video uncore reader so it will
basically provide frame by frame so
you're in a loop frame by frame and then
you take that each frame that is given
to you you take it and analyze it as if
it is an image individual image so
that's the way it works so it is very
easy to handle this all right so now
let's once again run just this cell rest
of the stuff Remains the Same so I will
run this cell again it will take a
little while so the our glasses come
back I will pause and then come back in
a little while all right so the
processing is done let's go and check
the annotated video go here so we have
cat annotated MP4 let's play this all
right so you can see here it is
detecting the cat and in the beginning
you also saw it detected something else
here there looks like it detected one
more object so let's just go back and
see what it has detected here let's see
yes so what is it trying to show here
it's too small not able to see but uh it
is trying to detect something I think it
is saying it is a car I don't know all
right okay so so in this video there's
only pretty much only one object which
is the cat and uh let's wait for some
time and see if it continues to detect
it when the cat turns around and moves
as well just in a little bit that's
going to happen and we will see there we
go and in spite of turning the other way
I think our network is able to detect
that it is a cat so let me freeze and
then show here it is actually still
continues to detect it as a cat all
right so that's pretty much it I think
that's the only object that it detects
in this particular video okay so close
this so that's pretty much it thank you
very much for watching this video and
you have a great day and in case you
have any questions please uh put them
below the video here and we will be more
than happy to get back to you and make
sure you put your email ID so that we
can contact you in case you have any
questions thank you once again bye-bye
today we're going to be covering the
convolutional neural netor Network
tutorial do you know how deep learning
recognizes the objects in an image and
really this particular neural network is
how image recognition works it's very
Central one of the biggest building
blocks for image recognition it does it
using convolution neural network and we
over here we have the basic picture of a
u hummingbird pixels of an image fed as
input you have your input layer coming
in so it takes that graphic and puts it
into the input layer you have all your
hidden layers and then you have your
output layer and your out put layer one
of those is going to light up and say oh
it's a bird we're going to go into depth
we're going to actually go back and
forth on this a number of times today so
if you're not catching all the image um
don't worry we're going to get into the
details so we have our input layer
accepts the pixels of the image as input
in the form of arrays and you can see up
here where they've actually um labeled
each block of the bird in different
arrays so we'll dive into deep as to how
that looks like and how those matrixes
are set up your hidden layer carry out
feature extraction by performing in
certain calculations and manipulation so
this is the part that kind of
reorganizes that picture multiple ways
until we get some data that's easy to
read for the neural network this layer
uses a matrix filter and performs
convolution operation to detect patterns
in the image and if you remember that
convolution means to coil or to twist so
we're going to twist the data around and
alter it and use that operation to
detect a new pattern there are multiple
hidden layers like convolution layer
real U is how that is pronounced and
that's the rectified linear unit that
has to do with the activation function
that's used pooling layer also uses
multiple filters to detect edges corners
eyes feathers beak Etc and just like the
term says pulling is pulling information
together and we'll look into that a lot
closer here so if you're if it's a
little confusing now we'll dig in deep
and try to get you uh squared away with
that and then finally there is a fully
connected layer that identifies the
object in the image so we have these
different layers coming through in the
hidden layers and they come into the
final area and that's where we have say
one node or one neural network entity
that lights up that says it's a bird
what's in it for you we're going to
cover an introduction to the CNN what is
convolution neural network how CNN
recognizes images we're going to dig
deeper into that and really look at the
individual layers in the convolutional
neural network and finally we do a use
case implementation using the CNN we'll
begin our ction to the CNN by
introducing the pioneer of convolutional
neural network Yan leun he was the
director of Facebook AI research group
built the first convolutional neural
network called lenette in
1988 so these have been around for a
while and have had a chance to mature
over the years it was used for character
recognition tasks like reading zip code
digits imagine processing mail and
automating that process CNN is a feed
forward neural network that is generally
used to analyze visual images by
producing data with a grid-like topology
a CNN is also known as a convet and very
key to this is we are looking at images
that was what this was designed for and
you'll see the different layers as we
dig in Mirror some of the other some of
them are actually now used since we're
using uh tensorflow and carass in our
code later on you'll see that some of
those layers appear in a lot of your
other neural network Frameworks uh but
in this case this is very Central to
processing images and doing so in a
variety that captures multiple images
and really drills down into their
different features in this example here
you see flowers of two varieties Orchid
and a rose I think the Orchid is much
more dainty and beautiful and the rose
smells quite beautiful I have a couple
rose bushes in my yard uh they go into
the input layer that data is then sent
to all the different nodes in the next
layer one of the Hidden layers based on
its different weights and its setup it
then comes out and gives those a new
value those values then are multiplied
by their weights and go to the next
hidden layer and some so on and then you
have the output layer and one of those
notes comes out and says it's an orchid
and the other one comes out and says
it's a rose depending on how was well it
was trained what separates the CNN or
the convolutional neural network from
other neural networks is a convolutional
operation forms the basis of any
convolutional neural network in a CNN
every image is represented in the form
of arrays of pixel values so here we
have a real image of the digit 8 uh that
then gets put onto its pixel value
represent in the form of an array in
this case you have a two-dimensional
array and then you can see in the final
in form we transform the digit 8 into
its representational form of pixels of
zeros and on where the ones represent in
this case the black part of the eight
and the zeros represent the white
background to understand the convolution
neural network or how that convolutional
operation Works we're going to take a
side step and look at Matrix in this
case we're going to simplify it we're
going to take two matrices A and B of
one dimension now kind of separate this
from your thinking as we learned that
you want to focus just on the Matrix
aspect of this and then we'll bring that
back together and see what that looks
like when we put the pieces for the
convolutional operation here we've set
up two arrays we have uh in this case
there a single Dimension Matrix and we
have a = 5
37597 and we have b = 1 23 so in the
convolution as it comes in there it's
going to look at these two and we're
going to start by doing multiplying them
eight time B and so we multiply the
arrays element wise and we get 5
66 where five is the 5 * 1 6 is 3 * 2
and then the other 6 is 2 * 3 and since
the two arrays aren't the same size
they're not the same setup we're going
to just truncate the first one and we're
going to look at the second array
multiplied just by the first three
elements of the first array now that's
going to be a little confusing remember
a computer gets to repeat these
processes hundreds of time so we're not
going to just forget those other numbers
later on we'll see we'll bring those
back in and then we have the sum of the
product in this case 5 + 6 + 6 = 17 so
in our a * B our very first digit in
that Matrix of a * B is 17 and if you
remember I said we're not going to
forget the other digits so we now have
325 we move one set over and we take 325
and we multiply that times B and you'll
see that 3 * 1 is 3 2 * 2 is 4 and so on
and so on WE sum it up so now we have
the second digit of our a * B product in
The Matrix and we continue on with that
same thing so on and so on so then we
would go from uh 375 to 759 to 597 this
short Matrix that we have for a we've
now covered all the different entities
in a that match three different levels
of B now in a little bit we're going to
cover where we use this math at this
multiplying of matrixes and how that
works uh but it's important to
understand that we're going through the
Matrix and multiplying the different
parts to it to match the smaller Matrix
with the larger Matrix I know a lot of
people get lost at is you know what's
going on here with these matrixes uh oh
scary math not really that scary when
you break it down we're looking at a
section of a and we're comparing it to B
so when you break that down your mind
like that you realize okay so I'm I'm
just taking these two matrixes and
comparing them and I'm bringing the
value down into one Matrix a * B where
producing that information in a way that
will help the computer see different
aspects let's go ahead and flip over
again back to our images and here we are
back to our images talking about going
to the most basic two-dimensional image
you can get to consider the following
two images the image for the symbol
backs slash when you press the back
slash the above image is processed and
you can see there for the image for the
forward slash is the opposite so we
click the forward slash button that
flips uh very basic we have four pixels
going in can't get any more basic than
that here we have a little bit more
complicated picture we take a real image
of a smiley face um then we represent
that in the form of black and white
pixels so if this was an image in the
computer it's black and white and like
we saw before we convert this into the
zeros and ones so where the other one
would have just been a matrix of just
four dots now we have a significantly
larger image coming in so don't worry
we're going to bring this all together
here in just a little bit layers in
convolutional neural network when we're
looking at this we have our convolution
layer and that really is the central
aspect of processing images and the
convolutional neural network that's why
we have it and then that's going to be
feeding in and you have your reu layer
which is you know we talked about the
rectified linear unit we'll talk about
that a little bit later the reu is an
how it Act is how that layer is
activated is the math behind it what
makes the neurons fire you'll see that
in a lot of other neural networks when
you're using it just by it itself is for
processing smaller amounts of data where
you use the atom activation feature for
large data coming in now because we're
processing small amounts of data in each
image the reu layer works great you have
your pooling layer that's where you're
pulling the data together pooling is a
neural network term it's very commonly
used I like to use a term reduce so if
you're coming from the map and reduce
side you'll see that we're mapping all
this data through all these networks and
then we're going to reduce it we're
going to pull it together and then
finally we have the fully connected
layer that's where our output's going to
come out so we have started to look at
matrixes we've started to look at the
convolutional layer and where it fits in
and everything we've taken a look at
images so we're going to focus more on
the convolution layer since this is a
convolutional neural network a
convolution layer has a number of
filters that perform convolution
operation every image is considered as a
matrix of pixel values consider the
following 5x5 image whose pixel values
are only zero and one now obviously when
we're dealing with color there's all
kinds of things that come in on color
processing but we want to keep it simple
and just keep it black and white and so
we have our image pixels uh so we're
sliding the filter Matrix over the image
and Computing the dot product to detect
the patterns and right here you're going
to ask where does this filter come from
this is a bit confusing because the
filter is going to be derived uh later
on we build the filters when we program
or train our model so you don't need to
worry what the filter actually is what
you do need to understand how a
convolution layer works is what is the
filter doing filter and you'll have many
filters you don't have just one filter
you'll have lots of filters that are
going to look for different aspects and
so the filter might be looking for just
edges it might be looking for different
parts we'll cover that a little bit more
detail in a minute right now we're just
focusing on how the filter works as a
matrix remember earlier we talked about
multiplying matrixes together and here
we have our two-dimensional Matrix and
you can see see we take the filter and
we multiply it in the upper left image
and you can see right here 1 * 1 1 * 0 1
* 1 we multiply those all together then
sum them and we end up with a convolved
feature of four we going to take that
and sliding the filter Matrix over the
image and Computing the dot product to
detect patterns so we're just going to
slide this over we're going to predict
the first one and slide it over one
notch predict the second one and so on
and so on all the way through until we
have a new Matrix and this Matrix which
is the same size as a filter has reduced
the image and whatever filter whatever
that's filtering out is going to be
looking at just those features reduced
down to a smaller uh Matrix so once the
feature maps are extracted the next step
is to move them to the reu layer so the
realu layer The Next Step first is going
to perform an element wise operation so
each of those Maps coming in if there's
negative pixels so it says all the
negative pixels to zero um and you can
see this nice graph where it just zeros
out the negatives and then you have a
value that goes from zero up to whatever
value is um coming out of the Matrix
this introduces nonlinearity to the
network uh so up until now we have a we
say linearity we're talking about the
fact that the feature has a value so
it's a linear feature this feature um
came up and it has let's say the feature
is the edge of the beak you know it's
like or that backslash that we saw um
you'll look at that and say okay this
feature has a value from -10 to 10 in
this case um if it was one and' say yeah
this might be a beak it might not might
be an edge right there a minus5 means no
we're not even going to look at it to
zero and so we end up with an output and
the output takes all these feature all
these filtered features remember we're
not just running one filter on this
we're running a number of filters on
this image and so we end up with an
rectified feature map that is looking at
just the features coming through and how
they weigh in from our filters so here
we have an input of a it looks like a
twocan
bird very exotic looking real image is
scanned in multiple convolution and the
reu layers for locating features and you
can see up here is tur it into a black
and white image and in this case we're
looking in the upper right hand corner
for a feature and that box scans over a
lot of times it doesn't scan one pixel
at a time a lot of times it will Skip by
two or three or four pixels uh to speed
up the process that's one of the ways
you can compensate if you don't have
enough resources on your computation for
large images and it's not just one
filter slowly goes across the image uh
you have multiple filters have been
programmed in there so you're looking at
a lot of different filters going over
the different aspects of the image and
just sliding across there and forming a
new Matrix one more aspect to note about
the reu layer is we're not just having
one reu coming in uh so not only do we
have multiple features going through but
we're generating multiple relu layers
for locating the features that's very
important to note you know so we have a
quite a bundle we have multiple filters
multiple railu uh which brings us to the
next step forward propagation now we're
going to look at the pooling layer the
rectified feature map now goes through a
pooling layer pooling is a down sampling
operation that reduces the
dimensionality of the feature map that's
all we're trying to do we're trying to
take a huge amount of information and
reduce it down to a single answer this
is a specific kind of bird this is an
irest this is a rose so you have a
rectified feature map and you see here
we have a rectified feature map coming
in um we set the max pooling with a 2X
two filters and a stride of Two And if
you remember correctly I talked about
not going one pixel at a time uh well
that's where the stride comes in we end
up with a 2X two pulled feature map but
instead of moving one over each time and
looking at every possible combination we
skip a we skip a few there we go by two
we skip every other pixel and we just do
every other one um and this reduces our
rectified feature map which as you can
see over here 16x 16 to a 4x4 so we're
continually trying to filter and reduce
our data so that we can get to something
we can manage and over here you see that
we have the Max uh 3 4 1 and two and in
the max pooling we're looking for the
max value a little bit different than
what we were looking at before so coming
from the rectified feature we're now
finding the max value and then we're
pulling those features together so
instead of think of this as image of the
map think of this as how valuable is a
feature in that area how much of a
feature value do we have and we just
want to find the best or the maximum
feature for that area they might have
that one piece of the filter of the beak
said oh I see a one in this beak in this
image and then it skips over and says I
see a three in this image and says oh
this one is rated as a four we don't
want to sum it together cuz then you
know you might have like five ones and
it say ah five but you might have uh
four zeros in 110 and that 10 says well
this is definitely a beak where the ones
will say probably not a beak a little
strange analogy since we're looking at a
bird but you can see how that pulled
feature map comes down and we're just
looking for the max value in each one of
those matrixes pooling layer uses
different filters to identify different
parts of the image like edges corners
body feathers eyes beak Etc um I know I
focus mainly on the beak but obviously
uh each feature could be each a
different part of the bird coming in so
let's take a look at what that looks
like structure of a convolution neural
network so far this is where we're at
right now we have our input image coming
in and then we use our filters and
there's multiple filters on there that
are being developed to kind of twist and
change that data and so we multiply the
matrixes we take that little filter
maybe it's a 2 by two we multiply it by
each piece of the image and if we step
two then it's every other piece of the
image that generates multiple
convolution layers so we have a number
of convolution layers we have um set up
in there just looking at that data we
then take those convolution layers we
run them through the reu setup and then
once we've done through the reu setup
and we have multiple reu going on
multiple layers that are reu then we're
going to take those multiple layers and
we're going to be pooling them so now we
have the pooling layers or multiple
poolings going on up until this point
we're dealing with uh sometimes it's
multiple Dimensions you can have three
dimensions some strange data setups that
aren't doing images but looking at other
things they can have four or five 5 six
seven dimensions uh so right now we're
looking at 2D image Dimensions coming in
into the pooling layer so the next step
is we want to reduce those Dimensions or
flatten them so flattening flattening is
a process of converting all of the
resultant two-dimensional arrays from
pulled feature map into a single long
continuous linear Vector so over here
you see where we have a pulled feature
map maybe that's the bird wing and it
has values 6847 and we want to just
flatten this out and turn it into 684 47
or a single linear vector and we find
out that not only do we do each of the
pulled feature Maps we do all of them
into one long linear Vector so now we've
gone through our convolutional neural
network part and we have the input layer
into the next setup all we've done is
taken all those different pooling layers
and we flattened them out and combined
them into a single linear Vector going
in so after we've done the flattening we
have a just a quick recap because we've
covered so much so it's important to go
back back and take a look at each of the
steps we've gone through the structure
of the network so far is we have our
convolution where we twist it and we
filter it and multiply the matrixes we
end up with our convolutional layer
which uses the reu to figure out the
values going out into the pooling and
you have numerous convolution layers
that then create numerous pooling layers
pulling that data together which is the
max value which one we want to send
forward we want to send the best value
and then we're going to take all of that
from each of the pooling layers and
we're going to flatten it and we're
going to combine them into a single
input going into the final layer once
you get to that step you might be
looking at that going boy that looks
like the normal input to most neural
network and you're correct it is so once
we have the flattened Matrix from the
pooling layer that becomes our input so
the pooling layer is fed as an input to
the fully connected layer to classify
the image and so you can see as our
flatten Matrix comes in in this case we
have the pixels from the flatten Matrix
fed as an input back to our toucan or
whatever that kind of bird that is um I
need one of these to identify what kind
of bird that is it comes into our Ford
propagation network uh and that will
then have the different weights coming
down across and then finally it selects
that that's a bird and that it's not a
dog or a cat in this case even though
it's not labeled the final layer there
in red is our output layer our final
output layer that says bird cat or dog
so quick recap of everything we've
covered so far we have our input image
which is twisted and multiply the
filters are multiplied times the uh
matri the two matrixes multiplied all
the filters to create our convolution
layer our convolution layers there's
multiple layers in there because it's
all building multiple layers off the
different filters then goes through the
reu as is activation and that creates
our pooling and so once we get into the
pooling layer we then on the pooling
look for who's the best what's the max
value coming in from our convolution and
then we take that layer and we flatten
it and then it goes into a fully
connected layer a fully connected neural
network and then to the output and here
we can see the entire process how the
CNN recognizes a bird this is kind of
nice cuz it's showing the little pixels
and where they're going you can see the
filter is generating this convolution
network and that filter shows up in the
bottom part of the convolution network
and then based on that it uses the relo
for the pooling the pooling then find
out which one's the best and so on all
the way to the fully connected layer at
the end or the classification in the
output layer so that'd be classification
neural network at the end so we covered
a lot of theory up till now and you can
imagine each one of these steps has to
be broken down in code so putting that
together can be a little complicated not
that each step of the process is overly
complicated but because we have so many
steps uh we have 1 two 3 four five
different steps going on here with
substeps in there we're going to break
that down and walk through that in code
so in our use case implementation using
the CNN we'll be using the CFR 10 data
set from Canadian Institute for advanced
research for classifying images across
10 categories Unfortunately they don't
let me know whether it's going to be a
toucan or some other kind of bird but we
do get to find out whether it can
categorize between a ship a frog deer
bird airplane automobile cat dog horse
truck so that's a lot of fun and if
you're looking anything in the news at
all of our automated cars and everything
else you can see where this kind of
processing is so important in today's
world and very Cutting Edge as far as
what's coming out in the commercial
deployment I mean this is really cool
stuff we're starting to see this just
about everywhere in Industry uh so great
time to be playing with this and
figuring it all out let's go ahead and
dive into the code and see what that
looks like when we're actually writing
our script before we go on let's do uh
one more quick look at what we have here
let's just take a look at data batch one
keys and remember in Jupiter notebook I
can get by with not doing the print
statement if I put a variable down there
it'll just display the variable and you
can see under data batch one for the
keys since this is a dictionary we have
the batch one label data and file names
uh so you can actually see how it's
broken up in our data set so for the
next step or step four as we're calling
it uh we want to display the image using
Matt plot Library there's many ways to
display the images you could even uh
well there's other ways to drill into it
but map plot library is really good for
this and we'll also look at our first
reshape uh setup or shaping the data so
you can have a little glimpse into what
that means uh so we're going to start by
importing our map plot and of course
since I am doing jupyter notebook I need
to do the map plot inline command so it
shows up on my page so here we go we're
going to import map plot library. pyplot
is PLT and if you remember matplot
Library the PIP plot is like a canvas
that we paint stuff on to and there's my
percentage sign map plot library in line
so it's going to show up in my notebook
and then of course we're going to import
numpy as NP for our numbers python array
setup and let's go ahead and set u x
equals to data batch one so this will
pull in all the data going into the x
value and then because this is just a
long stream of binary data uh we need to
go a little bit of reshaping so in here
we have to go ahead and reshape the data
we have 10,000 images okay that looks
correct and this is kind of an
interesting thing it took me a little
bit to I had to go research this myself
to figure out what's going on with this
data and what it is is it's a 32x32
picture and let me do this let me go
ahead and do a drawing pad on here uh so
we have 32 bits by 32 bits and it's in
color so there's three bits of color now
I don't know why the data is
particularly like this it probably has
to do with how they originally encoded
it but most pictures put the three
afterward so what we're doing here is
we're going to take uh the shape we're
going to take the data which is just a
long stream of information and we're
going to break it up into 10,000 pieces
and those 10,000 pieces then are broken
into three pieces each and those three
pieces then are 32 by 32 you could look
at this like an oldfashioned projector
where they have the red screen or the
red projector the blue projector and the
green projector and they add them all
together and each one of those is a 32x
32bit so that's probably how this was
originally formatted with in that kind
of Ideal things have changed so we're
going to transpose it and we're going to
take the three which was here and we're
going to put it at the end so the first
part is reshaping the data from a single
line of bit data or whatever format it
is into 10,000x 3x 32x 32 and then we're
going to transpose the color factor to
the last place so it's the image then
the 32x 32 in the middle that's this
part right here and then finally we're
going to take this uh which is three
bits of data and put it at the end so
it's more like we do we process images
now and then as type this is really
important that we're going to use an
integer 8 you can come in here and
you'll see a lot of these they'll try to
do this with a float or a float 64 what
you got to remember though is a float
uses a lot of memory so once you switch
this into uh something that's not
integer 8 which is goes up to 128 you
are just going to the the amount of ram
let me just put that in here is going to
go way up the amount of ram that it
loads uh so you want to go ahead and use
this you can try the other ones and see
what happens if you have a lot of RAM on
your computer but for this exercise this
will work just fine and let's go ahead
and take that and run this so now our X
variable is all loaded and it has all
the images in it from the batch one data
batch one and just to show we were
talking about with the as type on there
if we go ahead and take x0 and just look
for its max value let me go ahead and
run that
uh you'll see it doesn't oops I said 128
it's 255 uh you'll see it doesn't go
over 255 because it's an basically an
asky character is what we're keeping
that down to we're keeping those values
down so they're only 255 0 to 255 versus
a float value which would bring this up
um exponentially in size and since we're
using the map plot Library we can do um
oops that's not what I wanted since
we're using the map plot Library we can
take our canvas and just do a PLT do I
am for image show and let's just take a
look at what x0 looks like and it comes
in I'm not sure what that is but you can
see it's a very low grade image uh
broken down to the minimal pixels on
there and if we did the same thing oh
let's do uh let's see what one looks
like hopefully it's a little easier to
see run on there not enter let's hit the
run on that uh and we can see this is
probably a semi TR that's a good guess
on there and I can just go back up here
instead of typing the same line in over
and over and we'll look at three uh that
looks like a dump truck unloading uh and
so on you can do any of the 10,000
images we can just jump to 55 uh looks
like some kind of animal looking at us
there probably a dog and just for fun
let's do just one more uh uh run on
there and we can see a nice car for
image number four uh so you can see we
past through all the different images
it's very easy to look at them and
they've been reshaped to fit our view
and what the uh map plot Library uses
for its format so the next step is we're
going to start creating some helper
functions we'll start by a one hot
encoder to help us we're processing the
data remember that your labels they
can't just be words they have to switch
it and we use the one hot encoder to do
that and then we'll also create a uh
class uh sear helper so it's going to
have a knit and a setup for the images
and then finally we'll go ahead and run
that code so you can see what that looks
like and then we get into the fun part
where we're actually going to start
creating our model our actual neural
network model so let's start by creating
our one hot encoder we're going to
create our own here uh and it's going to
return an out we'll have our Vector
coming in and our values equal 10 what
this means is that we have the 10 values
the 10 possible labels and remember we
don't look at the labels as a number
because a car isn't one more than a
horse and that'd be just kind of bizarre
to have horse equals zero car equals 1
plane equal 2 cat equals 3 so a cat plus
a car equals what uh so instead we
create a numpy array of zeros and
there's going to be 10 values so we have
10 different values in there so you have
uh zero or one one means it's a cat zero
means it's not a cat um in the next line
it might be that uh one means it's a car
zero means it's not a car so instead of
having one output with a value of 0 to
10 you have 10 outputs with the values
of 0 to one that's what the one hot
encoder is doing here and we're going to
utilize this in code in just a minute so
let's let's go ahead and take a look at
the next helpers we have a few of these
helper functions we're going to build
and when you're working with a very
complicated python project dividing it
up into separate definitions and classes
is very important otherwise it just
becomes really ungainly to work with so
let's go ahead and put in our next
helper uh which is a class and this is a
lot in this class so we we'll break it
down here let's just start uh oops we
put a space right in there there we go
now this a little bit more readable at a
second space so we're going to create
our class the cyer for helper and we'll
start by initializing it now there's a
lot going on in here so let's start with
the uh nit part uh self. I equals z
that'll come in in a little bit we'll
come back to that in the lower part we
want to initialize our training batches
so when we went through this there was
like a meta batch we don't need the meta
batch but we do need the data batch one
two 3 four five and we do not want the
testing batch in here this is just the
self all train batches so we're going to
come make an array of of all those
different images and then of course we
leftt the test batch out so we have our
self. test batch uh we're going to
initialize the training images and the
training labels and also the test images
and the test labels so these are just
this is just to initialize these
variables in here then we create another
definition down here and this is going
to set up the images and let's just take
a look and see what's going on in there
now we could have all just put this as
part of the uh init part uh since this
is all just helper stuff but breaking it
up again makes it easier to read it also
makes it easier when we start executing
the different pieces to see what's going
on so that way we have a nice print
statement to say hey we're now running
this and this is what's going on in here
we're going to set up the self- trining
images at this point and that's going to
go to a numpy array vstack and in there
we're going to load up uh in this case
the data for D and S all train batches
again that points right up to here so
we're going to go through each one of
these uh files or each one of these data
sets CU they're not a file anymore we've
brought them in data batch one points to
the actual data and so our self-training
images is going to stack them all into
our into a numpy array and then it's
always nice to get the training length
and that's just a total number of uh
self-training images in there and then
we're going to take the selft trining
images let me switch marker colors
because I am getting a little too much
on the markers up here oops there we go
bring down our marker change
so we can see it a little better and at
this point this should look familiar
where did we see this well when we
wanted to uh uh look at this above and
we want to look at the images in the
matplot library we had to reshape it so
we're doing the same thing here we're
taking our self training images and uh
based on the training length total
number of images because we stacked them
all together so now it's just one large
file of images we're going to take and
look at it as our our three video
cameras that are each displaying uh 32x
32 we're going to switch that around so
that now we have um each of our images
that stays the same place and then we
have our 32 by 32 and then by our three
our last our three different values for
the color and of course we want to go
ahead and uh they run this where we say
divide by 255 that was from earlier it
just brings all the data into 0 to one
that's what this is doing so we're
turning this into a 0 to one array which
is uh all the pictures 32x 32 by 3 and
then we're going to take the
self-training labels and we're going to
pump those through our one hot encoder
we just made and we're going to stack
them together and uh again we're
converting this into an array that goes
from uh instead of having horse equals 1
dog equals 2 and then horse plus dog
would equal three which would be cat no
it's going to be uh you know an array of
10 where each one is zero to one then we
want to go ahead and set up our test
images and labels and uh when we're
doing this you're going to see it's the
same thing we just did with the rest of
them let me just change colors right
here this is no different than what we
were doing up here with our training Set
uh we're going to stack the different uh
images uh we're going to get the length
of them so we know how many images are
in there uh you certainly could add them
by hand but it's nice to let the
computer do it especially if it ever
changes on the other end and you're
using other data and again we reshape
them and transpose them and we also do
the one hot encoder same thing we just
just did on our training images so now
our test images are in the same format
so now we have a definition which sets
up all our images in there and then the
next step is to go ahead and batch them
or next batch and let's do another
breakout here for batches because this
is really important to understand tends
to throw me for a little Loop when I'm
working with tensor flow or carass or a
lot of these we have our data coming in
if you remember we had like 10,000
photos let me just put 10,000 down here
we don't want to run all 10,000 at once
so we want to break this up into batch
sizes and you also remember that we had
the number of photos in this case uh
length of test or whatever number is in
there uh we also have 32 by 32 by three
so when we're looking at the batch size
we want to change this from 10,000 to um
a batch of in this case I think we're
going to do batches of a 100 so we want
to look at just 100 the first 100 of the
photos and if you remember we set selfi
equal to 0er uh so what we're looking at
here is we're going to create X we're
going to get the next batch from the
very initialized we've already
initialized it for zero so we're going
to look at X from zero to batch size
which we set to 100 so just the first
100 images and then we're going to
reshape that into uh and this is
important to let the data know that
we're looking at 100x 32x 32x 3 now
we've already formatted it to the 3 2x
32x 3 this just sets everything up
correctly so that X has the data in
there in the correct order and the
correct shape and then the Y just like
the X uh is our labels so our training
labels again they go from zero to batch
size in this case they do selfi plus
batch size because the selfi is going to
keep changing and then finally we
increment the selfi because we have zero
so we so the next time we call it we're
going to get the next batch size and so
basically we have X and Y X being the
photograph data coming in and Y being
the label and that of course is labeled
through one hot encoder so if you
remember correctly if it was say a horse
is equal to zero it would be um one for
the zero position since this is the
horse and then everything else would be
zero in here let me just put lines
through there there we go there's our
array hard to see that array so let's go
ahead and take that and uh we're going
to finish loading it since this is our
class and now we're armed with all this
um uh our setup over here let's go ahead
and load that up and so we're going to
create a variable CH with the CFR helper
in it and then we're going to do ch.
setup images uh now we could have just
put all the setup images under the init
but by breaking this up into two parts
it makes it much more readable and um
also if you're doing other work there's
reasons to do that as far as the setup
let's go ahead and run that and you can
see where it says uh setting up training
images and labels setting up test images
and that's one of the reasons we broke
it up is so that if if you're testing
this out you can actually have print
statements in there telling you what's
going on which is really nice uh they
did a good job with this setup I like
the way that it was broken up in the
back and then one quick note you want to
remember that batch to set up the next
batch as we have to run uh batch equals
CH next batch of 100 because we're going
to use the 100 size uh but we'll come
back to that we're going to use that
just remember that that's part of our
code we're going to be using in a minute
from the definition we just made so now
we're ready to create our model first
thing we want to do is we want to import
report our tensor flow as TF I'll just
go ahead and run that so it's loaded up
and you can see we got a a warning here
uh that's because they're making some
changes it's always growing and they're
going to be depreciating one of the uh
values from float 64 to float type or
treeter does an NP float 64 uh nothing
to really worry about CU this doesn't
even affect what we're working on
because we've set all of our stuff to a
255 value or 0 to one and do keep in
mind that 0 to one value that we
converted to 255 is still a float value
uh but it'll easily work with either the
uh numpy float 64 or the numpy dtype
float it doesn't matter which one it
goes through so the depreciation would
not affect our code as we have it and in
our tensor flow uh we'll go ahead let me
just increase the size in there just a
moment so you can get better view of the
um what we're typing in uh we're going
to set a couple placeholders here and so
we have we're going to set x equals TF
placeholder TF float 32 we just talked
about the float 4 versus the numpy float
we're actually just going to keep this
at float 32 more than a significant
number of decimals for what we're
working with and since it's a
placeholder we're going to set the shape
equal to and we've set it equal to none
because at this point we're just holding
the place on there we'll be setting up
as we run the batches that's what the
first value is and then 32x 32x 3 that's
what we reshaped our data to fit in and
then we have our y true equals
placeholder TF float 32 and the shape
equal none comma 10 10 is the 10
different labels we have so it's an
array of 10 and then let's create one
more placeholder we'll call this uh hold
prob or hold probability and we're going
to use this we don't have to have a
shape or anything for this this
placeholder is for what we called
Dropout if you remember from our Theory
before we drop out so many nodes is
looking at or the different values going
through which helps decrease bias so we
need to go ahead and put a a placeholder
for that also and we'll run this so it's
all loaded up in there so we have our
three different placeholders and since
we're in tensor flow when you use carass
it does some of this automatically but
we're in tensor flow direct sits on
tensor flow we're going to go ahead and
create some more helper functions we're
going to create something to help us
initialize the weights initialize our
bias if you remember that each uh layer
has to have a bias going in we're going
to go ahead and work on our our
conversional 2D our Max pool so we have
our pooling layer our convolutional
layer and then our normal full layer so
we're going to go ahead and put those
all into definitions and let's see what
that looks like in code and you can also
grab some of these helper functions from
the MN the uh nist setup let me just put
that in there if you're under the tensor
flow so a lot of these are already in
there but we're going to go ahead and do
our own and we're going to create our uh
a knit weights and one of the reasons
we're doing this is so that you can
actually start thinking about what's
going on in the back end so even though
there's ways to do this with an
automation sometimes these have to be
tweaked and you have to put in your own
setup in here uh now we're not going to
be doing that we're just going to
recreate them for our code and let's
take a look at this we have our weights
and so what comes in is going to be the
shape and what comes out is going to be
uh random numbers so we're going to go
ahead and just knit some random numbers
based on the shape with a standard
deviation of 0.1 kind of a fun way to do
that and then the TF variable uh init
random distribution so we're just
creating a random distribution on there
that's all that is for the weights now
you might change that you might have a a
higher standard deviation ation in some
cases you actually load preset weights
that's pretty rare usually you're
testing that against another model or
something like that and you want to see
how those weights configure with each
other uh now remember we have our bias
so we need to go ahead and initialize
the bias with a constant uh this case
we're using 0.1 a lot of times the bias
is just put in as one and then you have
your weights to add on to that uh but
we're going to set this as 0.1 uh so we
want to return a convolutional 2d in
this case a neural network this is uh
would be a layer on here what's going on
with the con 2D is we're taking our data
coming in uh we're going to filter it
strides if you remember correctly
strides came from here's our image and
then we only look at this picture here
and then maybe we have a stride of one
so we look at this picture here and we
continue to look at the different
filters going on there the other thing
this does is that we have our data
coming in as
32 by 32 2 by 3 and we want to change
this so that it's just this is three
dimensions and it's going to reformat
this as just two Dimensions so it's
going to take this number here and
combine it with the 32x 32 so this is a
very important layer here CU it's
reducing our data down using different
means and it connects down I'm just
going to jump down one here uh it goes
with the convolutional layer so you have
your your kind of your pre- formatting
and the setup and then you have your
actual convolution layer that goes
through on there and you can see here we
have a knit weights by the shape a knit
bias shape of three CU we have the three
different uh here's our three again and
then we return the tfnn relu with the
convention 2D so this convolutional uh
has this feeding into it right there
it's using that as part of it and of
course the input is the XY plus b the
bias so that's quite a mouthful but
these two are the are the keys here to
creating the convolutional layers there
the convolutional 2D coming in and then
the convolutional layer which then steps
through and creates all those filters we
saw then of course we have our pooling
uh so after each time we run it through
the convectional layer we want to pull
the data uh if you remember correctly on
the on the pool side and let me just get
rid of all my marks it's getting a
little crazy there and in fact let's go
ahead and jump back to that slide let's
just take a look at that slide over here
uh so we have our image coming in we
create our convolutional layer with all
the filters remember the filters go um
you know the filters coming in here and
it looks at these four boxes and then if
it's a step let's say step two and then
goes to these four boxes and then the
next step and so on uh so we have our
convolutional layer that we generate or
convolutional layers they use the uh reu
function um there's other functions out
there for this though the reu is the uh
most the one that works the best at
least so far I'm sure that will change
then we have our pooling now if you
remember correctly the pooling was Max
uh so if we had the filter coming in and
they did the multiplication on there and
we have a one and maybe a two two here
and another one here and a three here
three is the max and so out of all of
these you then create an array that
would be three and if the max is over
here two or whatever it is that's what
goes into the pooling of what's going on
in our pooling uh so again we're
reducing that data down we reducing it
down as small as we can and then finally
we're going to flatten it out into a
single array and that goes into our
fully connected layer and you can see
that here in the code right here where
we're going to create our normal full
layer um so at some point we're going to
take from our pooling layer this will go
into some kind of flattening process and
then that will be fed into the full the
different layers going in down here um
and so we have our input size you'll see
our input layer get shape which is just
going to get the shape for whatever is
coming in uh and then input size initial
weights is also based on uh the input
layer coming in and the input size down
here is based on the input layer shape
so we're just going to already use the
shape shape and already have our sides
coming in and of course uh you have to
make sure you init the bias always put
your bias on there and we'll do that
based on the size so this will return
tf. matmo input layer w+b this is just a
normal full layer that's what this means
right down here that's what we're going
to return so that was a lot of steps we
went through let's go ahead and run that
so those are all loaded in there and
let's go ahead and uh create the layers
let's see what that looks like now that
we've done all the heavy lifting and
everything uh we get to do all the easy
part let's go ahead and create our
layers we'll create a convolution layer
one and two two different convolutional
layers and then we'll take that and
we'll flatten that out create a a
reshape pooling in there for our reshape
and then we'll have our full uh layer at
the end so let's start by creating our
first uh convolutional layer then we
come in here and let me just run that
real quick and I want you to notice on
here the three and the 32 this is
important because coming into this
convolutional layer we have three
different channels and 32 pixels each uh
so that has to be in there the four and
four you can play with this is your
filter size so if you remember you have
a filter and you have your image and the
filter slowly steps over and filters out
this image depending on what your step
is for this particular setup 44 is just
fine that should work pretty good for
what we're doing and for the size of the
image and then of course at the end once
you have your convolutional layer set up
you also need to pull it and you'll see
that the pooling is automatically set up
so that it would see the different shape
based on what's coming in so here we
have Max Two by 2 by two and we put in
the convolutional one that we just
created the convolutional layer we just
created goes right back into it and that
right up here as you can see is the X
it's coming in from here so it knows to
look at the first model and set the the
data accordingly set that up uh so so
matches and we went ahead and ran this
already I think I ran it let me go and
run it again and if we're going to do
one layer let's go ahead and do a second
layer down here and it's we'll call it
convo 2 it's also a convolutional layer
on this and you'll see that we're
feeding convolutional one in the pooling
so it goes from convolutional one into
convolutional one pooling from
convolutional one pooling into
convolutional two and then from
convolutional two into convolutional two
pooling and we'll go ahead and take this
and run this so these variables are all
loaded into memory and for our flatten
layer uh let's go ahead and we'll do uh
since we have 64 coming out of here and
we have a 4x4 going in let's do 8X 8X 64
so let's do
496 this is going to be the flat layer
so that's how many bits are coming
through on the flat layer and we'll
reshape this so we'll reshape our convo
2 pooling and that will feed into here
the convo 2 pooling and then we're going
to set it up as a single layer that's
4,096 in size that's what that means
there we'll go ahead and run this so
we've now created this variable the
convo two flat and then we have our
first full layer this is the final uh
neural network where we have the flat
layer going in and we're going to again
use the uh reu for our uh setup on there
in a neural network for evaluation and
you'll notice that we're going to create
our first full layer our normal full
layer that's our definition so we
created that that's creating the normal
full layer and our input for the data
comes right here from the this goes
right into it uh the convo to flat so
this tells it how big the data is and
we're going to have it come out it's
going to have uh 1024 that's how big the
layer is coming out we'll go ahead and
run this so now we have our full layer
one and with the full layer one we want
to also Define the full one Dropout to
go with that so our full layer one comes
in uh keep probability equals hold
probability remember we created did that
earlier and the full layer one is what's
coming into it and this is going
backwards and training the data we're
not training every weight we're only
training a percentage of them each time
which helps get rid of the bias so let
me go ahead and run that and uh finally
we'll go ahead and create a y predict
which is going to equal the normal full
one Dropout and 10 because we have 10
labels in there now in this neural
network we could have added additional
layers that would be another option to
play with you can also play with instead
of 1024 you can use use other numbers
for the way that sets up and what's
coming out going into the next one we're
only going to do just the one layer and
the one layer Dropout and you can see if
we did another layer it'd be really easy
just to feed in the full one Dropout
into full layer two and then full Layer
Two Dropout would have full Layer Two
feed into it and then you'd switch that
here for the Y prediction for right now
this is great this particular data set
is tried and true and we know that this
will work on it and if we just type in y
predict and we run that uh we'll see
that this is a tensor object uh shape
question mark 10 dtype 32 a quick way to
double check what we're working on so
now we've got all of our uh we've done a
setup all the way to the Y predict which
we just did uh we want to go ahead and
apply the loss function and make sure
that's set up in there uh create the
optimizer and then uh train our
Optimizer and create a variable to
initialize all the global TF variables
so before we Dive In to the um loss
function let me point out one quick
thing or just kind of a rehap over a
couple things and that is when we're
playing with this these setups um we
pointed out up here we can change the 44
and use different numbers there they
change your outcome so depending on what
numbers you use here will have a huge
impact on how well your model fits and
that's the same here of the 1024 also
this is also another number that if you
continue to raise that number you'll get
um possibly a better fit you might
overfit and if you lower that number
you'll use less resources and generally
you want to use this in um the
exponential growth an exponential being
2 4 8 16 and in this case the next one
down would be 512 you can use any number
there but those would be the ideal
numbers uh when you look at this data so
the next step in all this is we need to
also create uh a way of tracking how
good our model is and we're going to
call this a loss function and so we're
going to create a cross entropy loss
function and so before we discuss
exactly what that is let's take a look
and see what we're feeding it uh we're
going to feed it our labels and we have
our true labels and our prediction
labels uh so coming in here is we the
two different uh variables we're sending
in or the two different probability
distributions is one that we know is
true and what we think it's going to be
now this function right here when they
talk about cross entropy uh in
information Theory the cross entropy
between two probabil distributions over
the same underlying set of events
measures the average number of bits
needed to identify an event drawn from
the set that's a mouthful uh really
we're just looking at the amount of
error in here how many of these are
correct and how many of these um are
incorrect so how much of it matches and
we're going to look at that we're just
going to look at the average that's what
the mean the reduced to the mean means
here so we're looking at the average
error on this and so the next step is
we're going to take the error we want
want to know uh our cross entropy or our
loss function how much loss we have
that's going to be part of how we train
the model so when you know what the loss
is and we're training it you feed that
back into the back propagation setup and
so we want to go ahead and optimize that
here's our Optimizer we're going to
create the optimizer using an atom
Optimizer remember there's a lot of
different ways of optimizing the data
atoms the most popular used uh so our
Optimizer is going to equal the TF train
atom Optimizer if you don't remember
what the learning rate is let me just
pop this back into here here's our
learning rate when you have your weights
you have all your weights and your
different nodes that are coming out
here's our node coming out um and it has
all its weights and then the error is
being prop sent back through in reverse
on our neural network so we take this
error and we adjust these weights based
on the different formulas in this case
the atom formula is what we're using we
don't want to just adjust them
completely we don't want to change this
weight so it exactly fits the data
coming through because if we made that
kind of adjustment it's going to be
biased to whatever the last data we sent
through is instead we're going to
multiply that by 0.001 and make a very
small shift in this weight so our Delta
W is only 0.001 of the actual Delta W of
the full change we're going to compute
from the atom and then we want to go
ahead and train it so our training or
set up a training uh uh variable or
function and this is going to equal our
Optimizer minimize cross entropy and we
make sure we go ahead and run this so
it's loaded in there and then we're
almost ready to train our model but
before we do that we need to create one
more um variable in here and we're going
to create a variable to initialize all
the global TF variables and when we look
at this um the TF Global variable
initializer this is a tensor flow um
object it goes through there and it
looks at all our different setup that we
have going under our tensorflow and then
initializes those variables uh so it's
kind of like a magic W because it's all
hidden in the back end of tensor flow
all you need to know about this is that
you have to have the initialization on
there which is an operation um and you
have to run that once you have your
setup going so we'll go ahead and run
this piece of code and then we're going
to go ahead and train our data so let me
run this so it's loaded up there and so
now we're going to go ahead and run the
model by creating a graph session graph
session is a tensorflow term so you'll
see that coming up it's one of the
things that throws me because I always
think of graphic and Spark and graph as
just General graphing uh but they talk
about a graph session so we're going to
go ahead and run the model and let's go
ahead and walk through this uh what's
going on here and let's paste this data
in here and here we go so we're going to
start off with the with the TF session
as sess so that's our actual TF session
we've created uh so we're right here
with the TF uh session our session we're
creating we're going to run TF Global
variable initializer so right off the
bat we're initializing our variables
here uh and then we have for I in range
500 so what's going on here remember 500
we're going to break the data up and
we're going to batch it in at 500 points
each we've created our session run so
we're going to do with TF session as
session right here we've created our
variable session uh and then we're going
to run we're going to go ahead and
initialize it so we have our TF Global
variables initializer that we created um
that initializes our our session in here
the next thing we're going to do is
we're going to go for I in range of 500
batch equals ch. next batch so if you
remember correctly this is loading up um
100 pictures at a time and uh this is
going to Loop through that 500 times so
we are literally doing uh what is it uh
500 * 100 is uh 50,000 so that's 50,000
pictures we're going to process right
there in the first process is we're
going to do a session run we're going to
take our train we created our train
variable or Optimizer in there we're
going to feed it the diction
uh we had our feed dictionary that
created and we have x equals batch 0
coming in y true batch one hold the
probability
0.5 and then just so that we can keep
track of what's going on we're going to
every uh 100 steps we're going to run a
print So currently on step format C
accuracy is um and we're going to look
at matches equals tf. equal TF argument
y prediction one tf. AR Max y true comma
one so we're going to look at this is
how many matches it has and here our ACC
uh all we're doing here is we're going
to take the matches how many matches
they have it creates it generates a
chart we're going to convert that to
float that's what the TF cast does and
then we just want to know the average we
just want to know the average of the um
accuracy and then we'll go ahead and
print that out uh print session run
accuracy feed dictionary so it takes all
this and it prints out our accuracy on
there so let's go ahead and take this
oops screens there let's go ahead and
take this and let's run it and this is
going to take a little bit to run uh so
let's see what happens on my old laptop
and we'll see here that we have our
current uh we're currently on Step Zero
it takes a little bit to get through the
accuracy and this will take just a
moment to run we can see that on our
Step Zero it has an accuracy of 0.1 or
0128 um and as it's running we'll go
ahead you don't need to watch it run all
the way but uh this accuracy is going to
change a little bit up and down so we've
actually lost some accuracy during our
step two um but we'll see how that comes
out let's come back after we run it all
the way through and see how the
different steps come out I was actually
reading that backwards uh the way this
works is the closer we get to one the
more accuracy we have uh so you can see
here we've gone from a 0.1 to a 39 um
and we'll go ahead and pause this and
come back and see what happens when
we're done with the full run all right
now that we've uh prepared the meal got
it in the oven and pulled out my
finished dish here if you've ever
watched uh any of the old cooking shows
let's discuss a little bit about this
accuracy going on here and how do you
interpret that we've done a couple
things first we've defined accuracy um
the reason I got it backwards before is
you have uh loss or accuracy and with
loss you'll get a graph that looks like
this it goes oops that's an S by the way
there we go you get a graph that curves
down like this and with accuracy you get
a graph that curves up this is how good
it's doing now in this case
uh one is supposed to be really good
accuracy that mean it gets close to one
but it never crosses one so if you have
an accuracy of one that is phenomenal um
in fact that's pretty much Unos you know
unheard of and the same thing with loss
if you have a loss of zero that's also
unheard of the zero is actually on this
this axis right here as we go in there
so how do we interpret that because you
know if I was looking at this and I go
oh 0. 51 that's uh 51% you're doing
50/50 no this is not percentage let me
just put that in there it is not
percentage uh this is
logarithmic what that means is that. 2
is twice as good as 0.1 and uh when we
see 0. 4 that's twice as good as 0. 2
real way to convert this into a
percentage you really can't say this is
is a direct percentage conversion what
you can do though is in your head if we
were to give this a percentage uh we
might look at this as uh 50%
we're just guessing equals 0.1 and if
50% roughly equals 0.1 that's where we
started up here at the top remember at
the top here here's our 0.128 the
accuracy of 50% then 75% is about 0.2
and so on and so on don't quote those
numbers because that doesn't work that
way they say that if you have
.95 that's pretty much saying 100% And
if you have uh anywhere between you'd
have to go look this up let me go and
move all my drawings there uh so the
magic number is 0.5 we really want to be
over a 0.5 in this whole thing and we
have uh both 0504 remember this is
accuracy if we were looking at loss then
we'd be looking the other way but 0.0
you know instead of how high it is we
want how low it is uh but with accuracy
being over a 0. five is pretty valid
that means this is pretty solid and if
you get to a 095 then it's a direct
correlation that's what we're looking
for here in these num numbers you can
see we finished with this model at 5135
so still good um and if we look at uh
when they ran this in the other end
remember there's a lot of Randomness
that goes into it when we see the
weights uh they got
05251 so a little better than ours but
that's fine you'll find your own uh
comes up a little bit better or worse
depending on uh just that Randomness and
so we've gone through the whole model
we've created we trained the model and
we've also gone through on every 100th
run to test the model to see how
accurate it is welcome to the RNN
tutorial that's the recurrent neural
network so we talk about a feed forward
neural network in a feed forward neural
network information flows only in the
forward direction from the input nodes
through the hidden layers if any into
the output nodes there are no Cycles or
Loops in the network and so you can see
here we have our input layer I was
talking about how it just goes straight
forward into the hidden layers so each
one of those connects and then connects
to the next hidden layer connects to the
output layer and of course we have a
nice simplified version where it has a
predicted output and they refer to the
input as x a lot of times and the output
as y decisions are based on current
input no memory about the past no future
scope why recurrent neural network
issues in feed forward neural network so
one of the biggest issues is because it
doesn't have a scope of memory or time a
feed forward neural network doesn't know
how to handle sequential data uh it only
considers only the current input so if
you have a series of things and because
three points back effects what's
happening now and what your output
affects what's happening that's very
important so whatever I put as an output
is going to affect the next one um a
feed forward doesn't look at any of that
it just looks at this is what's coming
in and it cannot memorize previous
inputs so it doesn't have that list of
inputs coming in solution to feed
forward neural network you'll see here
where it says recurrent neural network
and we have our X on the bottom going to
H going to Y that's your feed forward uh
but right in the middle it has a value C
so it's a whole another process so it's
memorizing what's going on in in the
hidden layers and the hidden layers as
they produce data feed into the next one
so your hidden layer might have an
output that goes off to Y uh but that
output goes back into the next
prediction coming in what this does is
this allows it to handle sequential data
it considers the current input and also
the previously received inputs and if
we're going to look at General drawings
and um Solutions we should also look at
applications of the RNN image captioning
RNN is used to caption an image by
analyzing the activities present in it a
dog catching a ball in midair uh that's
very tough I mean you know we have a lot
of stuff that analyzes images of a dog
and the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using RNN and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that that it is very difficult and if
you're analyzing the whole stock the
stock market at the New York Stock
Exchange in the US produces somewhere in
the neighborhood if you count all the
individual trades and fluctuations by
the second um it's like three terabytes
a day of data so we're only to look at
one stock just analyzing One stock is
really tricky in here we'll give you a
little jump on that so that's exciting
but don't expect to get rich off of it
immediately another application of the
RNN is natural language processing text
Mining and analysis can be carried out
using RNN for natural language
processing and you can see right here
the term natural language processing
when you stream those three words
together is very different than I if I
said processing language natural leave
so the time series is very important
when we're analyzing sentiments it can
change the whole value of a sentence
just by switching the words around or if
you're just counting the words you might
get one sentiment where if you actually
look at the order there in you get a
completely different sentiment when it
rains look for rainbows when it's dark
look for stars both of these are
positive sentiments and they're based
upon the order of which the sentence is
going in machine translation given an
input in one language RNN can be used to
translate the input into a different
languages as output I myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking English you would say
big cat and if you're speaking Spanish
you would say cat big so that
translation is really important to get
the right order to get there's all kinds
of parts of speech that are important to
know by the order of the words here this
person is speaking in English and
getting translated and you can see here
a person is speaking in English in this
little diagram I guess that's denoted by
the flags I have a flag I own it no um
but they're speaking in English and it's
getting translated into Chinese Italian
French German and Spanish languages some
of the tools coming out are just so cool
so somebody like my myself who's very
linguistically challenged I can now
travel into Worlds I would never think
of because I can have something
translate my English Back in Forth
readily and I'm not stuck with a
communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down it'll make more
sense and usually we have a propagation
forward neural network with the input
layers the hidden layers the output
layer with the recurrent neural network
we turn that on its side so here it is
and now our X comes up from the bottom
into the hidden layers into Y and they
usually draw very simplified X to H with
c as a loop a to Y where a B and C are
the perimeters a lot of times you'll see
this kind of drawing in here digging
closer and closer into the H and how it
works going from left to right you'll
see that the C goes in and then the X
goes in uh so the x is going upward Ward
bound and C is going to the right a is
going out and C is also going out that's
where gets a little confusing so here we
have xn uh CN and then we have y out and
C out and C is based on HT minus one so
our value is based on the Y and the H
value are connected to each other
they're not necessarily the same value
because H can be its own thing and
usually we draw this or we represent it
as a function h of T equals a function
of C where H of T minus one that's the
last H output and X of T going in so
it's the last output of H combined with
the new input of x uh where HT is the
new state FC is a function with the
parameter C that's a common way of
denoting it uh HT minus one is the Old
State coming out and then xit T is an
input Vector at time of Step T well we
need to cover types of recurrent neural
networks and so the first one is the
most common one which is a one toone
single output one to one neural network
is usually known as a vanilla neural
network used for regular machine
learning problems why because vanilla's
usually considered kind of a just a real
basic flavor but because it's a very
basic a lot of times they'll call it the
vanilla neural network uh which is not
the common term but it is you know like
kind of a slang term people will know
what you're talking about usually if you
say that then we run one to mini so you
have a single input and you might have a
multiple outputs in this case uh image
captioning as we looked at earlier where
we have not just looking at it as a dog
but a dog catching a ball in the air and
then you have many to1 Network takes in
a sequence of inputs examples sentiment
analysis where a given sentence can be
classified as expressing positive or
negative sentiments and we looked at
that as we were discussing if it rains
look for a rainbow so positive sentiment
where rain might be a negative sentiment
if you were just adding up the words in
there and then of course if you're going
to do a one to one many to one one to
many there's many to many Networks takes
in a sequence of inputs and generates a
sequence of outputs example machine
translation so we have a lengthy
sentence coming in in English and then
going out in all the different languages
uh you know just a wonderful tool very
complicated set of computations you know
if you're a translator you realize just
how difficult it is to translate into
different languages one of the biggest
things you need to understand when we're
working with this neural network is
what's called The Vanishing gradient
problem while training an RNN your slope
can be either too small or very large
and this makes training difficult when
the slope is too small the problem is
known as Vanishing gradient and you'll
see here they have a nice U image loss
of information through time so if you're
pushing not enough information forward
that information is lost and then when
you go to train it you start losing the
third word in the sentence or something
like that or it doesn't quite follow the
full logic of what you're working on
exploding gradient problem Oh this is
one that runs into everybody when you're
working with this particular neural
network when the slope tends to grow
exponentially instead of decaying this
problem is called exploding gradient
issues in gradient problem long tring
time poor performance bad accuracy and
I'll add one more in there uh your
computer if you're on a lower-end
computer testing out a model will lock
up and give you the memory error
explaining gradient problem consider the
following two examples to understand
what should be the next word in the
sequence the person who took my bike and
blank a thief the students who got into
engineering with blank from Asia and you
can see in here we have our x value
going in we have the previous value
going forward and then you back
propagate the error like you do with any
neural network and as we're looking for
that missing word maybe will have the
person took my bike and blank was a
thief and the student who got into
engineering with a blank were from Asia
consider the following example the
person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the RNN must memorize the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the sequence the RNN must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
Asia it might be sometimes difficult for
the eror to back propagate to the
beginning of the sequence to predict
what should be the output so when you
run into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having uh whatever
information it's sending to the next
series we can truncate what it's sending
we can lower that particular uh set of
layers make those smaller and finally is
a gradient clipping so when we're
training it we can clip with that
gradient looks like and narrow the
training model that we're using when you
have a Vanishing gradient the OPA
problem uh we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choosing the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talked too
much about activation functions so we'll
look at that just minimally uh there's a
lot of choices out there and then
finally there's long short-term memory
networks the
lstms and we can make adjustments to
that so just like we can clip the
gradient as it comes out we can also um
expand on that we can increase the
memory Network the size of it so it
handles more information and one of the
most common problems in today's uh setup
is what they call longterm dependencies
suppose we try to predict the last word
in the text the clouds are in the and
you probably said sky here we do not
need any further context it's pretty
clear that the last word is going to be
Sky suppose we try to predict the last
word in the text I have been staying in
Spain for the last 10 years I can speak
fluent maybe you said Portuguese or
French no you probably said Spanish the
word we predict will depend on the
previous few words in context here we
need the context of Spain to predict the
last word in the text is possible that
the gap between the relevant information
and the point where it is needed to
become very larg
lstms help us solve this problem so the
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default Behavior All recurrent
neural networks have the form of a chain
of repeating modules of neural network
connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent H
layer lstm s's also have a chain-like
structure but the repeating module has a
different structure instead of having a
single neural network layer there are
four interacting layers communicating in
a very special way lstms are a special
kind of recurrent neural network capable
of learning long-term dependenc es
remembering information for long periods
of time is their default Behavior LST
tms's also have a chain-like structure
but the repeating module has a different
structure instead of having a single
neural network layer there are four
interacting layers communicating in a
very special way as you can see the
deeper we dig into this the more
complicated the graphs kit in here I
want you to note that you have X of T
minus one coming in you have x a t
coming in and you have x a t + one and
you have H A T minus one and H of T
coming in and H of t plus one going out
and of course uh on the other side is
the output a um in the middle we have
our tangent H but it occurs in two
different places so not only when we're
Computing the x of t+ one are we getting
the tangent H from X of T but we're also
getting that value coming in from the X
of T minus one so the short of it is as
you look at these layers not only does
it does the propagate through the first
layer goes into the second layer back
into it but it's also going into the
third layer so now we're kind of
stacking those up and this can get very
complicated as you grow that inze it
also grows in memory too and in the
amount of resources it takes uh but it's
a very powerful tool to help us address
the problem of complicated long
sequential information coming in like we
were just looking at in the sentence and
when we're looking at our long shortterm
memory network uh there's three steps of
processing sessing in the lstms that we
look at the first one is we want to
forget irrelevant parts of the previous
state you know a lot of times like you
know is as in a unless we're trying to
look at whether it's a plural noun or
not they don't really play a huge part
in the language so we want to get rid of
them then selectively update cell State
values so we only want to update the
cell State values that reflect what
we're working on and finally we want to
put only output certain parts of the
cell state so whatever is coming out we
want to limit what's going out too and
let's dig a little deeper into this
let's just see what this really looks
like uh so step one decides how much of
the past it should remember first step
in the lstm is to decide which
information to be omitted in from the
cell in that particular time step it is
decided by the sigmoid function it looks
at the previous state h of T minus one
and the current input X of T and
computes the function so you can see
over here we have a function of T equals
the sigmoid function of the weight of f
the H at T minus one and then exit plus
of course you have a bias in there with
any of your neural network so we have a
bias function so F of T equals forget
gate decides which information to delete
that is not important from the previous
time step considering an L STM is fed
with the following inputs from the
previous and present time step Alice is
good in physics John on the other hand
is good in chemistry so previous output
John plays football well he told me
yesterday over the phone that he had
served as a captain of his college
football team that's our current input
so as we look at this the first step is
the forget gate realizes there might be
a change in context after encountering
the First full stop Compares with the
current input sentence of xit T so we're
looking at that full stop and then
Compares it with the input of the new
sentence the next sentence talks about
John so the information on Alice is
deleted okay that's important to know so
we have this input coming in and if
we're going to continue on with John
then that's going to be the primary
information we're looking at the
position of the subject is vacated and
is assigned to JN and so in this one
we've seen that we've weeded out a whole
bunch of information and we're only
passing information on John since that's
now the new topic so step two is in to
decide how much should this unit add to
the current state in the second layer
there are two parts one is a sigmoid
function and the other is a tangent H in
the sigmoid function it decides which
values to let through zero or one
tangent H function gives the weightage
to the values which are passed setting
their level of importance minus1 to 1
and you can see the two formulas that
come up uh the I of T equals the sigmoid
of the weight of i h of T minus one X of
t plus the bias of I and the C of T
equals the tangent of H of the weight of
C of H of t minus1 x of t plus the bias
of C so our I of T equals the input gate
determines which information to let
through based on its significance in the
current time step if this seems a little
complicated don't worry cuz a lot of the
programming is already done when we get
to the case study understanding though
that this is part of the program is
important when you're trying to figure
out these what to set your settings at
you should also note when you're looking
at this it should have some semblance to
your forward propagation neural networks
where we have a value assigned to a
weight plus a bias very important steps
than any of the neural network layers
whether we're propagating into them the
information from one to the next or
we're just doing a straightforward
neural network propagation let's take a
quick look at this what it looks like
from the human standpoint um as I step
out in my suit again consider the
current input at X of John plays
football well he told me yesterday over
the phone that he had served as a
captain of his college football team
that's our input input gate analysis the
important information John plays
football and he was a captain of his
college team is important he told me
over the phone yesterday is less
important hence it is forgotten this
process of adding some new information
can be done via the input gate now this
example is as a human form and we'll
look at training this stuff in just a
minute uh but as a human being if I
wanted to get this information from a
conversation maybe it's a Google Voice
listening in on you or something like
that um how do we weed out the
information that he was talking to me on
the phone yesterday well I don't want to
memorize that he talked to me on the
phone yesterday or maybe that is
important but in this case it's not I
want to know that he was the captain of
the football team I want to know that he
served I want to know that John plays
football and he was the captain of the
college football team those are the two
things that I want to take away as a
human being again we measure a lot of
this from the human Viewpoint and that's
also how we try to train them so we can
understand these neural networks finally
we get to step three decides what part
of the current cell State makes it to
the output the third step is to decide
what will be our output first we run a
sigmoid layer which decides what parts
of the cell State make it to the output
then we put the cell State through the
tangent H to push the values to be
between minus1 and one and multiply it
by the output of the sigmoid gate so
when we talk about the output of T we
set that equal to the sigmoid of the
weight of zero of the H of T minus one
you know back One Step in Time by the x
of t plus of course the bias the H of T
equals the out of T times the tangent of
the tangent h of C of T so our o t
equals the output gate allows the past
in information to impact the output in
the current time step let's consider the
example to predicting the next word in
the sentence johon played tremendously
well against the opponent and won for
his team for his contributions Brave
blank was awarded player of the match
there could be a lot of choices for the
empty space current input Brave is an
adjective adjectives describe a noun JN
could be the best output after Brave
thumbs up for John awarded player of the
match and if you were to pull just the
nouns out of the sentence team doesn't
look right because that's not really the
subject we're talking about
contributions uh you know Brave
contributions or Brave team Brave player
Brave match um so you look at this and
you can start to train this these this
neural network so it starts looking at
and goes oh no JN is what we're talking
about so brave is an adjective Jon's
going to be the best output and we give
JN a big thumbs up and then of course we
jump into my favorite part a case study
use case implementation of lstm let's
predict the prices of stocks using the
lstm network based on the stock price
data between 2012 2016 we're going to
try to predict the stock prices of 2017
and this will be a narrow set of data
we're not going to do the whole stock
market it turns out that the New York
Stock Exchange generates roughly three
terabytes of data per day that's all the
different trades up and down of all the
different stocks going on and each
individual one uh second to second or
nanc to nanc uh but we're going to limit
that to just some very basic fundamental
Al information so don't think you're
going to get rich off this today but at
least you can give an a you can give a
step forward in how to start processing
something like stock prices a very valid
use for machine learning in today's
markets use case implementation of
lstm let's dive in we're going to import
our libraries we're going to import the
training set and uh get the scaling
going um now if you watch any of our
other tutorials a lot of these pieces
should start to look very familiar CU
it's very similar setup uh but let's
take a look at that and um just a
reminder we're going to be using
Anaconda the Jupiter notebook so here I
have my anaconda Navigator when we go
under environments I've actually set up
a carass python 36 I'm in Python 36 and
U nice thing about Anaconda especially
the newer version remember a year ago
messing with anaconda and different
versions of python and different
environments um Anaconda now has a nice
interface um and I have this installed
both on a Ubuntu Linux machine and on
windows so it works fine on there you
can go in here and open a terminal
window and then in here once you're in
the terminal window this is where you're
going to start uh installing using pip
to install your different modules and
everything now we've already
pre-installed them so we don't need to
do that in here uh but if you don't have
them installed on your particular
environment you'll need to do that and
of course you don't need to use the
anaconda or the Jupiter you can use
whatever favorite python ID you like I'm
just a big fan of this cuz it keeps all
my stuff separate you can see on this
machine I have specifically installed
one for carass since we're going to be
working with carass under tensorflow
when we go back to home I've gone up
here to application and that's the
environment I've loaded on here and then
we'll click on the launch Jupiter
notebook now I've already in my Jupiter
notebook um have set up a lot of stuff
so that we're ready to go kind of like
uh Martha Stewarts in the old cooking
shows we want to make sure we have all
our tools for you so you're not waiting
for them to load and uh if we go up here
to where it says new you can see where
you can um create a new Python 3 that's
what we did here underneath the setup so
it already has all the modules installed
on it and I'm actually renamed this so
if you go under file you can rename it
we I'm calling it RNN stock and let's
just take a look at start diving into
the code let's get into the exciting
part now we've looked at the tool and of
course you might be using a different
tool which is fine uh let's start
putting that code in there and seeing
what those Imports and uploading
everything looks like now first half is
kind of boring when we hit the rum
button cuz we're going to be importing
numpy as NP that's uh uh the number
python which is your numpy array and the
mat plot Library CU we're going to do
some plotting at the end and our pandas
for our data set our pandas is PD and
when I hit run uh it really doesn't do
anything except for load those modules
just a quick note let me just do a quick
uh draw here oops shift alt there we go
you'll notice when we're doing this
setup if I was to divide this up oops
I'm going to actually um let's overlap
these here we go
uh this first part that we're going to
do
is our
data prep a lot of prepping
involved um in fact depending on what
your system is since we're using carass
I put an overlap here uh but you'll find
that almost maybe even half of the code
we do is all about the data prep and the
reason I overlap this with uh carass we
just put that down cuz that's what we're
working in uh is because caras has like
their own preset stuff so it's already
pre-built in which is really nice so
there's a couple Steps A lot of times
that are in the Kass setup uh we'll take
a look at that to see what comes up in
our code as we go through and look at
stock and then the last part is to
evaluate and if you're working with um
shareholders or uh you know classroom
whatever it is you're working with uh
the evaluate is the next biggest piece
um so the actual code here cross is a
little bit more but when you're working
with uh some of the other packages you
might have like three lines that might
be it all your stuff is in your
pre-processing in your data since carass
has is is Cutting Edge and you load the
individual layers you'll see that
there's a few more lines here and carass
is a little bit more robust and then you
spend a lot of times uh like I said with
the evaluate you want to have something
you present to everybody else to say hey
this is what I did this is what it looks
like so let's go through those steps
this is like a kind of just general
overview and let's just take a look and
see what the next set of code looks like
and in here we have a data set train and
it's going to be read using the PD or
pandas read CSV and it's the Google
stock pric train.csv and so under this
we have training set equals data set
train. iocation and we've kind of sorted
out part of that so what's going on here
let's just take a look at let's let's
look at the actual file and see what's
going on there now if we look at this uh
ignore all the extra files on this um I
already have a train and a test set
where it's sorted out this is important
to notice because a lot of times we do
that as part of the pre-processing of
the data we take 20% of the data out so
we can test it and then we train the
rest of it that's what we use to create
our neural network that way we can find
out how good it is uh but let's go ahead
and just take a look and see what that
looks like as far as the file itself and
I went ahead and just opened this up in
a basic word pad text editor just so we
can take a look at it certainly you can
open up an Excel or any other kind of
spreadsheet um and we note that this is
a comma separated variables we have a
date uh open high low close volume this
is the standard stuff that we import
into our stock one of the most basic set
of information you can look at in stock
it's all free to download um in this
case we downloaded it from uh Google
that's why we call it the Google stock
price um and it specifically is Google
this is the Google stock values from uh
as you can see here we started off at 13
20102 so when we look at this first
setup up here uh we have a data set
train equals pdor CSV and if you noticed
on the original frame um let me just go
back there they had it set to home
Ubuntu downloads Google stock price
train I went ahead and change that
because we're in the same file where I'm
running the code so I've saved this
particular python code and I don't need
to go through any special paths or have
the full path on there and then of
course we want to take out um certain
values in here and you're going to
notice that we're using um our data set
and we're now in pandas uh so pandas
basically it looks like a spreadsheet um
and in this case we're going to do
iocation which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
in the data and the second one is we're
only going to look at columns one and
two and if you remember here from our
data as we switch back on over column
colums we should always start with zero
which is the date and we're going to be
looking at open and high which would be
one and
two I'll just label that right there so
you can see now when you go back and do
this you certainly can extrapolate and
do this on all the columns um but for
the example let's just limit a little
bit here so that we can focus on just
some key aspects of
stock and then we'll go up here and run
the code and uh again I said the first
half is very boring whenever you hit the
Run button it doesn't do anything cuz
we're still just loading the data and
setting it up now that we've loaded our
data we want to go ahead and scale it we
want to do what they call feature
scaling and in here we're going to pull
it up from the SK learn or the SK kit
pre-processing import min max scaler and
when you look at this you got to
remember that um biases in our data we
want to get rid of that so if you have
something that's like a really high
value um let's just draw a quick graph
and I have something here like the maybe
the stock has a value One stock has a
value of 100 and another stock has a
value of
five um you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100's going to
be the Max and five is going to be the
Min and then everything else goes and
then we change this so we just squish it
down I like the word squish so it's
between one and 0er so 100 = 1 or 1
equal 100 and 0 equal 5 and you can just
multiply it's usually just a simple
multiplication we're using uh
multiplication so it's going to be uh
minus5 and then 100 divided or 95
divided by one so or whatever value is
is divided by
95 and uh once we've actually created
our scale we've telling it's going to be
from 0 to one we want to take our
training set and we're going to create a
training set scaled and we're going to
use use our scaler SC and we're going to
fit we're going to fit and transform the
training Set uh so we can now use the SC
this this particular object we'll use it
later on our testing set because
remember we have to also scale that when
we go to test our uh model and see how
it works and we'll go ahead and click on
the run again uh it's not going to have
any output yet because we're just
setting up all the
variables okay so we pasted the data in
here and we're going to create the data
structure with a 60 time steps and
output first note we're running 60 time
steps and that is where this value here
also comes in so the first thing we do
is we create our X train and Y train
variables we set them to an empty python
array very important to remember what
kind of array we're in what we're
working with and then we're going to
come in here we're going to go for I in
range 60 to 1258 there's our 60 60 time
steps and the reason we want to do this
is as we're adding the data in there
there's nothing below the 60 so if we're
going to use 60 time steps uh we have to
start at 60 because it includes
everything underneath of it otherwise
you'll get a pointer error and then
we're going to take our X train and
we're going to append training set
scaled this is a scaled value between
zero and one and then as I is equal to
60 this value is going to be um 60 minus
60 is zero so this actually is 0 to I so
it's going to be 0 to 60 1 to 61 let me
just circle this part right here
1 to 61 uh 2 to 62 and so on and so on
and if you remember I said 0 to 60
that's incorrect because it does not
count remember it starts at zero so this
is a count of 60 so it's actually 59
important to remember that as we're
looking at this and then the second part
of this that we're looking at so if you
remember correctly here we go we go from
uh 0 to 59 of I and then we have a comma
a zero right here and so finally we're
just going to look at the open value now
I know we did put it in there for 1 to
two um if you remember correctly it
doesn't count the second one so it's
just the open value we're looking at
just open um and then finally we have y
train. append training set I to zero and
if you remember correctly I2 or I comma
0 if you remember correctly this is 0 to
59 so there's 60 values in it uh so we
do I down here this is number 60 so
we're going to do this is we're creating
an array and we we have 0 to
59 and over here we have number 60 which
is going into the Y train it's being
appended on there and then this just
goes all the way up so this is down here
is a 0 to 59 and we'll call it 60 since
that's the value over here and it goes
all the way up to 12 58 that's where
this value here comes in that's the
length of the data we're loading so
we've loaded two arrays we loaded one
array that has uh which is filled with
arrays from 0 to 59 and we loaded one
array which is just the value and what
we're looking at you want to think about
this as a Time sequence uh here's my
open open open open open open what's the
next one in the series so we're looking
at the Google stock and each time it
opens we want to know what the next one
uh 0 through 59 what's 60 1 through 60
what's 61 2 through 62 what's 62 and so
on and so on going up and then once
we've loaded those in our for Loop we go
ahead and take XT train and Y train
equals np. array XT tr. NP array YT
train we're just converting this back
into a numpy array that way we can use
all the cool tools that we get with
numpy array including reshaping so if we
take a look and see what's going on here
we're going to take our X
train we're going to reshape it wow what
the heck does reshape mean uh that means
we have an array if you remember
correctly um so many numbers by 60
that's how wide it is and so we're when
you when you do xtrain do shape that
gets one of the shapes and you get um XT
train. shape of one gets the other shape
and we're just making sure the data is
formatted correctly and so you use this
to pull the fact that it's 60 by um in
this case where's that value 60 by
1199 1258 minus
60199 and we're making sure that that is
shaped correctly so the data is grouped
into uh
1199 by 60 different arrays and then the
one on the end just means at the end
because this when you're dealing with
shapes and numpy they look at this as
layers and so the in layer needs to be
one value that's like the leaf of a tree
where this is the branch and then it
branches out some more um and then you
get the Leaf np. reshape comes from and
using the existing shapes to form it
we'll go ahead and run this piece a code
again there's no real output and then
we'll import our different carass
modules that we need so from carass
Models we're going to import the
sequential model dealing with sequential
data we have our dense layers we have
actually three layers we're going to
bring in our DSE our lstm which is what
we're focusing on and our Dropout and
we'll discuss these three layers more in
just a moment but you do need the with
the lstm you do need the Dropout and
then the final layer will be the dents
but let's go ahead and run this and
that'll bring Port our mod modul and
you'll see we get an error on here and
if you read it closer it's not actually
an error it's a warning what does this
warning mean these things come up all
the time when you're working with such
Cutting Edge modules that are completely
being updated all the time we're not
going to worry too much about the
warning all it's saying is that the
h5py module which is part of carass is
going to be updated at some point and uh
if you're running new stuff on carass
and you start updating your carass
system you better make sure that your H5
Pi is updated too other otherwise you're
going to have an error later on and you
can actually just run an update on the
H5 Pi now if you wanted to not a big
deal we're not going to worry about that
today and I said we were going to jump
in and start looking at what those
layers mean I meant that and uh we're
going to start off with initializing the
RNN and then we'll start adding those
layers in and you'll see that we have
the lstm and then the Dropout lstm then
Dropout lstm then Dropout what the heck
is that doing so let's explore that
we'll start by initializing in the RNN
regressor equals sequential because
we're using the sequential model and
we'll run that and load that up and then
we're going to start adding our LSM
layer and some Dropout regularization
and right there should be the CU Dropout
regularization and if we go back here
and remember our exploding gradient well
that's what we're talking about the uh
Dropout drops out unnecessary data so
we're not just shifting huge amounts of
data through um the network so and so we
go in here let's just go ahead and add
this in I'll go ahead and run this and
we had three of them so let me go and
put all three of them in and then we can
go back over them there's the second one
and let's put one more in let's put that
in and we'll go and put two more in I
meant to put I said one more in but it's
actually two more in and then let's add
one more after that and as you can see
each time I run these they don't
actually have an output so let's take a
closer look and see what's going on here
so we're going to add our first lstm
layer in here we're going to have units
50 the units is the positive integer in
the dimensionality of the output space
this is what's going out into the next
layer so we might have 60 coming in but
we have 50 going out we have a return
sequence because it is a sequence data
so we want to keep that true and then
you have to tell it what shape it's in
well we already know the shape by just
going in here and looking at x train
shape so input shape equals the xtrain
shape of one comma one makes it really
easy you don't have to remember all the
numbers that put in 60 or whatever else
is in there you just let it tell the
regressor what model to use and so we
follow our STM with a Dropout layer now
understanding the Dropout layer is kind
of exciting because one of the things
that happens is we can overtrain our
Network that means that our neural
network will memorize such specific data
that it has trouble predicting anything
that's not in that specific realm to fix
for that each time we run through the
training mode we're going to take 02 or
20% of our neurons and just turn them
off so we're only going to train on the
other one and it's going to be random
that way each time we pass through this
we don't overtrain these nodes come back
in in the next training cycle we
randomly pick a different 20 and finally
they see a big difference as we go from
the first to the second and third and
fourth the first thing is we don't have
to input the shape because the shape's
already the output units is 50 here this
Auto The Next Step automatically knows
this layer is putting out 50 and because
it's the next layer it automatically
sets that and says oh 50 is coming out
from our last layer is coming coming out
you know goes into the regressor and of
course we have our Dropout and that's
what's coming into this one and so on
and so on and so the next three layers
we don't have to let it know what the
shape is it automatically understands
that and we're going to keep the units
the same we're still going to do 50
units it's still a sequence coming
through 50 units and a sequence now the
next piece of code is what brings it all
together let's go ahead and take a look
at that and we come in here we put the
output layer the dense layer and if you
remember up here we had the three layers
we had uh LST Dropout and d uh D just
says we're going to bring this all down
into one output instead of putting out a
sequence we just know it want to know
the answer at this point and let's go
ahead and run that and so in here you
notice all we're doing is setting things
up one step at a time so far we've
brought in our uh way up here we brought
in our data we brought in our different
modules we formatted the data for
training it we've set it up you know we
have our y x train and our y train we
have our source of data and the answers
we're we know so far that we're going to
put in there we've reshaped that we've
come in and built our carass we've
imported our different layers and we
have in here if you look we have what uh
five total layers now carass is a little
different than a lot of other systems
because a lot of other systems put this
all in one line and do it automatic but
they don't give you the options of how
those layers interface and they don't
give you the options of how the data
comes in carass is Cutting Edge for this
reason so even though there's a lot of
extra steps in building the model this
has a huge impact on the output and what
we can do with this these new models
from carass so we brought in our dense
we have our full model put together a
regressor so we need to go ahead and
compile it and then we're going to go
ahead and fit the data we're going to
compile the pieces so they all come
together and then we're going to run our
training data on there and actually
recreate our regressor so it's ready to
be used so let's go ahead and compile
that and I can go ahe and run that and
uh if you've been looking at any of our
other tutorials on neural networks
you'll see we're going to use the
optimizer atom atom is optimized for Big
Data there's a couple other optimizers
out there uh beyond the scope of this
tutorial but certainly Adam will work
pretty good for this and loss equals
mean squared value so when we're
training it this is what we want to base
the loss on how bad is our error well
we're going to use the mean squared
value for our error and the atom
Optimizer for its differential equations
you don't have to know the math behind
them but certainly it helps to know what
they're doing and where they fit into
the bigger models and then finally we're
going to do our fit fitting the RN into
to the training set we have the
regressor do fit XT Trin y train epics
and batch size so we know where this is
this is our data coming in for the X
train our y train is the answer we're
looking for of our data our sequential
input epex is how many times we're going
to go over the whole data set we created
a whole data set of XT train so this is
each each of those rows which includes a
Time sequence of 60 and badge size
another one of those things where carass
really really shines is if you were
pulling this save from a large file
instead of trying to load it all into
RAM it can now pick smaller batches up
and load those indirectly we're not
worried about pulling them off a file
today this isn't big enough to uh cause
the computer too much of a problem to
run not too straining on the resources
but as we run this you can imagine what
would happen if I was doing a lot more
than just one column in one set of stock
in this case Google stock imagine if I
was doing this across all the stocks and
I had instead of just the open I had
open close high low and you can actually
find yourself with about 13 different
variables times 60 cuz it's a Time
sequence suddenly you find yourself with
a gig of memory you're loading into your
RAM which will just completely you know
if it's just if you're not on multiple
computers or cluster you're going to
start running into resource problems but
for this we don't have to worry about
that so let's go ahead and run this and
this will actually take a little bit on
my computer CU it's an older laptop and
give it a second to kick in there there
we go all right so we have Epic so this
is going to tell me it's running the
first run through all the data and as
it's going through it's batching them in
32 pieces so 32 lines each time and
there's 1198 I think I said $199 earlier
but it's 1198 I was off by one and each
one of these is 13 seconds so you can
imagine this is roughly 20 to 30 minutes
runtime on this computer like I said
it's an older laptop running at uh 0.9
GHz on a dual processor and that's fine
what we'll do is I'll go ahead and stop
go get a drink of coffee and come back
and let's see what happens at the end
and where this takes us and like any
good cooking show I've kind of gotten my
latte I also had some other stuff
running in the background so you'll see
these numbers jumped up to like 19
seconds 15 seconds which you can scroll
through and you can see we've run it
through 100 steps or 100 epics so the
question is what does all this mean one
of the first things you'll notice is
that our loss can is over here kind of
stopped at 0.0014 but you can see it
kind of goes down until we hit about 014
three times in a row so we guessed our
epic pretty close close since our loss
has remain the same on there so to find
out what we're looking at we're going to
go ahead and load up our test data the
test data that we didn't process yet and
uh real stock price data set test
iocation this is the same thing we did
when we prepped the data in the first
place so let's go ahead and go through
this code and we can see we've labeled
it part three making the predictions and
visualizing the results so the first
thing to we need to do is go ahead and
read the data in from our test CSV and
you see I've changed the path on it for
my computer and uh then we'll call it
the real stock price and again we're
doing just the one column here and the
values from ication so it's all the rows
and just the values from these that one
location that's the open Stock open
let's go ahead and run that so that's
loaded in there and then let's go ahead
and uh create we have our inputs we're
going to create inputs here and this
should all look familiar this is the
same thing we did before we're going to
take our data set total we're going to
do a little Panda concat from the data
sayate train now remember the end of the
data set train is part of the data going
in and let's just visualize that just a
little bit here's our train data let me
just put TR for train and it went up to
this value here but each one of these
values generated a bunch of columns it
was 60 across and this value here equals
this one and this value here equals this
one and this value here equals this one
and so we need these top 60 to go into
our new data so to find out we're
looking at we're going to go ahead and
load up our test data the test data that
we didn't process yet and real stock
price data set test iocation this is the
same thing we did when we prepped the
data in the first place so let's go
ahead and go through this code and we
can see we've labeled it part three
making the predictions and visualizing
the results so the first thing we need
to do is go ahead and read the data in
from our test CSV and you see I've
changed the path on it for my computer
and uh then we'll call it the real stock
price and again we're doing just the one
column here and the values from ication
so it's all the rows and just the values
from these that one location that's the
open Stock open and let's go ahead and
run that so that's loaded in there and
then let's go ahead and uh create we
have our inputs we're going to create
inputs here and this should all look
familiar this is the same thing we did
before we're going to take our data set
total we're going to do a little Panda
concat from the data Sate train now
remember the end of the data set train
is part of the data going in let's just
visualize that just a little bit here's
our train data let me just put TR for
train and it went up to this value here
but each one of these values generated a
bunch of columns it was 60 across and
this value here equals this one and this
value here equals this one and this
value here equals this one and so we
need these top 60 to go into our new
data cuz that's part of the next data or
it's actually the top 59 so that's what
this first setup is over here is we're
going in we're doing the real stock
price and we're going to just take the
data set test and we're going to load
that in and then the real stock price is
our data test. test location so we're
just looking at that first uh column the
open price and then our data set total
we're going to take pandas and we're
going to concat and we're going to take
our data set train for the open and our
data set test open and this is one way
you can reference these columns we've
referenced them a couple different ways
we've referenced them up here with the
one two but we know it's labeled as a
panda set as open so Pand is great that
way lots of Versatility there and we'll
go ahead and go back up here and run
this there we go and uh you'll notice
this is the same as what we did before
we have our open data set we pended our
two different or concatenated our two
data sets together we have our inputs
equals data set total length data set
total minus length of data set minus
test minus 60 values so we're going to
run this over all of them and you'll see
why this works because normally when
you're running your test set versus your
training set you run them completely
separate but when we graph this you
you'll see that we're just going to be
we'll be looking at the part that uh we
didn't train it with to see how well it
graphs and we have our inputs equals
inputs do reshapes or reshaping like we
did before we're Transforming Our inputs
so if you remember from the transform
between zero and one and finally want to
go ahead and take our X test and we're
going to create that X test and for I in
range 60 to 80 so here's our X test and
we're appending our inputs I to 60 which
remember is 0 to 59 and I comma 0 on the
other side so that's just the First
Column which is our open column and uh
once again we take our X test we convert
it to a numpy array we do the same
reshape we did before and uh then we get
down to the final two lines and here we
have something new right here on these
last two lines let me just highlight
those or or mark them predicted stock
price equals regressor do predicts X
test so we're predicting all the stock
including both the training and the
testing model here and then we want to
take this prediction and we want to
inverse the transform so remember we put
them between 0o and one well that's not
going to mean very much to me to look at
a at a float number between zero and one
I want the dollar amounts I want to know
what the cash value is and we'll go
ahead and run this and you'll see it
runs much quicker than the training
that's what's so wonderful about these
neural networks once you put them
together takes just a second to run the
same neural network that took us what a
half hour to train ahead and plot the
data we're going to plot what we think
it's going to be and we're going to plot
it against the real data what what the
Google stock actually did so let's go
ahead and take a look at that in code
and let's uh is cod up so we have our
PLT that's our uh oh if you remember
from the very beginning let me just go
back up to the top we have our mat plot
library. pyplot as PLT that's where that
comes in and we come down here we're
going to plot let me get my drawing
thing out again we're going to go ahead
and PLT is basically kind of like an
object it's one of the things that
always threw me when I'm doing graphs in
Python because I always think you have
to create an object and then it loads
that class in there well in this case
PLT is like a canvas you're putting
stuff on so if you've done HTML 5 you'll
have the canvas object this is the
canvas so we're going to plot the real
stock price that's what it actually is
and we're going to give that color red
so it's going to be in bright red we're
going to label it real Google stock
price and then we're going to do our
predicted stock and we're going to do it
in blue and it's going to be labeled
predicted and we'll give it a title
because it's always nice to give a title
to your uh graph especially if you're
going to present this to somebody you
know to your shareholders in the office
and uh the label is going to be time
because it's a Time series and we didn't
actually put the actual date and times
on here but that's fine we just know
they're incremented by time and then of
course the Y label is the actual stock
price pt. Legend tells us to build the
legend on here so that the color red and
and real Google stock price show up on
there and then the plot shows us that
actual graph so let's go ahead and run
this and see what that looks like and
you can see here we have a nice graph
and let's talk just a little bit about
this graph before we wrap it up here's
here's our Legend I was telling you
about that's why we have the legend to
showed the prices we have our title and
everything and you'll notice on the
bottom we have a Time sequence we didn't
put the actual time in here now we could
have we could have gone ahead and um
plotted the X since we know what the the
dates are and plotted this to dates but
we also know this only the last piece of
data that we're looking at so last piece
of data which ends somewhere probably
around here on the graph I think it's
like about 20% of the data probably less
than that we have the Google price PR
and the Google price has this little up
jump and then down and you'll see that
the actual Google instead of a a turn
down here just didn't go up as high and
didn't low go uh down so our prediction
has the same pattern but the overall
value is pretty far off as far as um
stock but then again we're only looking
at one column we're only looking at the
open price we're not looking at how many
volumes were traded like I was pointing
out earlier we talk about stock just
right off the bat there's six columns
there's open High low close volume then
there's WEA uh I mean volume shares then
there's the adjusted open adjusted High
adjusted low adjusted close they have a
special formula to predict exactly what
it would really be worth based on the
value of the stock and then from there
there's all kinds of other stuff you can
put in here so we're only looking at one
small aspect the opening price of the
stock and as you can see here we did a
pretty good job this curve follows the
curve pretty well it has like a you know
little jumps on it bends they don't qu
quite match up so this Bend here does
not quite match up with that bend there
but it's pretty darn close we have the
basic shape of it and the prediction
isn't too far off and you can imagine
that as we add more data in and look at
different aspects in the specific domain
of stock we should be able to get a
better representation each time we drill
in deeper of course this took a half
hour for my program my computer to train
so you can imagine that if I was running
it across all those different variables
might take a little bit longer to train
the data not so good for doing a quick
tutorial like this so we're going to
dive right into what is carass we'll
also go all the way through this into a
couple of tutorials because that's where
you really learn a lot is when you roll
up your sleeves so we talk about what is
carass carass is a highlevel deep
learning API written in Python for easy
impl implementation of neural networks
uses deep learning Frameworks such as
tensor flow pie torch Etc is backend to
make computation faster
and this is really nice because as a
programmer there is so much stuff out
there and it's evolving so fast it can
get confusing and having some kind of
high level order in there we can
actually view it and easily program
these different neural networks uh is
really powerful it's really powerful to
to um uh have something out really quick
and also be able to start testing your
models and seeing where you're going so
cross works by using complex deep
learning Frameworks such as tensor FL
flow P torch um ml play Etc as a backend
for fast computation while providing a
userfriendly and easy to learn frontend
and you can see here we have the cross
API uh specifications and under that
you'd have like TF carass for tensor
flow thano coros and so on and then you
have your tensorflow workflow that this
is all sitting on top
of and this is like I said it organizes
everything the heavy lifting is still
done by tensor flow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to um dig as deeply into the heavy
end stuff while still having a very
robust package you can get up and
running rather quickly and it doesn't
distract from the processing time
because all the heavy lifting is done by
packages like tensor flow this is the
organization on top of it so the working
principle of
carass uh the working principle of
carass is carass uses computational
graphs to express and evaluate
mathematical
Expressions you can see here we put them
in blue they have the expression um
expressing complex problems as a
combination of simple mathematical
operators uh where we have like the
percentage or in this case in Python
that's usually your uh left your um
remainder or multiplication uh you might
have the operator of x uh to the power
of3 and it uses useful for calculating
derivatives by using uh back propagation
so if we're doing with neural networks
when we send the error back up to figure
out how to change it uh this makes it
really easy to do that without really
having not banging your head and having
to handr write everything it's easier to
implement distributed computation and
for solving complex problems uh specify
input and outputs and make sure all
nodes are
connected and so this is really nice as
you come in through is that um as your
layers are going in there you can get
some very complicated uh different
setups nowadays which we'll look at in
just a second and this just makes it
really easy to start spinning this stuff
up and trying out the different models
so when we look at caros models uh caros
model we have a sequential model
sequential model is a linear stack of
layers where the previous layer leads
into the next
layer and this if you've done anything
else even like the sklearn with their
neural networks and propagation and any
of these setups this should look
familiar you should have your input
layer it goes into your layer one layer
two and then to the output layer and
it's useful for simple classifier
decoder models and you can see down here
we have the model equals a coros
sequential this is the actual code you
can see how easy it is uh we have a
layer that's dense your layer one has an
activation now they're using the ru in
this particular example and then you
have your name layer one layer Den Ru
name Layer Two and so forth uh and they
just feed right into each other so it's
really easy just to stack them as you
can see here and it automatically takes
care of everything else for you and then
there's a functional model and this is
really where things are at this is new
make sure you update your carass or
you'll find yourself running this um
doing the functional model you'll run
into an error code because this is a
fairly new release and he uses
multi-input and multi-output model the
complex model which Forks into two or
more branches and you can see here we
have our image input equals your carass
input shape equals 32x 32x 3 you have
your dense layers dense 64 activation
Rao this should look similar to what you
already saw before uh but if you look at
the graph on the right it's going to be
a lot easier to see what's going on you
have two different
inputs uh and one way you could think of
this is maybe one of those is a small
image and one of those is a full-sized
image and that feedback goes into you
might feed both both of them into one
Noe because it's looking for one thing
and then only into one Noe for the other
one and so you can start to get kind of
an idea that there's a lot of use for
this kind of split and this kind of
setup uh where we have multiple
information coming in but the
information's very different even though
it overlaps and you don't want it to
send it through the same neural network
um and they're finding that this trains
faster and is also has a better result
depending on how you split the data up
and and how you Fork the models coming
down
and so in here we do have the two
complex uh models coming in uh we have
our image inputs which is a 32x 32 by
three your three channels or four if
you're having an alpha channel uh you
have your dense your layer's dense is 64
activation using the railo very common
um x equals dense inputs X layers dense
x64 activation equals railu x outputs
equals layers dense 10 X model equals
coros model inputs equals inputs outputs
equals outputs name equals n
model uh so we add a little name on
there and again this is this kind of
split here this is setting us up to um
have the input go into different areas
so if you're already looking at corus
you probably already have this answer
what are neural networks uh but it's
always good to get on the same page and
for those people who don't fully
understand neural networks to dive into
them a little bit or do a quick overview
neural networks are deep learning
algorithms modeled after the human brain
they use multiple neurons which are
mathematical operations to break down
and solve complex maical
problems and so just like the neuron one
neuron fires in and it fires out to all
these other neurons or nodes as we call
them and eventually they all come down
to your output layer and you can see
here we have the really standard graph
input layer a hidden layer and an output
layer one of the biggest parts of any
data processing is your data
pre-processing uh so we always have to
touch base on that with a neural network
like many of these models they're kind
of uh when you first start using them
they're like a black box you put your
data in you train it and you test it and
see how good it was and you have to
pre-process that data because bad data
in is uh bad outputs so in data
pre-processing we will create our own
data examples set with carass the data
consists of a clinical trial conducted
on 2100 patients ranging from ages 13 to
100 with a the patients under 65 and the
other half over 65 years of age we want
to find the possibility of a patient
experiencing side effects due to their
age and you can think of this in today's
world with uh coid uh what's going to
happen on there and we're going to go
ahead and do an example of that in our
uh life Hands-On like I said most of
this you really need to have handson to
understand and so let's go ahead and
bring up our anaconda and uh open that
up and open up a Jupiter notebook for
doing the python code in now if you're
not familiar with those you can use
pretty much any of your uh setups I just
like those for doing demos and uh
showing people especially shareholders
it really helps it's a nice visual so
let me go and flip over to our anaconda
and the Anaconda has a lot of cool to
tools they just added datal lore and IBM
Watson Studio clad into the Anaconda
framework but we'll be in the Jupiter
lab or Jupiter notebook um I'm going to
do Jupiter notebook for this because I
use the lab for like large projects with
multiple pieces because it has multiple
tabs where the notebook will work fine
for what we're doing and this opens up
in our browser window because that's how
Jupiter notebook sorry Jupiter notebook
is set to run and we'll go under new
create a new Python 3 and uh it creates
an Untitled python we'll go ahead and
give this a title and we'll just call
this uh
carass
tutorial and let's change that to
Capital there we go we go and just
rename that and the first thing we want
to go ahead and do is uh get some
pre-processing tools involved and so we
need to go ahead and import some stuff
for that like our numpy do some random
number
Generation Um I mentioned sklearn or
your s kit if you're installing SK learn
the SK learn stuff it's a site kit you
want to look
up that should be a tool of anybody who
is um doing data science if if you're
not if you're not familiar with the
sklearn
toolkit it's huge uh but there's so many
things in there that we always go back
to and we want to go ahead and create
some train labels and train samples uh
for training our
data and then just a note of what we're
we're actually doing in here uh let me
go ahead and change this this is kind of
a fun thing you can do we can change the
code to
markdown and then markdown code is nice
for doing examples once you've already
built this uh our example data we're
going to do
experimental there we go experimental
drug was tested on 2100 individuals
between 13 to 100 years of age half the
participants are under 65 and 95% of
participants are under 65 experience no
side effects well 95% of participants
over 65 um experience side effects so
that's kind of where we're starting at
um and this is just a real quick example
because we're going to do another one
with a little bit more uh complicated
information uh and so we want to go
ahead and
generate our setup uh so we want to do
for I in range and we want to go ahead
and create if you look here we have
random integers train the labels a pen
so we're just creating some random
data uh let me go ahead and just run
that and so once we've created our
random data and if you if I mean you can
certainly ask for a copy of the code
from Simply learn they'll send you a
copy of this or you can zoom in on the
video and see how we went ahead and did
our train samples a pin um and we're
just using this I do this kind of stuff
all the time I was running a thing on uh
that had to do with errors following a
bell-shaped curve on uh a standard
distribution error and so what do I do I
generate the data on a standard
distribution error to see what it looks
like and how my code processes it since
that was the Baseline I was looking for
in this we're just doing uh uh
generating random data for our setup on
here and uh we could actually go in uh
print some of the data up let's just do
this
print um we'll do
train
samples and we'll just do the first um
five pieces of data in there to see what
that looks like and you can see the
first five pieces of data in our train
samples is 49 85 41 68 19 just random
numbers generated in there that's all
that is uh and we generated
significantly more than that um let's
see 50 up here 1,000 yeah so there's
1,000 here 1,000 numbers we generated
and we could also if we wanted to find
that out we could do a quick uh print
the link of
it and so or you could do a shape kind
of thing and if you're using
nump although the link for this is just
fine and there we go it's actually 2100
like we said in the demo setup in
there and then we want to go ahead and
take our labels oh that was our train
labels we also did samples didn't
we uh so we could also print do the same
thing
say oh
labels uh let's change this
to
labels and
[Music]
labels and run that just a double check
and sure enough we have 2100 and they're
labeled one one0 one0 I guess that's if
they have symptoms or not one symptoms
uh Zer none so we want to go ahead and
take our train labels and we'll convert
it into a numpy array and the same thing
with our samples and let's go ahead and
run that and we also Shuffle uh this is
just a neat feature you can do in uh
numpy right here put my drawing thing on
which I didn't have on earlier um I can
take the data and I can Shuffle it uh so
we have our so it's it just randomizes
it that's all that's doing um we've
already randomized it so it's kind of an
Overkill it's not really
necessary but if you're doing uh a
larger package where the data is coming
in and a lot of times it's organized
somehow and you want to randomize it
just to make sure that that you know the
input doesn't follow a certain pattern
uh that might create a bias in your
model and we go ahead and create a
scaler uh the scaler range uh minimum
Max scaler feature range 0 to
one uh then we go ahead and scale the uh
scaled train samples so we're going to
go ahead and fit and transform the data
uh so it's nice and scaled and that is
the age uh so you can see up here we
have 49 85 41 we're just moving that so
it's going to be uh between zero and one
and so this is true with any of your
neural networks you really want to
convert the data uh to zero and one
otherwise you create a bias uh so if you
have like a 100 creates a bias
versus the math behind it gets really
complicated um if you actually start
multiplying stuff there a lot of
multiplication addition going on in
there that higher end value will
eventually multiply down and it will
have a huge bias as to how the model
fits it and then it will not fit as well
and then one of the fun things we can do
in Jupiter notebook is that if you have
a variable and you're not doing anything
with it it's the last one on the line it
will automatically
print um and we're just going to look at
the first five samples on here and so
it's going to print the first five
samples and you can see here we go uh 9
195 791 so everything's between zero and
one and that just shows us that we
scaled it properly and it looks good uh
it really helps a lot to do these kind
of print UPS halfway through uh you
never know what's going to go on
there I don't know how many times I've
gotten down and found out that the data
sent to me that I thought was scaled was
not and then I have to go back and track
it down and figure it out on
there uh so let's go ahead and create
our artificial neural
network and for doing that this is where
we start diving into tensor flow and
carass uh tensor
flow if you don't know the history of
tensor
flow it helps to uh jump into we'll just
use
Wikipedia careful don't quote Wikipedia
on these things because you get in
trouble
uh but it's a good place to start uh
back in 2011 Google brain built
disbelief as a proprietary machine
learning setup tensorflow became the
open source for it uh so tensorflow was
a Google product and then it became uh
open sourced and now it's just become
probably one of the defa factos when it
comes for neural networks as far as
where we're at uh so when you see the
tensorflow
setup it it's it's got like a huge
following there are some other setups
like um the S kit under the sklearn has
our own little neural network uh but the
tensor flow is the most robust one out
there right now and carass sitting on
top of it makes it a very powerful tool
so we can leverage both the carass uh
easiness in which we can build a
sequential setup on top of tensor flow
and so in here we're going to go ahead
and do our input of tensor flow uh and
then we have the rest of this all carass
here from number two down uh we're going
to import from tensorflow the carass uh
connection and then you have your
tensorflow cross models import
sequential it's a specific kind of model
we'll look at that in just a second if
you remember from the files that means
it goes from one layer to the next layer
to the next layer there's no funky
splits or anything like
that uh and then we have from tensorflow
Cross layers we're going to import our
activation and our dense layer
and we have our Optimizer atom um this
is a big thing to be aware of how you
optimize uh your data when you first do
it Adam's as good as any atom is usually
uh there's a number of Optimizer out
there there's about uh there's a couple
main ons but atom is usually assigned to
bigger data uh it works fine usually the
lower data does it just fine but adom is
probably the mostly used but there are
some more out there inde depending on
what you're doing with your layers your
different layers might have different
activations on them and then finally
down here you'll see um our setup where
we want to go ahead and use the
metrics and we're going to use the
tensorflow cross metrics um for
categorical cross entropy uh so we can
see how everything performs when we're
done that's all that is um a lot of
times you'll see us go back and forth
between tensor flow and then pyit has a
lot of really good metrics also for for
measuring these things um again it's the
end of the you know at the end of the
story how good does your model do and
we'll go ahead and load all that and
then comes the fun part um I actually
like to spend hours messing with these
things and uh four lines of code you're
like ah you're G to spend hours on four
lines of code um no we don't spend hours
on four lines of code that's not what
we're talking about when I say spend
hours on four lines of code uh what we
have here I'm going to explain that in
just a second
we have a model and it's a sequential
model if you remember correctly we
mentioned the sequential up here where
it goes from one layer to the next and
our first layer is going to be our
input it's going to be uh what they call
DSE which is um usually it's just D and
then you have your input and your
activation um how many units are coming
in we have 16 uh what's the shape What's
the activation and this is where it gets
interesting uh because we have in here
uh
railu on two of these and soft Max
activation on one of these there are so
many different options for what these
mean um and how they function how does
the ru how does the softmax
function and they do a lot of different
things um we're not going to go into the
activations in here that is what really
you spend hours doing is looking at the
these different
activations um and just some of it is
just U um almost like you're playing
with it like an artist you start getting
a fill for like a uh inverse tangent
activation or the tan
activation takes up a huge processing
amount uh so you don't see it a lot yet
it comes up with a better solution
especially when you're doing uh when
you're analyzing Word documents and
you're tokenizing the words
and so you'll see this shift from one to
the other because you're both trying to
build a better model and if you're
working on a huge data set um it'll
crash the system it'll just take too
long to process um and then you see
things like soft Max uh soft Max
generates an interesting um
setup where a lot of these when you talk
about Ru it oops let me do this Ru there
we go Ru has um a setup where if it's
less than zero it's zero and then it
goes up um and then you might have what
they call lazy uh setup where it has a
slight negative to it so that the errors
can translate better same thing with
softmax it has a slight laziness to it
so that errors translate better all
these little
details make a huge different on your
model um so one of the really cool
things about data science that I like is
you build your uh they call you build to
fail and it's an interesting uh design
setup oops I forgot the end of my code
here the concept to build a fail is you
want the model as a whole to work so you
can test your model
out so you can do uh you can get to the
end and you can do your let's see where
was it overshot down here you can test
your test out the the quality of your
setup on there
and see where did I do my tensor flow oh
here we go I did it was right above me
here we go we start doing your cross iny
and stuff like that is you need a full
functional set of code so that when you
run
it you can then test your model out and
say hey it's either this model works
better than this model and this is why
um and then you can start swapping in
these models and so when I say I spend a
huge amount of time on pre-processing
data is probably 80% of your programming
time um well between those two it's like
8020 you'll spend a lot of time on the
models once you get the model down once
you get the whole code and the flow down
uh set depending on your data your
models get more and more robust as you
start experimenting with different
inputs different data streams and all
kinds of things and we can do a simple
model summary
here uh here's our sequential here's our
layer output a
parameter this is one of the nice things
about caros is you just you can see
right here here's our sequential one
model boom boom boom boom everything's
set and clear and easy to read so once
we have our model built uh the next
thing we're going to want to do is we're
want to go ahead and uh train that
model and so the next step is of course
model
training and when we come in here this a
lot of times is just paired with the
model because it's so straightforward
it's nice to print out the model setup
so you can have a
tracking but here's our model uh the
keyword in carass is
compile Optimizer atom learning rate
another term right there that we're just
skipping right over that really becomes
the meat of uh the setup is your
learning rate uh so whoops I forgot that
I had an arrow but I'll just underline
it a lot of times the learning rate set
to 0.0 uh set to 0 01 uh depending on
what you're doing this learning rate um
can overfit and underfit uh so you'd
want to look up I know we have a number
of tutorials out on overfitting and
underfitting that are really worth
reading once you get to that point in
understanding and we have our loss um
sparse categorical cross entropy so this
is going to tell carass how far to go
until it stops and then we're looking
for metrics of accuracy so we'll go
ahead and run that and now that we've
compiled our model we want to go ahead
and um run it fit it so here's our model
fit um we have our scaled train
samples our train labels our validation
split um in this case we're going to use
10% of the data for
validation uh batch size another number
you kind of play with not a huge
difference as far as how it works but it
does affect how long it takes takes to
run and it can also affect the bias a
little bit uh most of the time though
batch size is between 10 to 100 um
depending on just how much data you're
processing in there we want to go ahead
and Shuffle it uh we're going to go
through 30 epics and uh put a verbose of
two let me just go and run this and you
can see right here here's our epic
here's our training um here's our loss
now if you remember correctly up here we
set the loss let's see where was it um
compiled our data
there we go loss uh so it's looking at
the sparse categorical cross entropy
this tells us that as it goes how how
how much um how how much does the um
error go down uh is the best way to look
at that and you can see here the lower
the number the better it just keeps
going down and vice versa accuracy we
want let's see where's my
accuracy value accuracy at the end uh
and you can see 619 69 74 it's going up
we want the accuracy would be ideal if I
made it all the way to one but we also
the loss is more important because it's
a balance um you can have 100% accuracy
and your model doesn't work because it's
overfitted uh again you w't look up
overfitting and underfitting
models and we went ahead and went
through uh 30 epics it's always fun to
kind of watch watch your code going um
to be honest I usually uh um the first
time I run it I'm like oh that's cool I
get to see what it does and after the
second time of running it I'm like i'
like to just not see that and you can
repress those of course in your code uh
repress the warnings in the
printing and so the next step is going
to be building a test set and predicting
it now uh so here we go we want to go
ahead and build our test set and we have
uh just like we did our training
set a lot of times you just split your
your initial set up uh but we'll go
ahead and do a separate set on here and
this is just what we did above uh
there's no difference as far as
um the randomness that we're using to
build this set on here uh the only
difference is
that we already uh did our scaler up
here well it doesn't matter because the
the data is going to be across the same
thing but this should just be just
transformed down here instead of fit
transform uh because you don't want to
refit your data um on your testing
data there we go now we're just
transforming it because you never want
to transform the test data um easy
mistake to make especially on an example
like this where we're not
doing um you know we're randomizing the
data anyway so it doesn't matter too
much because we're not expecting
something
weird
and then we went ahead and do our
predictions the whole reason we built
the model is we take our model we
predict and we're going to do here's our
xcal data batch size 10 verbose and now
we have our predictions in here and we
could go ahead and do a um oh we'll
print
predictions and then I guess I could
just put down predictions and five so we
can look at the first five of the
predictions and what we have here is we
have our
age and uh the prediction on this age
versus on what what we think it's going
to be what what we think is going to are
going to have uh symptoms or not and the
first thing we notice is that's hard to
read because we really want a yes no
answer uh so we'll go ahead and just uh
round off the
predictions using the argmax um the
numpy argmax uh for prediction so it
just goes to a zer1 and if you remember
this is a Jupiter notebook so I don't
have to put the print I can just put in
uh rounded predictions and we'll just do
the first five and you can see here 0
one 0 0 0 so that's what the predictions
are that we have coming out of this um
is no symptoms symptoms no symptoms
symptoms no symptoms and just as uh we
were talking about at the beginning we
want to go ahead and um take a look at
this there we go confusion matrixes for
accuracy check um most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the pyit um the SK learn metrics uh pyit
being where that comes from import
confusion Matrix uh some iteration tools
and of course a nice matap plot library
that makes a big difference so it's
always nice to
um have nice graph to look at um picture
is worth a thousand
words um and then we'll go ahead call it
CM for confusion Matrix y true equals
test labels y predict rounded
predictions and we'll go ahead and load
in our
cm and I'm not going to spend too much
time on the plotting um going over the
different plotting
code um you can spend uh like whole we
have whole tutorial on how to do your
different plotting on there uh but we do
have here is we're going to do a plot
confusion Matrix there's our CM our
classes normalized false title confusion
Matrix cmap is going to be in
blues and you can see here we have uh to
the nearest cmap titles all the
different pieces whether you put tick
marks or not the marks the classes a
color bar um so a lot of different
information on here as far as how we're
doing the printing of the of the
confusion Matrix you can also just dump
the confusion Matrix um into a caborn
and real quick get an output it's worth
knowing how to do all this uh when
you're doing a presentation to the
shareholders you don't want to do this
on the Fly you want to take the time to
make it look really nice uh like our
guys in the back did and uh let's go
ahead and do this forgot to put together
our CM plot labels we'll go and run
that and then we'll go ahead and call
the little the
definition for our
mapping and you can see here plot
confusion Matrix that's our the the
little script we just wrote and we're
going to dump our data into it um so our
confusion Matrix our classes um title
confusion Matrix and let's just go ahead
and run
that and you can see here we have our
basic setup uh no side effects
195 had side effects uh 200 no side
effects that had side effects so we
predicted the 10 of them who actually
had side effects and that's pretty good
I mean I I don't know about you but you
know that's 5% error on this and this is
because there's 200 here that's where I
get 5% is uh divide these both by by two
and you get five out of a 100 uh you can
do the same kind of math up here not as
quick on the flight it's 15 and 95 not
an easily rounded number but you can see
here where they have 15 people who
predicted to have no uh with the no side
effects but had side effects kind of
setup on there and these confusion
Matrix are so important at the end of
the day this is really where where you
show uh whatever you're working on comes
up and you can actually show them hey
this is how good we are or not how
messed up it
is so this was a uh I spent a lot of
time on some of the parts uh but you can
see here is really simple uh we did the
random generation of data but when we
actually built the model coming up here
uh here's our model
summary and we just have the layers on
here that we built with our model on
this and then we went ahead and trained
it and ran the prediction now we can get
a lot more complicated uh let me flip
back on over here because we're going to
do another uh demo so that was our basic
introduction to it we talked about the
uh oops here we go okay so implementing
a neural network with carass after
creating our sample and labels we need
to create our coros neural network model
we will be working with a sequential
model which has three layers and this is
what we did we had our input layer our
hidden layers and our output layers and
you can see the input layer uh coming in
uh was the age Factor we had our hidden
layer and then we had the output are you
going to have symptoms or not so we're
going to go ahead and go with something
a little bit more complicated um
training our model is a two-step process
we first compile our model and then we
train it in our training data Set uh so
we have compiling compiling converts the
code into a form of understandable by
Machine we use the atom in the last
example a gradient descent algorithm to
optimize a model and then we trained our
model which means it let it uh learn on
training data uh and I actually had a
little backwards there but this is what
we just did is we if you remember from
our code we just had o let me go back
here
um here's our model that we created
summarized uh we come down here and we
compile it so it tells it hey we're
ready to build this model and use it uh
and then we train it and this is the
part where we go ahead and fit our model
and and put that information in here and
it goes through the training on there
and of course we scaled the data which
was really important to do and then you
saw we did the creating a confusion
Matrix with carass um as we are
performing classifications on our data
we need a confusion Matrix to check the
results a confusion Matrix breaks down
the various
misclassifications as well as correct
classifications to get the
accuracy um and so you can see here this
is what we did with the true positive
false positive true negative false
negative and that is what we went over
let me just scroll down
here on the end when we printed it out
and you can see we have a nice print out
of our confusion Matrix uh with the true
positive false positive false negative
true negative and so the blue ones uh we
want those to be the biggest numbers
because those are the better side and
then uh we have our false predictions on
here uh as far as this one so it had no
side effects but we predicted let's see
no side effects predicting side effects
and vice versa if getting your learning
started is half the battle what if you
could do that for free visit scaleup by
simply learn click on the link in the
description to know more now uh saving a
loading model with carass we're going to
dive into a more complicated
demo um and you're going to say oh that
was a lot of complication before well if
you broke it down we randomized some
data we created the um carass setup we
compiled it we trained it we predicted
and we ran our
Matrix uh so we're going to dive into
something a lot a little bit more fun is
we're going to do a face mask detection
with carass uh so we're going to build a
carass model to check if a person is
wearing a mask or not in real time and
this might be important if you're at the
front of a store this is something today
which is um might be very useful as far
as some of our you know making sure
people are
safe uh and so we're going to look at
mask and no mask and let's start with a
little bit on the
data and so in my data I have with a
mask you can see they just have a number
of images showing the people in masks
and again if you want some of this
information uh contact simply learn and
they can send you some of the
information as far as people with and
without masks so you can try it on your
own and this is just such a wonderful
example of this setup on here so before
I dive into the mass detection uh
talking about being in the current with
uh coid and seeing that people are
wearing masks this particular example I
had to go ahead and update to a python
3.8 version uh it might run into 37 I'm
not sure I I kind of skipped 37 and
installed 38
uh so I'll be running in a three python
38 um and then you also want to make
sure your tensor flow is up to date
because the um they call
functional uh layers with that's where
they split if you remember correctly
from back uh oh let's take a look at
this remember from here the functional
model and a functional layer allows us
to feed in the different layers into
different you know different nodes into
different layers and split them uh very
powerful tool
very popular right now in the edge of
where things are with neural networks
and creating a better model so I've
upgraded to python 3.8 and let's go
ahead and open that up and go through uh
our next example which
includes uh multiple layers um
programming it to recognize whether
someone wears a mask or not and then uh
saving that model so we can use it in
real time so we're actually almost a
full um end to end development of a
product here uh of course this is a very
simplified version and there'd be a lot
more to it you'd also have to do like uh
recognizing whether it's someone's face
or not all kinds of other things go into
this so let's go ahead and jump into
that code and we'll open up a new Python
3 oops Python 3 it's working on it there
we
go um and then we want to go ahead and
train our mask we'll just call this
train
mask and we want to go ahead and train
mask and save it uh so it's it's uh save
mask train mask detection not to be
confused with masking data a little bit
different we're actually talking about a
physical mask on your
face and then from the cross standpoint
we got a lot of imports to do here and
I'm not going to dig too deep on the
Imports uh we're just going to go ahead
and notice a few of them uh so we have
in here go alt D there we go have
something to draw with a little bit here
we have our uh image
processing and the image processing
right here me underline that uh deals
with how do we bring images in because
most images are like a a square grid and
then each value in there has three
values for the three different colors uh
cross and tensorflow do a really good
job of uh working with that so you don't
have to do all the heavy lisening and
figuring out what's going to go on uh
and we have the mobile net average
pooling 2D um this again is how do we
deal with the images and pulling them uh
dropouts a cool thing worth looking up
if you haven't when as you get more and
more into carass and tensor flow uh
it'll Auto drop out certain notes that
way you'll get a better um the notes
just kind of die uh and they find that
they they actually create more of a bias
than a help and they also add processing
time so they remove them um and then we
have our flatten that's where you take
that huge array with the three different
colors and you find a way to flatten it
so it's just a one-dimensional array
instead of a 2 by two
by3 uh DSE input we did that in the
other one so that should look a little
familiar oops there we go our input um
our model again these are things we had
on the last one here's our Optimizer
with our
atom um we have some pre-processing on
the input that goes along with bringing
in the data in uh more pre-processing
with image to array loading the image um
this stuff is so nice it looks like a
lot of works you have to import all
these different modules in here but the
truth is is it does everything for you
you're not doing a lot of pre-processing
you're letting the software do the
pre-processing um and we're going to be
working with the setting something to
categor
Oracle again that's just a conversion
from a number to a category uh 01
doesn't really mean anything it's like
true false um label binarized the same
thing uh we're changing our labels
around and then there's our train test
split classification report um our IM
utilities let me just go ahead and
scroll down here a notch for these this
is something a little different going on
down here this is not part of the uh
tensor flow or the SK learn this is the
S kit setup in tensor flow above uh the
path this is part of um open CV and
we'll actually have another tutorial
going out with the open CV so if you
want to know more about Open CV you'll
get a glance on it in uh this software
especially the ne the second piece when
we reload up the data and hook it up to
a video camera we're going to do that on
this round um but this is part of the
open CV thing and you'll see CV2 is
usually how this referenced um but the
IM utilities has to do with how do you
rotate pictures around and stuff like
that uh and resize them and then the map
plot library for plotting because it's
nice to have a graph tells us how good
we're doing and then of course our numpy
numbers array and just a straight OS
access wow so that was a lot of imports
uh like I said I'm not going to spend I
spent a little time going through them
uh but we didn't want to go too much
into
them and then I'm going to create um
some variables that we need to go ahead
initial ize we have the learning rate
number of epics to train for and the
batch size and if you remember correctly
we talked about the learning rate uh to
the ne4
.001 um a lot of times it's 0.001 or
0.001 usually it's in that uh variation
depending on what you're doing and how
many epics and they kind of play with
the epics the epics is how many times
are we going to go through all the
data now I have it as two um the actual
setup is for 20 and 20 works great the
reason I have it for two is it takes a
long time to process one of the
downsides of
Jupiter is that Jupiter isolates it to a
single kernel so even though I'm on an
eight core processor uh with 16
dedicated threads only one thread is
running on this no matter what so it
doesn't matter uh so it takes a lot
longer to run even though um tensor flow
really scales up nicely and the batch
size is how many pictures do we load at
once in process again those are numbers
you have to learn to play with depending
on your data and what's coming in and
the last thing we want to go ahead and
do is there's a directory with a data
set we're going to
run uh and this just has images of mass
and not
MKS and if we go in here you'll see data
set um and you have pictures with mass
they're just images of people with mass
on their face uh and then we have the
opposite let me go back up here without
masks so it's pretty straightforward
they look kind of a skew because they
tried to format them into very similar
uh setup on there so they're they're
mostly squares you'll see some that are
slightly different on here and that's
kind of an important thing to do on a
lot of these data sets get them as close
as you can to each other and we'll we
actually will run in the in this
processing of images up here and the
cross uh layers and importing and and
dealing with images it does such a
wonderful job of converting these and a
lot of it we don't have to do a whole
lot with uh so you have a couple things
going on there and so uh we're now going
to be this is now loading the um images
and let me see and we'll go ahead and uh
create data and labels here's our um uh
here's the features going in which is
going to be our pictures and our labels
going out and then for categories in our
list directory directory and if you
remember I just flashed that at you we
had uh uh face mask or or no face mask
those are the two options and we're just
going to load into that we're going to
pin the image itself and the labels so
we're just create a huge array uh and
you can see right now this could be an
issue if you had more data at some point
um thankfully I have a a 32 gig hard
drive or U
Ram even that does you could do with a
lot less of that probably under 16 or
even eight gigs would easily load all
this stuff
um and there's a conversion going on in
here I told you about how we are going
to convert the size of the image so it
resizes all the images that way our data
is all identical the way it comes
in and you can see here with our labels
we have without mask without mask
without mask uh the other one would be
with mask those are the two that we have
going in
there uh and then we need to change it
to the one not hot
encoding and this is going to take our
our um um up here we had was was it
labels and data uh we want the labels uh
to be categorical so we're going to take
labels and change it to categorical and
our labels then equal a categorical list
uh we'll run that and again if we do uh
labels and we just do the last or the
first 10 let's do the last 10 just
because um minus 10 to the end there we
go just so we can see what the other
side looks like we now have one that
means they have a mask one Zer one Zer
so
on uh one being they have a mask and
zero no mask and if we did this in
Reverse I just realized that this might
not make sense if you've never done this
before let me run this
01 so zero is uh do they have a mask on
zero do they not have a mask on one so
this is the same as what we saw up here
without mask 1 equals um the second
value is without mask so with masks
without mask uh and that's just a with
any of your data
processing we can't really a zero if you
have a01
output uh it causes issues as far as
training and setting it up so we always
want to use a one hot encoder if the
values are not actual uh linear Val or
regression values are not actual numbers
if they represent a
thing and so now we need to go ahead and
do our train X test X train y test y um
train split test data and we'll go ahead
and make sure it's going to be uh random
and we'll take 20% of it for testing and
the rest for um setting it up as far as
training their model this is something
that's become so cool when they're
training these Set uh they realize we
can augment the data what does augment
mean mean well if I rotate the data
around and I zoom in I zoom out I rotate
it um share it a little bit flip it
horizontally um fill mode as they do all
these different things to the data it um
is able to it's kind of like increasing
the number of samples I have uh so if I
have all these perfect samples what
happens when we only have part of the
face or the face is tilted sideways or
all those little shifts cause problem if
you're doing just a standard set of data
so we're going to create an augment and
our image data generator um which was
going to rotate zoom and do all kinds of
cool thing and this is worth looking up
this image data generator and all the
different features it has um a lot of
times I'll the first time through my
models I'll leave that out because I
want to make sure there's a thing we
call build def fail which is just cool
to know you build the whole process and
then you start adding these different
things in uh so you can better train
your model and so we go and run this and
then we're going to load um and then we
need to go ahead and you probably would
have gotten an error if you Hadad and
put this piece in right here um I
haven't run it myself cuz the guys in
the back did this uh we take our base
model and one of the things we want to
do is we want to do a mobile net
V2 um and this what we this is a big
thing right here include the top equals
false a lot of data comes in with a
label on the top row uh so we want to
make sure that that is not the case uh
and then the construction of the head of
the model that will be placed on the top
of the base model uh we want to go ahead
and set that
up and you'll see a warning here I'm
kind of ignoring the warning because it
has to do with the uh size of the
pictures and the weights for input shape
um so they'll it'll switch things to
defaults to saying hey we're going to
Auto shape some of this stuff for you
you should be aware of that with this
kind of imagery we're already augmenting
it by moving it around and flipping it
and doing all kinds of things to it uh
so that's not a bad thing in this but
another data it might be if you're
working in a different
domain and so we're going to go back
here and we're going to have we have our
base model we're going to do our head
model equals our base model
output um and what we got here is we
have an average pooling 2D pool size 77
head model um head model flatten so
we're flattening the data
uh so this is all processing and
flattening the images and the pooling
has to do with some of the ways it can
process some of the data we'll look at
that a little bit when we get down to
the lower level on this processing it um
and then we have our dense we've already
talked a little bit about a dense just
what you think about and then the head
model has a Dropout of 0. five uh what
we can do with a
Dropout the Dropout says that we're
going to drop out a certain amount of
nodes while training uh so when you
actually use the model it will use all
the nodes but this drops certain ones
out and it helps stop biases from up
forming uh so it's really a cool feature
in here they discovered this a while
back uh we have another dense mode and
this time we're using soft Max
activation lots of different activation
options here softmax is a real popular
one for a lot of things U so is the
ru and you know we could do a whole talk
on activation formulas uh and why what
their different uses are and how they
work when you first start out you'll
you'll use mostly the Rayo and the
softmax for a lot of them uh just
because they're they're some of the
basic setups it's a good place to
start uh and then we have our model
equals model inputs equals base model.
input outputs equals head model so again
we're still building our model here
we'll go ahead and run
that and then we're going to l sop over
all the layers in the base model and
freeze them so they will not be updated
during the first training process uh so
for layer and base model layers layers.
trable equals False A lot of times when
you go through your data um you want to
kind of jump in partway through um I I'm
not sure why in the back they did this
for this particular example um but I do
this a lot when I'm working with series
and and specifically in stock data I
wanted to iterate through the first set
of 30 Data before it does
anything um I would have to look deeper
to see why they froze it on this
particular one and then we're going to
compile our model uh so compiling the
model
atom and nit layer
Decay um initial learning rate over
epics and we go ahead and compile our
loss is going to be the binary cross
entropy which will have that print out
Optimizer for opt metric's accuracy same
thing we had before not a huge jump as
far as um the previous
code and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model uh so train the
head of the network print info training
head
run now I skipped a little time because
it you'll see the run time here is um at
80 seconds per epic takes a couple
minutes for to get through on a single
kernel one of the things I want you to
notice on here while we're while it's
finishing the
processing is that we have up here our
augment going on so anytime the train X
and train y go in there's some
Randomness going on there and is
jiggling it around what's going into our
setup uh of course we're batch sizing it
uh so it's going through whatever we set
for the batch values how many we process
at a time and then we have this steps
per epic uh the train X the batch size
validation data here's our test X and
Test Y where we're sending that
in uh and this again it's validation one
of the important things to know about
validation is our um when both our
training data and our test data have
about the same accuracy that's when you
want to stop that means that our model
isn't biased if you have a higher
accuracy on on your uh testing you know
you've trained it and your accuracy is
higher on your actual test data then
something in there is probably uh has a
bias and it's overfitted uh so that's
what this is really about right here
with the validation data and validation
steps so it looks like it's let me go
ahead and see if it's done processing
looks like we've gone ahead and gone
through two epics again you could run
this through about 20 with this amount
of data and it would give you a nice
refined uh model model at the end we're
going to stop at 2 CU I really don't
want to sit around all afternoon and I'm
running this on a single thread so now
that we've done this we're going to need
to evaluate our model and see how good
it is and to do that we need to go ahead
and make our predictions um these are
predictions on our test X to see what it
thinks are going to be uh so now it's
going to be evaluating the network and
then we'll go ahead and go down here and
we will need to uh turn the index Inc
remember it's it's either zero or one
it's uh 0 one 0 one so you have two
outputs uh not wearing uh wearing a mask
not wearing a mask and so we need to go
ahead and take that argument at the end
and change those predictions to a zero
or one coming out uh and then to finish
that off we want to go ahead and let me
just put this right in here and do it
all in one shot we want to show a nicely
formatted classification report so we
can see what that looks like on here
and there we have it we have our
Precision uh it's 97% with the mask
there's our F1 score support without a
mask
97% um so that's pretty high high uh
setup on there you know you three people
are going to sneak into the store who
are without a mask and that thinks they
have a mask and there's going to be
three people with a mask that's going to
flag the person at the front to go oh
hey look at this person you might not
have a mask um that's if I guess it's a
set up in front of a store
um so there there you have it and of
course one of the other cool things
about this is if someone's walking into
the store and you take multiple pictures
of them um you know this is just an it
it would be a way of flagging and then
you can take that average of those
pictures and make sure they match or
don't match if you're on the back end
and this is an important step because
we're going to this is just cool I love
doing this stuff uh so we're going to go
ahead and take our model and we're going
to save it uh so model save Mass
detector model we're going to give it a
name uh we're going to save the format
um in this case we're going to use the
H5
format and so this model we just
programmed has just been saved uh so now
I can load it up into say another
program what's cool about this is let's
say I want to have somebody work on the
other part of the program well I just
saved the model they upload the model
and now they can use it for whatever and
then if I get more information uh and we
start working with that that's some
point I might want to update this model
um make a better model and this is true
of so many things where I take this
model and maybe I'm uh running a
prediction on uh making money for a
company and as my model gets
better I want to keep updating it and
then it's really easy just to push that
out to the actual end user uh and here
we have a nice graph you can see the
training loss and accuracy as we go
through the epics uh we only did the you
know only show shows just the one Epic
coming in here but you can see right
here as the uh um value loss train
accuracy and value
accuracy starts switching and they start
converging and you'll hear converging
this is a convergence they're talking
about when they say you're you're um I
know when I work in the S kit with
sklearn neural networks this is what
they're talking about a convergence is
our loss and our accuracy come together
and also up here and this is why I'd run
it more than just two epics as you can
see they still haven't converged all the
way uh so that would be a cue for me to
keep
going but what we want to do is we want
to go ahead and create a new Python
3
program and we just did our train mask
so now we're going to go ahead and
import that and use it and show you in a
live action um get a view of uh both
myself in the afternoon along with my
background of an office which is in the
middle still of reconstruction for
another
month and we'll call this uh
mask
detector and then we're going to grab a
bunch of um a few items coming
in uh we have our um mobilet V2 import
pre-processing input CU we're still
going to need that um we still have our
tensor floral image to array we have our
load model that's where most of the
stuff's going on this is our CV2 or open
CV again I'm not going to dig too deep
into that we're going to flash a little
open CV code at you uh and we actually
have a tutorial on that coming out um
our numpy array our IM utilities which
is part of the open CV or CV2
setup uh and then we have of course time
and just our operating system so those
are the the things we're going to go
ahead and set up on here and then we're
going to create this takes just a
moment our module here which is going to
do all the heavy
lifting uh so we're going to detect and
predict a mask we have frame face net
Mass net these are going to be generated
by our open CV we have our frame coming
in and then we want to go ahead and and
create a mask around the face it's going
to try to detect the face and then set
that up so we know what we're going to
be processing through our
model um and then there's a frame shape
here this is just our height versus
width that's all HW stands for um
they've called it blob which is a CV2
DNN blob form image frame so this is
reformatting this Frame that's going to
be coming in literally from my camera
and we'll show you that in a minute that
little piece of code that just shoots
that in here uh and we're going to pass
the blob through the network and obtain
the face
detections uh so faet do set inport blob
detections face net forward print
detections
shape uh so these is this is what's
going on here this is that model we just
created we're going to send that in
there and I'll show you in a second
where that is but it's going to be under
faet uh and then we go ahead and
initialize our list of faces their
corresponding location and the list of
predictions from our face mask
Network we're going to Loop over the
detections and this is a little bit more
work than you think um as far as looking
for different faces what happens if you
have a fa a crowd of faces um so We're
looping through the detections and the
shapes going through here and
probability associated with the
detection uh here's our confidence of
detections we're going to filter out
weak detection by ensuring the
confidence is greater than than the
minimum
confidence uh so we've said it remember
0o to one so 0 five would be our minimum
confidence probably is pretty good um
and then we're going to put in compute
bounding boxes for the object if I'm
zipping through this it's because we're
going to do an open CV and I really want
to stick to just the carass
part and so I'm I'm just kind of jumping
through all this code you can get a copy
of this code from Simply learn and take
it apart or look for the open TV coming
out and we'll create a box uh the box
sets it around the
image Ure the bounding boxes fall within
the dimensions of the
frame uh so we create a box around
what's going to what we hope is going to
be the face extract the face Roi convert
it from BGR to RGB
Channel again this is an open CV issue
not really an issue but it has to do
with the order um I don't know how many
times I've forgotten to check the order
colors we working with open CV
because it's all kinds of fun things
when red becomes blue and blue becomes
red uh then we're going to go ahead and
resize it process it frame it uh face
frame setup again the face the CBT color
we're going to convert it uh we're going
to resize it image to array pre-process
the input uh pin the face locate face
start x. y and x boy that was just a
huge amount and I skipped over a ton of
it but the bottom line is we're building
a box around the face and that box
because the open CV does a decent job of
finding the face and that box is going
to go in there and see hey does this
person have a mask on
it uh and so that's what that's what all
this is doing on here and then finally
we get down to this where it says
predictions equals mass net. predict
faces batch size
32 uh so these different images of where
we're guessing where the face is are
then going to go through an generate an
array of faces if you will and we're
going to look through and say does this
face have a mask on it and that's what's
going right here is our prediction
that's the big thing that we're working
for and then we return the locations and
the predictions the locations just tells
where on the picture it is and then the
um prediction tells us what it is is it
a mask or is it not a mask all right so
we've loaded that all up so we're going
to load our serialized face detector
model from dis um and we have our the
path that it was saved in obviously
you're going to put it in a different
path depending on where you have it or
how however you want to do it and how
you saved it on the last one where we
trained it uh then we have our weights
path um and so finally our face net here
it is equals CB2 dn. read net uh protot
text path weights path and we're going
to load that up on here so let me go
ahead and run
that and then we also need to I'll just
put it right down here I always hate
separating these things in there um and
then we're going to load the actual mass
detector model from disk this is a the
the model that we saved so let's go
ahead and run that on there also so this
is pulling in all the different pieces
we need for our model and then the next
part is we're going to create open up
our video uh and this is just kind of
fun because it's all part of the open
CV video
setup and me just put this all in as one
there we go uh so we're going to go
ahead and open up our video we're going
to start it and we're going to run it
until we're
done and this is where we get some real
like kind of live action stuff which is
fun this is what I like working about
with images and videos is that when you
start working with images and videos
it's all like right there in front of
you it's Visual and you can see what's
going on uh so we're going to start our
video streaming this is grabbing our
video stream Source zero start
uh that means it's C grabbing my main
camera I have hooked up um and then you
know starting video you're going to
print it out here's our video Source
equals zero start Loop over the frames
from the video
stream oops a little redundancy there um
let me
close I'll just leave it that's how they
had it in the code so uh so while true
we're going to grab the frame from the
threaded video stream and resize it to
have the maximum with of 400 pixels so
here's our frame we're going to read it
uh from our visual uh stream we're going
to resize
it and then we have a returning remember
we returned from the our procedure the
location and the prediction so detect
and predict mask we're sending it the
frame we're sending it the face net and
the MK net so we're sending all the
different pieces that say this is what's
going through on
here and then it returns our location
and predictions and then for our box and
predictions in the location and
predictions um and the boxes is again
this is an open CV set that says hey
this is a box coming in from the
location um because you have the two
different points on there and then we're
going to unpack the box and predictions
and we're going to go ahead and do mask
without a mask equals
prediction we're going to create our
label no mask we create color if the
label equals mask lse 00 225 and you
know this is going to make a lot more
sense when I hit the Run button here uh
but we have the probability of the label
we're going to display the label and
bounding box rectangle on the output
frame uh and then we're going to go
ahead and show the output from the frame
CV2 IM show frame frame and then the key
equals CV2 weit key one we're just going
to wait till the next one comes through
from our
feed and we're going to do this until we
hit the stop button pretty much so are
you ready for this let's see if it works
we've distributed our uh our model we've
loaded it up into our distributed uh
code here we've got it hooked into our
camera and we're going to go ahead and
run
it and there it goes it's going to be
running and we can see the data coming
down here and we're waiting for the
popup and there I am in my office with
my funky headset
on uh and you can see in the background
my unfinished wall and it says up here
no mask oh no I don't have a mask on
uh I wonder if I cover my mouth what
would happen uh you can see my no
mask goes down a little bit I wish I
brought a mask into my office it's up at
the house but you can see here that this
says you know there's a 95 98% chance
that I don't have a mask on and it's
true I don't have a mask on right now
and this could be distributed this is
actually an excellent little piece of
script that you could start you know you
install somewhere on a a video feed on a
on a security camera CER or something
and then you'd have this really neat uh
setup saying hey do you have a mask on
when you enter a store or public
transportation or whatever it is or
they're required to wear a mask uh let
me goe and stop
that now if you want a copy of this uh
code definitely give us a hauler we will
be going into open CV in another one so
I skipped a lot of the open CV um code
in here as far as going into detail
really focusing on the kage
uh saving the model uploading the model
and then processing a streaming video
through it so you can see that the model
works we actually have this working
model that hooks into the video
camera which is just pretty cool and a
lot of
fun so I told you we're going to dive in
and really Roll Up Our Sleeve and do a
lot of coating today uh we did the basic
uh demo up above for just pulling in a
cross and then we went into a cross
model uh where we pulled in data to see
whether someone was wearing a mask or
not so very useful in today's world as
far as a fully running
application and we're going to take a
look at image classification using
carass and the basic setup we'll
actually look at two different demos on
here uh what's in it for you today what
is image
classification Intel image
classification data creating neural
networks with carass and the vgg16
model
what is image
classification the process of image
classification refers to assigning
classes to an entire image images can be
classified based on different categories
like whether it is a nighttime or
daytime shot what the image represents
Etc you can see here we have uh
mountains looking for mountains we'll
actually be doing some uh uh pictures of
scenery and stuff like that in deep
learning we perform image classification
by using neural networks to extract
features from images and classifies them
based on these
features and you can see here where it
says like what computer sees and this is
oh yeah we see mostly Forest maybe a
little bit of mountains because the way
the image is um and this is really where
one of the areas that neural networks
really shines um if you try to run this
stuff through uh more like a linear
regression model you'll still get
results uh but the results kind of miss
a lot of things as they as the neural
networks get better and better at what
they do with different tools we have out
there uh so Intel image classification
data the data being used is the Intel
image classification data set which
consists of images of six types of land
areas and so we have Forest building
glaciers and mountains sea and Street uh
and you can see here there's a couple of
the images out of there as a setup in
the in the um uh Intel image
classification data that they use
and then we're going to go into creating
a neural networks with
carass the convolutional neural network
that we are creating from scratch looks
uh as showing
below you'll see here we have our input
layer
um they have a listed Max pulling uh so
you have as you're coming in with a
input layer and this the input layer is
actually um before this but the first
layer it's going to go into it's going
to be a convolutional neural network uh
then you have a Max pooling that pulls
those the the convolutional neural
networks returns uh in this case they
have two of those that is very standard
with convolutional neural networks uh
one of the ones that I was looking at
earlier that was standard being used by
um I want one of the larger companies I
can't remember which one for doing a
large amount of identification had two
convolutional neural networks each with
their Max pooling and then about 17
dense layers after it we're not going to
do that heavy duty of a of a code but
we'll get you head in the right
direction and that gives you an idea of
what you're actually going to be looking
at when you look at the flatten part and
then the dense we're talking like 17
dense layers
afterwards uh I find that a lot of the
stuff I've been working on I end up
maxing it out right around nine dense
layers it really depends on what you
have going in and what you're working
with and the vgg16 model
uh vgg16 is a pre-trained CNN model
which is used for image classification
it is trained on a large varied data set
and fine-tune to fit image
classification data sets with
ease and you can see down here we have
the input coming in uh the convolutional
neural network 1: one 1:2 and then
pooling and then we do two: one 2:2
convolutional Network than pooling 3:2
and you can see there's just this huge
layering ing of convolutional neural
networks and in this case they have five
such layers going in and then three
dents going out or uh more now when they
took this setup this actually won an
award uh back in 2019 for this
particular
setup uh and it does it does really good
except that again um we only show the
three dense layers here and as you find
out depending on your data going in what
you have set up
uh that really isn't enough on one of
these setups and I'm going to show you
why we restricted it because it does
take up a lot of processing power and
some of these things so let's go ahead
and roll up our sleeves and we're going
to look at both the setups we're going
to start with the um the first
classification um and then we'll go into
the vgg16 and show you how that's set up
now I'm going to be using anaconda and
let me flip over to my anaconda so you
can see what that looks like now I'm
running in the Anaconda here uh you'll
see that I've set up a main python uh 38
I always put that in there because this
is where I'm doing like most of my kind
of playing around uh this is done in
Python version 3.8 we're not going to
dig too much into versions uh at this
point you should already have carass
installed on there usually carass takes
a number of extra
steps and then our usual um uh setup is
the numpy the pandas uh your SK your s
kit which is going to be the sklearn
your caborn and I'll I'll show you those
in just a minute um and then I'm just
going to be in the Jupiter lab where
I've created a new um notebook in here
and let's flip on over there to my blank
notebook now I'm there's a couple cool
things to note in here is that um one I
used the the um Anaconda Jupiter
notebook setup because it keeps
everything separate uh except for for
carass uh carass is actually running
separately in the back I believe it's a
a c program uh what's nice about that is
that it utilizes the multiprocessors on
the computer and I'll mention that just
in a little bit when we actually get
down to running the
code and when we look in here uh a
couple things to note is here's our uh
um oops I thought I grabbed the other
drawing thing uh but here's our numpy
and our pandas right here in our
operating system this is our s kit you
always import it as sklearn for the
classification report uh we're going to
be using well usually import like
Seaborn brings in all of your pip plot
Library
also kind of nice to throw that in there
I can't remember if we're actually using
caborn if they just the people in the
back just threw that together um and
then we have the SK learn shuffle for
shuffling data here's our map plot
library that the Seaborn is pretty much
built on uh
CV2 if you're not familiar with that
that is our image um module for
importing the image and then of course
we have our tens or flow down here which
is what we're really working
with and then the last thing is just for
visual effect while we're running this
um if you're doing a demo and you're
working with uh the partners or the
shareholders uh this TQ DM is really
kind of cool it's an extensible progress
bar for Python and I I'll show you that
too remember data science is not I mean
you know most this code when I'm looking
through this code I'm not going to show
half of this stuff to the shareholders
or anybody I'm working with they don't
really care about pandas and all that we
do because we want to understand how it
works uh so we need to go ahead and
import those different um uh setup on
there and then the next thing is we're
going to go ahead and set up our
classes uh now we remember if we had
Mountain Street Glacier building sea and
Forest
those were the different images that we
have coming
in and we're going to go ahead and just
do class name labels and we're going to
kind of match that class name of I for I
class name uh equals the class names so
our labels are going to match the names
up
here uh and then we have the number of
uh
classes and print the class names and
the labels and we'll go ahead and set
the image size this is important that we
resize everything because if you
remember with neural
networks they take one size data coming
in and so when you're working with
images you really want to make sure
they're all resized the same uh setup it
might squish them it might stretch them
that generally does not cause a problem
in these uh and some of the other tricks
you can do with if you if you need more
data um and this is one that's used
regularly we're not going to do it in
here is you can also take these images
and not only resize them but you can
tilt them one way or the other crop
parts of them um so they process
slightly differently and it'll actually
increase your accuracy of some of these
predictions uh and so you can see here
we have Mountain equals zero that's what
this class name label is Street equals 1
Glacier equals 2 buildings equals 3 C4
Forest equals
5 now we did this as an enumerator so
each one is z through five uh a lot of
times we do this instead as um uh uh 0 1
0 1 01 so you have five outputs and each
one's a zero or a one coming out so the
next thing we really want to do is we
want to go ahead and load the data up
and just put a label in there loading
data just just so you know what we're
doing we going to put in the loading
data down here uh make sure it's well
labeled uh and we'll create a definition
for
this and this is all part of your
preprocessing at this point you could
could replace this with all kinds of
different things depending on what
you're working on and if you once you
download you can go download this data
set uh send a note to the simply learn
team here in YouTube um and they'll be
happy to direct you in the right
direction and make sure you get this
path here um so you have it right
whatever wherever you saved it a lot of
times I'll just abbreviate the path or
put it as a sub thing and just get rid
of the directory um but again double
check your paths um we're going to
separate this into a segment for
training and a segment for testing and
that's actually how it is in the folder
let me just show you what that looks
like so when I have my uh lengthy path
here where I keep all my programming
simply learned this particular setup
we're working on image classification
and image classification clearly you
probably wouldn't have that lengthy of a
list and when we go in here uh you'll
see sequence train sequence test they've
already split this up this is what we're
going to train the data in and again you
can see buildings Forest Glacier
Mountain Sea Street uh and if we double
click let's go under Forest you can see
all these different Forest uh images and
and there's a lot of variety here I mean
we have wintertime we have
summertime um so it's kind of
interesting you know here's like a
Fallen Tree versus um a road going down
the middle that's really hard to train
and if you look at the
buildings A lot of these buildings
you're looking up a skyscraper we're
looking down the
setup here's some trees with one I want
to highlight this one it has trees in it
uh let me just open that up so you can
see it a little
closer the reason I want to highlight
this is I want you to think about this
we have trees growing is this the city
or a
forest um so this kind of imagery makes
it really hard for a classifier and if
you start looking at these you'll see a
lot of these images do have trees and
other things in the the foreground weird
angles really a hard thing for a
computer to sort out and figure out
whether it's going to be a forest or a
um
city and so in our loading of data uh
one we have to have the path the
directory we're going to come in here we
have our images and our labels so we're
going to load the images in one section
the labels in another
um and if you look through here it just
goes through the different folders uh in
fact let me do this let
me there we go uh as we look at this
we're just going to Loop through the
three the six different folders that
have the different Landscapes and then
we're going to go through and pull each
file
out and each label uh so we set the
label we set the folder for file and
list uh here's our image path join the
paths this is all kind of n General
stuff um so I'm kind of skipping through
it really quick
and here's our image setup uh if you
remember we're talking about the images
we have our CV2 reader so it reads the
the image in uh it's going to go ahead
and take the image and convert it to
from blue green red to red green green
blue this is a CV2 thing um almost all
the time it Imports it and instead of
importing it as a standard that's used
just about everywhere it Imports it with
the BG versus RGB um RGB is pretty much
a standard in here you have to remember
that was CV2 uh and then we're going to
go ahead and resize it this is the
important part right here we've set it
we've decided what the size is and we
want to make sure all the images have
the same size on
them and then we just take our images
and we're just going to pin the image
pin the label um and then the images is
going to turn into a numpy array this
just makes it easier to process and
manipulate and then the labels is also a
numpy array and and then we just return
the output pend images and labels and we
return the output down
here so we've loaded these all into
memory uh we haven't talked to much
there'd be a different setup in there
because there is ways to feed the files
directly into your cross model uh but we
want to go ahead and just load them all
it's
really for today's processing and the
what our computers can handle that's not
a big deal and then we go ahead and set
the uh train images train labels test
images test labels and that's going to
be returned in our output app pinned and
you can see here we did um uh images and
labels set up in there and it just loads
them in there so we'll have these four
different categories let me just go
ahead and run
that
uh so now we've gone ahead and loaded
everything on
there
and then if you remember from before uh
we imported just go back up
there Shuffle here we go here's our
sklearn utilities import Shuffle and so
we want to take these labels and shuffle
them around a little bit um just mix
them up so it's not having the same if
you run the same process over and over
uh then you might run into some problems
on
there and just real quick let's go ahead
and do uh um a plot so we just you know
we we've looked at them as far as from
outside of our code we pulled up the
files and I showed you what that was
going on we can go and just display them
here too and I tell you when you're
working with different
people this should be highlighted right
here um this thing is like when I'm
working on code and I'm looking at this
data and I'm trying to figure out what
I'm doing I skip this process the second
I get into a meeting and I'm showing
what's going on to other people
I skip everything we just did so and go
right to here where we want to go ahead
and display some images and take a look
at
it and in this display um I've taken
them and I've resize the images to 20 by
20 that's pretty small uh so we're going
to lose just a massive amount of detail
and you can see here these nice
pixelated images um I might even just
stick with the fold ER showing them what
images we're
processing uh again this is you got to
be a little careful this maybe resizing
it was a bad idea um in fact let me try
it without resizing it and see what
happens oops so I took out the image
size and then we put this straight in
here one of the things again this is
um put the D there we go one of the
things again that we want to
know whenever we're working on these
things uh is the
CV2 there are so many different uh image
classification setups it's really a
powerful package when you're doing
images but you do need to switch it
around so that it works with the p plot
and so make sure you take your numpy
array and change it to a u integer 8
format uh because it comes in as a float
otherwise you'll get some weird images
down there um and so this is just
basically we split up our we've created
a plot we went ahead and did the plot 20
by 20 um or the plot figure size is 20
by 20 um and then we're doing 25 so a
5x5 subplot um nothing really going on
here too exciting but you can see here
where we get the images and really when
you're showing people what's going on
this is what they want to see uh so you
skip over all the code and you have your
meeting you say okay here's our images
of the
building um don't get caught up in how
much work you do get caught up in what
they want to see so if you want to work
in data science that's really important
to
know and this is where we're going to
start uh having fun uh here's our model
this is where it gets exciting when
you're digging into these models and you
have here uh let me
get there we
go when you have here if you look here
here's our convolutional neural network
uh 2D and and uh 2D is an image you have
two different dimensions x y and even
though there's three colors it's still
considered 2D if you're running a video
you'd be convolutional neural network 3D
if you're doing a series going across um
a Time series it might be
1D and on these you need to go ahead and
have your convolutional Nal network if
you look here there's a lot of really
cool settings going on to dig into um we
have our input shape so everything has
been set to 150 by 150 uh and it has of
course three different color schemes in
it that's important to notice um
activation default is railu uh this is
small amounts of data being processed on
a bunch of little um neural
networks and right here is the 32 that's
how many of these convolutional null
networks are being strung up on here and
then the
3x3 uh when it's doing its steps it's
actually looking at uh a little 3x3
Square on each image and so that's
what's going on here and with
convolutional noral networks the window
floats across and adds up all these
numbers going across on this data and
then eventually it comes up with 30 in
this case 32 different feature options
uh that it's looking for and of course
you can change that 32 you you can
change the 3X3 so you might have a
larger setup and you know if you're
going across
150 um by 150 that's a lot of steps so
we might run this as 15 by 15 uh there's
all kinds of different things you can do
here we're just putting this together
again that would be something you would
play with to find out which ones are
going to work better on this setup um
and there's a lot of play
involved that's really where it becomes
an art form is guessing at what that's
going to be the second part I mentioned
earlier and I I can only begin to
highlight this um when you get to these
dense layers one is the activation is a
railu they use a railu and a softmax
here um it's a whole whole uh setup just
explaining why these are different um
and how they're different because
there's also an exponential there's a
tangent in fact uh there's just a ton of
these and you can build your own custom
activations depending on what you're
doing a lot of different things go into
these these activations uh there are two
or three major thoughts on these
activations and Ru and softmax or uh
well Ru uh you're really looking at just
the number you're adding all the numbers
together and you're looking at ukan
geometry um ax plus
bx2 plus
cx3 plus
bias with softmax this belongs to the
party of um it's activated or it's not
except it's they call it softmax because
when you get the the to zero instead of
it just being zero uh it's actually
slightly a little bit less than zero so
that when it trains it doesn't get lost
um there's a whole series of these
activations another activation is the
tangent um where it just drops off and
you have like a very narrow area where
you have from minus1 to one or
exponential which is 0 to one so there's
a lot of different ways to do the
activation again we can do that' be a
whole separate lesson on here we're
looking at the convolutional neural
network um and we're doing the two pools
this is so common you'll see two two
convolutional n networks stacked on top
of each other each with its own Max pull
underneath and let's go ahead and run
that so we built our model there and
then we need to go ahead and
um compile the model so let's go ahead
and do that
uh we are going to use the atom uh
Optimizer the bigger the data the adom
fits better on there there's some other
Optimizer but I think Adam is a default
um I don't really play with the
optimizer too much that's like the if
once you get a model that works really
good you might try some different
optimizers uh but Adam's usually the
most and then we're looking at a
loss pretty standard we want to minimize
our Lo we want
to maximize the loss of error and then
we're going to look at accuracy um
everybody likes say accuracy I'm going
to tell you right
now I start talking to people and like
okay what's what's the loss on this and
that and as a data science yeah I want
to know how the law what what's going on
with that and we'll show you why in a
minute but everybody wants to see
accuracy they want to know how accurate
this is uh and then we're going to run
the fit and I wanted to do this just so
I can show you even though
we're in uh python setup in here where
Jupiter notebook is using only a single
processor I'm going to bring over my
little CPU Tool uh this is eight cores
on 16 dedicated threats so it shows up
as 16
processors and actually I got to run
this and then move it over so we're
going to run this and hopefully it
doesn't destroy my
mic uh and as it comes in you can see
it's starting to do go through the epics
we said I said it for epics and then
this is really nice because carass uses
all the different uh threads available
so it does a really good job of doing
that uh this is going to take a while if
you look at here it's um ETA 2 minutes
and 25 seconds 24 seconds so this is
roughly 2 and a half minutes per epic uh
and we're doing five epics so this is
going to be done in roughly 15 minutes I
don't know about you but I don't think
you want to sit here for 15 minutes
watching The Green bars go across so
we'll go ahead and let that run and
there we go uh there was our 15 minutes
it's actually less than that uh because
I did when I went in here realized that
uh uh where was
it here we go here's our model compile
here's our model flp uh fit and here's
our epics uh so I did four epics so a
little bit better more like 10 to 11
minutes instead of uh uh doing the full
uh 15 and when we look at this here's
our model we did talked about the
compiler uh here's our history we're
going to um history equals the model fit
we'll go into that in just a
minute and what we're looking at is we
have our epics um here's our validation
split so as we train it uh we're
weighing the accuracy versus you kind of
pull some data off to the side
uh while you're training it and the
reason we do that is that um you don't
want to overfit and we'll look at that
chart in just a
minute uh here's batch
size this is just how many images you're
sending through at a time the larger the
batch it actually increases the
processing speed um and there's reasons
to go up or down on the batch size
because of the U the the smaller the
batch there's a certain point where um
you get too large of a batch and it's
trying to fit everything at once uh so I
128 is kind of big U depends on the
computer you're on what it can handle
and then of course we have our train
images and our train labels going in
telling it what we're going to train
on and then we look at our four epics
here uh here's our accuracy we want the
accuracy to go up and we get all the way
up to uh 83 or
83% uh this is actual percentage based
pretty much and we can see over here our
loss we want our loss to go down really
fluctuates uh 55 1.2
7748 uh so we have a lot of things going
on there let's go ahead and graph
those turn that out and our our team in
the back did wonderful job of putting
together um this basic plot setup um
here's our subplot coming in we're going
to be looking at um uh from the history
we're going to send it the accuracy and
the value
accuracy labels and setup on there um
and we're going to also look at loss and
value loss so you can see what this
looks like what's really interesting
about this setup and let me let me just
go ahead and show you because uh without
actually seeing the plots it doesn't
make a whole lot of sense
uh it's just basic plotting of uh of the
data using the piip plot library and I
want you to look at this this is really
interesting um when I ran this the first
time I had very different
results um and they they vary greatly
and you can see here our accuracy
continues to
climb um and there's a crossover
here put it in here right here is our
crossover
and I point that out because as we get
to the right of that crossover where our
accuracy U and we're like oh yeah I got
8% we're starting to get an overfit here
that's what this this switch over means
um as our value um as a training set
versus the value um accuracy stays the
same and so that this is the one we're
actually really want to be aware of and
where it
crosses is kind of where you want to
stop at um and we can see that also with
the train loss versus the value loss
right here we did one Epic and look how
it just flat lines right there with our
loss so really one Epic is probably
enough and you're going to say wow okay
8% um certainly if I was working with
the shareholders um telling them that it
has an 80% accuracy isn't quite true and
and we'll look at that a little deeper
it really comes out here that the
accuracy of our actual values is closer
to 41% right here um even after running
it this number of times and so you
really want to stop right here at that
crossover one Epic would have been
enough um so the data is a little
overfitted on this when we do four
epics and uh oops there we are
okay my drawing won't go away um let me
see if I can get there we
go uh for some reason I've killed my
drawing ability on my
recorder all right took a couple extra
clicks uh so let's go ahead and take a
look at our actual test loss um so you
see where cross is over that's where I'm
looking at that's where we start
overfitting the
model and this is where if uh we were
going to go back and continually upgrade
upgrade the model we would start taking
a look at the images and start rotating
them uh we might start playing with the
convolutional neural network instead of
doing the 3X3 window um we might expand
that or you know find different things
that might make a big difference as far
as the way of processes these things um
so let's go ahead and take a click at
our uh our test loss now remember we had
our training data now we're going to
look at our test images and our test
labels for our test loss here and this
is just model evaluate uh just like we
did fit up here where was it um one more
model fit with our training data going
in now we're going to evaluate it on the
and and this data has not been touched
yet so this model's never seen this data
this is on uh completely new information
as far as the model is concerned of
course we already know what it is from
the labels we
have and this is what I was talking
about here's the actual accuracy right
right here
048 uh or
4847 so this 49% of the Time guesses
what the image
is uh and I mean really that's a bottom
dollar uh does this work for what you're
needing does 49% work do we need to
upgrade the model more um in some cases
this might be uh oh what was I doing I
was working on uh stock
evaluations and
we were looking at what stocks were the
top
performers well if I get that 50%
correct on top
performers uh I'm good with that um
that's actually pretty good for stock
evaluation in fact the number I had for
stock was more like U um 30 something
percent as far as being a top performer
stock much harder to predict um but at
that point you're like well I'm you'll
make money off of that so again this
number right here depends a lot on the
domain you're working
on and then we want to go ahead and
bring this home a little bit more uh as
far as looking at the different setup in
here and one of the uh from SK learn if
you remember actually let's go back to
the top uh we had the classification
report and this came in from our sklearn
or S kit setup and that's right here you
can see it right here on the
um see there we go uh classification
report right here uh SK learn metrics
import classification report that's what
we're going to look at
next a lot of this stuff uh depends on
who you're working with so when we start
looking at um
Precision you know this is like for each
value I can't remember what one one one
was probably mountains so if 44% is not
good enough if if you're doing like um
you're in the medical Department and
you're doing cancer is it is this
cancerous or not and I'm only 44%
accurate not a good deal you know I
would not go with that um so it depends
on what you're working with on the
different labels and what they're used
for Facebook you know 44% I'm guessing
the right person I would hope it does a
little bit better than that um but
here's our main accuracy this is what
almost everybody looks at they say oh
48% that's what's important um again it
depends on what domain you're in and
what you're working
with and now we're going to do the same
model somehow I got my there it goes I
thought I was going to get stuck on
there again uh this time we're going to
be using the
vgg16 and remember this one has uh all
those layers going into it so it's
basically a bunch of convolutional n
networks getting smaller and smaller on
here and uh and so we need to go ahead
and um import all our different stuff
from carass uh we're importing the main
one is the V g16 setup on there just aim
that there we go
um there's kind of a pre-processing
images um applications pre-process input
this is all part of the VG g16 setup on
there uh and once we have all those we
need to go ahead and create our model
and we're just going to create a
vgg16 model in here um inputs model
inputs outputs model inputs I'm not
going to spend as much time as they did
on the other one uh we're going to go
through it really quickly one of the
first things I would do is if you
remember in carass you can treat treat a
model like you would a
layer um and so at this point I would
probably add a lot of dense layers on
after the vgg16 model and create a new
model with all those things in there and
we'll go ahead and uh run this uh cuz
here's our model coming in and our setup
and it'll take it just a moment to
compile that what's funny about this is
I'm I'm waiting for it to download the
um package since I pre- ran this um it
takes it a couple minutes to download
the vgg16 model into here um and so we
want to go ahead and train features for
the model we're going to predict the
we're going to predict the train images
and we're going to test features on the
predict test images on
here and then I told you I was going to
create another model too and the people
in the back uh did not disappoint me
they went ahead and did just that and
this is really an important part um this
is where stopping for I told you I was
going to go through this really quick so
here's our
uh we we have our model
two um coming in and we we've created a
model up here with the vgg16 model
equals model inputs model inputs and so
we have our
vgg16 this has already been
preprogrammed uh and then we come down
here I want you to notice on this um
right here layer model 2 layers minus 4
to 1 x layer X um we're basically taking
this model and we're adding stuff onto
it and so uh we've taken we've just
basically duplicated this model we could
have done the same thing by using model
up here as a layer um we could have had
the input go to this model and then have
that go down here so we've added on this
whole setup this whole block of code
from 13 to 17 has been added on to our
vgg16 model and we have a new model uh
with the layer input and X down here
let's go ahead and run that and compile
it
and that was a lot to go through right
there uh when you're building these
models this is the part that gets so
complicated did you get stuck playing in
and yet it's so fun uh it's like a
puzzle how can I Loop these models
together and in this case you can see
right here that the layers uh we're just
copying layers over and adding each
layer in um this is one way to build a
new model and we'll go ahead and run
that like I said the other way is you
can actually use the model as a layer I
had a little trouble playing with it uh
sometimes when you're using the Straight
model
over you run into issues
um it seems like it's going to work and
then you mess up on uh the input and the
output layers there's all kinds of
things that come
up let's go ahead and do the new model
we're going to compile it uh again
here's our metrics accuracy sparse
categorical loss uh pretty
straightforward just like we did before
you got to compile the
model and just like before we're going
to take our create a history uh the
history is going to be uh new model fit
train
128 and just like before if you remember
when we started running this stuff we're
going to have to go ahead and it's going
to light up our uh setup on here and
this is going to take a little bit to
get us all set up uh it's not going to
just happen in in a couple minutes so
let me go ahead and pause the video and
run it and then we'll talk about what
happened okay now when I ran that these
actually took about 6 minutes each um so
it's a good thing I put it on hold we
did four epics uh actually had to stop
it at 10 and switch it to four because I
didn't want to wait an
hour and you can see here our
accuracy um and our loss numberers going
down and just at a
glance it actually pered if you look at
the
accuracy.
2658 um so our accuracy is going down or
you know
26% um 34% 35% and you can see here at
some point it just kind of kicks a
bucket again this is
overfitting that's always an issue when
you're running on uh programming these
different neural
networks and then we're going to go
ahead and plot the accuracy um history
we built that nice little sub routine up
above so we might as well use it and you
can see it right here
um there's that crossover
again and if you look at this look how
the how the um uh the red shifts up how
the uh our loss functions and everything
crosses over we're overfitting after one
Epic um we're
clearly not helping the problem or doing
better um we're just going to it's just
going to Baseline this one actually
shows with the training versus the
loss um value loss maybe second epic so
here we're now talking more between the
first and the SE second epic and that
also shows kind of here so somewhere in
here it starts
overfitting and right about now you
should be saying uhoh uh something went
wrong there I thought that um when we
went up here and ran this look at this
we have the accuracy up here is hitting
that
48% and we're down here
um you look at the score down here that
looks closer to 20% not nearly anywhere
in the ballpark of what we're looking
for and we'll go ahead and run it
through the uh the actual test features
here and and there it is um we actually
run this on the Unseen data and
everything8 or
18% um I don't know about you but I
wouldn't want you know at 18% this did a
lot worse than the other one I thought
this is supposed to be the superm model
the model that beats all models the
vgg16 that won the awards and everything
well the reason is is that um one we're
not pre-processing the data uh so it
needs to be more there needs to be more
um as far as like rotating the data at
you know 45 degree angle taking partials
of it so you can create a lot more data
to go through here um and that would
actually greatly change the outcome on
here and then we went up here we only
added a couple dense layers uh we added
a couple convolutional neural
networks this huge pre-train setup is
looking for a lot more information
coming in as far as how it's going to
train and so uh this is one of those
things where I thought it would have
done better and I had to go back and
research it and look at it and say why
didn't this work why am I getting only
uh 18% here instead of 44% or better and
that would be wise it doesn't have
enough training data coming in uh and
again you can make your own training
data so it's not that we have a shortage
of data it's that that some of that has
to be switched around and moved around a
little bit and this is interesting right
here too if you look at the
Precision we're getting it on number two
and yet we had zero on everything else
so for some reason is not
seeing uh the different variables in
here so it'd be something else to look
in and try to find track down
um and that probably has to do with the
input but you can see right here we have
a really good solid 0 48 up here uh and
that's where I'd really go with is
starting with this model and then we
look at this model and find out why are
these numbers not coming up better is it
the data coming in um where's the setup
on there and that is the art of data
science right there is finding out which
models work better and
why so sequential models uh so what
makes this a sequential model sequential
models are linear stacks of layers where
one layer leads to the next it is simple
and easy to implement and you just have
to make sure that the previous layer is
the input to the next layer so uh you
have used for plain stack of layers
where each layer has one input and one
output tensor and this is what tensor
flow is named after is um each one of
these layers is like a tensor each each
node is a tensor and then the layer is
also considered a tensor of values
and it's used for simple classifier
declassify models you can it's also used
for regression models too so it's not
just about uh this is something this is
a teapot this is a cat this is a dog um
it's also used for generating um uh
reget the actual values you know this is
worth $10 that's worth $30 uh the
weather's going to be 90 de out or
whatever it is so you can use it for
both classifier and declassify uh
models and one more note when we talk
about sequential models the term
sequential is used a lot and it's used
in different areas and different
notations when you're in data science so
when we talk about time series we'll
talk about sequential that is something
very different uh sequential in this
case means it goes from the input to
layer one to Layer Two to the output so
it's very directional it's important to
note this because if you have a
sequential model can you have a
non-sequential model and the answer is
yes uh if you master the basics of a
sequential model you can just as easily
have another model that shares layers um
you can have another model where you
have an input coming in and it splits
and then you have one set that's doing
one set of uh nodes maybe they're doing
a yes no kind of node where it's either
putting out a zero or a one a classifier
and the other one might be regression
it's just processing numbers and then
you recombine them for the output um
that's what they call a cross the cross
API so there's a lot of different
availabilities in here and all kinds of
cool things you can do as far as
encoding and decoding and all kinds of
things and you can share layers and
things like that we're just focusing on
the basic cross model with the
sequential
model so let's dive into the meat of the
matter let's do a and do a demo on here
uh today's demo in this demo we will be
performing flower classification using
sequential model and carass and we'll
use our model to classify between five
different types of
flowers now for this demo and you can do
this demo on whatever platform you want
or whatever um user interface for
developing python um I'm actually using
anaconda and then I'm using Jupiter
notebooks to develop in and if you're
not familiar with this um you can go
under environment once you've created
environment you can come in here to open
a terminal window and if you don't have
the different modules in here you can do
youra install or whatever module it is
um just happened that this particular
setup didn't have a caborn in it which I
already installed
uh so here's our anaconda and then I'm
going to go
back and start up my jupyter
notebook where I already created a uh
new uh python project Python 3 I'm in
Python 3.8 on this particular particular
one um sequential model for flowers so
lots of fun there uh so we're going to
jump right into this the first thing is
to make sure you have all your modules
installed so if you don't have uh numpy
pandas mat plot library in Seaborn and
the carass um and sklearn or sidekit
it's not actually sklearn you'll need to
go ahead and install all of those now
having done this for years and having
switched environments and doing
different things um I get all my imports
done and then we just run it and if we
get an error we know we have to go back
and install something um right off the
bat though we have numpy pandas matplot
Library Seaborn these are built on top
of each other pandas the data frame and
built on top of numpy the uh um data
array and then we bring in our sklearn
or S kit this is the S kit setup SCI uh
kit even though you use sklearn to bring
it in it's a s kit and then our carass
we have our pre-processing the images
image data generator um our model this
is our basic model our sequential
model uh and then we bring in from coros
layers uh import dents um
optimizers these optimizers a lot of
them already come in these are your
different optimizers and it's almost a
lot of this is so automatic now um atom
is the a lot of times the default
because you're dealing with a large data
uh and then we get our SGD which is uh
smaller data does better on smaller
pieces of data and I'm not going to go
into all of these uh different
optimizers we didn't even use these in
the actual demo you just have to be
aware that they are different optimizers
and the Digger the more you dig into
these models um you'll hit a point where
you do need to play with these a little
bit but for the most part leave it at
the default when you're first starting
out and we're doing just the sequential
you'll see here layers
dense and then if we come down a little
bit more uh when they put this together
and they're running the dense layers
you'll also see they have Dropout they
have flatten they have activation uh
they have the uh convolutional layer 2D
Max pooling 2D batch
normalization what are all these layers
uh and when we get to the model we're
going to talk about them uh a lot of
times when you're just starting you can
just uh uh import cross. layers and then
you have your Dropout your flatten uh
your convolutional uh neural network 2D
and we'll we'll cover what these do in
the actual example when we get down
there uh what I want you to take from
here though is you need to run your
Imports um and load your different
aspects of this and of course your
tensor flow TF because this is all built
on tensor flow and then finally uh
import random as RN just for random
generation and then we get down here
here we have our uh
CV2 that is your um open image or your
open CV they call it for processing
images that's what the CV2
is uh we have our
tqdm the tqdm is for um is a progress
bar just a fancy way um of adding when
you're running a process you can view
the bar going across in the Jupiter uh
setup not not really necessary but it's
kind of fun to have um we want to be
able to shuffle some files uh again
these are all different things pill is
another um image processor it goes with
the CV2 a lot of times you'll see both
of those and so we run those we got to
bring them all
in and the next thing is to set up our
directories and so when we come into the
directories there's an important thing
to note on here other than we're looking
at a lot of flowers which is fun
uh is we get down here we have our
directory archive flowers that just
happens to be where the different files
for different flowers are put in we're
noting an X and a z and the x is the
data of the image and the Z is the tag
for it what kind of flower is this uh
and the image size is really important
because we have to re size everything if
you have a neural network and if you
remember from our neural networks uh let
me flip back to to that
slide when we look at this slide we have
two input nodes here uh with an image
you have an input node depending on how
you set it up for each pixel and that
pixel has three different color schemes
usually in it sometimes four so if you
have a picture that's 150 by
150 uh you multiply 150 * 150 * 3 that's
how many nodes input layers coming in I
mean so this is a massive input a lot of
times you think oh yeah it's just a a
small amount of data or something like
that uh no it's a full image coming in
then you have your hidden layers A lot
of times they match what the image Siz
is coming in so each one of those is
also just as big and then we get down to
just a single output so that's kind of a
a thing to note in here what's going on
behind the scenes and of course each one
of these layers has a lot of processes
and stuff going
on and then we have our our different uh
directories on here let me go and run
that so I'm just setting the directories
that's all this is um archive flowers
Daisy sunflower tulip dandelion Rose uh
just our different directories that
we're going to be looking
at uh and then we want to go ahead and
we're need to assign labels remember we
defined x and
z so we're just going to create a uh uh
definition here um and the first thing
is uh return flower type
okay just returns it what kind of flower
it is I guess assign label to it uh but
we're going to go ahead and make our
train data and when you look at this
there's a couple things to take away
from here uh the first one is we're just
appending right onto our numpy array the
image we're going to let numpy handle
all that different aspects as far as 150
by 150 by 3 uh we just dump it right
into the numpy which makes makes it
really easy we don't have to do anything
funky on the processing and we want to
leave it like that and I'm going to talk
about that in a minute uh and then of
course we have to have the string a pin
the label on there and I want you to
notice right here uh we're going to read
the image
in and then we're going to size it and
this is important because we're just
changing this to 150 by 150 we're
resizing the image so it's uniform every
image comes in identical to the other
ones uh this is something that's so
important is um when you're resizing or
reformatting your data you really have
to be aware of what's going on with
images is not a big deal because with an
image you just resize it so it looks
squishy or spread out or stretched um
the neural network picks up on that and
it doesn't really change how it
processes
it so let's go ahead and run that uh and
now we've got our definition set up on
there and then we want to go ahead and
make our uh
training data uh so make the train data
uh daisy flower daisy directory uh print
length of X so here we go let's go and
run that and we're just loading up the
flower daisy uh so this is going all in
there and it's setting um it's adding it
in to the our setup on there to our x
and z setup and we see we have
769 um and then of course you can see
this nice bar here this is the bar going
across is that little added uh code in
there that just makes it really cool for
doing demos uh not necessarily when
you're building your own model or
something like that but if you're going
to display this to other people adding
that little what was it called
um tqdm I can never remember that uh but
the tqdm module in there is really nice
and we'll go ahead and do sunflowers and
of course you could have just uh created
an array of these um but this has an
interesting problem that's going to come
up and I want to show you something it
doesn't matter how good the people in
the back are or how good you are at
programming errors are going to come up
and you got to figure out how to handle
them uh and so when we get all the way
down
to the um where is it dandelion here's
our dandelion directory we're going to
build
um Jupiter has some cool things it does
which makes this really easy to deal
with but at the same time you would want
to go back in there depending on how
many times you R run this how many times
you pull this so when you're finding
errors uh I'm going in here there's a
couple things you can do and we're just
going to um oh it wasn't there it is
there's our error I knew there was an
error this processed
1,62 out of
10,065 now I can do a couple things one
I can go back into our definition and I
can just put in here try and so if it
has a bad conversion this is where the
errors coming from um uh just skip it
that's one way to do it um when you're
doing a lot of work in data science and
you look at something like this where
you're losing three points of uh data at
the end you just say okay I lost three
points who cares um or you can go in
there and try to delete it um it really
doesn't matter for this particular demo
and so we're just going to leave that
error right alone and skip over because
it's already added all the other files
in there and this is a wonderful thing
about Jupiter notebook is that I can
just continue on there and the x and z
which we're creating is still uh running
and we'll just go right into the next
flower row so all these flowers are in
there um that's just a cool thing about
Jupiter
notebook uh and then we can go ahead and
just take a quick look and
see what we're dealing with and this is
of course really when you're dealing
with other people and showing them stuff
this is just kind of fun where we can
display it on the uh plot Library here
and we're just going to go through and
um let's see what we got here uh looks
like we're going to do like five of each
of them I think is that how they set
this up um plot Library 5x two okay oh I
see how they did it okay so two each so
we have 5 by two set up on our axes and
we're just going to go in and look at a
couple of these
flowers it's always a good thing to look
at some of your data uh no matter what
you're doing
we've reformatted this to 150 by 150 you
can see how it really blurs this one up
here on the Tulip that is that resize to
150 by 150 um and these are what's
actually going in these are all 150 by
150 images you can check the dimensions
on the side and you can see uh just a
quick sampling of the flowers we're
actually going to process on here and
again like I said at the beginning most
of your work in data science is
reprocessing
this different uh information so we need
to go ahead and take our
labels uh and run a label encoder on
there and then we're just going to Le is
a label encoder one of the things we
imported and then we always use the fit
um to categorical y comma 5 uh X here's
our array um X so if you look at this
here's our fit we're going to transform
Z that's our Z array we
created um and then we have Y which
equals that and then we go ahead and do
uh to categorical we want five different
categories and then we create our x uh
inpay of x x = x over
255 so what's going on here there's two
different Transformations one we've
turned our categories into 0 1 2 3 4 5
as the output and we have taken our X
array and remember the X array is three
values of your different colors
this is so important to understand when
we do this across a numpy array this
takes every one of those three colors so
we have 150 by 150 pixels out of those
150 by 150 pixels they each have three
um color arrays and those color arrays
ra range from 0 to 250 so when we take
the xal X over
255 I'm sorry range from 0 to 255 this
converts all those pixels to a number
between 0o and one
and you really want to do that when
you're working with neural networks uh
now if you do a linear regression model
um it doesn't affect it as much and so
you don't have to do that conversion if
you're doing straight numbers but when
you're running neural networks if you
don't do this you're going to create a
huge bias and that means they'll do
really good on predicting one or two
things and they'll just totally die on a
lot of other
predictions so now we have our um X and
Y values uh X being the data in y being
our no one
output and with any good setup we want
to divide this data into our training so
we have XT train uh we have our X test
this is the data we're not going to
program the model with and of course
your y train corresponds to your X train
and your y test corresponds to your X
test the outputs and this is uh when we
do the train test split this was from
the S kit sklearn we imported train test
split and we're just going to go ahead
and the test size at about a quarter of
the data 0.25 and of course random is
always good this is such a good tool I
mean certainly you can do your own
division um you know you could just take
the first you know 0.25 of the data or
whatever do the length of the data not
real hard to do but this is randomized
so that if you're running this test a
few times you can kind of get an idea
whether it's going to work or
not sometimes what I will do is um I'll
just split the data into three parts and
then I'll test it on two with one being
the U or train it on two of those parts
with one being the test and I rotate it
so I come up with three different
answers which is a good way of finding
out just how good your model is uh but
for setting up let's stick with the XT
train X test and the SK learn
package and then we're going to go ahead
and uh do a random
seed uh now a lot of times the cross
actually does this automatically but
we're going to go ahead and set it up on
here and you can see we did an NP random
seed um from 42 and we get a nice RN
number um and then we do TF random we
set the seed so you can set your
Randomness at the beginning of your uh
tensor flow and that's what the tf.
random. set
is so that's a lot of prep um all this
prep and then we finally get to the
exciting part um this is where you
probably spend once you have the data
prepped and you have your pipeline going
and you have everything set up on there
this is the part that's exciting is
building these
models and so we look at this model one
we're going to designate it sequential
um they have the API which is a cross
the cross tensorflow API versus
sequential sequential means we're going
one layer to the next so we're not going
to split the layer and bring it back
together it looks almost the same with
the exception of um bringing it back
together so it's not a huge step to go
from this to an
API and the first thing we're going to
look at is um our convolutional neural
network in 2D uh so what's going on here
there's a lot of stuff that's going on
here um the default for well let's start
with the beginning what is a
convolutional 2d
Network well convolutional 2D Network
creates a number of small windows and
those small Windows float over the
picture and each one of them is their
own neural network and it's basically U
becomes like a
um a categorization and then it looks at
that and it says oh if we add these
numbers up a certain way uh we can find
out whether this is the right flower
based on this this little window
floating around which looks at different
things and we have filters 32 so this is
actually creating 32 Windows is what
that's
doing and the kernel size is 5x5 so
we're looking at a 5x5 Square remember
it's 150 by 150 so this narrows it down
to a 5x5 it's a 2d so it has your XY
coordinates um and when we look at this
5x5 remember each one of these is is
actually looking at 5x5
by3 uh so we're actually looking at 15
by 15 different um
pixels and padding is just um H usually
I just ignore that activation by default
is railu we went ahead and put the ru in
there there's a lot of different
activations Ru is for your smaller uh
when you remember I mentioned atom when
you have a lot of data data used in atom
kind of activation or using atom
processing we're using the railu here uh
it kind of gives you a yes or no but it
it doesn't give you a full yes or no it
has a um a zero and then it kind of
shoots off at an angle very common this
the most common wand and then of course
here's our input shape 150 by 150 by 3
pixels
and then we have to pull it so whenever
you have a two convolutional 2D um uh
layer we have to bring this back
together and pull this into uh neural
network and then we're going to go ahead
and repeat
this uh so we're going to add another
Network here one of the cool things if
you look at this is that it as it comes
in it just kind of automatically assumes
you're going down to the next layer and
so we have another convolutional null
Network 2D here's our Max pooling again
we're going to do that again Max pooling
uh and we're just going to filter on
down now one of the things they did on
this one is they changed the kernel size
they changed the number of filters and
so each one of these steps kind of looks
at the data a little bit differently and
that's kind of cool because then you get
a little added filtering on there this
is where you start playing with the
model you might be looking at a
convolutional no network which is great
for image classification
um we get down to here one of the things
we see is flatten so we had we just
flatten it remember this is 150 by 150
by 3 well and actually the pool size
changes so it's actually smaller than
that flatten just puts that into a 1D
array uh so instead of being you know a
tensor of this really complexity with
the the pixels and everything it's just
flat and then the
DSE is just another activation on there
um by default it is probably Ru as far
as its
activation and then oh yeah here we go
in sequential they actually added the
activation as railu so this just because
this is sequential this activation is
attached to the dents uh and there's a
lot of different activations but railu
is the most common one and then we also
see a soft Max uh soft Max is similar
but it has its own kind of variation and
one of the cool things you know what let
me bring this up because if we if you
don't know about these activations this
doesn't make
sense and I just did a quick Google
search on images of tensorflow
activations um I should probably look at
which website this is but this is the
output of the values uh so as your X as
it adds in all those uh weighted X
values going into the node it's going to
activate it a certain way and that's a
sigmoid activation and you can see it
goes between zero and one and has a nice
curve there this also shows the
derivatives um if we come down the seven
popular activation functions nonlinear
activations there's a lot of different
options on this let me see if I can find
[Music]
the oop let me see we can find the
specific to
railu so this is a leaky
railu and you can see instead of it just
being zero and then a value between uh
going up it has a little leaky there
otherwise your railu loses some noes
they just become inactive um but you can
see there's a lot of different options
here here's a good one right here with
the Rayo you can see the raayo function
on the upper on the upper left here and
then the Leaky Rayo over here on the
right which is very commonly used
also one of the things I use with
processing um language is the S is the
exponential one or the tangent H the
hyperbolic tangent because they have
that nice uh funky curve that comes in
that um has a whole different meaning
and captures word use better again these
are very specific to domain and you can
spend a lot of time playing with
different models for our basic model uh
we'll stick to the railu and the softmax
on here and we'll go and run and build
this
model so now that we've had fun playing
with all these different models that we
can add in there uh we need to go ahead
and have a batch size on here uh
128 epics
10 this means that we're going to send
128
uh rows of data or flowers at a time to
be processed and the Epic 10 that's how
many times we're going to Loop through
all the
data um and then there's all kinds of
stuff you can do again this is now built
into a lot of coros models already by
default
um so there's different ways to reduce
um the values and verbose verbose equals
one means that we're going to show
what's going on um value monitor what
we're monitoring we'll see that as we
actually train the model this is what's
what's going to come out of there if you
set the verbos equal to zero um you
don't have to watch it train the model
although it is kind of nice to actually
know what's going on
sometimes and since we're still working
on U bringing the data in here's our
batch side here's our epics we need to
go ahead and create a data generator uh
this is our image data
generator and it has all the different
settings in here almost all of these are
defaults uh so if you're looking at this
going oh my gosh this is confusing most
of the time you can actually just ignore
most of this um vertical flip so you can
randomly flip pictures you can randomly
horizontally flip them um you can shift
the picture around this kind of helps
gives you multiple data off of them uh
zooming rotation there's all kinds of
different things you can do with images
most of these we're just going to leave
as false we don't really need to do all
that um um set set up because we already
have a huge amount of data if you're
short data you can start flipping like a
horizontal picture and it will generate
it's like doubling your data almost um
so the upside is you double your data
the downside is that if you already have
a bias in your data you already have um
5,000 sunflowers and only two roses
that's a huge bias it's also going to
double that bias uh that is the downside
of
that and so we have our model compile
and this you're going to see in all the
carass we're going to take this model
here we're going to take all this
information as far as how we want it to
go and we're going to compile it this
actually builds the model and so we're
going to run that and I want you to
notice uh learning
rate very important this is the default
001 um there's there you really don't
this is how slowly it adjust to find the
right answer and and the more data you
have you might actually make this a
smaller number um with larger with you
have a very small sample of data you
might go even larger than that and then
we're going to look at the loss
categorically categorical cross entropy
most commonly used and this is uh how
how much it improves the model is
improving is what this number means or
yeah that's that's important on there
and then the accuracy we want to know
just how good our model is on the
accuracy and then uh one of the cool
things to do is if you're in a group of
people who are studying the model if
you're in shareholders you don't want to
do this is you can run the model summary
I do this by default and you can see the
different layers that you built into
this model just a quick summary on there
so we want ahead and we're going to go
ahead and create a
um we'll call it history but we want to
do a model fit
generator and so what this history is
doing is this is tracking what's going
on as while it fits the
model now there's a lot of new setups in
here where they just use fit and then
you put the generator in here um we're
going to leave it like this even though
the new default um is a little different
on that doesn't really matter it does
the same thing and we'll go ahead and
just run
that and you can see while it's running
right here uh we're going through the
epics we have one of 10 now we're going
through to 25 here's our loss we're
printing that out so you can see how
it's improving and our accuracy the
accuracy gets better and better and this
is 6 out of 25 this is going to take a
couple minutes to process uh because we
are training 150 by 150 by 3 pixels
across uh six layers or eight layers
whatever it was that is a huge amount of
processing so this will take a few
minutes to process this is when we talk
about the hardware and the problem s
that come up in data science and why
it's only now just exploding being able
to do neural networks this is why this
process takes a long
time now you should have seen a jump on
the screen here because I did uh uh
pause the recorder to let this go ahead
and run all the way through its epics
let's go ahead and take a look and see
what these epics are and um if you set
the verbos to uh zero instead of one it
won't show what's going on in the behind
the scenes as it's training it so when
we look at this epic 10 epics we went
through all the data 10 times uh if I
remember correctly there's roughly a gig
of data there so that's a lot of data
the first thing you're going to notice
is the 270 seconds um that's how much
each of those epics took to run and so
if you divide 60 in there you roughly
get about 5 minutes worth of each epic
so if I have 10 epics that's 50 minutes
almost an hour of
runtime that's a big deal when we talk
about processing uh in on this
particular computer um I actually have
what is it uh uh eight cores with 16
dedicated threads so it runs like a 16
core computer it alternates the threads
going in and it still takes it five
minutes for each one of these epics so
you start to see that if you have a lot
of data this is going to be a problem if
you have a number of models you want to
Fe find out how good the models are
doing and what model to use and so each
of those models could take all night to
run in fact I have a model I'm running
now that takes over uh takes about a day
and a half to test each model um it
takes four days to do with the whole
data uh so what I do is I actually take
a small piece of the
data test it out to find out uh get an
idea of of how the different setups are
going to do and then I increase that
size of the data and then increase it
again and I can just take that that
curve and kind of say okay if um the
data is doing this then I need to add in
more dense layers or whatever uh so you
can do a small chunks of data then
figure out what it cost to do a large
set of data and what kind of model you
want the loss as we see here continues
to go down uh this is the error this is
how much errors is in there it really
isn't a um userfriendly number other
than the more it Trends down the better
and so if you continue to see the loss
going down eventually it'll get to the
point where it stops going down and it
goes up and down and kind of waivers a
little bit that point you know you've
run too many epics you're you're
starting to get a bias in there and it's
not going to give you a good model fit
the accuracy just turns this into
something that uh we can use and so the
accuracy is what percentage of guesses
in this case it's categorical so this is
the percentage of guesses are correct um
value loss is similar you know it's a
minus a value loss and then you have the
value accuracy and you'll see the value
accuracy is pretty similar to the
accuracy um just rounds it off basically
and so a lot of times you come down here
and you go okay we're doing .5
6 7 and that is 70% accuracy or in this
case 68 uh 59% accuracy that's a very
usable number and it's very important to
have if you're identifying uh flowers
that's probably good enough if you can
get within a close distance and knowing
what flower you're you're identifying uh
if you're trying to figure out whether
someone's going to die from a heart
attack or not might want to rethink it a
little bit or Rey how you're building
your model so if I'm working with a uh
uh a group of um clients um shareholders
in a company or something like that you
don't really want to show them this um
you don't want to show them hey you know
this is what's going on with the
accuracy these are just numbers and so
we want to go and put the finishing
touches just like when you are building
a house and you put in the frame and the
trim on the house it's nice to have
something a nice view of what's going on
and so so we'll go ahead and do a pi
plot and we'll just plot the history of
the loss uh the history of the value
loss over here um epics train and test
and so we're just going to compute these
this is really important uh and what I
want you to notice right here is when we
get to about oh five epics a little more
than five six epics you see a cross over
here and it starts Crossing as far as
the um uh value loss and what's going on
here is you have the loss in your actual
model in your actual data and you have
the value loss where it's testing it
against the the test data the the data
wasn't used to program your model wasn't
used to train your model on and so when
we see this crossing over this is where
the bias is coming in this is becoming
overfitted and so when you put these two
together uh right around five and six
you start to see how it does this this
switch over here and and that's really
where you need to stop right around five
yeah six um it's always hard to guess
cuz at this point the model is kind of a
black box uh so but you know that right
around here if you're saving your model
after each run you want to use the one
that's right around five epics because
that's the one that's going to have the
least amount of bias so this is really
important as far as guessing what's
going on with your model and its
accuracy and when to stop uh it also is
you know I don't show people this mess
up here um I show somebody this kind of
model and I say this is where the
training and the testing comes in on
this model uh it just makes it easier to
see and people can understand what's
going
on so that completes our demo and you
can see we did what it we were set out
to do we took our flowers and we're able
to classify them uh within about you 68
70% accuracy whether it's going to be a
dollia sunflower cherry blossom Rose um
a lot of other things you can do with
your output as far as a uh different
tables to see where the errors are
coming from and what problems are coming
up and that's a wrap for our python AI
course 2023 Journey we hope you are as
accelerated as we are about the
boundless possibilities of AI with
python if you enjoyed this video don't
forget to smash that like button
subscribe and bring the notification
Bell to stay updated on all the latest a
advancements and if you have any
questions you can drop them in the
comment section below thanks for
watching and stay tuned for more from
Simply learn staying ahead in your
career requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here what