did you know that devops has become a
critical practice for organizations
seeking to improve their software
development and deployment processes
well according to a survey conducted by
statista the global devops market is
expected to reach 18 billion by 2023
growing at a compound annual growth rate
of 19 from 2018 to 2023. in recent years
the demand for devops professionals has
skyrocketed due to the increased
adoption of agile methodologies and the
need for faster and more efficient
software delivery as companies try to
stay competitive in today's digital
landscape they are turning to devops to
streamline their development and
operations workflow so at simply learned
we understand the importance of keeping
up with the latest trends and acquiring
the necessary skills in devops and
that's why we are excited to announce a
new video on devops Advanced full course
in this comprehensive video we will dive
deep into the world of devops covering
essential Concepts and Advanced
Techniques the course begins with an
introduction to devops and its
fundamental principles you will learn
how devops bridges the gap between
development and operations team enabling
seamless collaboration and continuous
delivery
we will explore popular devops tools and
Technologies like Docker kubernetes and
Jenkins providing you with Hands-On
demonstrations and real world examples
you will also discover the various
career opportunities available in the
devops field and the skills required to
excel as a devops engineer by the end of
this video you will have a solid
understanding of devops principles
methodologies and tools empowering you
to drive Innovation and deliver high
quality software Solutions so don't miss
out on this exciting opportunity to
level up your devops skills make sure to
subscribe to our YouTube channel and hit
the Bell icon to stay updated with the
latest content from Simply learn so
let's embark on this transformative
devops journey together get ready for
the devops advanced food course but
before that if you are planning to learn
about devops in a more detailed form and
want to square your skills with industry
standards then simply learns Catholic
postgraduate program in devops can be
the right choice for you this devops
course designed in collaboration with
Celtic ctme is a professional
development option that has Blended
learning combined with live online
devops certification classes with
interactive Labs that will give you
hands-on experience
so in this course you will learn how to
formalize and document development
processes and create a self-documenting
system devops certification course will
also cover Advanced tools like puppet
saw stack and ansible that help
self-governance and automation
management at scale so don't hold
yourself check out the course Link in
the description
also to support you and boost your
confidence here's a success story of one
of our Learners that will help you gain
insights over your career trajectory to
watch it
[Music]
hey there my name is Manuel canalon and
I'm Rafael Canada we're actually
brothers and we live here in Alberta
Canada we do a lot of things together
and this time we decided to take a
course in devops from Simply learn the
course proved to be very beneficial for
both our careers yeah I got a promotion
after the course and I go to hired with
a new company
I'm actually a mechanical engineering
graduate and I've been working at my
company for the last four years in 2020
I came to know that my company was
starting the hiring process for a devops
infrastructure team
at the time I was working as an
implementation specialist and my
colleagues recommend that I take a
course in devops if I wanted to be part
of the team so I started doing my
research and then he came to me saying
that he was also looking for a devops
course yeah funny enough I actually
found the course through Instagram but
you know my situation was slightly
different I had been working as a
geomatics field technician for about
four years but I wanted to change my
career into something that involved more
programming and automation so when I
came across with this course I said you
know what let's take it
and caltex name was definitely a
deciding factor in July 2021 we got our
certifications and I immediately posted
that on my LinkedIn profile you know
coincidentally in November of that same
year we got the good news
and I got hired as a GIS developer
which has allowed me to do way more
programming which is what I wanted to do
around that time I got promoted to the
devops infrastructure team and so at 21
salary hike the overall course was
awesome but the part that I enjoyed the
most was a Docker certification training
you know what I also really enjoyed the
docker certification training I think
there were brothers after all
yeah the experience was amazing and
being able to study together was
definitely very nice yeah it brought us
back to those good old days when we were
going to school together now what is the
purpose of Jenkins here now Jenkins is
normally a kind of a CI tool which we
use for performing the build automations
and the test cases automation there it's
one of the open source tool which is
available there and one of the most
popular CI tool also available into
their market now this tool makes it
easier for the developers to integrate
the changes to the project here so we
can easily integrate the changes and
whatever the modifications we want to
manage we will be able to do that with
the help of Jenkins
now Jenkins also achieves The Continuous
integration with the help of couple of
plugins each and every tool which you
want to integrate have its own plugins
which is available there for example you
want to integrate Maven we have a maven
plugin in Jenkins which you can install
you can configure in that case you will
be able to use the maven there now you
can deploy the mavener to build tool
onto the Jenkins server and then you can
prepare or you can configure any number
of Maven jobs in case of Jenkins
so uh what exactly the maven or the
Jenkins really do is the mavenwen
integrates with Jenkins through the
particular plugin so you can able to
automate the pills because for
automation the build you require some
integration with the maven and that
integration is what we are getting from
the maven plugin so in Jenkins you have
to install the maven plugin and once the
plugin is installed so what you can do
is that you can proceed with the
configurations you can proceed with the
setup and this a particular plugin can
help you to build out some of the Java
based projects which is available there
in the git repositories and once that is
done you will be able to go ahead and
you will be able to process a complete
integration of Maven within Jenkins
right so let's see that how we can go
for the integration now I have already
installed the maven onto the Linux
virtual machine which we are using so
using the app utility or using the Yum
utility you can actually download the
Jenkins package and the maven package
onto the server onto the virtual machine
and now I'm going to proceed further
with the plugin installation and the
configuration of a maven project so I
have a GitHub repository which is having
a maven project Maven uh source code and
the mavenized test cases over there so
let's see let's log into the uh Jenkins
and see that how it works
so this is the Jenkins interface which
we have over here now in this one what
we can do is that we can create some
Maven jobs over here and once those jobs
are created we will be able to do a
custom build onto of these Jenkins so
first of all we have to install the uh
particular plugin here
for that we have to go to the manage
Jenkins in manage Jenkins you have the
manage plugins option there so you have
to click on that now here you will be
having different tabs like updates
available installed Advanced all these
different tabs are available there so
what you can do is that you can click on
the available one when you go to the
available tab so what will happen that
here you can actually put up that what
exactly uh plugin you want to fetch here
so I can put a plugin called maven
now you can see that the very first one
the maven integration tool is available
so I'm going to select that particular
plugin and click on download now and
install after restart
now once that is done so what will
happen that the plugin will be
downloaded but in order to reflect the
changes we have to do a couple of
restart now for that you don't have to
go to the virtual machine you have the
option here itself that will allow you
to do the restart over here when you
click on this button so you check this
option and say that restart Jenkins when
the installation is done so what will
happen that the installation will be
automatically attempted whenever the
particular plugin installation is
completed here so you just have to
refresh the page again and you will be
able to see that the particular Jenkins
is being processed as such here
right so you can see that the screen is
coming up that Jenkins is restarting so
it will take a couple of five to six
seconds to do the restart and the login
screen to come up again over there
you can do the refresh also if you feel
automatically it will be reloaded once
the Jenkins is ready but sometimes we
have to refresh it so that we can get
the screen over there so once the login
is done so my Maven integration is done
so next thing which I will be doing is
that I will be creating a maven related
project so I'm going to put the admin
user
and the password so whatever the user
and password you have created you are
going to put that so that you will be
able to log into the Jenkins portal now
this is the Jenkins which is available
here so all you have to do is that you
have to click on create a new job or new
item so both the option is pretty much
same only
so here you will be able to see a maven
project here so I'm going to select like
maven build that's the name which I'm
going to give here and the maven project
I'm going to select here
and then press ok
now here you will be providing the first
of all the repository from which you
will be checking out the source code now
I can have a discard old builds over
here so if I feel that I want to have
like low rotation so all the previous
builds should be deleted so I'm just
saying that dates to keep build should
be 10 over here and the number of bills
which I need to keep over here is 20.
you can adjust these settings according
to your requirement but uh over here we
are you know doing a kind of
configurations which we are trying to do
a lot of configurations and settings
same so these are the particular
settings which we are looking forward as
such over here so now we are going to
have the log rotation here so we can
have it like how many days we want to
keep and how many number of bills we
want to keep here so both the values we
are providing over here and then now I'm
going to put the get uh integration here
like the repo URL so I have this
repository here in which I have the Java
source code and some uh particular uh
genuine test cases and all I also have
the uh particular source code and it's
kind of a maven project so that's what
I'm trying to clone over here with the
help of this plugin so this plugin will
download this repository it will clone
it on to the Jenkins server and then
depending on our integration with mav1
the maven build will be triggered here
so now I'm going to process with the uh
Maven here so you can see here that it's
saying that uh Jenkins needs to know
that where the maven is installed
because that Maven version it needs to
configure it needs to process on that
part so I'll just do the save over here
and or I can click on this tool
configuration so I'll just save or do
the apply click on this tool
configuration here
now here you have the options like where
you can have the jdk installation but
what happens that thing Jenkins is
running there so jdk is automatically
installed so in the tools configuration
you don't have to put the jdk
configuration but at least for the maven
configurations you have to provide that
where exactly the maven is available
there so I'm just saying that maven
three I want to process and the latest
moving Apache web server I want to
configure here so I just want to have
like I just want to save this settings
so that it will be automatically
download the latest version Apache 3.6.3
version there and that same should be
utilized over here in this case now I'm
just going to the maven Builder
configuration here and click on the
configure part so these git repository
is available here and in the build step
it automatically builds up that uh what
a maven environment you want to select
so you see that previously since I did
not configure my Maven environment so it
was throwing an error but once I have
configured that I have to download it
during the build process or before the
build that utility should be downloaded
so instead of doing the physical
installation of Maven on the server what
I have chosen over here is that I have
selected the particular version like I
have selected that a particular
3.6.3 version should be installed for
the maven purposes over here now once
the that is done I'm going to put the
particular
steps over here you can have it like
clean install you can have clean compile
test clean test or test alone you can
give it's just a part of the setup or
the goals which you want to configure
here it by default says that pom.xml
file is the current one in the current
directory you need to refer you need to
pick on that one what it's up to you
only that how you want to configure and
how you want to process as such this
information so according to your
requirement you can say that okay I'm
just want to go for these particular
goals and you can say like save over
here the particular configuration will
be saved now you can just click on the
build now and you will be able to see
that the first of all the git clone will
happen and then the desired Mi 1
executable will be uh the build tool
will be configured and according to that
it will be processed here so you can see
here that uh the maven is uh getting
downloaded it's getting configured here
and once it's configured because I have
explained over there that 3.6.3 version
I have to select so that specific
version will be configured and will be
picked up over here now even if you
don't have the maven installed on the
physical machine on which the Jenkins is
running still you will be able to do the
processing using this particular
component here so you can see here that
we have some particular test cases
executed and in the end we are able to
get a particular artifices also there
since I did not call upon the package or
install goal that's the reason why the
particular artifacts was not generated
War file or jar file whatever the
packaging mode is available at form
level but still what happens that my
test cases do gets executed and that's
what I have got over here in this case
so this is a kind of a mechanism where
we feel that how we can configure a git
repository once the git repository is
configured you are going to integrate
the maven plugin in the maven plugin you
are going to configure in the tools
configuration that this and so and so
version I want to configure to run my
build and once that is done after that
you just have to trigger the build and
click on the bill now option and once
that is done you will be able to get a
particular full-fledged build or
compilation happened onto the Jenkins
and this log will give you the complete
details that what are the different
steps which has happened on this one so
let's have a look at what we have in our
current environment so today when you
actually have your standard machine you
have the infrastructure you have the
host operating system and you have your
applications and then when you create a
virtual environment what you're actually
doing is you're actually creating
virtual machines but those virtual
machines actually are now sitting within
a hypervisor solution that sits still on
top of your host operating system and
infrastructure and with a Docker engine
what we're able to do is we're able to
actually reduce significantly the
different elements that you would
normally have within a virtualized
environment so we're able to get rid of
the the bins and the so we're able to
get rid of the guest OS and we're able
to eliminate the hypervisor environment
now this is really important as we
actually start working and creating
environments that are consistent because
we want to be able to make it so it's
really easy and stable for the
environment that you have within your
Dev and Ops environment now critical is
getting rid of that hypervisor element
it's just a lot of overhead so let's
have a look at a container as an example
so here we actually have a couple of
examples on the right hand side we have
different containers we have one
container that's running Apache Tomcat
with Java a second container is running
SQL server and microsoft.net environment
the third container is running python
with mySQL these are all running just
fine within the docker engine and
sitting on top of a host OS which could
be Linux it really could be any host OS
within a consistent infrastructure and
you're able to have a solution that can
be shared easily amongst your teams so
let's have a look at an example that
you'd have today if if a company is
doing a traditional Java application so
you have your developers working in
JBoss on his system and he's coding away
and he has to get that code over to a
tester now what will happen is that
tester will then typically in your
traditional environment then have to
install JBoss on their machine and get
everything running and cool and
hopefully set up identically to the
developer chances are they probably
won't have it exactly the same but
they're trying to get it as close as
possible and then at some point you want
to be able to test this within your
production environment so you send it
over to a system administrator who would
then also have to install JBoss on their
environment as well yeah this just seems
to be a whole lot of duplication so why
go through the problem of installing
JBoss three times and this is where
things get really interesting because
the challenge you have today is that
it's very difficult to almost impossible
to have identical environments if you're
just installing software locally on
Divine Aces the developers probably got
a whole bunch of development software
they could be conflicting with the JBoss
environment the tester has similar
testing software but probably doesn't
have all the development software and
certainly the system administrator won't
have all the tools that the developer
and tester have their own tools and so
what you want to be able to do is kind
of get away from The Challenge you have
of having to do local installations on
three different computers and in
addition what you see is that this uses
up a lot of effort because when you're
having to install software over and over
again you just keep repeating doing
really basic foundational challenges so
this is where Docker comes in and Docker
is the tool that allows you to be able
to share environments from one group to
another group without having to install
software locally on a device you install
all of the code into your Docker
container and simply share the container
so in this presentation we're going to
go through a few things we're going to
cover what Docker actually is and then
we're going to dig into the actual
architecture of darker and kind of go
through what Docker container is and how
to create a Docker container and then
we'll go out through the benefits of
using Docker containers and then the
commands and finalize everything out
with a brief demo so what is darker so
Docker is as you'd expect because all
the software that we cover in this
series is an open source solution and it
is a container solution that allows you
to be able to containerize all of the
necessary files and applications needed
to run the solution you're building so
you can share it from different people
in your team whether it's a developer a
tester or system administrator and this
allows you to have a consistent
environment from one group to the next
so let's kind of dig into the
architecture so you understand why
Docker runs effectively so the docker
architecture itself is built up of two
key elements there is the docker client
and then there is a rest API connection
to a Docker Daemon which actually hosts
the entire environment within the docker
host and the docker demo you have your
different containers and each one has a
link to a Docker registry the docker
client itself is a rest service so as
you'd expect a rest API and that sends
command line to the docker Daemon
through a terminal window or command
line interface window and we'll go
through some of these demos later on so
you can actually see how you can
actually interact with Docker the docker
Dem then checks the request against
um the docket components and then
performs the service that you're
requesting now the docker image itself
all it really is a collection of
instructions used to create container
and again this is consistent with all
the devops tools that we have the devops
tools that we're looking to use
throughout this series of videos are all
environments that can be scripted and
this is really important because it
allows you to be able to duplicate and
scale the environments that you want to
be able to build very quickly and
effectively the actual container itself
has all of the applications and the
dependencies of those applications in
one package you kind of think of it as a
really effective and efficient zip file
it's a little bit more than that but
it's one file that actually has
everything you need to be able to run
all of your Solutions the actual Docker
registry itself is an environment for
being able to host and distribute
different Docker images among your team
so say for instance you had a team of
developers that were working on multiple
different solutions so say you have a
team of developers and you have 50
developers and they're working on five
different applications you can actually
have the applications themselves the
containers shared in the docker registry
so each of those teams at any time check
out and have the latest container of
that latest image of the code that
you're working on so let's dig into what
actually is in a container so so the
important part of a docking container is
that it has everything you need to be
able to run the application it's like a
virtualized environment it has all your
Frameworks and your libraries and it
allows the teams to be able to build out
and run exactly the right environment
that the developer intended what's
interesting though is the actual
applications then will run in isolation
so they're not impacting other
applications that using dependencies on
other libraries or files outside of the
container because of the architecture it
really uses a lot less space and because
it's using less space it's a much more
lightweight architecture so the files
and the actual folder itself is much
smaller it's very secure highly portable
and the boot up time is incredibly fast
so let's actually get into how you would
actually create a Docker container so
the docker container itself is actually
built through command line and ninja
it's built of a file and Docker image so
the actual Docker file is a text file
that contains all the instructions that
you would need to be able to create that
Docker image and then we'll actually
then create all of the project code with
inside of that image then the image
becomes the item that you would share
through the docker registry you would
then use the command line and we'll do
this later on select Docker run and then
the name of the image to be able to
easily and effectively run that image
locally and again once you've created
the document you can store that in the
docker registry making it available to
anybody within your network so something
to bear in mind is that Docker itself
has its own registry called Docker Hub
and that is a public restrict so you can
actually go out and see other Docker
images that have been created and access
those images as your own company you may
want to have your own private repository
so you want to be able to go ahead and
either do that locally through your own
Repository or you can actually get a
licensed version of Docker Hub where you
can actually then share those files now
something that's also very interesting
to know is that you can have multiple
versions of a Docker image so if you
have a different version control
different release versions and you want
to be able to test and write code for
those different release versions because
you may have different setups you can
certainly do that within your Docker
registry environment okay so let's go
ahead and we're going to create a Docker
image using some of our basic Docker
commands and so there are essentially
really you know kind of just two
commands that you're going to be looking
for one is the build command another one
is to actually put it into your registry
which is a push command so if you want
to get a image from a Docker registry
then you want to use the pull command
and a pull command simply pulls the
image from the registry and in this
example using nginx as our registry and
we can actually then pull the image down
to to our test environment on our local
machine so we're actually running the
container within our Docker application
on a local machine we're able to then
have the image run exactly as it would
in production and then you can actually
use the Run command to actually use the
docker image on your local machine so
just a you know a few interesting
tidbits about the docking container once
the container is created a new layer is
formed on top of the docker image layers
called the container layer each
container has a separate read write
container layer and any changes made in
that docking container is then reflected
upon that particular container layout
and if you want to delete the container
layer the container layer also gets
deleted as well so you know why with
using Docker and containers be of
benefit to you well you know some of the
things that are useful is that
containers have no external dependencies
for the applications they run once you
actually have the container running
locally it has everything it needs to be
able to run the application so you
there's no having to install additional
pieces of software such as the example
we gave with JBoss at the beginning of
the presentation now the containers are
really lightweight so it makes it very
easy to share the containers amongst
your teams whether it's a developer
whether it's a tester whether it's
somebody on your operations environment
it's really easy to share those
containers amongst your entire team
different data volumes can be easily
reused and shared among multiple
containers and again this is another big
benefit and this is a reflection of the
lightweight nature of your containers
the container itself also runs in
isolation which means that it is not
impacted by any dependencies you may
have on your own local environment so
it's a completely sandboxed environment
so some of the questions you might ask
us you know can you run multiple
containers together without the need to
start each one individually and you know
what yes you can with Docker compose
docking compose allows you to run
multiple containers in a single service
and again this is a reflection on the
lightweight nature of containers within
the docker environment so we're going to
end our presentation by looking at some
of the basic commands that you'd have
within Docker so we have here on the
left hand side we have a Docker
container and then the command for each
item we're actually going to go ahead
and use some of these commands and the
demo that we're going to do after this
presentation you'll see that in a moment
but just you know some of the basic
commands we have are committing the
docket image into the Container kill is
a you know standard kill command to you
know terminate one or more of the
running containers so they stop working
then restart those containers but
suddenly you can look at all the image
all of the commands here and try them
out for yourself so we're going to go
ahead start a demo of how to use the
basic commands to run Docker so to do
this we're going to open up terminal
window or command line now depending
whether you're running Linux PC or Mac
and we're going to go ahead and the
first thing we want to do is see what
our Docker image lists are so we can go
sudo Docker images and this will give us
well first we'll enter in our passwords
let's go enter that in and this will now
give us a list of our Docker images and
here are the docker images I have
already been created in the system and
we can actually go ahead and actually
see the processes that are actually
running so I'm going to go ahead and
open up this window a little bit more
but this will show you the actual
processes and the containers that we
actually have and so on the far left
hand side you'll see under names we have
learned simply learn be unscore cool
these are all just different ones that
we've been working on so let's go ahead
and create a Docker image so I'm going
to do sudo
docker
run
Dash Dash p
0.0.0.0 go on a t colon 80.
Ubuntu and this will allow us to go
ahead and run an Ubuntu image and this
will run the latest image and what we
have here is a hash number and this hash
number is a unique name that defines the
container that we've just created and we
can go ahead and we can check to make
sure that the container actually is
present so we're going to do
pseudo.co.ps and this actually show us
down there so it's not in a running
state right now but that doesn't mean
that we don't have it so let's list out
all the containers that are both running
and in the exits see so let's do sucker
PS Dash a and this lists all the
containers that I have running on my
machine
and this shows all the ones that have
been in the running State and in the
exit State and here we see one that we
just created about a minute ago and it's
called learn
and these are all running Ubuntu and
this is the one that we had created just
a few seconds ago
let's open it up and
there we go so let's change that to that
new Doc container to a running state so
scroll down and we're going to type sudo
docker
run
Dash it Dash Dash
name my
um so this is going to be the new
container name it's going to be my
Docker so this is how we name our Docker
environment
and we'll put in the image name which is
Ubuntu
and dash bin Dash Bash
and it's now in our root and we'll exit
out of that
so now we're going to go ahead and start
the new my Docker container so sudo
docker
start
my
and we'll get the container image which
will be my docker
my docker
return and that started that Docker
image and let's go ahead and check
against the other running Docker images
to make sure it's running correctly so
sudo docker
PS
and there we are underneath name on the
right hand side you have to see my
Docker along with the other Docker
images that we created and it's been
running for 13 seconds
quite fast so we want to rename the
container let's use the command sudo
docker
rename we can take another Docker image
this to scrub this one and we'll put it
in rename
and we'll rename and put in the old
container name which is image and then
we'll put in the new container name and
let's call it
purple
so now the container image that had
previously been called image is now
called Purple
so do sudo Docker PS
to list all of our Docker images
and if we scroll up and there there we
go purple
how easy is that to rename an image
and we can go ahead and use this command
if we want to stop container so we're
going to write sudo docker
stop
and then we'll have to put in the
container name
and we'll put in my Docker the container
that we originally created
and that image has now stopped
and let's go ahead and prove that we're
going to list out all the docker images
and what you see is that it's not listed
in the active images it's uh not on the
list on the far right hand side
but if we go ahead and we can list out
all of the docker images so you actually
see it's still there as an image it's
just not in an active stage it's what's
known as in an exit state
so here we go
and there's my Docker it's in an exited
state so that happened 27 seconds ago
so if we want to to remove a container
we can use the following command
so pseudo docker
RM
for remove
my docker and that will remove it from
the exited state
and we're going to go ahead and we're
going to double check that
and yep
yep that's not not listed there under
exit State anymore
it's gone
and there we go
there that's where it used to be all
right let's go back
so if we want to exit a container in the
running state so we do sudo kill and
then the name of the container
I think one of them is called yellow
let's just check and see if that's going
to kill it
oops no I guess we don't have one called
yellow so let's find out name of
container that we actually have
so sudo Docker kill oh we're going to
list out of the ones that are running oh
okay there we go now yellow isn't in
that list so let's take I know let's
take simply learn and so we can actually
go ahead and let's write sudo Docker
kill simply learn
and that will actually kill an active
Docker container
boom there we go
and we list out all the active
containers you can actually see now
that's they simply learn container is
not active anymore
and these are all the basic commands for
Docker container
so what we're going to do in this
session is we're going to go through
what Docker and Docker container is and
why you'd want to use it we'll then
break out what are the characteristics
and structure of a Docker swarm
environment and then finally we'll go
through and do a demo using actual code
in the command line interface to
actually run and build out the main
functions and commands that you would
use for a Docker swarm environment so
with that said let's get started so
Docker is a tool that we actually use to
help automate applications in our
environment so that we have a consistent
way that we're actually building and
testing and validating codes that's been
created think of it as a virtualization
but much more sophisticated and
efficient way of doing virtualization
the thing that you have that's a huge
Advantage when you're working with
Docker is that it's a very lightweight
software package it allows you to have
all of the tools needed for actually
running the code without having to have
the significant overhead that you would
normally have with virtualization so one
of the characteristics you have with
Docker is that you can actually run
multiple Docker environments in the same
host environment and for you to be able
to do this we had to figure out how this
would actually run and so to actually do
this we actually do what's called a
Docker swarm and a Docker swarm is an
environment where you can actually have
multiple Docker images running on the
same host operating system so you can
have running in each of those Docker
containers different solutions so let's
dig into what a Docker swarm is so a
Docker swarm is essentially a tool that
allows you to very easily create and
schedule multiple Docker nodes so really
two or more Docker nodes and you can
have quite a large number of Docker
nodes in a single swarm each node itself
is actually a Docker demon and that
demon is able to interact with the
docker API and have all the benefits of
being a full Docker environment one of
the other advantages you have is that
the each duck container within the Swarm
can then be deployed and managed but as
a node in that entire clustered
environment so what we have here is a
breakdown of the five key elements
within a Docker environment you have the
docker container you have the Daemon the
docker images and the docker client and
Docker registry so the docker Daemon
itself is what does all the work
interacting with the actual host
operating system to be able to manage
the docker containers so if we look here
here we have set up an environment where
we have three docket container being run
with Docker and what we want to be able
to do is be able to interact with the
environment because what would happen if
we actually have something changing our
environment so we have the environment
set up there's a Docker swarm and one of
our containers fails what we're able to
do is use the Swarm to be able to
correct that failure so the docker swarm
manager is able to come in and
reschedule containers and as you would
imagine the actual swarm note has full
backups and full redundancy for any kind
of failures that would happen and we
would do all of this work through
command line interface so let's go
through some of the features that you
have within the actual Docker swarm
itself so a key feature for Docker swarm
is that it is fully decentralized which
means that it makes it very easy for
teams to be able to access and manage
the environment as you would expect as
well is that the communication that
happens between the manager and client
notes within the swan is highly secure
and of course this is really just a
fundamental that you should have for any
kind of solution but it is good to know
that Docker swarm has that built in
there is also Auto load balancing within
your environment you can actually script
that into how you write out and
structure your swarm environment and
that load balancing then also allows you
to then convert that swarm environment
into a highly scalable infrastructure
and then rollback task allows you to be
able to roll back environments to
previous safe environments so say
something does get pushed out and
something breaks you're able to
immediately roll back into a safe
environment so each of the containers
are pushed out and and are controlled
using services and they actually happen
to be breast Services which make it very
easy for you to be able to integrate
within your environment and each of the
different Services contains a group of
containers of the same image Now by
having this structure it allows you to
be able to scale your application
appropriate to the demands on your
server so if you have a service that
needs to have a significantly larger
number of services for it to run you can
actually scale that appropriately and
they can be either Geo or demand based
one of the requirements for setting up a
double swarm is you must have at least
one node deployed so the way that the
architecture is set up is that you have
a manager node and a client node and
there must be one of each for the entire
environment to be able to work
effectively so you know here we are just
jumping ahead of sales a little bit we
have two types of nodes in a Docker
swamp we have the manager node and we
have the worker node which is the client
that does the actual execution of the
tasks the manager knows very similar to
other systems that we've talked about on
simply learn allows you to actually
control and manage the actual tasks that
are being executed by the worker nodes
and the working nodes as you can imagine
and then actually execute the
instructions that the manager are
sending out to it so here we have a
situation where we can illustrate what
would happen with a manager node sending
out commands to different worker nodes
the manager is fully aware of the staff
matters of the entire swarm environment
at all times this is because of the
two-way secure communication that's
going from the manager to the worker
environment the workers as you'd expect
are accepting tasks that are being sent
from the manager so the manager sends
out a task saying that you need to be
running as a MySQL environment then the
work and node will then convert to a
MySQL environment and all of these
environments are scripted and controlled
by you as the manager the actual worker
nodes themselves actually have a client
agent and that client will then
communicate all different states of the
infrastructure back to the manager so
that anytime the manager node is in full
control of the entire swarm the major is
the controller of this environment and
so you always want to be able to ensure
that the manager has full access to all
the work that's happening within the
Swarm and this allows you as the manager
to be able to control your swarm and
very quickly be able to react to any
changes without having to rely on manual
installations of software and hardware
and as we covered earlier on there is a
rest API that uses the communication
over HTTP from the manager to the worker
node it's interesting to note that it is
a rest API because if you wanted to you
could actually integrate that API into
custom applications and even then create
automated Docker images to be created on
demand from third-party solutions that
you may want to create so one of the
things that's a really a big Advantage
with having a swarm is that once you've
actually created the form any of the
services that you create can be accessed
by any node of the same cluster one of
the things you do have to do though is
you have to specify what container image
that you're going to use when you're
creating a new servers and you can do
that either through a centralized Docker
Hub environment or through your own
private Docker Hub environment one of
the things that's interesting is that
you can set up commands and services to
be either Global or replicated a global
Service will allow you to run a service
consistently on every node within the
Swarm whereas a replicated service will
only push out functionality and tasks to
specific worker nodes within a swarm so
you may be asking yourself hey is it a
service and task the same thing no that
they're kind of not in the docker world
and the difference is is a service is a
description of a task for the state
whereas the actual task is the work that
needs to get done and that's the
differentiator here so what you can do
as a Docker user is you actually create
services and then you can then Define
when you want them to start as tasks now
what's interesting is that when you do
assign a task to a node that same task
cannot be assigned to another node also
what's interesting is that you can
actually have multiple managers within a
Docker swarm environment if you do
however go down this path you have to
elect one manager to be the primary
manager and the other managers to be
secondary managers in many ways those
secondary managers are really similar in
concept to worker nodes in which they
have the capability of a manager but
they are dependent on that single
primary manager to be able to provide
the right instructions and for services
and tasks to the entire swarm
environment so if we kind of recap some
of these we have a command line
interface which allows us to create and
connect via apis and that those apis
that we connect to in our soil
environment allows us to do
orchestration via tasks and services the
task allocation allows us to allocate
work to tasks via their IP address which
allows them to execute them on the work
and then the worker nodes themselves
have to connect to the manager node to
be able to check for when tasks come in
so that they are are keeping a
consistent communication back and forth
across the entire swarm and then the
final stage is to actually execute the
tasks that have been assigned from the
manager node to the working node so that
you have a successful execution of the
solution you're looking to build all
right so with that said let's go ahead
and we're going to do a demo so we're
going to go ahead and do a demo and
showing how you can run with Docker Swan
for this you'll need to have both a
virtual environment running your manager
and you're working in environment so
here we have our worker and here we have
our manager so we're just going to open
up terminal window and we're going to go
into the manager for terminal window so
the following command here is used to
initialize Docker Swan which is going to
type this in so sudo Docker swamp in it
and we purchased the IP address for the
network we're connecting to
open in our password
and here we have now connected
and this is our manager node and we want
to be able to highlight is the specific
token which identifies the docker swarm
environment and this is our token right
here so we're going to copy that
because we'll need to use that for our
workout environment so here we are in
the worker environment and we'll use the
token that we just copied
um as a way to be able to connect to the
manager and connect to the docker swarm
environment
foreign
we'll join that I will now here we shows
that we have joined the Swarm
environment as a worker
and now we're in the manager we can
actually show that we actually have
that worker in the environment so
there's Docker node LS to list out all
of the
items in that environment
sorry we actually did that in the wrong
area so we're on the right pseudo Docker
node allows to list out all the nodes in
the Swarm and there you are you see
we've actually connected the worker to
the manager node and both are active and
the manager is the leader and the worker
virtualbox is in the Swarm
okay we're going to go ahead and create
a new service so Dr Swann create and
we're going to just change the name to
hello world for the new service and
we'll use the Alpine image from
docker.com
takes just a moment for it to run
there we go
and there we go Services converged
so what we have now is a new service
that has been created
let's go ahead and we'll list out the
services
and here we have our new Hello World
Service
and we go ahead and check the docking
containers and we're going to do Docker
PS and
here's our Alpine it's running the
latest image
and that's the docker container ID
and we can see that it was just updated
and so we're going to do is use the same
command in the worker node
and make sure everything is working over
there
so go over to our worker paste that in
so what we see here is there is no image
or container created because it's not in
the worker node it would be in the
manager node
so we can go ahead and create the new
service
and we'll make the Mode Global and that
way the surface is available across all
of the Swarm
so we're just going to execute that work
takes a moment
and
there we are
done work has been completed and we have
the two different IDs which shows that
two different Services running and if we
go back to the working mode and we run
sudo.gov PS again and there we are now
we see that we have the hello world
new virtual environment has been added
with the image of Alpine and we also
have the new token ID for that
if we want to kill a node from the Swarm
we can use the following command which
is suitable Docker swamp leave dash dash
force and that will force the node to
leave the Swarm so in this session what
we're going to do is we're going to go
through what Docker is and then by
extension what Docker compose is and how
it improves the performance of Docker
what the benefits of working with Docker
compose is we are going to cover as well
what the difference between Docker
compose and Docker Swan is because the
two products are similar but they have
two different types of job and at the
end of this presentation we'll take you
through some basic commands that allow
you to get up and running working with
Docker and Docker compose finally after
this presentation we'll do a brief demo
that allows you to get up and running
using some of these commands to do your
own Docker compose environment so let's
get started with Docker compose so first
of all we have to kind of cover what
Docker is so Docker has come as an
evolution of technologies that started
with virtual machines VM machine and a
VM machine allows you to run at a
complete operating system and complete
operating system and its environment on
your own computer in your own operating
system without interfering that the
problem was is that it was very heavy
and complex and in contrast Docker is a
much more lightweight and more flexible
environment that you can automate with
scripts so we have to remember that the
container itself is just software and it
has dependencies that run within that
application that are managed by a Docker
so if we have a look at the way Dockers
broke now first of all we have the
docker image and we have the docker
image created by a Docker file and the
docker file is a text-based script and
that text file within Docker file
actually has all the commands that allow
you to run the appropriate Technologies
within the docker image so the docker
image allows you to be able to have a
wide group of people and consistently
run the same environments and it if you
want to be able to share that Docker
image effectively you would do so by
uploading to Docker harp and Docker Hub
can be available either publicly through
the public Docker Hub website or you can
actually set up your own private
Enterprise Docker Hub to share your own
images across your own network and then
from Docker Hub people can then pull
their own Docker image and build new
containers that are consistent on the
original Docker container that you've
created so it's a great way of being
able to have your entire team scale up
easily and effectively so one of the
questions that will come up though is
what if you want to use two containers
in a single service and an example of
this might be like a web server that is
using a database but the database may be
in one container and the web server may
be in another container which is very
typical for your own web and database
infrastructure so very similar I'm
scenario but how would you do this with
dark app so by using just Docker itself
it's actually very difficult to do this
you can do it but it's just very time
consuming and just not a very good use
of your productivity but when Docker
compose you can actually do the whole
process quite quickly and easily and
hence the reason why we have Docker
compose to allow you to have two or more
Docker containers running in the same
environment being able to communicate as
if they were running in a production
environment so let's dig into what
Docker compose is so let's consider a
scenario that you'd have today so Mitra
is a fashion website similar to Amazon
and you go to Mantra with your web
browser and on that website you would go
through a number of activities such as
logging into your account browsing a
catalog you'd have your checkout
application server and you go through
the table process of building out an
application behind each of these
services are different products such as
you know to have an account database
base you have a product database
different checkout processes and these
would all be run behind the scenes and
each of these can be considered a micro
service so the more microservices you
build into your environment the more
complexity you're adding and the more of
value it would be to have each of these
services in their own container but as a
developer you want to be able to jump
from one container to another container
so your login account can then be passed
on to your product catalog and then be
able to move through the whole process
so the environment that you would see
today would be small like what we're
showing right now where you'd have your
server and your database and you may
have a server running on an Apache
Tomcat or a SQL Server you'd have
different databases and you'd want to be
able to have each of those in their own
containerized environment so the way
that Docker compose runs is that it
works as a single service so you have
the docker compose running multiple
containers but the perception is of a
single service run the thing that's
great though is that each of those
containers will run in their in
isolation and but they can interact with
each other so unlike a normal Docker
environment where you have multiple
Docker images running completely in
isolation of each other these are images
that will run in isolation but can
interact with each other the dock
compose files are very easy to write
they're all written with a scripting
language called yaml and yaml is an XML
based language it actually stands for
yet another markup language but it's an
XML based language and it's very easy to
use the great thing about learning yaml
is that a lot of Open Source tools in
the devops environment use yaml as the
scripting lane and then finally in
Docker compose as a user you can
actually go in and Trigger all your
services within the containers to start
with a single camera on as you can
imagine this dramatically reduces the
amount of work that you as a developed
other have to do so let's take an
example of if you're running a container
one with a nijex server and one with a
redis database uh you can create a yaml
Docker compose file that actually has
the instructions on the containers
needed to build out both environments
and then you can run each of those
environments separately but have them
connected as a single service so the
benefits of dock and compose is that you
have a single host deployment
environment you can run this all on one
piece of Hardware you can have a quick
and easy configuration using yaml
scripts the productivity that you get as
a developers significantly increases
because you're not having to have time
wasted trying to configure just
traditional Docker containers by
themselves you now have a way of being
able to interact with those containers
and then finally security is at the
center of all of these containers so
each of those containers are completely
isolated from each other and they are
controlled with the same level of
security that you'd have with a
traditional Docker image but there is a
question here around Docker compose
isn't it similar in concept to Docker
Swan because at a high level it kind of
seems like both do the same thing and
the answer is well actually no they're
different there are similarities but
there are also differences and
particularly for a developer these
differences will really start coming out
in scale as you start working on your
Solutions so let's look at Docker
compose right so Docker compose will
allow you to create multiple containers
on a single host and that's the
important part of being a single host
which is maybe your development PC with
Docker swarm it also allows you to
create multiple contains however you
have to manage those multiple containers
on multiple hosts which makes a lot of
sense if you're running an operations
environment but not so much if you're
doing a development environment and
Docker compose is scripted with yaml
which is very easy for you to be able to
control your scripts Docker swarm
doesn't have a scripting technology like
yaml so it's a little bit harder to work
with so what we have here is just some
basic commands that allow you to get up
and started working with Docker compose
in our next demo we're actually going to
take you through how to use all of those
commands so we're going to go ahead and
do a demo on how to use Docker compose
and the first thing you want to do is
check that you have Docker installed so
what we're going to do is we're going to
open up the command window and we're
going to type in Docker you'll be able
to see now that we have Docker installed
if you don't have dog installed go ahead
and install Docker you can go to Google
Search to install Docker and it'll give
you the instructions for that we also
have videos that we've already done if
once you have dock installed then you
want to do a Google search on the docker
compose or go to the docker.com website
and select for the compose those section
and look for the install Docker compose
instructions so we already have another
tab which has the instructions already
installed you'll see on the left hand
side there are a number of Docker tools
you want to expand the docker compose
section select install compose and then
on the right hand side you'll have
different install options for Mac
windows and Linux if you're installing
on those platforms make sure you're
copying the correct command line so
there's a couple command ones you want
to copy copy the first one over and we
paste that in and it's going to go ahead
and download all of the files for
compose it takes a while but we have to
get another file so while it's
downloading we'll go get the other
command line just go back and grab it
real quick it's a little faster but
we'll copy that over there was a second
command instruction and post that
command yep there's the command right
there and that will apply the
appropriate binary sorry and once you've
done that you can actually go ahead and
validate that you have the correct
version of Docker compose installed and
for that you want to write the command
Docker Dash compose space dash dash
version and that will then give you a
version build once you know that you
have a version build that means that you
have everything installed correctly on
your machine so you can see here that we
have a version number which means we
have everything installed you might see
a different version number depending on
how old the video is when you're
watching it but they're using very
content at the moment and just go ahead
and clear the screen and now what we're
going to do is create a folder where
we're actually going to install the yaml
file that will have the instructions for
uh docking compose environment so we're
moving the cursor to the desktop and on
the desktop we're going to create a new
folder and using the MK derive command
and we'll call this one Docker compose f
for file and then we'll move the cursor
into that new folder by using CT command
and now we are in the folder called
Docker compose F and you can go look at
your desktop and you'll see it's there
and now you want to select and pound
touch Docker Dash compose Dot yml and
they'll actually create the new yaml
file for you and you can use the ls list
command LS Docker Dash compose.yaml and
they'll allow you actually now to step
in and edit that file so let's go ahead
and hit return and they'll allow us to
go in and edit the file so now I'm in
the editor and I'm going to write some
instructions
um so I'm I'm going to put a couple of
mistakes in these instructions so we can
see how Docker pose catches those
mistakes and so but most this is going
to be fairly correct so you can just
type this out so we're going to start
with Services colon then we do web colon
and then we create image colon and we'll
do ninjix and then we'll do a database
colon and and we call the image redis
and so what you can see because we have
two reference images we're actually
calling two different images two
different containers that will be
created by Docker Hub uh so what we're
going to do is uh where we can actually
show you um how that's pulling in you
can go to Docker Hub and you can
actually see the references to the
images that we've just created so go to
Docker Hub which is Hub hub.docker.com
and you'll see that uh ninjix redis
those are the default names for the
images so you could do other images like
HTTP for Apache web server if you wanted
to there are a lot of images in Docker
Hub so certainly have a lot of fun
building uh tools from that environment
so we're going to save those files in
the dock image and we're just going to
do colon WQ to exit this screen I save
the text and let's go and run our yaml
file and you'll see the two different
different error messages that will come
up so we're going to do cat Docker Dash
compose Dot yml and here we have a
display of the text that we just wrote
and now let's go check the validity of
the file that we just created so we'll
do Docker Dash compose config and we
should get an error message and there we
are error message and the reason why we
have an error message is because the
spacing is not correct in the yaml file
itself so let's go to the actual yaml
file that we created let's go through
and we're going to use uh just the file
finder and you'll actually open up the
yaml file in your text and see your
favorite text editor and here we have
the code that created it and let's use
the correct spacing so that everything
is spaced out appropriately in the image
and so this is the spacing that you'd
expect to see a space a line between
surfaces and the first image and the
first image is a tab in and then it
refers what the image is and let's go
ahead and clear the screen and we'll see
whether or not we've got that all
correct this clear screen and we're
going to check that everything is
running correctly and we're seeing we're
still getting an error message and the
reason why we're getting another error
is that there's still a config error in
this the actual yaml file so one of the
things you have to do when you're
creating a yaml file for Docker
composers you actually have to have the
appropriate version number in there so
what we have to do is we have to go and
find which is the appropriate version
and there are three major versions one
two and three depending on the age of
Docker that you have installed on your
computer so if you have the very latest
version of Docker 18 and newer you would
put 3.7 as the version number and what
we're going to do is we're going to just
check on the version that we have
installed and so just clear the screen
and then we'll just do Docker version
and what we have is our version of
Dockers 7
18.12 now this is the version of Docker
not Docker compose it's your main Docker
environment and so if we go back to our
web browser we'll actually see that
version 17 should align with version
number 3.5 yep there we are version 3.5
so what we want to do is go back into
our yaml file in the very first line
that we want to put in above the
surfaces is the version number and that
version number we're going to put in is
3.5 so we type in version version and
we'll put colon and then version number
is in in quotes and you save that and
now what we can actually do is we can
actually go ahead and use the single
line instruction to run both of these uh
at two images that we've created because
everything is configured correctly or
just one note you'll see that the
version number is actually at the bottom
of the list and that's just the way
Docker compose words it pulls out the
services and the images that you'd
create and then put the image the
version number at the bottom of the list
so in the presentation we talked about
how you can use one line to trigger two
or more images and so to do that we're
going to do sudo Docker Dash compose D
and it's going to create the two
environments we have the web environment
and the database environment and let's
use pseudo Docker PS to actually list
out that all the processes that are
running right now and there we are we
actually see all of the images are
running we have the names the commands
the images and everything's up and
running and if we want to close both of
those images we can actually use a
single line of command as well and that
command will be pseudo Docker Dash
compose down and that closes both of the
images and we can see that all the
databases and the web server says images
have been closed and sudo Docker PS will
actually lists that there is nothing
running and that's how to use Docker
compose how the docker writes an angular
application my name is Miro Uso and in
the next few minutes I'll be working
YouTube building a Docker image for an
angular application and then running the
image locally on your Workstation
as angular is a front-end framework
we'll be using a web server called nginx
to serve the application HTML files
after this session you will understand
three things first what is Docker and
what is it used for you will understand
the basics of Docker and what problem
Docker fixes
next you'll learn how to write a Docker
file which you can think of as the
recipe Docker uses to build your image
in this session we'll be using a
multi-stage Docker file to build our
application
third you will learn how to use an nginx
server to serve an angular application
inside a Docker container
to follow through the examples in this
session you should have an angular
application ready and the latest version
of Docker installed on your Workstation
you can install Docker by following
instructions on docker.com
first let's look at what Docker is
Docker is the most popular
containerization technology and it has
quickly become the de facto standard
when talking about containers
containerizing an application means
packaging it and all its dependencies
into a single easily transportable and
isolated container
this container will run in exactly the
same fashion regardless of the computer
it is run on
by providing this layer of consistency
Docker fixes the traditional but it
works on my machine problem
instead of Distributing just our
application we're Distributing a full
runtime environment along with our
application
well not exactly correct it might help
you to think of a Docker container as a
lightweight virtual machine inside your
computer
your computer can run multiple Docker
containers at the same time stopping and
starting them individually as required
a common problem in software delivery is
dependency management
when one application is run on multiple
development machines and multiple server
environments a small difference in the
version of an external Library can
change the functionality of your
application making it behave differently
on different environments
the beauty of Docker is that if you
build your application into a container
image and transfer the same container
image to your colleague's computer you
can be sure that the application will
function identically on both computers
this is because the container includes
all dependencies for the application
inside it
on the other hand a Docker container
should not have any dependencies to the
host it's running on apart from Docker
itself
it's important for you to understand
what images and containers are and
what's the difference between them
for the purposes of this session you can
think of a Docker image as something
that holds a file system and some
metadata in it
the metadata includes things like the
name or tag of the image and
instructions for Docker like what
command to run by default when the image
is started
while the official Docker documentation
is a bit vague about the meaning of
names and tags you should learn the
basics of it
every Docker image has at least one tag
and in fact the same Docker image can
have multiple tags pointing to it
every image tag has a name part and a
version part separated by a colon
for example the nginx image has multiple
tags representing different versions and
different flavors of the image
there's a special version tag called
latest which points the latest version
of the image
all Docker commands default to using the
latest version tag if no other version
tag is explicitly defined
when you build an image on your local
machine or for example a build server
you can upload the image into a Docker
registry
when you configure other machines they
use the same Docker registry you can
download the same image to as many other
machines as you like
there is an official Docker registry
called the docker hub and the model is
the same as for example GitHub you can
store public images free of charge under
your account but you will need to pay to
host private images
having your images public is great for
hosting open source projects but for any
commercial proprietary applications you
will most likely want to opt for hosting
the images privately
if you don't want to use a third-party
service you can always run your own
Docker registry
my advice is that unless you're planning
on hosting a very large number of images
it's absolutely worth the cost to spend
a few dollars per month on a hosted
Duggar registry such as Docker hub
once your application ecosystem starts
to build around Docker the registry
becomes a critical piece of
infrastructure and keeping it up and
running can prove to be a substantial
piece of work
when you build or download a Docker
image to a computer and run it it
becomes a running container
if you don't tell Docker otherwise
Docker will run a pre-specified command
inside the container
as far as this command is concerned it
is running inside a Linux server and it
has access to other commands and
resources that are available inside the
same container
you can think of a container as a
Sandbox where processes running inside
the container do not have access to the
host system or other containers unless
you explicitly specify otherwise
note that the complete container runs
around a single process created by the
initial command used to start the
container
if this process terminates for any
reason the container will stop
an example for our container would be
the nginx server process
when you start a container the nginx
server process starts in it
and when the nginx server process dies
or is killed the container will stop as
well
it is possible to run multiple processes
inside a single container by for example
using an init system to spawn the
processes you need but for the purposes
of this session we'll use the concept of
just a single process per container
the basic workflow with Docker is as
follows
first you will build a Docker image
either on your workstation or on a
continuous integration server
second you will push this Docker image
into a Docker registry to make it
available for other computers
third you will most likely want to run
the image on a server that is accessible
via the Internet so clients can access
whatever service software is running
inside your container
most often this is an HTTP server and in
this session we'll be using the nginx
HTTP server
in software delivery environments that
Embrace continuous delivery this
workflow is often part of a fully
automated pipeline
a pipeline like this can trigger from
committing a piece of code into a
version control system and thus
automatically deploy a new version of
your application into a testing or even
production environment whenever you
change your application code
as we discussed earlier a container is
isolated from the machine it's running
on as well as any other containers
running on the same machine unless you
explicitly Define connections between
them
this is out of the scope of this session
but I recommend you spend some time
after the session to learn about Docker
networking and links between containers
in this session we'll be running a
single container that can connect to the
internet to the host machine and expose
a single TCP port for the nginx server
process
building Docker images is done with
Docker files you can think of a Docker
file as a text file containing a recipe
to tell Docker how to build an image
the basic function of a Docker file is
to be a list of instructions that
incrementally manipulate an existing
Docker image
the word existing here is important
while you can theoretically build a
Docker image from scratch in most use
cases you will actually be using another
Docker image as the base for your new
image
many Frameworks and open source projects
publish their own Docker images which
can be used by developers to build
images on top of
in this session we'll be using a new
Docker feature called multi-stage builds
this feature is available from Docker
version 17.05 forward and it allows you
to use one base image for building our
application and another base image for
serving it
this works brilliantly for our use case
while angular is often used as part of
the full mean stack in this session
we're only concentrating on angular
I'm assuming you know angular and how
the angular command line interface works
and that you have a package.json in your
application directory
if you're not quite there yet check out
the excellent getting started guide on
the angular website
please note that the functionality of
angular varies wildly between major
versions
however the fundamental idea behind
containerizing an angular application is
the same
our Docker file is going to have two
stages
first we'll need to build our angular
application using the NGA build command
this will output the packaged
application in the disk directory under
our application route
in the second stage we will place the
files from the disk directory into a
directory that's served by a web server
and finally run the web server itself
this is where the multi-stage build
feature comes in handy as you remember a
Docker container only has the software
available which we've explicitly added
into it
this means that for the NGA build
command to work we need to have the
angular command line interface installed
inside our Docker container
this is something we wouldn't want on
our final Docker image because it's just
extra weight and on the other hand we
wouldn't want the web server inside the
image we're building our application in
because the angular CLI is distributed
as a node package we can use the
official node Docker image as the base
of our first Builder stage
because angular is a front-end framework
it doesn't come with a server-side
component other than the test server
shipped with the angular CLI
unfortunately this test server isn't fit
to be run on production so we'll want to
use something else to serve the angular
HTML files
in this example we will use a robust web
server called nginx to serve the angular
application
please note you could just as well be
using a server application like Apache
or Lighty instead I'm not going to be
diving into the specifics of configuring
nginx nginx does come with good
documentation but more importantly it
also comes with an official Docker image
we can use the official nginx Docker
image as the base for our angular
application image
the good thing is that this makes our
Docker file very short and simple let's
have a look
this is the docker file and it's divided
in the two stages
the way the multi-stage build Works in
Docker is that we're actually creating
multiple Docker images with a single
Docker file but we're only keeping the
last image we've defined
this allows us to build the application
first and then copy the resulting build
artifacts into the next Docker image
the idea behind this is that our final
Docker image won't have all the build
time dependencies in it which makes the
resulting image nice and small
in the first stage we'll use the from
directive to instruct Docker that we
want to use the Note 8 image as our base
image
we're also using the S Builder keyword
so we'll be able to reference this stage
in our second stage later
the next directive is the copy directive
this command is for copying files and
directories from our local machine to
the docker image
we're giving this directive two
arguments the location of our
application code directory and the
target path within the docker container
in this example our application code
sits in test app and we want to place it
in the directory test app inside the
container
the next directive is work there
this tells Docker that all following
commands should be run within the
specified directory which in this
example is the application directory
within the container
the next directives are the Run
directives
as you might guess these directives
instruct Docker to run commands inside
the container
first we're running npm install to
install alt node packages defined in the
package.json file of our application
then we're running NG build to package
our angular application into this
directory
now we'll get to the second stage which
is copying the contents of the disk
directory into a directory that's served
by default by nginx
we're using the from directive again
this tells Docker to start building
another image
we're defining the from image to be an
image called nginx
which is the official nginx Docker image
this image is pre-configured to run the
nginx server and serve HTML files that
are stored in a predefined directory
next we'll use the copy directive
but this time with the from argument
this tells Docker to not look for the
source path in our workstation but
instead the previous stage in this
Docker file
essentially we're copying the contents
of the disk directory in the previous
Docker image into a directory called
user share nginx HTML inside our new
image
the nginx docker image will then serve
the contents of this directory
finally we're using a directive called
expose to tell Docker that the port 80
has a server running in this image
the port 80 is the default HTTP Port as
you might know
note that the exposed directive alone
isn't enough to actually expose the port
when the image is run
you can merely consider this as a
documentation feature
and will still need to explicitly map
Port 80 from the container into a port
on the host machine
this enables you to connect to the
mapped port on the host machine which
will turn forward the connection into
the container
to follow this session for your angular
application create a file with these
contents and place it in The Parent
Directory of your application code
remember to customize the paths in the
file accordingly
make sure the name of the file is Docker
file written as one word with a capital
D
now that you've learned what the docker
file is and what one looks like for an
angular application let's learn how to
use it to containerize an example
angular application
for this we'll be using the docker
command line interface
after you've installed Docker on your
workstation you can use the docker
command line interface to build and run
images
to find the command line shell on a Mac
click on the spotlight search icon on
the top right corner of your screen and
right terminal followed by enter
while the docker command line provides
many features in this session we'll
concentrate on two the docker build and
the docker run commands first let's
navigate to the directory with our
Docker file
the command we'll be using is Docker
build
we'll be giving the command the minus t
argument which defines the tag for the
image that will be built
for this we'll be using the tag test app
the second argument for the docker build
command is the build context
this is where Docker expects to find the
docker file
we'll use a single dot to denote the
current directory and press enter to
start building
you can see Docker runs through every
directive in the docker file as a step
outputting the directive and the output
of running it
finally you'll see that Docker has
successfully built the image and tagged
it as test app latest
as you might remember from before since
we didn't Define the version tag the
attack latest is used automatically
great work
now let's run the image we just built
for this we'll be using the docker run
command
we'll be giving the docker run command a
few arguments
first minus I and minus t to denote that
we want to run the image in an
interactive terminal that is running the
container attached to our current
terminal session so we can see the
output and interact with the process
we'll also be using the minus P argument
to map the port 8080 on our workstation
to Port 80 inside the container
as you remember the nginx server is
listening on Port 80. with this mapping
anyone connecting to port 8080 on our
workstation will get forwarded to Port
80 inside the container
finally the last argument is Attack of
the image we want to run
we're using test app colon latest now
that the container is running let's use
our web browser to navigate to port 8080
on our local machine
and we can see our angular application
being served well done you can see that
nginx by default outputs all the
requests in the standard output of the
container
after you've finished testing press Ctrl
C in your terminal to terminate the
nginx process and thus kill the
container you were running and that's
how you dockerize an angular application
in this session you will have a good
understanding of what a virtual machine
is and what a Docker is followed by that
we will have a major difference between
Docker and the virtual machines also we
will discuss some of the key differences
between a Docker and virtual machines
and finally a use case on what made BBC
to use docker
so the first one what exactly is a
virtual machine it is an isolated
Computing environment that enables a
person to use an operating system via a
physical machine virtual machines
provide the functionality of a physical
computer as you can see on my screen
there is a virtual machine and this is
how a typical virtual machine looks like
now let's look at what a Docker is
Docker is an operating system level
virtualization software platform that
enables software developers and it
administrators to create deploy and run
applications in a Docker container with
all their dependencies and a Docker
container is a lightweight software
package that includes all the
dependencies like Frameworks libraries
and many more which are essentially
required to execute an application
this is the architecture of Docker tool
now let's move on to our next main topic
for the discussion that is Docker versus
virtual machine when it comes to
comparing the two we could say the
docker containers have much more
potential than virtual machines as you
can see on the left and right hand side
both the images look similar now let's
define the layers of virtual machine
from the bottom up let's begin with the
infrastructure infrastructure could be a
computer system a laptop or a virtual
private server such as an Amazon ec2
instance or Azure virtual machine or gcp
virtual machine instance on the top of
the infrastructure runs the operating
system and operating system is a
software that manages hardware and
software resources for the computer
programs on your laptop this will be
likely Windows Mac OS or Linux flavor
operating systems such as Ubuntu next
comes the hypervisor a hypervisor is a
firmware that builds and runs virtual
machines there are two types of
hypervisors type 1 hypervisors are
hyperkit for Mac OS hyper-v for Windows
and kvn for Linux operating systems and
type 2 hypervisors are virtualbox and
VMware then we have the guest operating
systems consider an example where you
want to run two apps on your server in
total isolation this process would
require two guest operating systems
which will be controlled by a hypervisor
virtual machines come with many
dependencies where each guest operating
system will at least occupy 512 MB of
your RAM this is worse because each
guest operating system needs its own CPU
and memory resources eventually this
will be expensive then on top of that
each guest operating system requires its
own dependencies such as binaries and
libraries since every application has
different dependencies it requires its
own set of libraries finally it's the
application this layer consists of the
source code for the application you have
built this was all about the virtual
machine on a server here's what the same
setup looks like when you're using
Docker containers here you'll notice
that there are a lot fewer layers Docker
doesn't require a bunch of massive guest
operating systems let me break it down
from the bottom up again infrastructure
and host operating systems are the same
as we'll discussed for the virtual
machine coming into the third layer
instead of a hypervisor Docker uses
Docker engine Docker engine or Docker is
a client server application that builds
and executes containers using Docker
component
next we have our dependencies just like
on the virtual machines here
dependencies are built into a template
called Docker images as you can see on
the screen each application is still
isolated and occupies less space now
let's have a look at the significant
differences between Docker and virtual
machine let's start with the operating
system first Docker is a container based
model where containers are software
packages used for executing an
application on any operating system
on the other side virtual machine is a
container based model it utilizes user
space along with the kernel space of the
operating system a kernel space is where
the core of the operating system runs
and provides its services to the user a
user cannot modify the space and user
space is the portion of the system
memory in which the user's processes run
Docker containers share operating system
kernels with other applications and
result in a higher server efficiency
hence Docker provides the most
substantial default isolation
capabilities among the other
configuration tools on the other hand
virtual machines did not share the
operating system also it does not
provide isolation in the host kernel in
Docker multiple workloads can run on a
single operating system but in virtual
machine each workload needs a complete
operating system or a hypervisor the
next significant difference between the
two is the performance in the case of
Docker they use the same operating
system without any additional software
like hypervisor in case of virtual
machines performance issues are a major
problem it can be due to the several
reasons like CPU constraints memory
allocation Network latency and many more
running multiple virtual machines lead
to an unstable performance Docker
containers can start up quickly and
result in less boot up time whereas
virtual machines do not start quickly
and lead to a poor performance now let's
talk about the next difference which is
portability with Docker containers a
developer can build an application and
store it into a Docker image which later
he or she can run it across any host
environment but when it comes to Virtual
machines it has portability issues
virtual machines do not have a central
Hub like Docker dust and it requires
more memory space to store data Docker
containers are smaller than virtual
machines due to which the process of
transferring files on the host file
system is easier on the other hand
dependency on the host operating system
and Hardware makes virtual machines less
portable while transferring files
virtual machines should have a copy of
the operating system and its
dependencies due to which image size is
increased and becomes a tedious process
to share data
the boot up time between the both is
very different the application in Docker
containers start without any delay since
the operating system is already up and
running on the other hand virtual
machines take much longer time than it
takes for a container to run
applications these containers were
basically designed to save time in the
deployment process of an application
whereas in Virtual machines to deploy a
single application virtual machine needs
to start an entire operating system
which would result in a full board
process so those were the major
differences between Docker and a virtual
machine now let's discuss the minor
differences between them
Docker is not always in a running state
it stops when the stop command is
executed whereas the virtual machines
are always running in the background
which will result in huge RAM
consumption talking about snapshots
Docker has a lot of snapshots a snapshot
is an image that you can upload on a
private repository to access it on
another host but virtual machines do not
consist of any snapshots in Docker
images can be version controlled just
like git they have local registry called
Docker Hub where uses store and
distribute container images on the other
hand virtual machine does not have a
central Hub they are not version
controlled
Docker can run multiple containers on a
system users can connect multiple
containers using user-defined networks
and shared volumes multiple containers
can be accessed on the same machine and
share the operating system kernel with
other containers each container is
isolated in user space Also Docker
containers occupy less space than
virtual machines and start instantly
virtual machine can run only a limited
number of virtual machines on a system
since each virtual machine requires a
certain amount of CPU RAM memory and
other resources your physical systems
will have less space multiple containers
can be started at a time on the docker
engine the isolation environment allows
the user to run and deploy several
containers simultaneously on a given
host virtual machines can run only a
limited number of virtual machines on a
system since each virtual machine
requires certain amount of CPU RAM
memory and other resources your physical
systems will have less space now that I
have told you the differences between
Docker containers and virtual machines
let me show you a real life case study
of how PCC uses Docker BCC news is a
British News Channel over 500 developers
working across the globe BCC news
delivers broadcasts in almost 30
different languages and with over 80 000
Daily News in English alone the news
channel ran more than 26 000 Jobs worth
more than 10 continuous Integrations
with sequential scheduling the company
had issues with identifying a way to
unify the coding process and monitor the
continuous integration consistently also
the existing jobs took up to more time
that is up to 60 Minutes to schedule and
perform its task
Docker allowed BCC news to eliminate job
wait times and run jobs in parallel it
also gave the users the ability to work
in a more flexible CI environment where
entire core processes were unified and
stored in a single place however Docker
succeeded in spreading up the whole
continuous integration process so we're
going to go through a bunch of things
today around kubernetes we're going to
cover what kubernetes is and why you
should be using it along with the
features of kubernetes we're also going
to take a little bit of a look at
kubernetes versus Docker swarm as the
two technologies do tend to get referred
to quite frequently and then we're going
to do a deep dive into the actual
kubernetes architecture itself and how
kubernetes runs and operates within your
network and then finally we're going to
look at some kubernetes use cases of how
you can actually apply best learnings
that other companies have used with
kubernetes we'll finish everything off
with a demo where we actually step
through and use some of the actual codes
that you can actually use to run from
your terminal window to actually
activate kubernetes itself so with that
said let's jump in so why kubernetes
well kubernetes is really being built
from allowing you to be able to have a
more reliable infrastructure the tool
itself is designed to manage the
containers you have within your network
it's very modular in architecture which
allows it to be very easy to be able to
maintain and to be able to scale and
meet the demands that your current
customers are offering you today the
actual ability for you to be able to
deploy and update software at scale is
really at the heart and center of
kubernetes it's really well the one of
the reasons why kubernetes was created
was to be able to scale up the
deployment of software into tens of
thousands of networks and servers within
a Data Center and then finally when you
start working with kubernetes really
what you're doing is laying the
foundation for building Cloud native
applications and part of this comes from
the architecture and history of
kubernetes which we'll get to in a
little bit but it's good to know that as
you're using kubernetes and
containerized solutions that you're
really building out an infrastructure
that's designed for cloud-centric
solution delivery so let's dig into why
what kubernetes is so the history of
kubernetes is that it was originally
developed by Google Google actually used
the precursor to kubernetes to manage
and maintain the data structure and data
infrastructure that they had within
their Network they really liked it and
one of the big benefits that kubernetes
that Google provided is that they
converted their code into an open source
program that they released as kubernetes
in 2014 and made an open source this
means that any company can use
kubernetes there's no charge for it all
you have to do is know how to use the
command line to actually get up and
running the benefit of kubernetes being
open source is that most Cloud providers
now support kubernetes so whether you're
using Microsoft Azure you're using
Google cloud or AWS you'll find that you
can actually use kubernetes and
kubernetes and at a core really helped
eliminate manual processes so it allowed
for deploying and scaling your
container-based Solutions much more
effortless within a cloud environment so
let's jump into the features of
kubernetes and why you'd want to be
using it and we have a number of
features that we want to kind of Step
through again the first thing that
kubernetes is a strong benefit of is
that it eliminates those manual
processes and allows you to automate
them the benefit you have from this is
that you can then scale up your
automation across your entire network so
whether you are managing 10 nodes within
your network or whether you're managing
10 000 it doesn't matter you have the
ability to scale that something you
simply didn't have before kubernetes is
also will manage the actual containers
themselves out including the security
and the storage and the networking
within those containers the great
benefit of working with kubernetes is
that not only is it a cloud-centric
solution but it was really built from a
core need of a secure solution so
security is at the heart and center of
all the work you're doing which really
helps benefit any teams scaling out
containers your kubernetes environment
will be constantly monitored so that
when you have nodes that have your
containers the health of those nodes is
being constantly communicated back to uh
the kubernetes environment so that the
entire environment has always been
managed and checked so if you have any
additional hosting and launching that
needs to be done kubernetes will help
you automate those manual processes if
something does go wrong you can actually
set up your kubernetes environment to do
an automatic rollback again this is just
allowing you to have that confidence
that when you're deploying a solution
that everything is going to work within
that environment and then finally
kubernetes will allow you to mount and
add the storage needed to run your apps
so whether you have storage such as a
local SSD storage device physically on
the hardware or if you're connecting to
storage fire the cloud kubernetes will
allow you to access the storage you need
how you're accessing it so let's look a
little bit into the differences between
kubernetes and Docker swarm so if we
just step back a little bit kubernetes
and Docker often get connected with each
other and largely because kubernetes
manages containers and Docker is really
famous for creating the most popular
container management solution which is
called Docker if you're a developer
you're probably already using Docker
today so one of the things that
kubernetes is really really good at is
that it manages containers at scale and
just massive volume of containers Docker
swarm is docker's alternative to
kubernetes it's a tool that allows you
to also manage large number large
numbers of containers there are
differences between the two of them and
right now without doubt the most popular
open source solution out there is
kubernetes from Google it just has a
massive Community behind it and the
community really have embraced
kubernetes and yes the original product
was by developed by Google but Google
has been great stewards and they've
allowed the community to drive the
future evolution of the product Docker
swarm is actually managed and developed
by Docker there is a smaller Community
but it is also a very passionate
community and the reason being is that
both products are actually pretty good
and you just really kind of have to
choose which one you want to go with
without doubt there is more
customization and extensions built for
kubernetes simply because the community
is just so much bigger than the docker
swarm Community but with that said
Docker swarm is really easy to set up so
if you are used to working within the
docker environment you'll be able to set
up Docker swarm very very quickly
whereas kubernetes does require some
heavy lifting to get up and running now
with that said if you are using
Microsoft Azure or Google Cloud very
often those environments will actually
set up kubernetes for you so you don't
have to do it and you can actually then
focus on writing the instructions to
maintain your kubernetes environment
kubernetes has extremely high voltage
tolerance again it's because kubernetes
is has been Road tested the foundation
for kubernetes is based on managing
sites such as Gmail google.com YouTube I
mean just incredibly robust environment
compared to Docker swarm which has lower
fault time tolerance
um and this really kind of leads into
the final areas where kubernetes does
guarantee strong clusters management
just a really reliable environment Dr
swarm however is able to take continuous
deploy them into large casters very very
quickly two interesting differences is
that with kubernetes the load balancing
is still manual and be interesting to
watch how kubernetes matures that skill
set in contrast load balancing is
automatic within Docker swarm so let's
dig into the kubernetes architecture and
just kind of expose some of those key
elements um if you're interested in
actually having a very deep dive into
kubernetes architecture we have already
got another video and we should have a
link in the comments that you can select
that will take you to that video as well
but this will provide you an overview as
well um so the way that kubernetes runs
is that it uses nodes to actually manage
all the individual Hardware within a
specific unit and so a node is really a
physical machine in a data center or a
virtual machine and when you're setting
up an environment on a cloud environment
such as Microsoft Azure or Google cloud
or AWS kubernetes will be managing the
containers within that node so it's
important to know that kubernetes itself
doesn't work with the individual nodes
but actually works with the cluster as a
whole and and why that's important is
that it's a tool to manage your entire
environment it's not going to come down
and manage the individual node itself
it's going to match the entire
environment the actual control of data
within a cluster and it's very very
flexible within kubernetes you can have
local storage such as an SSD card or a
traditional local storage or you can
actually connect to the cloud and use
cloud-based storage and the benefit that
cloud-based storage offers you is that
you are increasing Moving Your solution
into more of a hardware free environment
cloud-based architecture where you're
not reliant on a physical piece of
Hardware within your data center and why
that's important is that it gives you
the opportunity to scale up your
Solutions much more effectively so if
you need to increase the hard drive
space you can do that very easily from
the cloud whereas local hardware is a
little bit harder to manage great news
kubernetes gives you both options to be
able to choose from and so the the next
layer down is the actual management of
the application and that's within a
container and as mentioned earlier
Docker is the most popular container
solution out there and again if you're a
developer you're probably already using
Docker as it was a natural maturity from
using a virtual machine but kubernetes
itself will actually manage the
containers that you have within your
nodes and so you have a collection of
containers working together and then you
pull those together into what's called A
Part and what's great is kubernetes will
actually manage the health of that pod
so if a container or a pod fails
kubernetes will automatically deploy out
a replica so your entire environment can
stay running and being efficient and so
the pods themselves will actually be
managed by kubernetes and you really
want to let kubernetes do this because
it's allowing you to step away from the
manual management of Parts your
environment is just going to be
healthier relying on kubernetes to do
the actual management of all the pods
you have out there so you are able to
allow access to from Services outside of
your kubernetes and this is done through
a process called Ingress and going back
earlier we talked about how security is
a key part of all the work that goes
into kubernetes and this is a great
example of where you can use an external
service but still have that layer of
security that is vital for your solution
because to be able to provide the
Ingress connection kubernetes requires
an SSL connection which automatically
allows for secure data transfer
absolutely fantastic so here we have an
overview of the actual architecture on
the right hand side we have our nodes
and the pods within our nodes and within
those pods are the containers on the
left hand side we have the master
architecture and the master architecture
is the actual brains behind the work
that you're doing it's actually managing
the schedules the controllers for all of
the nodes and then and the bottom left
hand side we have commands which are
written on a local workstation and those
commands are pushed up to the master so
let's kind of dig a little bit further
into the architecture of the master so
the master itself is comprised of a
number of key components again this is
the tool that's going to manage your
entire kubernetes infrastructure the key
components that you have are the etcd
control manager scheduler API server and
the key for components are etcd or the
cluster store the controller manager the
scheduler and the API server the cluster
store is the tool where all the details
of the work that you're doing is stored
now it's going to manage the rules it's
going to manage all the instructions and
this is where you're going to post that
information the controller is actually
going to then perform the tasks that are
stored in the cluster store and as you'd
expect the schedule is the tool that
actually controls when instructions and
tasks are pushed out to all the pods and
nodes within your network and then
finally you have an API server which
allows you to be able to control all the
operations the actual operations
themselves are all instructions written
as rest instructions and which allows
you to take advantage of the security
that has been built into rest and so you
end up with a worker slave architecture
and slaves are all the different nodes
and pods within your network and so a
part itself will have multiple one or
more multiple containers and then each
container and then each part will have a
Docker container a kublet and a
kubernetes proxy and the these are tools
to allow you to communicate back up to
the master layer so the docker is the
actual container itself and this is
going to actually run your application
and the instructions here are to pull
down the correct container from the
docker images that you have within your
library the cubelet is going to manage
your containers and your container
instructions that will be deployed to
the cubelet are going to be written in
either yaml or Json format yaml is just
graded if you haven't had a chance to
start working with yaml I certainly
encourage you to do so it's very easy to
pick up if you know any XML you'll pick
up yaml very quickly and finally the
kubernetes proxy is the tool that
interacts with the master environment
within the kubernetes network and allows
all the instructions front that are
pushed to the cubelet and to Docker to
be able to perform effectively so let's
have a look at some of the companies
using kubernetes there are a lot and if
you're doing any work today in the cloud
and or even if you're just starting off
and doing some simple work in the cloud
there's a really good chance that you
are using kubernetes for some of the
companies that you would probably not
surprised at using kubernetes are
Spotify sap eBay and of course some of
the the big companies out there Google
with their entire network of solutions
and Microsoft with office and their
tools so let's take a step into aube now
let's have a look at a kubernetes use
case and in this example we're going to
look at the New York Times and how they
use kubernetes to solve a problem that
their it Department was struggling with
so when New York Times started working
after their data centers their their
actual deployments were small and the
applications were managed across VMS but
the problem is is that as New York Times
became more of a digital company they
started building more digital Solutions
and at some point they realized that
what they needed to do was to embrace
the cloud and move all their solutions
to the cloud however their first step of
using the cloud and this is I think
typical for a lot of companies is that
they treated like the cloud hosted
provider in the same way as they would
with a Data Center and they at some
point they realized they had to take a
change and this is where the development
team I came in and they said look how
about we stop thinking of our Solutions
as a data center even though they're
being run in Amazon and start thinking
them as Cloud first Solutions and so
what they proposed and what they ended
up doing was using Google Cloud platform
with kubernetes as a service and to
provide greater efficiency and so by
using kubernetes they have the following
advantages they're able to deliver
Solutions at a much faster speed and in
the world of digital content speed is
everything they're able to reduce the
time for deployment from minutes to
seconds and by doing this they were able
to increase the amount of time that
uptime for the actual infrastructure um
updates began to be deployed
independently and when required so you
didn't have to schedule an event when
you do the deployments it just happened
and there was more unified approach to
deployment from the engineering staff
which just made the whole solution more
portable so the fundamental end line for
the New York Times is that by leveraging
kubernetes they were able to
automatically manage their deployments
and scheduling of solutions instead of
having to do Solutions through a more
traditional ticket based system it just
really speed up the efficiency of the
New York Times website so this finally
kind of look into some of the important
terminologies before we take it our time
to do a demo so we have six terms that
we're going to look through and we'll
let you uh read through this but uh we
have a cluster which is a set of
machines physical vertical in which
applications are managed and run we have
a node that are the working machines
that run containerized applications and
other workloads there is the pod which
is within a node and there's a group of
containers that are deployed together on
the same host we have replication
controllers and a replication controller
is used to define a pods life cycle
rather than to create the pods directly
we have a selector which is an
expression matched label to filter
specific resources labels are key value
pairs that are attached to objects such
as pods and the key value pair or the
label can be filtered organized and
perform operations on resources
replication sense Define how many
replicas of each pod will be running and
managed when you want to either replace
a pod when it dies and patient is a
label with much larger data capacity a
name is a resource as it is identified
volume is the direction which data is
accessible to a container namespaces
provide additional qualification to a
resources name and then finally service
is an abstraction of a top-tier pods
which provide a single IP address and
DNS Name by which the pods can be
accessed so this time what we want to do
is take you through kubernetes demo
actually runs through some of these
basic controls so I'm going to take you
through a video of how to actually
install kubernetes and to use some of
the basic instructions so kubernetes is
the most wildly used platform for
container management the first step is
to actually install the necessary
dependencies so let's see if there's any
updates first so the first so to check
if there's any updates we'll use the
command sudo app dash get update we're
going to enter in our password and this
looks like it's going to take a few
minutes because we've got a few updates
that need to be running all right we're
done all right next we're going to write
sudo apt-get install Dash y
apt-transport https and this will
install all of the dependencies we need
to actually start installing the
solutions needed for kubernetes all
right that was quick now we're going to
put in the docker dependency and the way
we do that is write the command sudo apt
install Docker dot IO I've got a typo
correct that quickly and put in y so vs
and we continue and it's going to take a
while because we're going to download
and install Docker off the internet and
it's quite a large installation now
would be a good time to go get a cup of
coffee because it'll take us about a
minute to get this all done once we
actually have Docker installed we'll let
you go ahead and we'll start Docker and
enable Docker using our sudo commands
all right so that's all done so let's
start to enable Docker we're going to be
using so we're going to use sudo
systemctl start Docker and sudo
systemctl enable Docker this will be
quite quick
so the next step is to install all the
components for kubernetes and we're
going to install the curl command and
then kubolate Cube C and then qbadm and
we're going to install all the necessary
tools we'll do the call command cubelet
cubic C2 and Cube ADM and so let's go
ahead and put in the sudo apt-get
install Curl we're going to get a prompt
we can say yes for that
and then after this we'll go ahead and
install kubernetes so the first what
we're going to do is we're going to
actually put in the kubernetes key so
that's going to be pseudocal dash s and
then we have a long URL which is https
packages.cloud google.com apt dark
apt-key dot gpg
sudo
apt-key there we go and create my typo
there okay now we have to go ahead and
change permissions of a folder so we do
that with sudo chmod 777 slash Etc apt
slash
sources.list.d all right we're good okay
now you want to go ahead and open up
your text editor and we're going to make
um an actual edit and we're going to
create a simple file and so what we're
going to type in is the the following
command DB hey cgbt
apt-cubernetes Dot IO slash kubernetes
Dash zenial Main and we're going to save
this file in the ETC folder and then
look for and then a APT which is going
to be down a little bit and there we
have it and we're going to save it in
the
apt-get updates and we're going to name
this file as kubernetes dot list and
we'll save that and then we can close
out the notepad and go back to the
terminal window and let's look and see
see if there are any updates again so
we're going to go ahead and run apt-get
update and it's going to run any updates
from what we've created and we want to
check what's in the folder and so we do
cat slash X at ATT
Etc apt
sources.d kubernetes dot list and there
we are that shows what we have in that
text file and so we're going to go ahead
and we're going to install uh cubelet
cubic cubic 2 and Cube ADM but first
we're going to have to initialize the
master I'm going to show you what an
error you would get if you were to
install cubelet and the other files too
quickly you get the fine error
all right so we have installed cubelet
qbadm cubic tool and kubernetes cni and
so we're going to go ahead and use sudo
swap up Dash a and now we're going to
initialize the master node and we're
going to do that with sudo Cube ADM in
it and this will take a few minutes for
it to initialize
all right so that's all done all right
so we're going to deploy the Pod to a
network and we have two commands that
we're going to use we use sudo cuboo
Cube CTL apply and
we have a long Pathway to type out which
is dash out of hqbs raw githubs
content.com choreos flannel Master
documentation Cube file Dot yml
what we we have to do is we have to
actually go ahead and deploy the nodes
first so we're going to do that by
creating mkdr Dash p and then we have
dollar sign home dash dot Cube and
create that new directory and sudo CP
Dash I and then the path will be
Etc
kubernetes
admin.conf and then dollar sign home
slash dot Cube dot slash config
and then final one will be pseudo
c-h-o-w-n dollar sign open Brain ID Dash
U closed paren colon dollar sign open ID
slash Dash G close paren door sign home
slash dot Q slash config all right now
we're going to go ahead and deploy and
uh let's see and oh yeah I think I have
a typo in my command here yep oh yeah I
did have a typo and so
um I've got that incorrect now so
documentation is actually spelled with a
t not an S and then we're going to go
ahead and we're going to put in the next
line which is pseudo cubic Cube CTL
apply dash dash F and then we're just
going to change a little bit of the
actual link here and that runs great and
now we want to be able to see all of the
pods that we have in our Network and so
we're going to do that by writing the
commands sudo cubic TCL get pods Dash or
all dash dash name spaces and it should
list all of our pods we only have one
there we are there's all the content now
we're going to so there's only one node
so let's see what we have here so we're
going to use the command sudo you know
cubic TL get nodes there we are just one
node so our final step is to actually go
ahead and deploy a service and so what
we're going to go ahead is deploy the
ngi and access service and we're going
to do that with the command sudo cubic
CTL run Dash Dash image equals nginx and
we're going to put this into Port 80 so
it's going to be nginx app dash dash
Port equals 80 dash dash environment EMV
equals uh ubunt open quota domain equals
cluster close quotes
and we're going to write one more
command which is sudo Cube CTL exposed
deployment nginx Dash app dash dash Port
equals 80 dash dash name equals nginx
Dash HTTP and once we've done this we'll
be able to then run the command to
actually see our services and there'll
be quite a few of them
and our final command is sudo docker
ps-a
yeah we have quite a lot of lists quite
a lot of services listed so there you
are that's the instructions on how to
get Docker up and running and uh with
some basic commands and it is worth
going through a few times to get right
um but once you actually have the um the
kubernetes environment up and running
and managing your Docker environment and
containers it's extremely powerful so
we're going to break out the
presentation into the following areas
we're going to cover what kubernetes is
and why you'd want to be using it we're
going to introduce the actual
architecture and provide an overview of
how it contains the containers and the
other components that you'd have within
the architecture we'll also compare
kubernetes with Docker swarm and one of
the things that you hear with kubernetes
almost in the same breath as Docker and
we have to be careful not to confuse
Docker from a container point of view
and Docker swarm which is a container
management tool and kubernetes is also a
container management tool so compare the
two of those and then we'll look at the
hardware and software components and
then find finally do a deep dive into
the architecture of kubernetes and
provide a case study of where kubernetes
is being used successfully in the past
so let's jump into this so what actually
is kubernetes so kubernetes is a tool
that was designed to actually help
manage and contain large groups of
containers it was developed originally
by Google out of the Google book
solution and then Open Source by Google
the actual environment allows you to
manage large groups of containers so if
you're a developer in devops and you're
working with Docker then you're used to
the concept of a container and you'll
also know that containers and Cloud
Technologies tend to be almost breathed
in the same breath so the work that
you're doing with kubernetes is to be
able to manage large groups of
containers inside of the cloud the thing
that's great about working with
kubernetes is that it is incredibly
flexible and allows you to have
incredibly complex applications run
efficient efficiently so this step into
why you'd want to use kubernetes so a
couple of key points kubernetes itself
is an open source solution originally
developed by Google but it is available
on most Cloud platforms today so AWS
Google Cloud Microsoft Azure all of them
support kubernetes and what you'll find
is as you're setting up your
infrastructure particularly if you're
using containers you'll see that the
support for kubernetes is flooded up
very very efficiently and effectively by
all three vendors it's extremely um
useful for being able to manage large
modular environments and that's really
kind of these I think one of the big
benefits of kubernetes is its modularity
is that it really breaks down containers
into their smaller parts and once you do
that it becomes much more efficient for
you as an administrator to be able to
manage that entire entire environment
the reproductibility of kubernetes is
extremely high you can build out
infrastructures very quickly and have
them being able to manage and have
containers coming on and off and killed
and created to be able to help load
balance your entire environment and so
again you know we keep talking about
containers but that's really what
kubernetes is all about is being able to
manage your applications that are
managed through containers and being
able to do that in a virtualized
infrastructure and the thing that's also
really good with kubernetes is it's
really easy to deploy Solutions you just
have to use a simple curl call and you
can actually push out your kubernetes
infrastructure so let's have an overview
of the kubernetes architecture so
kubernetes has really broken up into
three key areas you have your
workstation where you develop your
commands and you push out those commands
to your master and the master is
comprised of four key areas which
essentially control all of your nodes
and the node contains multiple pods and
each pod has your Docker container built
into it so consider that you could have
a really almost an infinite number of
PODS I'm sorry infinite number of nodes
are being managed by the master
environment so you have your cluster
which is a collection of servers that
maintain the availability and the
compute power such as RAM CPU and disk
utilization and you have the master
which is really components that control
and schedule the activities of your
network and then you have the node which
actually hosts the actual Docker virtual
machine itself and be able to actually
control and communicate back to the
master the health of that pod and we'll
get into more detail now later in the
presentation so you know you keep
hearing me talk about containers but
they really are the center of the work
that you're doing with kubernetes and
can the concept around kubernetes and
containers is really just a natural
evolution of where we've been with
internet and digital Technologies over
the last 10 15 years so before
kubernetes you had tools where you're
either running virtual machines or
you're running a data centers that had
to maintain and manage and notify you of
any interruptions in your network
kubernetes is the tool that actually
comes in and helps address those
interruptions and manages them for you
so the solution to this is the use of
containers so you can think of
containers as that Natural Evolution
from you know uh 15 20 years ago you
would have written your code and posted
it to a Data Center and more recently
you probably posted your code to a
virtual machine and then move the
virtual machine and now you actually
just work directly into a container and
everything is self-contained and can be
pushed out to your environment and the
thing that's great about containers
they're they're isolate environments
very easy for developers to work on them
but it's also really easy for operations
teams to be able to move a container
into production so let's kind of step
and back and look at a competing product
to kubernetes which is Docker swarm now
one of the things we have to and
remember is that darker containers which
are extremely popular I'm built by the
company Docker and made open source and
Docker actually has other products one
of those other products is Docker swarm
and Docker swarm is a tool that allows
you to be able to manage multiple
containers so if we look at some of the
uh the benefits of using Docker swarm
versus kubernetes now one of the things
that you'll find is that both tools have
strengths and weaknesses but it's really
good that they're both out there because
it helps keep it really kind of
justifies the importance of having these
kind of tools so kubernetes was designed
originally from the ground up to be Auto
scaling whereas took a swarm isn't the
load balancing is automatic on Docker
Swan whereas with kubernetes you have to
manually configure load balancing across
your nodes the installation for Docker
swarm is really fast and easy I mean you
can be up and running within minutes
kubernetes takes a little bit more time
is a little bit more complicated aided
eventually you'll get there I mean it's
not like it's going to take you days and
weeks but it's it is a tool that's a
when you compare the two Docker swarms
much easier to get up and running now
what's interesting is that kubernetes is
incredibly scalable and it's you know
that's its its real strength is its
ability to have strong clusters whereas
with Docker swarm it's cluster stream
isn't um as strong when compared to
kubernetes now you compare it to
anything else on the market it's really
good um so this is kind of a splitting
hairs kind of comparison but kubernetes
really does have the advantage here if
you're looking at the two compared to
each other for scalability I mean
kubernetes was designed for by Google to
scale up and support Google Cloud
Network infrastructure they both allow
you to be able to share storage volumes
with Docking you can actually do it with
any container with that is managed by
the docker swamp whereas with kubernetes
it manages the storage with the pods and
a product can have have multiple
containers within it but you can't take
it down to the level of the container
interestingly kubernetes does have a
graphical user interface for being able
to control and manage the environment
the reality however is that you're
likely to be using terminal to actually
make the controls and Commands to
control your either Docker swarm or
kubernetes environment and it's great
that it has a GUI and to get you started
but once you're up and running you're
going to be using terminal window for
those fast quick administrative controls
that you need to make so let's look at
the hardware components for kubernetes
so and what's interesting is that
kubernetes is extremely light of all the
systems that we're looking at is
extremely lightweight
um it's allows you to have if you
compare it to like a virtual machine
which is very heavy at you know
kubernetes is extremely lightweight and
other uses any resources at all
interesting enough though is that if you
are looking at the usage of CPU it's
better to actually take it for the
cluster as a whole rather than
individual nodes because the nodes will
actually combined together to give you
that whole compute power again this is
why kubernetes works really well in the
cloud where you can do that kind of
activity rather than if you're running
in your own data center
um so you can have persistent volumes
such as a local SSD or you can actually
attach to a cloud data storage again
kubernetes is really designed for the
cloud I would encourage you to use cloud
storage wherever possible rather than
relying on physical storage the reason
being is that if you connect to cloud
storage and you need to flex your your
storage the cloud will do that for you I
mean that's just an inherent part of why
you would have cloud storage whereas if
you're connecting to physical storage
you're always restricted to the
limitations of the physical Hardware so
that's um kind of pivot and look at the
software components as compared to the
hardware components so the main part of
the components is the actual container
and all of the software running in the
container runs on Linux so if you have
documents stored as a developer on your
machine it's actually running inside of
Linux and that's what makes it so
lightweight and really one of the things
that you'll find is that most data
centers and Cloud providers now are
running predominantly on Linux inside of
the um the the container itself is then
managed inside of a pod and a pod is
really just a group of containers
bundles together and the kubernetes
scheduler and proxy server then actually
manage what how the pods are actually
pushed out into your kubernetes
environment the the pods themselves can
achieve and share resources both
networking and storage so the products
aren't um pushed out manually they're
actually managed through a layer of
abstraction and part of their deployment
and and this is the strength of
kubernetes you use to find your
infrastructure and then kubernetes will
then merge you for you and there isn't
that problem of manual management of
PODS if you have to manage the
deployment of them and you that's simply
taken away and it's completely automated
and the the final area of software
Services is on Ingress and this is
really the secure way of being able to
have communication from outside of the
cluster and passing of information into
that cluster and again this is done
securely through SSL layers and it
allows you to ensure that security is at
the center of the work that you have
within your kubernetes environment so
let's dive now into the actual
architecture before we start looking at
a use case of how kubernetes is being
employed so kubernetes again is we
looked at this diagram at the beginning
of the presentation and there are really
three key areas there's the workstation
where you develop your commands and then
you have your master environment which
controls the scheduling the
communication and the actual commands
that you have created and pushes those
out and manages the health of your
entire node Network and each node has
various pods so we like break this down
so the master node is the most vital
component um with the master uh you have
four key controls you have Etc the
controller manager schedule an API
server the cluster store Etc this
actually manages the details and values
that you've developed on your local
workstation and then we'll work with the
out the control schedule and API server
to communicate that out those
instructions of how your infrastructure
should look like to your entire network
the control manager is really an API
server and again this is all about
security so we use restful apis which
can be packaged in SSL to communicate
back and forth across your pods and the
master and indeed the services within
each of them as well so at every single
layer of attraction the communication is
secure the schedule as the name would
imply really schedules and when tasks
get sent out to the actual nodes
themselves the nodes themselves are are
done nodes they just have the
applications um running on them the
master and the scum is really doing all
of the work to make sure that your
entire network is running efficiently
and then you have the API server which
has your rest commands and the
communication and back and forth across
your networks that is secure and
efficient so your node environment is
where all the work that you do with your
containers gets pushed out too so um a
work is really a it's a combination of
containers and each container will then
logically run together on that node so
you'd have a collection of containers on
a node that all make logical sense to
have together within each node you have
uh Docker and this is your isolated
environment for running your container
you have your cubelet which is a service
for convenient information and back and
forth to the service about the actual
health of the kubernetes node itself and
then finally you have the proxy server
and the proxy server is able to manage
the nodes the volumes the the creation
of new containers and actually helps
pass the community the the health of the
container back up to the master to see
whether or not the containers should be
either killed stop started or updated so
finally let's look at see where
kubernetes is being used by other
companies so you know kubernetes is
being used by a lot of companies and
they're really using it to help manage
complex existing systems so that they
can have greater performance and with
the end goal of being able to Delight
the customer increase value to the
customer and enhance increased value and
revenue into the organization so example
of this is a company called BlackRock
where they actually went through the
process of implementing kubernetes so
they could so BlackRock had a challenge
where they needed to be able to have
much more Dynamic access to their
resources and they were running complex
installations on people's desktops and
it was just really really difficult to
be able to manage their entire
infrastructure so they actually went and
pivoted to using kubernetes and this
allowed them to be able to be much more
scalable and expansive in the in the
management of their infrastructure and
as you can imagine kubernetes was then
hooked into their entire existing system
and has really become a key part of the
success that BlackRock is now
experiencing of a very stable
infrastructure and the bottom line is
that BlackRock is now able to have
confidence in their infrastructure and
be able to give their confidence as back
to their customers through the
implementation and more rapid deployment
of additional features and services so
let's go through a couple of scenarios
let's do one for kubernetes and then one
for Docker and we can actually go
through and understand what the problems
specific companies have actually had and
how they're able to to use the two
different tools to solve them so our
first one is with Bose and Bose had a
large catalog of products that kept
growing and their infrastructure had to
change so the way that they looked at
that was actually establishing two
primary goals to be able to allow their
product groups to be able to easier more
easily catch up to the scale of their
business so after going through a number
of solutions they ended up coming up
with a solution of having kubernetes
running their iot platform as a service
inside of Amazon's AWS cloud service and
what you'll see with both these products
is they're very Cloud friendly but here
we have um Bose and kubernetes working
together with AWS to be able to scale up
and meet the demands of their product
catalog and so the result is that we
were able to increase the number of
non-production deployments significantly
by taking the number of services from
being large bulky is down to small micro
Services being able to handle as many as
1250 plus deployments every year an
incredible amount of time and value has
been opened through the use of
kubernetes now let's have a look at
Docker and see what a similar problem
that people would have so the problem is
with PayPal and PayPal and processes
something in the region of over 200
payments per second across all of their
products and PayPal doesn't just have
PayPal they have Braintree and venmo so
the challenge that PayPal was really
being given is that they had different
architectures which resulted in
different maintenance cycles and
different deployment times and an
overall complexity from having a
decade-old architecture with PayPal
through to a modern architecture with a
venmo through the use of Docker paper
was able to unified application delivery
and be able to centralize the management
of all all of the containers with one
existing group the net net result is
that PayPal was able to migrate over 700
applications into Docker Enterprise
which consists of over 200 000
containers this ultimately opened up a
50 increase in availability for being
able to add in additional time for
building testing and deploying of
applications just a huge win for PayPal
now let's dig into kubernetes and Docker
and so kubernetes is an open source
platform and it's designed for being
able to maintain a large number of
containers and what you're going to find
is that your argument for kubernetes
versus Docker isn't a real argument it's
kubernetes and Docker working together
so kubernetes is able to manage the
infrastructure of a containerized
environment and Docker is the number one
container management solution and so
with Docker you able to automate the
deployment of your applications being
able to keep them in a very lightweight
environment and being able to create a
nice consistent experience so that your
developers are working in the same
containers that are then also pushed out
to production so with Docker you're able
to manage multiple containers running on
same Hardware much more efficiently than
you are with a VM environment the
productivity around Docker is extremely
high you're able to keep your
applications very isolated the
configuration for dark here is really
quick and easy you can be up and running
in minutes with Docker once you have it
installed and running on your
development machine or inside of your
devops environment so we look at the
deployment between the two
um and the differences kubernetes is
really designed for a combination of
PODS and services in its deployment
whereas with Docker it's around about
deploying services in containers so the
the difference and here is that
kubernetes is going to manage the entire
environment and then and that
environment consisting of PODS and
inside of a pod you're going to have all
of your containers that you're working
on and those containers are going to
control the surfaces that actually power
the applications that are being deployed
kubernetes is by default an auto scaling
solution it has it turned on and is
always available whereas a Docker does
not and that's not surprising because
Docker is a tool for building out
Solutions whereas kubernetes is about
managing your infrastructure kubernetes
is going to run health checks on the
liveness and Readiness of your entire
environment so not just one container
but tens of thousands of containers
whereas Docker is going to limit the
health check to the services that it's
managing within its own containers now
I'm not going to kid you kubernetes is
quite hard to set up it's it's if of the
tools that you're going to be using in
your devops environment it's not it's
not an easy setup for you to use and for
this reason you want to really take
advantage of the surfaces within Azure
and other similar Cloud environments
where they actually will do the setup
for you Docker in contrast is really
easy to set up you can as I mentioned
earlier you can be up and running in a
few minutes as you would expect the
fault tolerance within kubernetes is
very high and this is by Design because
the architecture of kubernetes is built
on the same architecture that Google
uses for managing its entire Cloud
infrastructure in contrast Docker has
lower fault tolerance but that's because
it's just managing the the services
within its own containers what you'll
find is that most public Cloud providers
will provide support for both kubernetes
and Docker here we've highlighted
Microsoft Azure because they were very
quick to jump on and support kubernetes
but the realities is that today Google
Amazon and many other providers are just
having first level support for
kubernetes it's just become extremely
popular in a very very short time frame
the company is using both kubernetes and
Docker is vast and every single day
there are more and more companies using
it and you should be able to look and
see whether or not you can add your own
company to this list any more hesitation
let's jump into some of the popular
interview questions that you're going to
get asked we're going to break up the
interview questions into three
categories it's going to be beginner
intermediate and advanced the beginner
questions are really going to cover
about three quarters of the questions
that we're going to ask here and then
the intermediate and advanced will
finalize out the we'll finish out the
final quarter and and the reason is is
that quite typically most interviews
you'll be asked you know intermediate to
beginner level questions mainly beginner
level questions just so you can
demonstrate your understanding of
kubernetes and then there's always one
Advanced question that gets thrown out
just to make sure that you really do
know what you're talking about and
you've done at least one or two
kubernetes environment setups to be able
to validate your experience so let's
kind of jump into that first section
which is you know why is kubernetes
widely used and your answer to this
really comes into the fact that
kubernetes is widely used because of
three distinct areas one is that
kubernetes is an open source platform
that is readily available for the
management containers and allows you to
manage the deployment scaling and
management of those containers and so
the first area is that it allows you for
easy deployment of your containers the
ability to scale effectively within
kubernetes not just vertically but
horizontally for your containers and
then finally the kubernetes environment
itself was really designed to make it
easy to manage containers
um in small parts other key areas of why
kubernetes is popular is that kubernetes
is the single largest open source
project out there since its launched in
2014 is just grown in Leaps and Bounds
around that project there is just a huge
Community both an online and physical
Community there are just a large number
of online groups that are supporting
kubernetes and allow you to get fast
access to information but in addition to
that there are also a lot of local
meetup groups that cover specifically
kubernetes whether as part of a devops
group or as part of a dedicated
kubernetes group the actual environment
itself is a really robust way of being
able to manage your containers and
containers are becoming the most popular
way of being able to deploy cloud-based
applications kubernetes is an effective
and persistent storage environment so
kubernetes is also able to effectively
manage persistent storage whether it's
local storage as a SSD drive or a local
hard drive or whether it's cloud-based
storage and because of the way that
kubernetes architecture is redesigned
for the cloud from the ground up so it's
a great way to get started in building
out your Cloud Solutions and then
finally the actual tools within
kubernetes are designed to help and
manage the health and monitoring of the
environment itself so what is Google
container engine so the Google container
engine is another open source solution
but it's designed for managing the
docker containers and clusters and so
kubernetes is a container management
solution and Docker is the most popular
container solution out there and this is
just a way of being able to bring the
two effectively with into each other so
that you can have Docker containers
manage the application and then
kubernetes match the scale of your
applications as their vertically and
horizontally scaled across your network
so what is the difference between
kubernetes and Docker and this is
important because kubernetes and Docker
are often dropped in the same sentence
and there's a lot of people that get
confused between the two so
fundamentally kubernetes is about taking
pods which contain containers which have
applications in and deploying them at
scale across a cloud Network Docker is
the container solution itself so the
actual application itself is put into
the docker container because of this
difference kubernetes is designed for
auto scaling is highly available whereas
Docker simply isn't because it doesn't
have to worry about that within the
container itself the kubernetes
environment will check for liveness and
Readiness of the health of the entire
infrastructure whereas with Docker it's
just really going to check the services
that are coming in and out of the actual
application itself within the container
kubernetes is complicated to set up it
really is just simply complicated setup
and what you'll find is a lot of cloud
services will actually do the setup for
you whereas Docker is very easy to set
up the as you would expect with the
tolerance ratio there is high fault
tolerance River kubernetes whereas with
Docker is low fault tolerance and again
this is fundamental in the two different
types of strategy that kubernetes and
Docker have they're both connected
tightly with each other but they both do
different things so what are the notable
features of kubernetes one is the
ability to do contain minor burns
kubernetes always knows where to place a
container within its entire network
within the pods and with its
infrastructure the services are managed
for Security Network and storage it is
self-monitoring so it goes in and is
able to check its own nodes for health
and the containers within the nodes so
it makes sure that the applications are
always running effectively you can do
the scaling effectively both vertically
and horizontally within kubernetes the
solution is open source so there's no
actual fee for the actual kubernetes
service itself and kubernetes allows for
storage orchestration what does it mean
by storage orchestration now storage
orchestration is that you're able to
access and share either local storage
such as a hard drive or SSD drive or a
cloud-based drive and my preference is
always to go with cloud-based drive
because then you're really positioning
Your solution to be a true cloud-based
application so what are the main
advantages of kubernetes oh well you get
automatic rollback for changes that go
wrong the a you're able to automate a
lot of the processes that would have
been manual processes previously you're
able to scale the resources both
vertically and horizontally easily and
quickly and then these three areas the
rollback automating roll Bank automating
services and being able to scale
indirectly allows you to save money by
reducing the amount of time and people
working on your operations environment
so let's break out the architecture of
kubernetes so you're going to be
explaining us to explain how is
kubernetes architecture structured and
there are three key areas there's the
application layer there's kubernetes
layer and then there's the
infrastructure layer we're going to
start off with the infrastructure layer
and so the infrastructure layer is our
base layer and this is where all your
where you make your clusters for your
collections of storage and networking
resources within kubernetes and this is
where you would have if your databases
are your data stored locally on a hard
drive or in the cloud your core Network
and the VMS that you'd be building out
your infrastructure on these allow you
to group many sheens together into a
single unit so that you're able to scale
up and down very easily to meet customer
demand so let's talk about the middle
layer which is the kubernetes layer now
so if we're looking at our architecture
for kubernetes layers really broken down
for the the core operating system the
docker container environment and the
kubernetes code itself and this is where
you're able to take machines and pull
them together as a cluster so that they
work within that kubernetes system you
have a master controller within the
kubernetes that actually then we'll go
ahead and manage the schedule the
control manager the API server and then
you'll be able to Cluster all of these
kubernetes together so that they're all
working and processing as one single
unit and then finally let's look at the
application layer where you actually
have your app and so the application
lamb where you have your pool and your
services you know kubernetes will use
API to connect in and out of this layer
it's to run your applications within
containers and you use instructions that
are written in yaml or Json to actually
set up those containers and you actually
have the environment running on a plan
that examines these uh instructions so
it keeps the entire environment healthy
API ecosystem is used to interact with
the cluster it's so you don't have to do
that by yourself so make sure that
you're referencing that the apis are
secured and connecting in and out of the
system and so you have your scheduling
control manager actually from the
kubernetes environment doing the hard
work and then the workers themselves
execute the the commands that coming
from kubernetes so you will be asked to
kind of dig into some of the key areas
such as explaining the master node and
listing its components so the master
note is uh price of four key components
there's the cluster store the controller
manager the scheduler and the API server
so four key components remember four
fingers four components and then you're
going to be asked to maybe dig deeper
into what is a cluster within kubernetes
and so a customer is really a
combination of resources such as
machines that into a single node and
then those nodes are pulled together and
work as a single cluster as a whole
kubernetes will work with that whole
cluster and then once you're actually
working with that whole cluster if a
node fails or needs to be removed
kubernetes is able to do that quickly
and effectively so let's talk a little
bit about what are the different types
of controllers in kubernetes and there
are five types of control manager you
have the node controller the service
account the token controller the
endpoint controller and the replication
controller and so the no controller
controls and handles the node in the
system the service account enables
access controls in the system the token
controller cleans up any tokens for
non-existent service accounts the
endpoint controller joins services in
pods to the endpoint objects and the
replication controller manages the Pod
life cycle so what is the role of the
cloud controller manager within this
environment and so the role of the cloud
control manager is really to help manage
persistent storage and so persistent
storage is storage that's either local
storage to the physical machine such as
an SSD or hard drive or cloud-based
storage and this allows the storage to
be shared across the entire network what
is the role of the cube API server and
the cube scheduler so the cube API
server responsible for the establishment
communication between the kubernetes
nodes and the master components and and
it does this through the through apis
the apis themselves are all commands
that are rest based commands and it
implements an interface which means
different tools and libraries can
communicate effectively with each other
so what is the role of the cube API
server and the cube scheduler so the
cube schedule actually does the
distribution of work across the entire
infrastructure so the actual yaml script
that you've written are executed
appropriately at the right time so you
can be asked to explain persistent
volumes in kubernetes so a persistent
volume is just a place where you can
store data again this can be local data
as in a hard drive local to the physical
machine or it can be cloud-based data so
what is a cubelet and a cube proxy so a
cubelet is a service responsible for
gaming information to and from the
control plan itself the process and the
Q proxy itself is responsible for
maintaining the work and the standards
within the node server in addition every
node in the cluster runs a simple
Network proxy and Cube proxy routes
requests to the correct container in
that node in addition you'll have some
basic and primitive load balancing and
management of the node from the proxy
itself and there is additional layers of
management that get done outside of the
proxy but the proxy is also there to
help do some localized load management
so what is Cube CTL so Cube CTL is your
command line or terminal window
interface that allows you to execute
commands and later in this video we'll
actually go through some of those
commands
etcd is the store that configures the
details and essential values such as key
value pairs that are distributed across
your entire network it also manages the
network rules and the post-forwarding of
activity so what are the disadvantages
of kubernetes well the first is it's
really hard to install and configure and
this is why Azure and many cloud
services such as Google Cloud will
actually do the installation and
configuration of kubernetes for you it
does take a long time to get up and
running with kubernetes not so much for
the actual software but you own
experience of being able to use
kubernetes it is not simple to manage
the service you actually have to have a
lot of experience and this is why people
get paid a lot for kubernetes skills
it's a difficult environment to get up
and running and be effective in that
management again kubernetes is just not
an easy platform to learn it takes a lot
of time and effort and one of the
disadvantages is that while kubernetes
itself is free because it is an open
source project getting the people who
are skilled you to using kubernetes is
expensive right so those were kind of
the introductory questions that you can
expect to be asked in your interview the
next set of questions are really going
to be on an intermediate and advanced
level and these are the questions that
will be thrown out you know that you see
these as being that the trick questions
which are really there to test that you
have the skill set to be able to
implement and manage kubernetes within a
company so what happens if a master node
hours what happens if a worker nerve
fails how would kubernetes manage that
well kubernetes is really designed for
this kind of scenario and so you can
actually look at it and the response
would be is that the Pod itself does not
get affected and kubernetes will
actually respond and fix the uh the
failed node whether it is uh needs to be
repaired but there needs to be restarted
or needs to be replaced so what does a
service role in kubernetes so a service
role is an extraction for all of the
pods it provides a virtual IEP address
it allows clients to connect to
Containers running in the pods using
that virtual IP address and the command
that you would use to get that would use
some of these Cube CTL commands it's
quite a simple one it's Cube CTL get
services and you get a full list then of
all the services that are available so
how do I do a rollback in deployment and
so there's a couple of ways you can do
the rollback and the first is if you
want to see the rollout history
deployment you would write Cube CTL
rollout history deployment image and
then in square brackets deployment if
you want to just restore to the previous
rollout the command would be Cube CTL
rollout undo deployment and then in
square brackets deployment and that
would take you back to the previous uh
deployment so what is an Ingress
controller so an Ingress controller it
really is a pod that manages that
inbound traffic and a prominent feature
that traffic is that it is secured with
an SSL termination and so what we're
going to do to now address some of the
key ways in which you can provide API
Securities on kubernetes and we have
eight of those so the first four are
implementing TL TLS for your security
the second would be API authentication
the third is to make the Cubist
protection the cubeless protected via
authorization mode equals webhook
command the fourth is to monitor rbac
failures the fifth is to to remove those
default service account permissions six
would be to filter egress to Cloud API
metadata apis the seventh is to use pod
security policy to restrict container
policies and protect the node and the
eighth and final is to keep the cube at
the latest version all right let's go
into some of the more advanced commands
and so the advanced questions you're
going to get are all going to be
commands that you're going to be asked
to write now my advice for you for these
questions is to make sure that you have
some of these basic commands memorized
they don't have to be perfect when
you're in the interview environment but
it's good to be able to get up to a
whiteboard and actually write out what
these commands are just so you can
actually demonstrate that you know what
the commands are the interviewer isn't
going to ask you questions on how would
you apply kubernetes to the company that
you're being hired for because you
simply don't have that knowledge but
they do want to be able to see clearly
that you understand the commands and
terminal window instructions that will
be able to demonstrate how you would
actually go in and access some of that
information so fun first questions you
might be asked is how do you package
kubernetes applications and so you would
use the package manager within
kubernetes to be able to manage to and
configure and deploy applications into
your cluster and so here we have the the
helm command that you would use so what
are init containers and a new container
gets executed for any other containers
can run on the Pod and so you want to be
able to illustrate what the init
instructions would look like and here we
have an example and I would again use
this example and maybe memorize it for
when you have this question come up in
your interview so what is the difference
between a config map and a secret and
these are more uh fundamentals so a
config map just has the instructions
that are clearly available in plain
tanks on what the cube is supposed to do
whereas the secrets are actually
encrypted and these are like passwords
and other sensitive data that you don't
want to have um easily available these
are encrypted information that have to
be decrypted to access the content so if
a node is Tainted is there a way to
schedule the pods to that node and the
answer is no there isn't and so what you
have to do then is schedule the Pod to
be fixed and so that schedule the Pod to
fix the tainted node to allow for
tolerations to that and we have here the
example code that you would use to do
that so how do you deploy a feature with
zero downtime in kubernetes well
kubernetes is designed to be able to
roll out applications with zero downtime
and here we have four examples using the
rolling update as a strategy for zero
downtime deployments in kubernetes again
memorize one of these so you can
actually illustrate how you use rollout
to be able to deploy a zero downtime
solution so how do you monitor a pod
that is always running and so to do that
you would use the liveness probe to
check the health of the application
within the Pod and see whether or not
there are any failures or whether or not
the Pod itself needs to be restarted so
how do you drain traffic from pod during
a maintenance so again
um this is a simple command for you to
remember but to be able to drain an
application so that it's easy for you to
be able to do maintenance on the
application you would use the cube CTL
drain and then in square brackets the
node name and that would then
temporarily drain that particular pods
you can actually do maintenance on it
what you have to then and this would
probably give you bonus points with your
in an interview is then reference the
uncordant command so that you can
uncoordinate the actual node itself so
that the Pod itself can come back up
into life and work as a fully
functioning solution so to be able to
tie surfaces to a podder to a set of
Parts is that you can apply labels to
the actual Parts themselves and then use
the labels as selectors to be able to
glue services to that pod and again we
have here an example of how you'd
actually do that and again this is one
of those where you can probably won't be
asked to write this out but it'd be good
to have this memorized how to get all
pods on a node and so the following
command will actually go ahead and bring
all the pods into a kubernetes cluster
and this is quite lengthy and you know
maybe you want to like talk to how this
would be done but we have the
instructions on how to do that right
here the important part is to make sure
that you are when you're talking about
the node name that you reference to
whoever you're interviewing that you
would what you would switch out to
reference the actual nodes that you're
trying to pull together into the
kubernetes cluster so how do pods Mount
NFS volumes and so in these examples
here we actually Define the NFS server
and then the Pod and how you'd actually
Mount two together and again what you
want to be able to do is illustrate why
you would use yaml and how you'd
actually then point to specific IP
addresses within your kubernetes
environment within either the production
or test environments to be able to
illustrate how you would Mount up those
NFS volumes before you start
understanding any automation tool it's
good to look back into what manual
testing is all about what are its
challenges and how automation tool
overcomes these challenges challenges
are always overcome by inventing
something new so let's see how selenium
came into existence and how did it
evolve to become one of the most popular
web application automation tool selenium
Suite of tools selenium is not a single
tool it has multiple components so we
will look into each of them and as you
know every automation tool has its own
advantages and limitations so we will be
looking at what the advantages are and
the limitations of selenium and how do
we work around those limitations all
right so let's get started manual
testing a definition if you can say a
manual testing involves the physical
execution of test cases against various
applications and to do what to detect
bugs and errors in your product it is
one of the Primitive methods of testing
a software this was the only method
which we knew of earlier it is execution
of test cases without using any
automation tools it does not require the
knowledge of a testing tool obviously
because everything is done manually also
you can practically test any application
since you are doing a manual testing so
let's take an example so say we have a
use case you are testing say a Facebook
application and in Facebook application
let's let's open the Facebook
application and say create an account
this is your web page which is under
test now now as a tester what would you
do you would write multiple test cases
to test each of the functionalities on
this page you will use multiple data
sets to test each of these fields like
the first name the surname mobile number
or the new password and you will also
test multiple links what are the
different links on this page like say
forgotten account or create a new page
so these are the multiple links
available on the web pages also you look
at each and every element of the web
page like your radio buttons like your
drop down list apart from this you would
do an access ability testing you would
do a performance testing for this page
or say a response time after you say
click on the login button literally you
can do any type of tests manually once
you have these test cases ready what do
you do you start executing this test
cases one by one you will find bugs your
developers are going to fix them and you
will need to rerun all these test cases
one by one again until all the bugs are
fixed and your application is ready to
ship now if one has to run test cases
with hundreds of transactions or the
data sets and repeat them can you
imagine the amount of effort required in
that now that brings us to the first
demerit of the manual testing manual
testing is a very time consuming process
and it is very boring also it is very
highly error prone why because it is
done annually and human mistakes are
bound to happen since it's a manual
executions tester's presence is required
all the time one is to keep doing manual
Steps step by step again all the time he
also has to create manual reports group
them format them so that we get good
looking reports also send this reports
manually to all stakeholders then
collection of logs from various machines
where you have run your test consoliding
all of them creating repositories and
maintaining them and again since it's
all as a manual process there is a high
chance of creating manual errors there
scope of manual testing is limited for
example let's say regression testing
ideally you would want to run all the
test cases which you have written but
since it's a manual process you would
not have the luxury of time to execute
all of them and hence you will pick and
choose your test cases to execute that
way you are limiting the scope of
testing also working with large amount
of data manually is Impractical which
could be the need of your application
what about performance testing you want
to collect metrics on various
performance measures as a part of your
performance testing you want to simulate
multiple loads on application under test
and hence manually performing these kind
of test is not feasible and to top it
all up say if you're working in an agile
model where code is being churned out by
developers testers are building their
test and they are executing them as and
when the bills are available for testing
and this happens iteratively and hence
you will need to run this test multiple
times during your development cycle and
doing this manually definitely becomes
very tedious and burning and is this the
effective way of doing it not at all so
what do we do we automate it so this
tells us why we automate one for faster
execution two to be less error prone and
three the main reason is to help
frequent execution of our test so there
are many tools available in the market
today for automation one such tool is
selenium birth of selenium much before
selenium there were various tools in the
market like say rft and qtp just to name
a few popular ones selenium was
introduced by gentlemen called Jason
Huggins way back in 2004. he was an
engineer at thoughtworks and he was
working on a web application which
needed frequent testing he realized the
inefficiency in manually testing this
web application repeatedly so what he
did was he wrote a JavaScript program
that automatically controlled the
browser actions and he named it as
JavaScript test run later he made this
open source and this was renamed as the
selenium core and this is how selenium
came into existence and since then
selenium has become one of the most
powerful tool for testing web
applications so how does selenium help
so we saw all the demerits of manual
testing so we can say by automation of
test cases one selenium helps in Speedy
execution of test cases since manual
execution is avoided the results are
more accurate No human errors since your
test cases are automated Human Resources
require to execute automated test cases
is far less than manual testing because
of that there is a lesser investment in
human resources it saves time and you
know time is money it's cost effective
as selenium is an open source it is
available free of cost early time to
Market since you save effort and time on
manual execution your clients will be
merrier as you would be able to ship
your product pretty fast lastly since
your test cases are automated you can
rerun them any point of time and as many
times as required so if this tool offers
so many benefits we definitely want to
know more detail about what selenium is
selenium enables us to test web
applications on all kind of browsers
like Internet Explorer Chrome Firefox
Safari Edge Opera and even the Headless
browser selenium is an open source and
it is platform independent the biggest
reason why people are preferring this
tool is because it is free of cost and
the qtp and the rft which we talked
about are chargeable selenium is a set
of tools and libraries to facilitate the
automation of web application as I said
it is not a single tool it has multiple
components which we'll be seeing in
detail in some time and all these tools
together help us test the web
application you can run selenium scripts
on any platform it is platform
independent why because it is primarily
developed in JavaScript it's very common
for manual testers not to have in-depth
programming knowledge so selenium has
this record and replay back tool called
the selenium ID which can be used to
create a set of actions as a script and
you can replay the script back however
this is mainly used for demo purposes
only because selenium is such a powerful
tool that you should be able to take
full advantage of all its features
selenium provides support for different
programming languages like Java python
c-sharp Ruby so you can write your test
scripts in any language you like one
need not know in-depth or Advanced
knowledge of these languages also
selenium supports different operating
systems it has purpose for Windows Macs
Linux even Ubuntu as well so so you can
run your selenium test on any platform
of your choice and hence selenium is the
most popular and widely used automation
tools for automating your web
applications selenium set of tools so
let's go a little more deeper into
selenium as I said selenium is not a
single tool it is a suite of tools so
let's look at some of the major
components or the tools in selenium and
what they have to offer so selenium has
four major components one selenium ID
it's the most simplest tool in the suite
of selenium it is integrated development
environment earlier selenium IDE was
available only as a Firefox plugin and
it offered a simple record and Playback
functionality it is a very simple to use
tool but it's mainly used for
prototyping and not used for creating
Automation in the real-time projects
because it has its own limitations like
any other record and replay tool
selenium RC this is nothing but selenium
remote control it is used to write web
application test in different
programming language what it does it
basically interacts with the browser
with the help of something called as RC
server and how it interacts is it uses a
simple HTTP post get request for
communication this was also called as
selenium 1.0 version but it got
deprecated in selenium 2.0 version and
was completely removed in 3.0 and it was
replaced by Webdriver and we will see in
detail as why this happened selenium
Webdriver this is the most important
component in the selenium Suite it is a
programming interface to create and
execute test cases it is obviously the
successor of the selenium RC which we
talked about because of certain
drawbacks which RC hat so what Webdriver
does is it interacts with the browsers
directly unlike RC where the RC required
a server to interact with the browser
and the last component is the selenium
grid so selenium grid is used to run
multiple test scripts on multiple
machines at the same time so it helps
you in achieving parallel execution
since the selenium Webdriver with you
can only do sequential execution grid is
what comes into picture where you can do
your parallel execution and why is
parallel execution important because in
real time environment you always have
the need to run test cases in a
distributed environment and that is what
grid helps you to achieve so all this
together helps us to create robust web
application test Automation and we will
go in detail about each of these
components so before that let's look at
the history of selenium version so what
did selenium version comprised of it had
an IDE RC and grid and as I said earlier
there were some disadvantages of using
RC so RC was on its path of deprecation
and web driver was taking its path so if
you look at selenium 2 version it had an
earlier version of Webdriver and also
the RC so they coexisted from three dot
onwards RC was completely mode and
Webdriver took its place there is also a
four dot version around the corner and
it has more features and enhancements
some some of the features which are
talked about are w3c Webdriver
standardization improved ID and improved
grid now let's look at each of the
components in the selenium Suite
selenium IDE is the most simplest tool
in the suite of selenium it is nothing
but an integrated development
environment for creating your automation
scripts it has a record and Playback
functionality and is a very simple and
easy to use tool it is available as a
Firefox plugin and a Chrome extension so
you can use either of this browser to
record your test scripts it's a very
simple user interface using which you
can create your scripts that interact
with your browser the commands created
in the scripts are called selenis
commands and they can be exported to the
supported programming language and hence
this code can be reused however this is
mainly used for prototyping and not used
for creating automation for your
real-time projects why because of its
own limitation which any other recorded
replay tool has so a bit history of
selenium ID so earlier selenium ID was
only a Firefox extension so we saw that
IDE was available since the selenium
version one selenium ID died with the
Firefox version 55 that was ID was
stopped supporting from 55 version
onwards and this was around 2017 time
frame however very recently all new
brand selenium ID has been launched by
apply tools and also they have made it a
cross browser so you can install the
extension on Chrome as well as as an
add-on on Firefox browser so they
completely revamped this IDE code and
now they have made it available on the
GitHub under the Apache 2.0 license and
for the demos today we'll be looking at
the new ID now with this new ID also
comes a good amount of features
reusability of test cases better
debugger and most importantly it
supports pattern test case execution so
they have introduced a utility called
selenium side Runner that allows you to
run your test cases on any browser so
you can create your automation using IDC
on Chrome or Firefox but through command
prompt using your site Runner you can
execute this test cases on any browser
thus by achieving your cross browser
testing control flow statement so
initially in the previous versions of
idea they were control flow statements
available however one had to install a
plugin to use that but now it is made
available out of box and what are these
control flow statements these are
nothing but your if else conditions the
while Loops the switch cases so on it
also has an improved locator
functionality that means it provides a
failover mechanism for locating elements
on your web page so let's look at how
this ID looks and how do we install it
and start working on that so for that
let me take you to my browser so say
let's go to the Firefox browser so on
this browser I already have the ID
installed so when you already have an ID
installed you will see an icon here
which says selenium ID and how do you
install this you simply need to go to
your Firefox add-ons here whether it
says find more extension so just type in
selenium ID and search for this
extension so in the search results you
see the selenium ID just click on that
and now since I've already installed
here it says remove otherwise for you it
is going to give you an add button here
just click on the add button it will
install this extension once it is
installed you should be able to see this
selenium ID icon here okay so now let's
go ahead and launch this ID so when I
click on that it is going to show me a
welcome page where it's going to give me
few options the first option is this is
record a new test case in a new project
so straight away if you choose this
option you can start recording a test
case in which case it's going to just
create a default project for you which
you can save it later then open an
existing project so you can open if you
already have a saved project create a
new project and close so I already have
an existing project with me for the demo
purpose so I'll go ahead and open that
so I'll say open existing project and I
have created a simple script what the
script does is it logs me into the
Facebook using a dummy user mail sorry
username and password that's all it's a
very simple script with few lines and
this is what it's going to do so what we
will simply do is we'll just run the
script and see how it works for that I'm
just going to reduce the test execution
speed so that you should be able to see
every step of execution here all right
so what I'll do now here is I'll just
adjust this window and I'll just simply
say run current test all right so I'll
just get this side by side so that you
should be able to see what exactly the
script is doing okay so now you are able
to see both the windows okay so now it's
going to type in your user email here
there you go and now it will enter the
password and it is clicked on the login
button so it's going to take a while to
say login and since these are the dummy
IDs it is you are not able to log in
here and you're going to see this error
window fine that is what is the expected
output here now on the ID if you look
here after I execute the test case every
statement or every command which I have
used here is color coded in green so
that means this particular step was
executed successfully and then here in
the log window it will give you a
complete lock of this test case right
from the first step till the end and
your end result is it says FB login
which is my test case name completed
successful let's look at few components
of this ID the first one is the menu bar
so let's go to our ID call it so the
menu bar is right here on the top so
here is your project name so either you
can add a new project here or rename
your project so since we already have
this project which is named as Facebook
and then on the right you have options
to create a new project open an existing
project or save the current project and
then comes on toolbar so using the
options in this toolbar you can control
the execution of your test cases so
first one here is the recording button
so this is what you use when you start
recording your script and then on the
left you have two options here to run
your test cases the first one is run all
tests so in case you have multiple test
cases written here you can execute them
one by one sequentially by using this
run all test else what you can do so if
you just want to run your current test
this is what you would use then ID has
this debugger option which you can use
to do a step execution so say for
example now whenever I run the script
it's going to execute each and every
command here sequentially so instead if
I just select the first command and say
do step execution all right so what it
does is the moment it finishes the First
Command which is opening of Facebook
right I think which is already done here
yeah all right so once this is done it
is going to wait immediately on the
second command and it says paused
debugger so from here you can do
whatever you would like to do in case
you want to change the command here you
can do that you can pause your execution
you can resume your execution here right
you can even completely stop your test
execution or you can just select this to
run the rest of the test case so if we
say run the test case what it is going
to do is it's just going to Simply go
ahead and complete the com complete the
test case now there is another option
here which is you see the timer there
which says test execution speed so to
execute your test cases in the speed you
want say whenever you're developing an
automation script right and say you want
to give a demo so you need to control
the speed sometimes so that the viewer
is able to exactly see all the steps
which is being performed and this gives
you an option to control that complete
execution right so do you see the
grading here so we have somewhere from
Fast to completely slow execution so the
previous demo which I showed was I
control the speed and then I executed it
so that we could see every command how
it is being executed all right so what's
the next this is called as an address
bar so whichever Wherever Whenever you
enter an URL here that is where you want
to conduct your test and another thing
what it does is it gives a history of
all the URLs which I've used for running
your test then here is where your script
is recorded So each and every
instruction is displayed here in the
order in which you have recorded the
script and then if you look here you
have something called as log and
reference so now log is an area where it
records each and every step of your
command as in when they get executed
right so if you see here it says open
https facebook.com and ok so that means
this command was executed successfully
and after the complete test case is done
it gives you whether the test case
passed or filled so in case there is a
failure you will immediately see this
test case is filled in red color also
there is something called as reference
here for example say if I click on any
of this command the reference tab what
it is going to to show me is a details
of this command which I have used in the
script it gives you the details of the
command as well as what the arguments
have been used or how how is that you
need to be using this particular command
okay so now what we'll do is let's go
ahead and write a simple script using
this ID so with this you'll get an ideas
how do we actually record scripts in ID
so for that I have a use case here a
very very simple use case so whatever
we'll do is we will open amazon.in then
we'll search simply search for say a
product iPhone and once we get that
search page where all your iPhones are
displayed we will just do an assert on
the title of the page simple all right
so let's do that so first thing what I
need is an URL okay so first let me go
to my Firefox browser here and say
amazon.in so why I'm doing this just to
Simply get the right URL absolute URL
path here and so that I don't make any
mistakes while typing in the URL okay so
I got this so let me close all this
windows I don't need any of this let's
minimize this all right so here what I
will do in the tests tab I'll say add a
new test and name this test as uh Amazon
search done I'll say add now I'll enter
this URL which I just copied it from my
browser okay and then I'll just say
start recording so what it did was since
I have entered the URL in this address
box it just opened the amazon.in URL now
let's do the test case so in my test
case what I said was I want to search
for iPhone once I have this I'm just
going to click on my search button so
now this gives me a list of all iPhones
and then I said I want to add an
assertion on the title of this page so
for me to do that what id gives me an
option is I have to just right click
anywhere on this page and you'll see the
selenium ID options here so in this I
will select assert title and then I will
close this browser so that kind of
completes my test case so now take a
look at all the steps which is created
for me so it says open slash because
I've already provided the URL here so
either you can replace it with your
regular URL or you can just leave it as
it is so what I will do since this is
going to be a proper script and I might
be using this to run it from a command
prompt also so I'll just replace this
target with the actual URL and then what
it is doing it is setting a window size
then there are whatever I did on that
particular URL on that website it has
recorded all the steps for me so this is
where it says type into this particular
text box which is my search box and what
did it type iPhone this was the value
which I entered now there was one more
feature which I told you in this new ID
which had which I said it has a failover
mechanism for your locating techniques
now that is what this is now if you look
here this ID is equal to two tab search
text block this is nothing but the
search box where we entered the text
iPhone and it has certain identification
through which this IDE identifies that
web element and that has multiple
options to select that particular search
box so right now what it has used is ID
is equal to two tab search box however
if you know the different locating
techniques you will be able to see here
that it has other techniques also which
it has identified like the name and the
CSS and the XPath so how does this help
in failovers say tomorrow if Amazon dot
in website changes the ID of this
element right you are not going to come
and rewrite the scripts again instead by
using the same script what it will do is
if this particular ID fails if it is
unable to find the element using the
first locator which is the ID it simply
moves to the next available ones and it
tries to search for that element until
one of this becomes true that is what
was the failure mechanism which has got
added now it's a very brilliant feature
because most of our test cases break
because of element location techniques
well let's come back to this so then we
added an assert title right so what is
assert Title Here it simply captures the
title of that particular page and it
checks this is all a very simple test
case so what we will do now is we will
stop the recording and then I've also
given a closed browser so right now what
I will do is I'll just comment this out
why because if I just run this test case
it's going to be very fast and you might
not be able to catch the exact command
execution what has happened all right so
right now I'll just disable it so that
it will just do all these test cases and
it just stays there without closing the
browser so now I'll just say run the
current test so your Amazon in is
launched okay it is typed in the iPhone
it's also clicked on the search so it is
done so now if you look here since we
are in the reference tab it is not able
to show so let's go to the log and now
let's see the lock so it's going to be
running log so if you notice here the
previous examples which we have run for
Facebook is also in the same lock so we
all have to see the log from running
Amazon search because that's our test
case so if you see here every command
line right it was executed success fully
assert title was also done and your test
case was executed successfully so it
passed now what we will do is on this
assert title I'll just modify this and
let's say just add some text I'll just
add double s here now this by
intentionally I am going to fail this
test case just to show you that whenever
there is a test case failure how does
the ID behaves and how do you get to
know the failures all right so I'll just
run the test test case again so before
that let's close the previous window all
right done and now here I'll also
uncomment the close because anyway it's
a failure which I'm going to see which I
should be able to see it in the logs so
I'll close the browser after the
execution of test case Okay so let's
simply go and run the test case Okay
amazon.in is launched it should search
for iPhone now yeah there you go all
right now it should also close the
browser yes let us close the browser and
it has failed now see here now this is
the line where our Command fit why
because the expected title was not there
and if you look in the logs it says your
assert title on Amazon dot in failed at
actual result was something different
and it did not match with what we had
asked it for so this is how simple it is
to use your ID to create your automation
scripts so we saw all the components of
ID we saw the record button then I
showed you the toolbar I showed you the
editor box and also the test execution
log so now let's come to what are the
limitations of this ID with IDE you
cannot export your scripts your test
scripts to web driver scripts this
support is not yet added but it is in
The Works Data driven testing like using
your Excel files or reading data from
the CSV files and passing it to the
script this capability is still not
available also you cannot connect to
database for reading your test data or
perform any kind of database testing
with selenium Webdriver yes you can also
unlike selenium Webdriver you do not
have a good reporting mechanism with the
ID like say for example test NG or
report NG so that brings us to the next
component of The Suite which is selenium
RC selenium remote control so selenium
RC was developed by Paul Hammond he
refactored the code which was developed
by Jason and was credited with Json as a
co-creator of selenium selenium server
is written in Java it is used to write
web application tests in different
programming languages as it supports
multiple programming languages like your
Java C sharp Pearl Python and Ruby it
interacts with a browser with the help
of an RC server so this RF save server
uses a simple HTTP get and post request
for communication and as I said earlier
also selenium RC was called as selenium
1.0 overshill but it got deprecated in
selenium 2.0 and was completely removed
in 3.0 and it got replaced by what
Webdriver and we'll see why this
happened and what was that issue which
we had with the RC server so this is the
architecture of selenium remote control
at a very high level so when Jason
Huggins introduced selenium you know the
tool was called as JavaScript program
and then that was also called as
selenium core so every HTML has a
JavaScript statements which are executed
by web browser and there is a JavaScript
engine which helps in executing this
command now this RC had one major issue
now what was that issue say for example
you have a test script say test dot
JavaScript here which you are trying to
access a limits from anywhere from the
google.com domain so what used to happen
is every element which is accessible are
the elements which can belong only to
google.com domain like say for example
mail the search or the drive so any
elements from this can be accessible
through your test scripts however
nothing outside the domain of say
google.com in this case was accessible
say for example if your test scripts
wanted to access something from
yahoo.com this was not possible and this
is due to the security reasons obviously
now to overcome that the testers what
they had to do was they had to install
the selenium core and the web server
which contained your web application
education which is under test on the
same machine and imagine if you have to
do this for every machine which is under
test this is not going to be feasible or
even effective all the time and this
issue is called as the same origin
policy now what the same origin policy
issue says is it prohibits a JavaScript
from accessing elements or interacting
with scripts from a domain different
from where it is launched and this is
purely for the security measure so if
you have written a scripts which can
access your google.com or anything
related to google.com these scripts
cannot access any elements outside the
domain like as we said in the example
yahoo.com this was the same origin
policy now to overcome this what this
gentleman did was he created something
called as selenium remote control server
to trick the browser in believing that
your course your selenium core and your
web application under test are from the
same domain and this is what was the
selenium remote control so if you look
at again high level architecture or how
did this actually work first you write
your test scripts which is here right in
any of the supported language like your
PHP or your Java or Python and before we
start testing we need to launch this RC
server which is a separate application
so this selenium server is responsible
for receiving the Cellini's commands and
these Cellini's commands are the ones
which you have written in your script it
interprets them and reports the result
back to your test so all that is done to
your RC server the browser interaction
which happens through RC server right
from here to your browser so this
happens through a simple HTTP post and
get request and that is how your RC
server and your browser communicate and
how exactly this communication happens
this RC server it acts like a proxy so
say your test scripts asked to launch a
browser so what happens is this commands
goes to your server and then your RC
server launches the browser it injects
the JavaScript into the browser once
this is done all the subsequent calls
from your test script right from your
test scripts to your browser goes
through your RC and now upon upon
receiving these instruction your
selenium core executes this actual
commands as JavaScript commands on the
browser and then the test results are
displayed back from your browser to your
RC to your test scripts so the same
cycle gets repeated right until the
complete test case execution is over so
for every command what you write in your
JavaScript here or your test script here
goes through a complete cycle of going
through the RC server to the browser
collecting the results again from the RC
server back to your test scripts so this
cycle gets repeated for every command
until your complete test execution is
done so RC had definitely a lot of
shortcomings and what are those so RC
server needs to be installed before
running any test scripts which we just
saw so that was an additional setup
since it acts as a mediator between your
commands which is your selling is
commands and your browser the
architecture of RC is complicated it why
because of its intermediate RC server
which is required to communicate with
the browser the execution of commands
takes very long it is slower we know why
because every command in this takes a
full trip from the test script to your
RC server to the core engine to the
browser and then back to the same route
which makes your overall test execution
very slow lastly the AP is supported by
RC are very redundant and confusing so
RC does have a good number of apis
however it is less object oriented so
they are redundant and confusing say for
example say if you want to write into a
text box how and when to use a type key
command or just a type command is always
confusing another example is some of the
mouse commas using a click or a mouse
door both kind of you know almost
providing a similar functionality so
that is the kind of confusion which
developers use to create hence
selenium RC got deprecated and is no
more available in later selenium
versions it is obsolete now now to
overcome these shortfalls web driver was
introduced so while RC was introduced in
2004 web driver was introduced by Simon
Stewart in 2006. It's a class platform
testing platform so Webdriver can run on
any platform like say Linux Windows Mac
or even if you have a Ubuntu machine you
can run your selenium scripts on this
machine it is a programming interface to
run test cases it is not an ID and how
does this work actually so test cases
are created and executed using web
elements or objects using the object
locator and the web driver method so
when I do a demo you will understand
what this Webdriver methods are and how
do we locate the web elements on the web
page it does not require a core engine
like RC so it is pretty fast why because
web driver interacts directly with the
browser and it does not have that
intermediate server like the RC hat so
each browser in this case what happens
is each browser has its own driver on
which the application runs and this
driver is responsible to make the
browser understand the commands which
you will be passing from the script like
say for example click of a button or you
want to enter some text so through your
script you tell which browser you want
to work with say Chrome and then the
Chrome driver is responsible for
interpreting your instructions and to
execute it on the web application
launched on the Chrome browser so like
RC Webdriver also supports multiple
programming languages in which you can
write your test Scripts
so another advantage of web driver is it
supports various Frameworks like test NG
junit and unit and Report entry so when
we talk about the limitations of
Webdriver you will appreciate how this
support for various Frameworks and Tool
help in making the selenium a complete
automation solution for web application
so let's look at the architecture of
Webdriver at a high level what is in
Webdriver so Webdriver consists of four
major components
the first one is we have got client
libraries right or what we also call it
as language bindings
so since selenium supports multiple
language and you are free to use any of
the supported languages to create your
automation script these libraries are
made available on your selenium website
which you need to download and then
write your scripts accordingly so let's
go and see from where do we download
this so if I go to my browser
so
seleniumhq.org right so if you're
working with selenium this website is
your Bible so anything and everything
you need to know about selenium right
you need to come here and use all the
stabs here in this website so right now
what we are going to look at is what are
those language binding so for that I'll
have to go to this download tab here
okay and if you scroll down here you
will see something like selenium client
and web driver language bindings and for
each of the supported language of
selenium you have a download link
right so say for example if you're
working with Java here what you need to
do is you need to download your Java
language finding so let's go back to the
presentation so this is where your
language bindings are available next so
selenium provides lots of apis for us to
interact with the browser and when we do
the demo I'll be showing you some of
this APS right and these are nothing but
the rest apis and everything whatever we
do through the script happens through
the rest course then we have a Json wire
protocol what is Json JavaScript object
notation it is nothing but a standard
for exchanging data over the web so for
example you want to say launch a web
application through your script
so what selenium does it it creates a
Json payload and posts the request to
the browser driver that is here and then
we have this browser drivers themselves
and as I said there is a specific driver
for each browser
as you know every tool has its own
limitation so does selenium so let's
look at what these limitations are and
if there are any workarounds for them
cannot test mobile applications requires
framework like APM
selenium is for automating web
application it cannot handle mobile
applications mobile applications are
little different and they need its own
set of automation tool however what
selenium provides is a support for
integrating this APM tool which is
nothing but a mobile application
automation tool and using APM and
selenium you can still achieve mobile
application automation
and when do you usually need this when
your application under test is also
supported on mobile devices you would
want a mechanism to run the same test
cases on web browser as well as your
mobile process right so this is how you
achieve it the next limitation so when
we talked about the components of
selenium I said that with Webdriver we
can achieve only sequential execution
however in real time scenario we cannot
just live with this we need to have a
mechanism to run our test cases
parallelly on multiple machines as well
as on multiple browsers so though this
is a limitation of web driver but what
selenium offers is something called as
grid which helps us achieve this and we
will see in shortly what the selenium
grid is all about also if you want to
know more details as how do we work with
the grid how do you want to install that
grid so do check out our video on simply
learn website on selenium grid third
limitations so limited reporting
capability so selenium Webdriver has a
limited reporting capability it can
create basic reports but what we
definitely need is a more
so it does support some tools like say
test engine report NG and even extend
reports which you can integrate with
selenium and generate beautiful reports
powerful isn't it also there are other
challenges with selenium like selenium
is not very good with image testing
especially for the ones which are
designed for web application automation
but then we have other tools which can
be used along with selenium like autoit
and securely so if you look at all this
selenium still provides a complete
solution for your automation needs and
that's the beauty of selenium and that
is why it makes the most popular tool of
today for automation
okay let's do a quick comparison between
the selenium RC and the web driver
so RC has a very complex architecture
you know why because of the additional
RC server whereas due to direct
interaction with the browser web driver
architecture is pretty simple execution
speed
it is slower in RC and much faster than
Webdriver why because in Webdriver we
have eliminated the complete layer of
selenium server right that's the RC
server and we established a direct
communication with the browser through
browser drivers it requires an RC server
to interact with the browsers we just
talked about it and whereas Webdriver
can directly interact with the browser
so RC again we talked about this as one
of the limitations that we have lot of
redundant ABS which kept developers
guessing as which API to use for what
functionality however Webdriver offers
pretty clean apis to work with
RC did not offer any support for
headless browser whereas in Webdriver
you do have a support for using headless
browsers
let's see the web driver in action now
now for the demo we will use this
particular use case and what this use
case says is navigate to the official
simply learn website then type the
selenium in search bar and click on it
and click on the selenium 3.0 training
so we are basically searching for
selenium 3.0 training on the simply
learn website first let's do the steps
manually and then we will go ahead and
write the automation script so let's go
to my browser on my browser what I'll do
is let me first launch the simply learn
website
okay and here what my use case step says
is I need to search for selenium and
click on the search button so once I do
that it is going to give me a complete
list of all kind of selenium trainings
which is available with simple learn and
what I'm interested in is the selenium
3.0 training here once I find this on
the web page I need to go and click on
that all right so this is all the steps
which we are going to perform in this
use case okay now so for writing the
test cases I'll be using an ID which is
Eclipse I've already installed my
eclipse and also I have installed
selenium in this instance of my Eclipse
all right so if you can see the
reference library folder here you will
see all the jars which are required for
the selenium to work next another prereq
which is required for selenium and that
is your driver files now every browser
which you want to work with has its own
driver file to execute your selenium
scripts and since for this demo I'll be
working with the Firefox browser I will
need a driver file for Firefox now
driver file for Firefox is the gecko
driver which I have already downloaded
and placed in my folder called drivers
now where did I download this from let's
go ahead and see that so if I go back to
my browser and if you go to your
selenium HQ dot website you have to go
to this download tab here in the
download tab when you scroll down you
will see something like third party
drivers bindings and plugins in this
you'll see the list of all the browsers
which is supported by selenium and
against each of this browser you will
find a link which has the driver files
now since we'll be using the gecko
driver
this is the link where you need to go to
and depending on which operating system
which you're working on you need to
download that particular file now since
I am working on Mac this is the file
which I'm using if you are a Windows
user you need to download this ZIP file
and unzip it so once you unzip that you
would
get a file called gecko driver for your
Firefox or a chrome driver for your
Chrome browser and then what you do is
you just create a directory called
drivers under your project and just
place the driver files here so these are
the two prereqs for your selenium one is
importing your jar files like this and
then having your drivers downloaded and
keep them under a folder where you can
reference to okay so now we'll go ahead
and create a class I already have a
package created in this project so I'll
use this project and create a new class
so I'll say create new Java class and
let's call this as search training I'll
be using a public static wordman and
I'll click on finish so let's remove
this Auto generated lines as we do not
need all right now the first statement
which you need to write before even you
start writing the rest of your code is
what you need to do is you need to
define or declare your driver variable
using your class web driver so what I
would do is I'll say Webdriver
driver
done all right now you will see that
this ID is going to flash some errors
for you that means it is going to ask
you to import certain libraries which is
required by the web driver so simply
just go ahead and say import Webdriver
from
org.opensq.sellini this is the package
which we will need all right so you have
a driver created which is of the class
web driver and now after this I'm going
to create three methods all right so
first method I will have for launching
the Firefox browser okay and then I will
write a simple method for searching
selenium training and clicking on it
this is the actual use case what we'll
be doing and then third method I'm going
to write is just to close the browser
which I'm going to be opening right so
these are the different methods which
I'll be creating and from the public
static wordman I will just call these
methods one after the other okay so
let's go ahead and write the first
method now my first method is launching
the Firefox browser so I'll say public
void since my return type is null or
there is no return type for this let's
call it as launch browser okay all right
now in this for launching any browser I
need to mention two steps now the first
step is where I need to do a system.set
property okay let's do that first and
then I'll explain what this does I'll
just say system dot set property so this
accepts a key and a value pair so what
is my key here my key here is Webdriver
dot gecko dot driver and I need to
provide a value so value is nothing but
the path to the gecko driver and we know
that this gecko driver which I'm going
to use here is right here in the same
project path under the drivers folder
okay and that is what the path which I
am going to provide here so here simply
I need to say drivers slash gecko driver
gec KO all right done and let me close
this sentence all right now since I am a
Mac User my gecko driver installable is
just the name gecko driver if your
windows use user and if you are running
your selenium scripts on the Windows
machine you need to provide a complete
path to this including dot exe because
driver executable on your machines is
going to be gecko driver.exe all right
so just make sure that your path which
you mentioned here in the system.set
property is the correct path okay then
the next thing what we need to do is I
need to just say driver is equal to new
Firefox driver okay so this command new
Firefox driver creates an instance of
the Firefox browser now this is also
flagging me error why because again it's
going to ask me to import the packages
where the Firefox driver class is
present okay we did that now these two
lines are responsible for launching the
Firefox browser form so this is done so
what's my next step in the use case now
I need to launch the website simply
learn so for that we have a command
called driver dot get driver dot get
what it does is whatever URL you're
going to give it here in this double
quotes as an argument it is going to
launch that particular website and for
us it's a simply learn website so what I
do as a best practices instead of typing
out the URL I go to my browser launch
that URL which I want to test and I
simply copy it come back to your eclipse
and just simply paste it so this ensures
that I do not make any mistakes in the
URL okay so done so our first method is
ready where we are launching the browser
which is our Firefox browser and then
launching the simply learn website now
the next method what is my next method
in my next method I need to give the
search string to search selenium
training on this particular website now
for that we need to do few things what
are those few things let's go to the
website again all right so let me
relaunch this let's close this okay let
me remove all this and let's go to the
home page first okay this is my home
page so as you saw when I did a manual
testing of this I entered the text here
so now since I have to write a script
for this first I need to identify what
this element momentous for that what I'm
going to do is I'm just going to say
right click here and I'll say inspect
element all right now this element let's
see what attribute it has which I can
use for finding this element so I I see
that there is an ID present so what I'm
going to do is I'm just going to Simply
use this ID and then I'll just copy this
ID from here go back to Eclipse let's
write a method first so I'll say public
void and what do we give the method name
say search training or just search all
right now in this I need to use a
command called driver dot find element
by ID is what I'm going to use as a
locating technique and in double quotes
the ID which I copied from the website
is what I'm going to paste here okay and
then what am I going to do on this
element is I need to send that text the
text which I'm going to search for which
is selenium so I'll just say send keys
and whatever text I want to send I need
to give it in double quotes So for that
selenium so this is done so now I've
entered the text texture and after
entering the text I need to click on
this button so for that I need to first
know what that button is so let's
inspect that search button okay now if
you look at the search button other than
the tag which is span and the class name
I do not have anything here all right so
what I can do is I can either use the
class name or I can write an X path
since this is a demo which we have
already used ID locating technique I
would go ahead and use the X path here
so for me to construct an X path I will
copy this class first okay and then I
already have a crow path installed on my
Firefox so I'll use the crow path and
first test my X bar so I'll just say
double slash let's see what was that
element it has a span tag okay so I'll
have to use span and at class equal to
and I'll just copy the class name here
and let's see if it can identify that
element yeah so it is able to identify
so I'll just use this x path in my code
so I'll go back to eclipse and I'll say
driver dot find element buy Dot X path
and the X path which I just copied from
cropath is what I'm going to paste here
and what is the action I need to do here
I need to say click done so I have
reached a stage where I have entered
this selenium okay and then I have
clicked on the search button once I do
this I know that expected result is I
should be able to find this particular
link here selenium 3.0 training okay and
I should be able to click on that so for
that again I need to inspect this so
let's inspect this selenium 3.2 all
right so now what are the elements this
has now this particular element has
attributes like it has a tag H2 then it
has got some class name and some other
attributes so I would again would like
to use a x path here now this time while
using the X path I am going to make use
of a text functionality so that I can
search for this particular text so I'll
simply copy this I'll go to my Crow path
the tag is H2 so I'll say simply H2 okay
and here I'll say text equal to and this
is the text which I copied I missed all
that yes there so I'm just going to add
an S okay so let's first test here
whether it is able to identify that
element yeah so it is able to identify
so can you see your blue dotted line it
is able to show us which element it is
identified so I'll copy this x path now
and let's go to my ID Eclipse so now
here what I need to do is I'll have to
again simply say driver dot find element
by dot X path and paste the X path which
we just did and then again I have to do
a click operation done all right so
technically we have taken all the steps
of the use case and we have written the
commands for that now let's add an
additional thing here say after coming
to this page after finding this we want
to uh say print the title of this page
now what is the title of this page if
you just hover your mouse on this it
says online and classroom training for
professional certification courses
simply now so what I will do is after
doing all these operations I will just
print out this page title on our console
so for that I have to just do this drive
fiber dot so let's do a sys out so I'll
say sys out
system.out.println okay and here I would
say let's add a text here the page title
is and then let's append it with driver
dot get title so this is the command
which we'll be using to fetch the page
title done now what is the last method I
need to add just to close the browser
all right so let me add a method here
I'll say public void close browser and
it's a one single command which I need
to call I'll say driver dot quit Okay
and then I need to call all these
methods from my public static void main
so I let me use my class name which is
this so I'm going to create an object
obj is equal to new class name and then
using this object first is I need to
call the method launch browser and then
I'll call the method search right and
then I'll call the method close browser
so technically our script is is ready
with all the functionality which we
wanted to cover from our use case now
there are few other tweaks which I need
to do this and I'll tell you why I need
to do this now for example after we
click here right after we click on the
search if you observed on your website
it took a little while before it listed
out all the selenium trainings for us
and usually when you are actually doing
it you wait for the selenium 3.0
training to be available and then you
click on that now same thing you also
need to tell your scripts to do that you
need to tell your scripts to wait for a
while until you start seeing the
selenium 3.0 training or it appears on
your web page there are multiple ways to
do that in your script and it is a part
of overall synchronization what we call
where we use kind of implicit and
explicit kind of favorites now since
this is a demo for demo purpose what I'm
going to do is I am going to use a
command curve thread.sleep and I'm just
going to give an explicit weight of say
three seconds so you can use this Main
only for the demo purposes you can use a
thread.sleep command now this
thread.sleep command needs us to handle
some exceptions so I'm just going to
click on ADD throws declaration and say
interrupted exception now same thing
I'll have to do it in my main function
also okay so let's do that and complete
it all right so this is done so by doing
this what am I doing I'm ensuring that
before I click on the selenium three dot
training we are giving enough time for
the script to wait until the web page
shows this link to the selenium 3.0
training that's one thing I'm doing all
right and also now since you are going
to be seeing this demo through the video
recording the script when it starts
running it is going to be very fast so
you might just miss out saying how it
does the send keys and how did it click
on the search button for us to enable us
to see it properly I'll just add some
explicit weights here just for a demo
purpose so after entering the keys right
so what I'll do is I'll just give a
simple thread.sleep here now
okay so probably a three seconds or two
seconds which should be good enough okay
at three seconds weight should be good
enough here so that we should be able to
see how exactly this works on your
browser when we execute this okay now
our complete script is ready so what
I'll do is I'll just save the script and
then we will simply run the script so to
run the script I just say right click
run as Java application okay it says ask
me to select and save I have saved the
script now so let's observe how it runs
okay the simply learn.com the website is
launched so the selenium text has been
entered in the search box it has clicked
on the search okay all right so now it
did everything whatever we wanted it to
do all right so since we are closing the
browser you are unable to see whether
the selenium three dot training was
selected or not however what I have
given here is to fetch the title after
all these operations were complete and
if you see here the complete operations
was done and and we were able to see the
page title here okay so now what we'll
do since we are unable to see whether it
clicked on the selenium 3. training or
not I'll just comment out the closed
browser the command okay so we will not
call the close browser so that the
browser remains open and we get to see
whether did it really find the training
link or not okay so let me close this
window we don't need this Firefox window
close all tabs and then I'll just
re-execute the script so I'll say run as
Java application so save the file okay
simplylearn.com is launched so search
text is entered now it's going to click
on the search button yes all right so
we've got the search results it should
click on selenium 3.0 training and yes
it is successfully able to click on that
all right so now it's not going to close
the browser because we have commented on
that line however it did print us the
title here all right so this is a simple
way of using the selenium Scripts
selenium grid so grid is used to run
multiple test scripts on multiple
machines at the same time with Webdriver
you can only do sequential execution but
in real time environment you always have
the need to run test cases in
distributed environment and that is
where selenium grid comes into picture
so grid was conceptualized and developed
by Patrick the main objective is to
minimize test execution time and how by
running your test parallelly so design
is in such a way that commands are
distributed on multiple machines where
you want to run tests and all these are
executed simultaneously what do you
achieve by this methodology of course
the parallel execution on different
browsers and operating system grid is
pretty flexible and can integrate with
many tools like say you want a reporting
tool integrated to pull all the reports
from the multiple machines where you're
running your test cases and you want to
present that report in a good looking
format so you have an option to
integrate such report okay so how does
this grid work so grid has as a hub and
node concept which helps in achieving
the parallel execution let's take an
example say your application supports
all browsers and most of the operating
system like as in this picture you could
say one of them is a Windows machine one
of them is a Mac machine and another one
is say a Linux machine so your
requirement is to run the test on all
supported browsers and operating system
like the one which is depicted in this
picture so what you have to do is first
thing is you configure a Master machine
or what you also call it as a hub by
running something called a selenium
Standalone server and this Talent
Standalone server can be downloaded from
the selenium HQ website using the server
you create a hub configuration that is
this node and then you create nodes
specific to your machine requirement and
how are these nodes created you again
use the same server which is your
Standalone selenium server to create the
node configuration so I'll show you
where the selenium server can be
downloaded so if we go back to our
selenium HQ website so you can see here
right on the top it says selenium
Standalone server and this is the latest
version available so you just need to
download once you download this keep a
version of this one on your master
machine that is your Hub and this should
also be present on your node machines
and there is a certain command which you
can use to install the server as a node
and as a master depending on your
configuration so once that is done you
have your master and your master or the
Hub and the notes created your master
can control on which nodes you want to
execute test now say if this is a Mac
machine this is a Windows machine
through your master or your Hub you
control on which machine you want to
execute what is then also you could have
a multiple combination of tests like say
for example smoke test to be run on all
these nodes that's the kind of
configuration you can create or a
certain component and test to be run on
say only Windows 10 with an edge browser
or say regression test on Mac machine so
any such things whatever configurations
you have for your node what happens is
the Hub picks the right node which
confirms to your requirements or the
configurations which you have set up and
then sends a command to the node to
execute those test cases on the Node
everything is controlled through your
master machine only here what happens on
the Node so once the node gets the
command from the master the node
actually runs all the test cases which
is it which it intends to run on the
specific browser which you have
mentioned in your configuration file so
this is a very high level architecture
and how a grid works so if you want to
know more details of how the grid works
you can refer to our video on simply
learn all right again limitations so
what limitation does is grid have now
the first one it has relatively poor
scalability compared to many modern
applications it is pretty static each
server is configured in advance with a
subset of required browser instances
available so if you want to vary this
you'll need to reconfigure so any
configuration tomorrow on any machines
you want to change it cannot be done
automatically you need to completely
reconfigure that node and then add it
back to the grid thirdly although it is
available to run on Virtual servers it
is actually not designed as a cloud
native application and because of as a
result of this it isn't optimized to
take advantage of things like say
distributed storage or dynamic scaling
or even automatic failover so these are
some of the limitations of the grid
advantages of selenium speed and
accuracy of course since there is no
manual execution your test can run
faster and with grid it helps in
parallel execution and you can run or
execute large volumes of test within a
very short time frame let's take an
example say before just before the
release of your product say during the
last lap of your testing you find a bug
and of course it has to be fixed so the
developer fixes the bug and you want to
run all the test cases pertaining to
that area now can you imagine that if
you have to do this testing manually how
long it's going to take so by automation
you can achieve this in a very short
duration of time and it will also help
you release your product in time with
very minimal or absolutely no human
errors the second Advantage it's an open
source so it's available free of cost so
anyone can download it and start using
it it supports wide spectrum of
programming languages we have been
seeing throughout this presentation it
is not restricted to any particular
programming language so whatever
language you are comfortable with you
have an option to use that and write
your selenium scripts selenium has
support for almost all browsers and
operating system we also talked about
headless browser support So this helps
you create test cases once using any of
the browser and run them across all
browsers and all operating system thus
saving you lot of time from manual
execution and helping in achieving a
very broad test coverage and we also saw
that pretty easy to use tool right so
you can check out all our videos and you
can it's just a matter of time that you
can Master this killer writing your
automation scripts reusability of course
like any programming language it
provides you a mechanism to reuse your
code and avoid redundancy well again
let's see an overall what are the
limitations of selenium itself now since
selenium is open source that is one of
the biggest Advantage now there is a
little disadvantage which comes with
that and what is it you do not have much
technical support because it's an open
source code however there are loads and
loads of documentations and forums
available which you can definitely refer
to and selenium as we said it is only
for automating web applications it
cannot handle the mobile applications or
even the desktop application however it
does provide support for integrating
tools like APM for mobile testing
selenium is not very good with image
testing especially designed for your web
application tools again we have tools
like autoit and securely which can be
integrated with selenium selenium
Webdriver has limited a limited
reporting capability but again it does
provide us support for integrating tools
like test NG report NG and extend report
it does not have test management
capabilities it need not right not one
tool need not have each and everything
what is required but there is always a
way to integrate it so selenium does
provide a way to integrate any of the
test management tools now since selenium
supports multiple programming language
the developer of test automation will
require to have some basic knowledge of
any of the programming language it
supports to write effective automation
scripts right so this you could look at
it as an advantage or as an disadvantage
so overall if you look at all this right
selenium still provides a complete
solution for our automation need and
that is why we can still say that
selenium is one of the most popular tool
used in the industry today for your web
application automation so this video
consists of three parts basically three
different levels of set of questions one
is the beginner level second is the
intermediate level and then the advanced
level questions and each level has a set
of 10 most probable questions which you
might be asked in the interview so let's
begin with the beginner level questions
question one what are selenium Suite
components so selenium is a set of tools
and libraries to facilitate automation
on web applications you already know
this it is not a single tool so what are
those components which are comprised in
the selenium so selenium comes with four
major components the first one is the id
id happens to be the most simplest tool
in the whole Suite of selenium it is
nothing but an integrated development
environment which has a record and
Playback functionality this was earlier
available as only Firefox plugin however
you need to know that the new version of
selenium IDE is available as a Chrome
extension as well as a Firefox plugin so
that means you can either use any of the
browsers that is Firefox or the Chrome
for recording your test scripts also you
should also know that the new ID comes
with many new features and now this new
ID has become more powerful than what
was available earlier so some of these
new features are like reusability of
your test cases it comes with a better
debugger now and most importantly it
supports parallel test execution and how
does it do that it has a utility called
selenium side Runner that's about
selenium ID the next component is the
selenium remote control so selenium
remote control or popularly called a
selenium RC is used to write web
application test in different
programming languages it supports
multiple programming languages like Java
C sharp Perl python Ruby and PHP it is
nothing but it's a server which
interacts with your browser using a
simple HTTP get and post request for
communication selenium RC is also called
called as selenium 1.0 version or
selenium core third component selenium
Webdriver it is a programming interface
to run your test cases and Webdriver was
introduced by Simon Stewart in 2006 and
this was introduced to overcome some of
the limitations of selenium RC so asses
or architecture was very complex because
it required an additional server in its
setup and why was that additional server
for communicating with the browser and
what Webdriver did was it removed this
dependency completely Webdriver
interacts directly with the browser and
also makes your test cases run much
faster than your RCA the last component
the selenium grid a very important tool
this is used to run multiple test
scripts on multiple machines at the same
time now with the selenium Webdriver you
can only do sequential execution but in
real time environment you always have
the need to run test cases in a
distributed environment so this grid
allows parallel execution of tests on
different browsers as well as different
operating system so the design is in
such a way that commands are distributed
on multiple machines where you want to
run the test and they are executed
simultaneously this uses a simple hover
node concept where Hub controls the all
execution of your test cases on the
nodes and this is how we achieve
parallel execution and what is the major
advantage of having this grid it is to
reduce your overall test execution type
question 2 what are the limitations of
selenium now like any other tool
selenium also has certain limitations so
when you are answering this question you
will also be expected to talk about what
is the workaround available for all
these limitations so let's look at them
one by one no reliable tech support of
course selenium is an open source so if
you look at it you do not have much
technical support available however
there are loads of documentation
available there are forums available
which you can refer to
seleniumhq.org is the website is your
Bible if you're working with the selling
it and then there are there are a lot of
selenium Google Groups and the forums
which are available for any kind of
support So selenium has been there for
quite some time so it's it's one of the
most mature toll available today so even
though we do not have a technical
support available there is enough
documentation available on the internet
the second limitation selenium can test
only web application selenium is the
open source tool that is designed to
automate web-based applications on
different browsers but it cannot handle
Windows GUI or non-html pop-ups in the
application now what is the workaround
available for that selenium provides a
support for integrating other tools like
autoit so Auto edit is a tool for
handling your windows based activity
selenium also cannot automate mobile
applications but then what is available
with that it can integrate a tool called
APM tool which is a mobile mobile
automation tool the third limitation
limited support for image testing
selenium is a de facto tool for
functional web web application tests so
standard API allows interacting only
with the browser and it's hard to test
the images based applications using
selenium what is the other tool
available securely is a very good tool
to use for image testing and this again
you can integrate securely with selenium
that's how you can achieve your image
testing through selenium but using the
securely tool no built-in reporting
facility so selenium does not have any
reporting capability as such of course
it does provide a very limited reporting
capability where some basic reports are
created but definitely when you are
working in a framework you need a better
support for a good reporting tools so
that way selenium supports some of the
tools like test NG report NG and
extended reports using which you can
integrate these tools with selenium and
generate beautiful reports Limited Test
management selenium does not provide any
test tool integration with test
management tools may require knowledge
of programming languages so since
selenium supports multiple programming
languages the developer of the test
automation will require to have some
basic knowledge of any of the supported
programming language why one in order to
write your effective automation scripts
and of course because selenium tool you
want to use it at its full potential
question 3 what are the testing types
supported by selenium so selenium
supports regression testing and
functional testing these can be called
as a very high level categories so what
is regression testing regression testing
is a full or partial selection of
already executed test cases which are
re-executed to ensure your existing
functionality does not break so let's
look at what are these attributes of
regression testing and why we say that
selenium helps us in doing regression
testing so regression tests are run
every time a new fix has been received
or a new feature has been deployed so
that is called as retesting you can
always select a full Suite or selected
test to test a fix or a complete product
so your selection can be based on
recently added features a test area
where most effects are there or
integration areas or even end-to-end
kind of tests you can also prioritize
your test cases based on critical
functionalities and business impact of
this test cases according to the
priority set so all this can be done
using selenium now what is the second
category of the testing which it
supports functional testing so
functional testing includes what your
smoke test sanity test say install test
database test basically any kind of
functionality which you are going to
test and if you look at the kind of
steps which are involved in that it
involves identification of your test
inputs expected outputs or the results
the test Case Logic itself and the
assertions so a typical flow will be you
identify your test input first then you
derive your test outcomes or the
expected outputs you design and execute
your test cases and then assert on the
expected versus actual outcomes question
4 what are selenium 2.0 and 3.0 versions
so selenium was introduced by a
gentleman called Jason Huggins way back
in 2004 and that was selenium version
since then this tool has gone through
multiple changes to make it better and
better so if selenium 2 dot version had
a major change it took a Leap Forward in
terms of browser automation the selenium
RC the remote control had a disadvantage
of having the need of an additional
server to talk to the web browsers and
because of this the text execution was
much slower and the architecture itself
became very complex so if you look at
the selenium 2 it had two components a
successor to RC and the RC itself which
is called as your core so this successor
was nothing but your web Drive API so
Webdriver apis made it easier to write
your automation scripts for any browser
by simply using a suitable driver for
any browser which you want to run the
tests on apart from these two component
the selenium 2.0 version also had the ID
and the grid components so in summary if
you look at it selenium 2.4 version has
an ID it has selenium RC which was on
its way of deprecation Webdriver API
which was just introduced in there a
very very earlier version of Webdriver
API and the grid so what did selenium
3.0 consists so selenium 3.0 RC was
completely eliminated now because in 2.0
it was already on its path of
deprecation and Webdriver replaced RC
completely so if you look at the
components of 3.0 you have IDE we have a
web driver and grid as the major
components so now with this introduction
of web driver a full-fledged web driver
selenium became the most powerful tool
for web application education automation
question 5 a very important question
what is same origin policy and how is it
handled now same origin policy is a
security ID feature so it is a very
important Concept in web application
security model and what is this
according to this policy a web browser
allows scripts from one web page to
access the contents of another web page
provided both these pages are on the
same origin now what does origin mean
origin is nothing but a combination of
your url schema the host number and the
port number it's basically your domain
now if a such question is asked how are
you going to explain it so let's say you
have a JavaScript program which runs in
a certain domain say google.com Okay so
this program can access all the programs
which are from the same origin now
origin here is what google.com so for
example you can access other Pages like
your Google Drive your Google email or
your Google calendar however now now if
you use the same JavaScript right so if
you use the same script from the
google.com to access another page or an
element in a different domain Say
yahoo.com then the security model of the
web application itself prohibits you to
do so so what happens because of that is
you are limited with access only to
those elements and Pages which are in
the same domain but now look at a case
that you want to run your test on
multiple servers and you might also want
to access multiple domains so due to the
same origin policy you will not be able
to do so and this became a major
limitation with a very early selenium
tool so how was this handled so this got
handled by Paul Hammond who created the
selenium remote control so he created a
selenium remote control server to trick
the browser in believing that the
selenium core and the web application
under test actually came from the same
domain and this is how the same origin
policy was handled and this is selenium
RC which is selenium remote control
became the version one of selenium
question 6 what is selenis and how is it
classified selenium commands for
selenium ID are often called as selenis
they are a set of commands that run your
test and sequence of this command is
called as a test script now these
commands are categorized into three
major categories one is the action
command now Action commands are the
commands that generally manipulate the
state of your application for example
they can do things like click on this
link or select a particular option open
a URL type certain text into an input
box click or double click on any web
element and so on so these are called as
Action commands the second set of
commands is the accesses now accessors
examine the state of the application and
store the results in a variable so it
basically allows the user to store
certain values to a user defined
variable for example if I want to store
the page title of a web page I can do
that by defining a user defined variable
and then these stored values can be
later used to create your assertions or
verifications in your test now few
examples of such commands are you can
just use a store s t o r e or a store
text store title or a store value so
these are some of the accessor commands
the third category is the assertions
assertions are like your accesses but
they verify the state of your
application and conforms to what is
expected so basically it's a comparison
of your actual versus expected results
there are two types of assertions
available one is the soft assertions and
the hard assertions an example of a soft
assertion can be verify title wherein
you verify the title check whether the
expected and the actual outcomes are the
same and continue with your test case
now whereas an hard assertion is you do
an assert title using the command assert
title and if at all the assertion fails
the test case stops from executing
question 7 mention the types of web
locator what is web locator a locator is
a command that tells the selenium which
UI element you want to operate on
selenium uses locators to find and match
the elements of your web page which it
needs to interact with and there are
around 8 locating techniques in selenium
the first one is by ID now this is the
most common way of locating elements why
since the IDS are supposed to be unique
for each element the IDS can also be
dynamic in nature so you can identify
them by inspecting the attributes of the
web element so let's look at an example
so if we go to a Facebook right so if
you go to the Facebook and say if I want
to identify this element email so if you
look at this highlighted text here it
has a unique ID which is email so if I
am going to operate on this particular
element email I can directly use the ID
locating technique link text and the
partial link text so this type of
locator applies only to the hyperlink
text so we access the link by using this
locator which needs the hyperlink text
to be provided so for link text you can
give a complete text in the link text or
let's let's look at an example of that
so let's if we go to say one of the
e-commerce site which is say amazon.in
and say you want to click on this
today's d link so if you look here in
amazon.in there are a lot of hyperlinks
available here and if an example if you
have to take an example say we want to
click on the today's deal now if you
inspect this element you will see that
it has an anchor tag a it has an
Associated href link here and also it
has a text which says today's deal right
which is what is displayed on your web
page now similarly there is also a link
available here called customer service
so if you inspect that customer service
it also has something similar what the
today still had it has an anchor tab it
has an etch ref link and it has a long
text which says customer service which
is what is displayed now if you have to
use your web locator which which is a
link text you need to provide the
complete text which is displayed on your
web page and there is another way of
using the link text by saying partial
link text where you can just give a
partial text which can be matched
against the complete text of your link
and this is how you can use your partial
link text or the link text the next ID
is by name so locating Elements by name
is very similar to locating by ID just
that your attribute instead of using an
ID you're going to use name in this case
now if you look at the same example of
say Facebook we identified this email
element using its ID you can also locate
the same element by using the name
attribute which is also unique in this
case which has a value called email tag
name that's the next identifier so
locate element using tag name of the
element now say if you want to identify
all the elements on your webpage which
are of type button so that you can do by
using the tag name as button there for
example again on your Facebook page I
know that there is a single button here
which is of type button and which has a
tag as button and this is the only
button which is available on this page
so if I want to access this particular
element I can simply use a locator
technique called tack which is button in
this case next locator class name so
locate an element using its class name
attribute so you can use this identifier
only if you find that a class name is
unique for a web element which you want
to work with class names are sometimes
unique a web element can also have
multiple class name or even you will
find some elements which has no class
name so depending on what is available
in the attributes you can use this class
name for identification so example the
Shonen on your screen here where it says
find element by class name input text
basically this is going to list out all
elements on your web page which has a
class name called input text the next
identifier is by XPath now XPath is a
language used when locating XML nodes
what is XML extendable markup language
it can access almost any element even
those without any class names or names
or even ID attributes so whenever you do
not find a unique attribute for ID name
or class name you can construct an expat
to create an unique identifier for that
particular element now for example on
this Facebook page if You observe here
there is a label here called create an
account so if you inspect this
particular label it has no attributes
other than a tag called SPAC so if you
want to identify this particular element
and say display it this is one of the
ways to do it is through XPath CSS
selector so CSS selector are string
patterns used to identify an element
based on the combination of either the
HTML tag ID class or any other
attributes locating by CSS selector is
little more complicated than any of the
previous method however the advantages
CSS selectors work much faster than the
expat the performance of your test
scripts is much better when you use CSS
selector than any other locating
technique now again as an example if you
see the Facebook again you can identify
the same email here right I used an ID
here or a name attribute here to
identify it so assume that if you did
not have any of those things or if you
want to use a CSS selector here you can
still do that by using this syntax this
is how you use the CSS selector to
identify an element question 8 what are
the types of Weights supported by
Webdriver now before even you answer
this question you should know why do we
need weights in selenium weights play a
very important role in executing test
cases most of the web applications are
developed using using Ajax and
javascripts so when a page is loaded by
the browser the elements which you want
to interact May load at a different time
intervals not only this makes it
difficult to identify the elements but
also if the element is not look located
it will start throwing element not
visible exception and how do we resolve
this by using the weights so there are
three different kind of Weights
available in selling it first one is the
implicit weight the implicit weight
tells the browser or your web driver to
wait for a certain amount of time before
it can throw no such element exception
Now the default setting is zero for this
once we set a time what Webdriver does
it it will wait for that time so if you
have mentioned 10 seconds it will wait
for 10 seconds until it finds the
element and even in the 10 seconds it is
unable to find that element only after
that it throws an exception now what is
the syntax of this implicit weight is
this is the syntax of this where you use
diver dot manage timers implicit weight
and there is a timeout value say 10
seconds and the unit whether it's in
minutes seconds or milliseconds is what
you can mention implicit weight is
applied globally to all the elements
which means it is available for all the
web elements throughout the driver
instance implicit weight is defined only
once and it will remain same throughout
the driver object instance now let's see
what explicit weight is so explicit
weight is used to tell the web driver to
wait for certain conditions before
throwing an element not visible
exception it is defined to wait for
certain expected conditions only now
these expected conditions can check
whether the element is displayed on the
web page or not or can be clicked or not
so any kind of conditions you can
mention there so these are called as
conditional weights and they are applied
on a web element on a single web element
unlike the implicit weight which was
kind of Applied globally to all elements
it is definitely recommended to use this
explicit weight when your elements are
taking longer time to load and also for
verifying the property of the elements
like say visibility of the element
located element to be clickable
clickable or element to be selected then
we have fluent weight now fluent weight
is used to tell the web driver to wait
for a condition that is very similar to
the explicit weight as well as a
frequency with which we want to keep
checking the condition before throwing
the not visible exception so fluent
weight is used to define the maximum
amount of time you want the web driver
to wait for a condition then if you look
in the syntax then there is polling
every second so polling every and you
give a timeout this could be like five
seconds so start pulling to see whether
the web element is available or not say
once in three seconds once in four
seconds depending on your use case and
then dot ignoring so you can also try to
ignore specific type of exceptions while
waiting now for example such as no such
element exception which is sometimes
thrown when searching for an elemental
page so you can mention even that here
so fluent weight takes two parameters
one is the timeout value and the polling
frequency so when we have web elements
which sometimes are visible after a few
seconds or sometimes it takes more time
than usual like we have Ajax
applications as an example we can set a
default polling period based on our
requirement and then as I said you can
also ignore some of the exceptions by
pulling this element question 9 mention
the types of navigation commands
available in selenium so Webdriver
provides a basic browser navigation
commands that allows the browser to move
forward backward refresh and using the
browser history and this is done using
some of the driver.navigate methods
right so these are some of the commonly
used navigation methods first one is
navigate 2. so this method loads a new
web page in the existing browser window
and it accepts string as a parameter and
returns nothing so it's a void so the
URL you mentioned here needs to be a
fully formed URL driver.navigate dot
refresh so this method refreshes or
reloads the current web page in the
existing browser window it neither
accepts anything so there is no value
path or it returns nothing forward so
this method enables the web browser to
click on the forward button in the
existing browser window again this has
this has no arguments to be passed or it
returns nothing
driver.navigate dot pack so this method
enables the web browser to click on the
back button in the existing browser
window again this returns nothing nor it
needs any argument question 10 what is
the major difference between
driver.close and Driver dot quit so this
is again a very important question and
pretty confusing also sometimes so
selenium Webdriver provides two methods
for closing the browser window one is
browser.close and the other one is the
browser.quit so you might think to use
them interchangeably and that's that's
what everybody thinks that both anyway
does the same function so it can be used
however it is not so both are two
different methods Now driver.close
command is used to close the current
browser window which is having the focus
and in case if there is a only single
browser window open then calling this
driver.close actually quits the complete
browser session now it is used when
you're working with say multiple browser
tabs or Windows and you want to just
close one of them which is in focus in
the test right whereas the driver.quit
is used to quit the complete browser
session along with all its Associated
Windows tabs pop-ups and everything so
when do you use browser.quate it is best
to use use browser.quit or sorry
driver.quit when you no longer want to
interact with the driver object along
with all its Associated Windows tabs and
pop-ups generally it is one of the last
statements of your automation scripts so
in case you are like working with
selenium with test NG or junit we
usually call this driver.quate in the
after Suite that is at after switch
method of our test Suite thus by by
doing that we close the complete browser
session at the end of the test fit
execution now let's look at some
intermediate level questions question 11
how to type in an input box using
selenium so selenium provides a method
or a command you can say to insert value
into any of the input box and this
command is called as a send Keys command
so send Keys is a method available in a
web element now this is how you use the
send keys so here is an example so first
you need to find the element using any
of the eight locating techniques and
once you find the element in this case
which is an email field you need to call
a method called send Keys send Keys
takes an argument which is of type
string so you can pass the value which
you want to pass to the web element so
in this case you'll be passing a Gmail
ID or any ID which will be your email ID
here and then similarly you do it for
your password fit so first you find the
element password and then you send the
password using send keys this is a very
simple method to use question 12 how to
click on a hyperlink in selenium so
wherever you are testing a e-commerce
application you are going to find lot of
hyperlinks on your web pages which you
will obviously need to test now selenium
provides two locating techniques using
which you can identify this hyperlinks
and using these is what you need to
operate on the hyperlinks on your web
pages so one is using link text and
other one is using partial link text so
using link text you give a complete text
in the link text that is if you look at
in your e-commerce let's say amazon.in
let's let's go to that website and see
what it means so in the amazon.in if you
look at some of these hyperlink text
called today's deals or Amazon pay or
sell or customer service if you inspect
this element you will see that this is
associated with a link text which is
called as today's deal along with its
Edge ref and of course an anchor tag so
one way of identifying this link is to
use this complete text which is today's
tip also there is another way by using
partial link text for say example you
have a link text which is pretty long
and you do not want to use this complete
text instead you can use the locating
technique called partial link text and
give a partial text from that in this
example instead of giving the complete
customer service here you you can just
say use a customer or you can just use
Service as your text only thing what
will be required here is this has to be
unique on this page there shouldn't be
two elements which would have this text
which you are going to use so let's
quickly see this in action how do we use
this link text and partial index now we
will be using the same example of
amazon.in so my use cases first I will
click on this today's deal and after a
sleep of 2 or 3 seconds I will also
click on customer service and I am going
to achieve this one by using link text
so if you see in this program so I have
this method written which is going to
click on today's deal and this is using
a locating technique link text and then
I also have another meth head here by
using partial link text I am going to be
clicking on the customer service link
right so let's see how this works so
I'll just go ahead and execute this so
This launches the amazon.in and the
first thing what we are doing is it is
going to click on today still all right
that is done and then it will click on
the customer service link right so this
is how you can achieve clicking on the
hyperlinks in your web applications
question 13 how to scroll down a page
using JavaScript this is a very
important question and usually this gets
asked quite often in the interview you
should know that you can run javascripts
through selenium and selenium provides a
way to do this selenium provides an
interface called JavaScript executor
JavaScript executor and this helps in
executing any kind of javascripts
through the selenium Webdriver so this
JavaScript executor it has basically two
methods one is execute script and the
other one is execute it's async script
to run the JavaScript on the selected
window or on your current bridge but why
do you need this right so if if you're
answering this question you should also
have a background that why at all this
is required because sometimes you need
to perform certain functions which
cannot be done using the locators what
has been provided let's say scrolling
down the web page itself also sometimes
you will see that you are unable to even
identify some web elements using all the
locating techniques which is made
available by selenium so in such
situations what you can do is you can
directly write a JavaScript and execute
it using this JavaScript executor and if
you look at it the syntax is something
like this so it says JavaScript executor
JS is equal to JavaScript executor and
Driver now if you look at this
JavaScript executor is an interface that
means you cannot create an object of it
and hence you type cast it to your
driver object now how do you do this now
first thing what you do is you launch
your web application on whatever web
application is under test and then on
that current window you call this
particular execute script method using
the JavaScript executor and then you can
perform any functions here in this
example we are seeing a function which
is crawling down vertically so this
function takes an attributes or argument
like x coordinate and the y coordinate
now in this example our x coordinate is
0 and y coordinate is one thousand that
means when this command gets executed
the page is going to get scroll down
vertically by thousand pixels so why
don't we just see this in action so
let's go to another program here called
the scrolling test on Java so basically
what I'm doing here is I am going to be
logging into amazon.in and then I will
show you how to scroll down by 600
pixels so it's a very very simple code
to write you launch your application
then you create your object of
JavaScript executor right actually you
Typecast it to the driver and then you
just use an execute script command or a
method this you see is your actual
script which gets executed which is
Windows dot scroll by 0 comma 600 which
is x coordinate and y coordinate let's
go ahead and execute this so in
amazon.in is launched and you should see
the screen scrolling down yes there we
go so it just scroll down by few pixels
and that is 600 pixels what we mentioned
see I could scroll back again and that
is how you can achieve scrolling
functions and when do you usually use
this say if you have a very long web
page and you want to go and identify or
do some operations on the web element
which is probably available to you
towards the end of the page and that is
when you can use some of the scrolling
functions question 14 how to assert the
title of a web page selenium Webdriver
provides Webdriver methods and one such
method is called get title so what get
title does is it fetches the title of
the current page how to use this command
get titles returns string which is
nothing but your title of your page that
means you need a string variable in
which you can store this value so what
you do is you define a string variable
like it is defined in this example to
store your title and then you execute
this command driver dot get title which
is going to fetch the title and that
value gets stored in your actual title
once you get the title you can do any
operations with that in this case what
we are doing is we are trying to verify
the title there are two ways which you
can verify them one is using an if else
statement where you compare your actual
title with the expected title or
alternatively you can also use assert
statements here right so selenium
provides a search statement so using
assert you can validate whether your
title of the web page is as expected
question 15 how to mouse hover over an
element in automation many times it is
required to perform some action on the
element which gets visible only on the
mouse hovering on those elements now in
selenium action is a class which we can
use to achieve this functionality so
action class is defined in a package
called as selenium dot interactions this
has a set of apis to emulate complex
user gestures it helps in building the
patterns of action and then perform that
action on a web element now this action
class provides a rich set of apis like
so for Mouse events and even the
keyboard events so say for to perform
form any kind of mouse event it provides
a method called move to element now in
this example what you do is you initiate
an object of mouse class and then use
this move to element method and what it
does is the element is scrolled into the
view first even before the move
operations happen so you could have
multiple actions like here in this
example we are just using move to
element but also you can if you want to
do a drag and drop observation you can
still have that so in such case where
you have multiple actions like a drag
and drop then you use a method called
build along with this so you say
actions. move to element dot build and
then dot perform so dot build method is
called before the perform method and
what it does is it generates the
composite actions which has all those
multiple actions which you want to
perform on a web element then after the
built-in action is done you call a
perform method which actually performs
that action on your web element and this
is how the move to element method of the
action class is used why don't we just
see that in action let's say let's go to
our Eclipse here and I have a mouse
hover test written here so let's say we
go to amazon.website so in the Amazon uh
if you go to the shop by category you
see that so the moment I Mouse hover it
it is going to give me this drop down
list so for me to select any element
from this my first action on this web
page should be to go and just hover my
mouse over this web element called shop
by category and that's exactly I'm going
to achieve through my automation now
here in my methods here what I have
written I have said Okay first launch
your website which is amazon.in and then
I'm finding that element which is shop
by category using an X path so you could
use any available locating technique to
find the element first and then I
instantiate an object of actions and
then simply I'm doing is I'm using the
method move to element and then I'm just
going to say perform that means my mouse
would or chili get moved or over onto
that particular element okay so let's
see this in action so I'll just execute
this test case Okay so Amazon dot is
launched and now we should see yeah the
mouse actually moved to shop by category
and because of that we got this drop
down list so and this is how you can
achieve your mouse hover functionality
question 16 how to retrieve the CSS
properties of an element now CSS is
nothing but cascading style sheets it's
a stylesheet language used for
describing the presentation of a
document written in your markup language
it basically describes your HTML
elements as how they need to be
displayed on the screen it's basically
the look and feel like your font size
the width height color and so on so it
provides a special and easy way to
specify various properties of the
element now why do we need this we need
to use a special method called get CSS
value to retrieve such CHS properties of
web element why because we already have
a method called attribute which can
retrieve the value of the attributes
right however the get attribute method
can only retrieve those properties where
the attribute and its value are
mentioned in your Dom and hence all this
font size color cannot be retrieved
through the get attribute method and
that is why we use the get CSS value so
basically it works like your get method
and you need to provide the attribute
which you want to fetch using the get
CSS file so in this example what we are
trying to do is we have a web element
called the email ID and of that we are
trying to fetch the font size so let's
again check this in action so if I go to
my web page here let's close this so if
I go to my Facebook and if I inspect
this particular element which is the
email ID and on the right if you see
there are multiple other attributes of
that element you see right so you see
the background color you see the color
here then you see something called as
font size and say for example V1 want to
retrieve the font size of this
particular web element and you will also
see on the left where you are seeing the
Dom here you do not see that font says
anywhere as an attribute and its value
here and that is why I cannot use the
get attribute method to retrieve it now
instead of that what I am going to do is
I'm going to use the get CSS value and
use this font size for that so let's go
to my Eclipse I have a method written
here where I log into the Facebook and
then I'm going to first find that
element which is by ID and I am going to
retrieve the CSS value and I'm just
simply printing it on the console so if
I execute this so it's going to launch
the Facebook and since I'm not going to
do anything on the Facebook web page as
such let's go to my console here because
this is where I'm expecting the output
so here it has given me that an output
in the console saying 12px which is
nothing but your actual font says what
you see here in your HTML page right and
this is how you can extract your CSS
properties of any web element on your
web page question 17 what is Page object
model popularly known as form page or
object model is a design pattern that
helps create object repositories for web
elements so writing selenium test cases
without say using any patterns may lead
to code duplicacy and also the code
becomes unmaintainable after some time
and say if you have web element locators
which changes over time then you will
have to go through the entire code base
to make all the required changes so
under this model for each web page in
the application there should be a
corresponding page class and this page
class will have all the web elements of
the web page and also it contains the
page methods which perform operations on
the web elements and the verification
steps required in your test cases are
the separate test cases they are not
combined in the same class now the tests
that use the methods of this page object
class whenever they need to interact
with the UI of that page the benefit
here is if the page UI changes the tests
need not be changed only the code within
this page object class needs to be
teached that is you go and just change
the web elements or the locators of the
web elements only in one place which is
your page class and not in any of your
test classes or not test classes so how
does this form help so it makes the code
more readable maintainable and reusable
so there are two ways you can Implement
a form or a page object model one is
done using the page Factory which is
offered by selenium and then you can
also achieve a page object model without
using the page Factory right using
directly the objects and the classes
question 18 can capture be automated
this is again a very favorite question
of the interview capture is a security
feature that prevents Bots and automated
programs from accessing sensitive
information yes and captcha cannot be
automated you will see that there are
many applications today who have
captured why because it's a security
feature it is a concept to differentiate
between human and computers the idea of
captcha itself is to prevent any
automated process or a robotic process
to do any kind of actions and say if
you're going to automate it then the
entire idea of having the captcha is
lost so you should never attempt to
automate it but then what do you how can
you handle this then one either you can
ask your developers to disable it in the
test environment so that you can test
the whole application without having to
hit on this captcha other way of doing
is you can use static data for your
captcha so that you can hard code that
value in your test for the time being
and then since your application has
captures one of its functionality if you
really need to test that you can just
test that separately as a manual testing
itself but not automated question 19 how
does selenium handle Windows based popup
selenium is an automation tool for
automating web applications it cannot
handle Windows based application however
we can use tools like Auto it and robot
class along with selenium to handle such
pop-ups so what are these tools robot
class is a Java class which along with
selenium can simulate your keyboard and
mouse events it also helps in upload and
download of files and also to manage the
pop-ups so wherever you have a
requirement of doing a file upload
operation in which if your native dialog
box opens up then one of the way to
handle it is using robot class other
tool which is available for us is the
autoit tool it is a freeware and what it
does is it uses a combination of your
mouse movement keystrokes and Window
Control manipulation to automate any
such task which is not possible by
selenium web driver and again this
autoit tool can be integrated with
selenium and thus you can achieve the
automation of native window based
pop-ups question 20 how to take
screenshots in Webdriver take screenshot
interface can be used to take
screenshots in web driver so selenium
provides us this interface for managing
our screenshot operations now
screenshots are desirable for bug
analysis so selenium can automatically
take this screenshots during your
execution you know that it is very
important to take screenshots when we
execute the test scripts say if we have
a huge number of test scripts running
and if some of the test case fail we
need to check why this test case failed
so using the screenshot functionality it
helps us debug and identify the problem
by looking at those screenshots so
Webdriver has this built-in
functionality to take the screenshot
across the page and it's pretty easy to
use take screenshot interface is used to
capture the screenshot during the
execution now this interface has a
method called grid get screenshot as
which captures a screenshot and it also
stores in a location specified by us so
let's take an example of our Facebook
page and say we fail a particular test
case on the Facebook application and
then let's see how it take a screenshot
at that particular moment when the test
case fails so let's go to the Facebook
page first and let's understand the use
case so in this Facebook page say if I
am signing up on this page I need to
enter all this field now as a part of
demo what I will do is I'll just enter
the first name and the surname and I
will click on the sign up button with
this test case it should basically not
sign me up rather it should give me some
errors saying that you need to enter all
the fees and at that minute we are going
to take a screenshot okay so that's our
use case so let's see how do we achieve
that so if I go to my eclipse and let's
say we go to my screenshot test.java I
have a method written here so and these
are the operations which I just
described which I am doing it here so
I'm going to just send keys my first
name to the first name field second name
into the surname fit and then I'm just
going to click on the sign up button now
there are few things required for us to
do before we use the take screenshot
interface along with its method which is
the get screenshot us so I said that
this method stores the screenshot taken
in a file so that means I need to
provide it a file name and also a
location where it needs to be placed so
in our demo here in this test class what
I've done is I've created a folder
called screenshot in the same project
directory and then I am giving it a file
name called FB login.png so that means
after the screenshot is taken it is
saved in a file called FB login.png and
then it is stored in the location
screenshot folder and these are the
simple file operations of java which I
use to achieve this so let's execute
this and see how this happens so right
now I have the screenshot folder and
it's an empty folder so after the
execution I am expecting that this
particular file would get created so
let's see that in action so I'll go
ahead and execute this Java program okay
so we have launched the facebook.com and
then I entered the my first name second
name and it has also clicked on Signum
after which it has given all this red
marks Here and Now now this is what I'm
expecting in the screenshot so let's go
back to our Eclipse I will refresh this
folder so I'll say right click and
refresh okay I have this file created so
let's open this file and see okay so do
you see this this whole image has been
captured so now as a tester if at all
this test case fails for me I can always
open this PNG file and see what actually
happened and I can debug my test case so
this is how you use screenshot
functionality in selenium level
questions question 21 can you type in a
text box without using send keys so this
is a very favorite question of any
interviewer so you should know that
sometimes on an IE browser whenever you
use the send Keys command it is pretty
slow so some of the performance of your
some of your test cases gets impacted so
you need an alternative way to use
instead of using the send Keys function
and one such alternative is using
JavaScript executor so selenium provides
an interface called JavaScript executor
and this this helps in executing any
JavaScript through your selenium
Webdriver so you can do any kind of
function using this JavaScript and one
of the functions which we can do is the
send Keys kind of a function now how to
replicate a send Keys function using
JavaScript so let's see an example here
now since this JavaScript executor is an
interface you cannot create an object of
this interface so what you do you type
cast it to your driver object the way it
is done here in this example and then
you use document dot get element ID to
get the actual element on which you're
going to perform the operations of
sending the keys now what is this
document so it is an object right so
this document it's an object that
represents your HTML document which is
displayed in that window and this
document object has various properties
that refer to other object which allows
you the access to modify any of your
document content so the argument which
you're passing through the Java execute
script is nothing but your JavaScript
now this JavaScript uses the document
object to identify your element and then
send the value which you need to send to
that element so let's see this in action
as how it works so let me show you a
small program here all right so I have a
method here wherein I'm trying to
identify the email field of your
Facebook page so if this is your
Facebook application this is my email
field and instead of using sent Keys
here I'm going to pass a email value to
this and how am I doing that using a
JavaScript executor right so if I
execute this particular Java program it
is exactly going to do the same function
what I would do otherwise using the send
case so let's execute this and see if I
can see my user's ID email in the
Facebook application so the Facebook
application is launched and there we go
so we this is the value which I have
sent using my JavaScript okay so this is
how you can use a JavaScript executor to
wrap applicate a send Keys method
question 22 how to select a value from
your drop down list in the selenium
Webdriver so select class is a selenium
Webdriver it is used for selecting and
deselecting options in your drop down
list the objects of Select type can be
initialized by passing the drop down web
element as a parameter to its
Constructor like in this example I am
saying select drop down equal to new
select of test drop so test drop is
nothing but the web element which I have
identified using any locating techniques
after you create the select object then
you have three different methods
available for actually selecting your
methods and what are these methods one
is Select by index now the index starts
with 0 in the list so you can select any
element from your list using the index
site and then the second one is Select
by values so this is used to select an
option based on its value attribute
select by visible text so this is an
option wherein you can select a
particular list item or an option based
on the text displayed in the option so
that means whatever text you can see on
your drop down list is what you will use
it to identify that particular element
so let's see the demo for this so I have
again a same Facebook application which
I've considered here for demo so if you
go to our Facebook application first you
know that there is a drop down menu here
now this drop down is for selecting your
birth date this has a day values which
you can select the month values right
and the year now what I have done in my
code is first thing is I've identified
all these drop down boxes using one of
I'm using mostly the ID locating
technique here so you can use any of
your choice so once I identify the
element I created the select object
class for each of this and I pass that
web element to the Constructor of the
select class so now my select element or
the object is ready to use so my next
step what I need to do is I need to go
and select the actual dates now in this
use case what I want is I want to select
a date which is 10th June 2005. that
means on my select drop down of the day
I should be able to select the day 10.
now this I am achieving by using select
by value and then I am selecting the bun
month based on the index which is the
index in this case is 6 which will
select the June form and then the third
one I'm trying to select it by visible
text wherein I am selecting the year
2000 y so when I execute this program
let's see how the date selection happens
okay there we go so it selected the 10
the day 10 the June month and the 2005.
so this is how you use select objects in
selenium question 23 what does the
switch to command do so switch to
command is used to switch between
Windows frames or even the alert pop-ups
within your application right so there
are multiple things which you can do
with switch to now sometimes in your web
application say when you click on a
button it opens up a completely new
browser window or you'll also see it
opens up another tab in your browser say
for example let's look at one of this
application which I have let me go to my
browser and say I launch HDFC bank.com
all right so this is my application say
which you want to test now when you
launch this first thing is I see this
advertisement pop-up which comes up
which is right now an unwanted pop-up so
right now I'll just close this and now
if I want to enter this HDFC Bank
application first I need to do a login
so if I click on this login button see
there is a new window which opens up and
I need to go and operate into this
window so for any further operations I
want to do it on this web page I need to
switch to this window and then say I
want to click on continue to net banking
and then continue with my use case so
now in this application there are two
windows open one is your base window
from where we actually logged in and
after I clicked on the login there is an
new window opened up right so this is
where your switch to command will help
us to switching to this particular
window to do any of the operations on
this web elements present on this window
okay now let's see how do we actually do
this so in order to do this first you
need to get the handle of the window by
using a command called driver dot get
window handle all right so now when you
use driver.getwindow handle what you get
is a unique alphanumeric value which is
an identifier for that window and once
you get this you use a command called
driver.switch2 dot window and you pass
that handle which you have fetched
previously and this command will
actually switch to the current window
which is in your focus now you will also
come across a case where there are
multiple windows open right now the
example what we saw we just had a single
window but say if you whenever you
launch an application say there are
multiple windows which are open so in
such case you can use something called
as get Windows handles right now get
Windows handle is what it does it it
picks up all the active windows at that
point of attack and using a for Loop you
can read into every handle and then you
do your operation specific to your
window handles also there is another way
wherein you can switch back to your
parent window so if ever you want to
switch back to your parent window you
have to use the same command driver dot
switch to and the handle what you need
to provide here is your parent handle
all right so this is about Windows then
I also said switch to can be used for
your frames so what are frames frames
are usually used to divide your web
application or your web page or the on
the same domain into various different
windows or sections right so for us to
operate on Elements which are in
different frames we need to First switch
to a particular frame and then use the
web elements on that so the same command
we'll be using now instead of window
here I would say driver dot switch to
dot free similarly even the alert
pop-ups can be handled using the same
command so in the case of alert what you
need to do is you have to say driver dot
switch to dot question 20 how to upload
a file in selenium Webdriver right a
very important Concept in selenium
Webdriver we know that selenium
Webdriver is designed for automating web
application right and file upload is a
very common scenario which we get with
every web application which we are
working with now there are two ways to
do a file window a file upload operation
One is using the send Keys which is the
simplest method and the other way is
using robot class now there are certain
prereqs required for you to work with
the send Keys option now if you are
using the send Keys option what you
basically need to do is you need to type
in the complete path of the file along
with the file name right in the file
select input field now this element
where you are going to enter your file
name should have certain attributes one
of the attributors it should have the
tag as input right and also it should
have an attribute called type which
should have a value or file only then it
is possible for you to enter a file name
and upload the file okay that is by
using the send Keys method then the
other way is using robot now when when
is that we need to use robot where
whenever you have an application and in
your application usually for your file
upload you have a button which says
upload a file and when you click on that
it opens a new window and this window is
your Windows Explorer window or a finder
window depending on what operating
system you're working and these are the
native windows and we know that selenium
Webdriver directly cannot handle the
native applications so in such case the
robot class helps us in managing these
native windows so what happens in the
robot classes it helps you manage your
file upload dialog box by simulating all
the actions required for selecting a
particular file from your dialog box and
then you go and upload them all right so
this is the example of using the send
Keys question 25 how to set browser
window size in Webdriver so you can can
either maximize the window or set it to
a certain value now to do this you use a
simple functions provided by
driver.manage one of the functions using
driver dot manage is
driver.manage.window.maximize now this
command maximizes the current window and
say you want to set a particular window
size rather than maximizing it and then
for that you have a command called
driver.manage dot window dot set size
now this set size takes an argument of
the type Dimension class so this is
basically when you create a dimension
class you are passing the size of the
window which you want to create here
right so we use the dimension class
Define the new dimensions of the window
and then pass the object of this
Dimensions to the set size method this
is one way of doing it we also saw that
using JavaScript executor we can do any
kind of functionality so if you want to
minimize or maximize or set a different
in size to your browser windows this can
also be done using a JavaScript okay
let's see a quick demo on how do we use
this driver.manage dot set size or the
maximize command so let's go to my
Eclipse here so I have this method
written here so I have two methods one
method where I am saying I am going to
maximize the window this particular
command is where I'm saying
driver.manage Dot window.maxims and I'm
launching a Facebook application so
first let's run this particular method
and see how our browser window behaves
so I'll just say execute this
so now you will see the whole window
your browser window is in maximized mode
so now from here I'm going to execute
the other set of lines here using which
I'm going to set a certain size now I'll
just comment out these two lines
and then these are the two commands
which I'm going to use right so here
what I have done is I've defined an
object of the class Dimension and I've
used 600 600 which is the size of the
window which I want it to be and then
using the set size I'm passing this
Dimension object to the set size method
now let's see how do we look at this
browser window now did you see that so
it was at a certain size and it just
shrunk it shrunk because I have used a
smaller size window question 26 when do
we use find element and find elements
Now find element you can use this
command to access any single element on
your web page and what does it return it
returns an object of the first matching
element of the specified locator and
this is the most common command what we
use for working with every web element
on the web page what is find elements
find elements is used to return a list
of web elements right so each element
which it identifies is indexed with the
number starting from zero it's just like
an array I'll show you an example of
that so that you can understand it
better so let's go to my Eclipse here
right so if you look at this
locators.java I have different elements
which I have identified using different
locating techniques right so for that I
use a simple command called find element
now say I have a used case right wherein
I want to fetch all the web elements who
have a class name starting with the text
input let's see what that is so if I go
to my Facebook application if I try to
inspect this say first name right so
I'll say inspect element now here if you
see this particular web element has a
class name starting with text called as
input text likewise there are many
elements here like even the surname if
you look at it it has a class name
starting with input text okay so if I
have a use case like that where I want
all the web elements on the Facebook
page who have the class name starting
with the text input if I have to do that
that is when I say I am going to use the
driver dot find elements so I say driver
dot find elements I give an XPath which
is going to pick up all those elements
which has the class name starting with
input this is my expat expression once I
get all the elements what am I doing I
am going to store it in a list array
once I have all these elements created
it's in an array I'm just going to
Traverse through the array using an
enhanced for Loop so in this program in
this test case I'm just picking each and
every element and printing the attribute
ID of each element so if I execute this
now we should see all the elements on
the Facebook page which has the class
name starting with input and we should
see all the IDS of that so this is what
we are getting so we are around getting
9 to 10 elements it has picked up and
all of them have the class name starting
with input and these are the different
IDs
question 27 what is pause on exception
in selenium ID so pause on exception is
a new feature of selenium ID it is one
of the brilliant debugger function which
this new selenium ID provides us what it
does is it enables us or when this
particular option is enabled the
execution of your test pauses if at all
there is any exception which occurs in
your test scripts and because of this
what you can do is you can go and handle
that error by say changing your command
in the script and then continue your
test execution so this helps in
debugging your code pretty well so
whenever there is an exception say for
example an web element which you are
trying to identify has a wrong identify
or it changes on your web page then you
can correct that command which caused
the exception and then resume the test
execution from that point onwards so
let's look at a simple use case and see
how exactly this works so for example
let's go to my browser here okay so have
a simple use case here where I want to
login into Facebook using my email ID
and password and what I have done is I
have created a script or recorded a
script using my selenium ID so these are
all the steps which it has wherein it
logs into the Facebook account then it
clicks on the email field enters the
email this is my value which I'm passing
clicks on the password field enters the
password and then clicks on the login
button now this is a clean test script
and all my web elements are correct here
so whenever I'm running this test with
this test script is going to pass now
say as an example say this particular
element which is my password element say
this ID changes so now deliberately what
I'm going to do I'm just going to give a
wrong idea so if I give a wrong ID and
if I execute a test script now it is
going to throw an exception at this
point so let's do that so it's going to
throw an exception but now since I want
to use the pause on exception feature
what I do is it's available here it says
pause and Exception by default it is
Switched Off so when you select this
that means this has been switched on and
now if I execute the script this is
going to come into a debug mode at this
point where my exception is going to be
true so let's see that so now I have
this I have changed this ID to a wrong
ID so I'll just say go and run this
current test so let's see how it
operates so let me just bring this side
by side so you can see here it has
entered the email and if you look here
in the console it is saying trying to
find the ID for which I've given PA and
4S which is a wrong ID so it will look
for some time whether it can find an
element with this particular ID so let's
wait for it to throw the exception there
we go so now it tried enough and
actually that step has failed and there
is an exception saying that that
particular web element is not found
however my test case or the test script
did not exit it has come and paused and
it has entered into a debug mode here
paused in debug now as a tester or an
automation scriptwriter what I can do
here is I can simply go and and correct
this now I know this is my correct ID
once I correct this I can execute the
rest of the script right from this point
so now I have this option here if you go
here it says resume test execution so
I'm just going to click on that so it is
going to go ahead and enter everything
else if you see here I entered email
password and in this description what
I'm doing is I'm entering my first name
and my surname instead of clicking on
the login button so it went ahead and if
you look here it says FB test completed
successfully this is how the pause and
exception feature helps us question 28
how to login into any website if it is
showing an authentication popup for
username and password which will be
something like this so this is a typical
authentication pop-up which you normally
see when you are dealing with some web
application right and to handle them we
use something called as an alert class
and alert class comes with a method
called authenticate using basically by
using this method what we are trying to
do is it helps us skip the basic http
authentication box so how do we handle
it it is handled exactly the way we
handle any kind of alert box in selenium
now in this example first what we need
to do is we need to create a weight
object using an explicit width now this
weight is for what it is for the alert
to pop up so we basically wait until
that authentication alert pops up and
set the conditions in this explicit way
for as alert is present so that means
you're going to wait until your
authentication pop-up is seen on your
screen and then once you have that
authentication pop-up which is on your
screen that is when you need to go and
handle it now to handle it for your
alert class you're going to call a
method as I said using authenticate
using and to this you pass a parameter
now this parameter has to be your
username and password however the way
the parameters are passed here is using
a class object called user and password
now this user and password class it is a
part of selenium Security package all
right and this is implemented from an
interface called credentials so you
create this object of class user and
password using the authentication
information which you have and then pass
this as parameter to your method called
authenticate using and this way you can
handle your authentication pop-up
question 29 what is the difference
between single and double slash in XPath
a very important question and it's a
very important thing for each of you to
understand single slash is used to
create an XPath with absolute path that
is the X path would be created right
from the starting node and the starting
node is slash HTML and the double slash
is created or it's a XPath with relative
path now relative path this x path would
be created using any starting location
or anywhere within the document for
example I could start with a div tag
wherein if I mention double slash enter
so this is going to be treated as a
relative path and whatever conditions I
am going to give it is going to try and
match those attributes with the stack
right this is called as a relative path
so why don't we just go and take a look
at how do we identify an element using a
relative as well as an absolute path so
if I go back to the same application of
the Facebook this is my Facebook
application now I have this grow path
installed which helps me in identifying
X parts of an element now say I want to
try and see what is the X path of my
element first name so in the inspector
tool I can see that it has a tag input
and it has a long class name and it has
multiple attributes which I can use to
identify this element so if I go to my
Crow path and again try to match this
element so if I just hover my mouse here
now here you can see this cropath gives
me both the path one is the relative X
path and one is the absolute X path now
look at the big difference between these
two now relative path was as simple as
using a slash slash input which is the
tag and an attribute called ID we are
able to identify this particular element
same thing thing if you want to use an
absolute path and you if you see here it
will start right from the slash HTML
which is your starting node of your HTML
page and then it travels down looking
into each of the parent and child under
this until it found this element input
right so this is absolute path so you
can use either of these paths to
identify your elements question 30 how
do you find broken links in selenium
Webdriver now HTTP is designed for what
to enable Communications between your
clients and servers and if you know
there are few commands like get put post
now get is used to request data from a
specified resource put command is used
to send data over your server and post
is used to send data to the server right
now while using these links and trying
to figure out if the link is valid or
not what you can do is you can use
something called as head requests now
what are head requests head requests are
nothing but there's something like a get
request but they do not have a response
body and usually this is used for
checking that what a get request can
return before actually making a get
request like say before downloading a
large file you would send a head request
before sending the complete get just to
know whether the link which is provided
for downloading a large file is a
genuine link or not and then looking at
the response code is what you decide
that so how do we do this
programmatically now selenium provides
something called as HTTP URL connection
class which has some methods to send
HTTP request and to capture the HTTP
response codes like the ones which you
see it on the screen so if we have to
see how do we do that now first thing
the logic behind how do we get this
broken links is you get all the URLs on
your website first after getting all the
URLs now how do you get the URLs by
using the tag a like it is done here so
once you get the complete list of all
urls you also check for a non-val A
non-null or a blank once that is one
kind of a check which you can put after
that you create get an HTTP connection
object with each of the URL and you open
the connection of the URL and you send
the request now this request whatever
you are going to send you are going to
send the header request to just to get
the head and not the complete get
request now only thing what you are
interested in getting a response code so
once you get the Response Code what you
have to do is you have to check the
Response Code against all your response
code now typically when you're checking
for your response code any response code
above 400 is treated as a client error
rest all the links below 400 are all the
valid links so here if you see 404 you
have a response called as link not font
400 is a bad request 401 is unauthorized
so you can do a simple check there where
it says if your response code is above
500 then that particular link has an
error and that is probably going to be
your invalidly so let's see that in
action as how exactly the code is going
to look and how do we achieve that now I
have a test case written here called
broken links.java now here what I'm
doing is I'm logging into facebook.com
and then I'm fetching all the elements
with the tag a now when I say I'm
fetching all the elements with the tag
that means I'm going to get all the href
element from the page and once I get
that using a for Loop I'm going to
inspect each and every URL what I'm
going to fetch as I said my first check
would be to see if it is a null or is it
an empty if I find an empty or a null
value in the URL I'm just going to
ignore that and continue the loop and
this is where I actually do the
connection with my URL so I create an
HTTP URL connection with the URL which I
have picked it up from my website then I
do this I send a request here the head
request to get a response code so once I
get this response code is what I put a
if fill statement and check if my
response code is greater than or equal
to 400 so if I find that if my response
code is more than 400 that means this
particular URL link is broken and that
is how I identify so once you identify a
particular URL which is broken you can
make use of that information share it
with your developers and get it fixed so
let's go ahead and execute this so in
this test case basically what I'm going
to do is I'm just printing out all the
URLs and I'm printing the ones which are
valid and ones which are broken uh since
it's a facebook.com website I do not
have any broken URL but this is just a
logic of how do we find a broken URL so
if you look here there are a lot of
websites which is getting listed which
says is a valid link so that means it is
getting a response code which is more
than 500. so this is what we saw in the
demo as how do I iterate after I select
all the links on the page and then how
do I determine whether it's a valid link
or it's a broken link so by definition
devops is a collaboration between the
development and the operations team
which enables continuous delivery of
application and services to the end
users so let's expand on this definition
first the development team is the one
that actually produces the products the
operation team is related with the
management or the maintenance of these
products now there's a link that devops
forms between these two teams which
makes this entire process much smoother
and this approach this collaborative
approach is what's called devops so
devops focuses on continuous delivery of
application which means that when the
development team is ready with the
product it immediately goes into the
maintenance or the management phase and
it's then pushed on to the end users
with minimum delay just in case you
doubt the stand of devops today or its
growth in the near future you must know
that according to the information week
devops will be in high demand for
upcoming years and has no chance of
slowing down also according to LinkedIn
over the past few years devops has
increased in demand by 50 percent now
that's quite a lot of growth so clearly
devops is a field that will be great to
dive into and grow in so before we begin
how you can actually become a devops
engineer let's look at what we'll be
going through in this video first we
look at who is a devops engineer
followed by the devops career roadmap
we'll then look into a devops
certification and then finally the
salary trends for a devops engineer so
who is a devops engineer now a devops
engineer is an ID professional who
understands the software development
life cycle and when I say software
development lifecycle I do not just mean
the devops approach but right from a
waterfall model which is a traditional
software development life cycle and then
as you know we moved on to the agile and
finally we reached the devops lifecycle
so a devops engineer needs to understand
the whole purpose of this life cycle why
we had to move from one model to the
other what are the shortcomings that the
previous models had and what all does
devops make up for also other than
understanding this entire process they
also need to know how to use the various
automation tools for developing The
Continuous integration and continuous
deployment pipelines so of course your
next question must be what is the
continuous integration and continuous
deployment pipeline as you can see on
your right we have an image of this
pipeline so there are various stages so
first you have your planning stage where
you plan things out and write a an
algorithm for your product then it's the
building stage where the algorithm
actually transforms into a code and is a
product which could be used then you
have the testing stage in the testing
stage a product is tested all the bugs
are caught and all of this is corrected
and once all of that's done it's then
sent to the deployment stage which means
now your product is ready to be deployed
once it's deployed of course a product
needs continuous monitoring this is now
something new which is being introduced
in the devops lifecycle that although
your product is out in the now the
monitoring stage is something that the
devops approach focuses on much more
than our previous model models and for
very good reason which is that once your
product is deployed into the real world
to ensure customer satisfaction it's
important that you continuously monitor
the product this way you're able to
catch the bugs fix them and push it back
to the clients and ensure that they are
happy with your product and would go for
it again now in simple words a devops
engineer is expected to collaborate with
the developers and the operations team
to deliver high quality products within
a minimum amount of time now devops
engineer as such is not a profile that
you might come across very often but of
course there are number of career paths
that you can take under the devops
engineer but of course there are a
number of career paths under a devops
engineer some of these are the devops
architect automation engineer software
tester integration specialist security
engineer and release manager now let's
look at the devops career roadmap so
basically if you are looking forward to
bagging that devops engineer job where
should you start and how should you go
about it first thing first there are a
few programming languages which go hand
in hand with the devops tools you need
to pick up these and also know the Linux
fundamentals devops uses these
programming languages for developing and
automating software now three most
common languages used with the devops
tools are Ruby Python and JavaScript it
is always advised that you know at least
one of these of course we must consider
the fact that these three languages are
used with completely different tools so
knowing just one of these languages does
not mean that you can now work with any
tool under the devops approach but it
does give you a good hang of at least
one of them which is more than most can
say it's also important to know the
fundamentals of the Linux command line
interface of course this is not very
unique to devops whichever career path
you are choosing Linux fundamentals is
one of the most important things let's
now discuss some of the mandatory Linux
skills that devops Engineers should be
aware of the first is of course the
Linux shell now you must either know
bash or the kernel shell dense the Linux
commands now there are a huge number of
Linux commands but the basic ones you
must know no matter what are the find
grep orc and the said command and
finally we have the networking commands
nslookup which is for querying the DNS
and netstat for monitoring your
connections our next Milestone would be
learning the source code management now
source code management is especially
important when you're dealing with huge
projects so that all your codes
irrespective of the number of files you
have or the number of versions of a
particular code you have everything is
organized and can be easily accessible
so it's important that you are
comfortable with at least one of the
source code management tools here the
most common ones we see are git CVS and
Mercurial I would personally recommend
meant you to choose git and why git
because git is used to track changes in
the source code it can manage large
projects efficiently and it allows
multiple developers to work together
with great ease now our next stage is to
learn to build applications so a devops
engineer must know how to build an
application and commit to the source
code management tool such as git now
popular tool used to build applications
which makes things much simpler is Maven
why Maven well it supports parallel
builds instant access to new features
without any additional configuration and
has an easy build process now Maven is
actually an automation tool which helps
to build and manage software projects
within a short period of time so here
you have the pom file which is required
to build an application under which you
have sets of jars you have the commands
which are to be executed the builds
plugins and the bills profile but in
order to automate the entire entire
process require a continuous integration
or a continuous development tool and
this brings us to our fourth stage which
is learning to automate the process
using CI and CD tools one of the most
extensively used tools in this field is
Jenkins Jenkins is an open source
continuous integration tool it helps to
automate continuous development testing
and deployment of newly created codes
well why Jenkins because it has multiple
plugins it is easily distributed across
multiple machines and has an easy
installation and configuration process
our next stage is to learn to test the
applications once you have completed the
build process learn how to automate the
testing process of web applications one
of the best testing tools for QA teams
is selenium and why selenium above all
the other tools that provide the same
functionality well because it provides
fast execution allows scripting in
several languages and all also supports
parallel test execution the next thing
is to learn to deploy applications in
production servers so you should learn
how to deploy and run the applications
in the production servers and in order
to deploy an application you should have
the knowledge of containers such as
Docker and configuration management
tools like ansible so Docker helps with
the containerization while configuration
management tools such as ansible so
Docker helps with containerization while
tools such as ansible helps you to
provide you the capability of pushing
configurations onto multiple servers and
maintaining them in the required state
so why choose Docker High scalability
and efficiency reusable data volumes
isolated applications and why ansible
because unlike other tools in this field
it has a push-based configuration it is
agentless and uses SSH for secure
connections the next stage is to learn
to monitor the applications so
monitoring your applications is another
important aspect of a devops engineer in
this stage we identified the issue and
implement the changes as quickly as
possible the most popular tool which
provides for monitoring applications is
nagios it has a comprehensive monitoring
system high availability and immediate
remediation and finally a devops
engineer must know the working of cloud
providers for a devops engineer it's
important to know about the cloud
service providers and their Basics cloud
computing is an important skill to learn
irrespective of which field you are in
today as a lot of companies have the
infrastructure on the cloud Amazon web
services is the most popular cloud
provider whereas Microsoft Azure and
Google Cloud platform are slowly
catching up to Amazon Amazon web
services provides high scalability and
flexibility is cost effective and
provides better security let's now look
at some of the devops certifications you
can opt for the first one is the AWS
certified devops engineering program
which is offered by Amazon web services
and then of course we have the devops
certification training offered by our
very own simply learn simply learn also
provides programs which are more
specific to tools such as Chef puppet
ansible and selenium certification and
we also provide a devops architect
Masters program so now these are some of
the most popular certifications
available in the market let's get a
better idea of what these certifications
provide so we'll move on to the simple
learn web page so here we are on the
landing page for devops certification
training course as you can see here this
course provides 56 hours of in-depth
learning you will be working on more
than 10 real industry projects which are
integrated with the labs there are 24
Live demos of the most popular devops
tools you have a 24x7 support dedicated
to Mentor you throughout your project
and we also provide self-paced learning
so there are videos which are always
available in case you have a very
stringent timetable let's now look at
some of the tools which are covered
under this course so under build
automation we cover Maven among other
tools configuration management this
ansible Chef puppet continuous
integration and monitoring tools
includes Jenkins steam City Docker and
testing Frameworks we have a clan junit
Version Control bit bucket git so we are
going through a number of tools but as
you can see all the popular ones are
definitely covered other than this let's
also have a quick look at all the topics
that is covered under this course so we
have the course introduction followed by
introduction to devops and then we focus
on the various processes under devops
Version Control continuous integration
deployment and the build tools software
automation testing Frameworks
configuration management
containerization all of this is followed
up by need of cloud in devops and of
course you have practice projects we
also are provided with two free courses
one covering the Linux Basics and one
with the fundamentals of JavaScript
since as we have already mentioned these
are two very important skills that a
devops engineer must possess and now
this is the devops architect Masters
program offered by simply learn this is
the go to program if you take becoming a
devops engineer seriously as you can see
here guys it covers more than 40 in
demand skills and 15 plus tools you have
access to 120 life instructor-led online
classes you'll also be working on more
than seven real life projects and at the
end of your entire course you receive
your Master's certification there's a
well thought out timeline to proceed
with your course which covers
instructor-led and self-paced classes
your projects and your lab sessions also
some of the tools that we'll be covering
under this program include Maven Chef
puppet ansible solstice attack team City
Jenkins nagios J unit jit and many more
so guys why procrastinate anymore get on
to Simply learns webpage register
yourself to a course and get certified
get ahead of course if you're taking up
a career you must know what kind of
salary to expect an average salary of a
devops engineer in the United States is
115
666 dollars per year so this is the
average salary as you can see here guys
there's a graph which shows how there's
been a growth this graph here will give
you a good idea of the salary Trend an
average salary of a devops engineer in
India on the other hand is six lakh 12
000 per year so again here we have a
graph on the right which demonstrates
the pattern in growth in the salary so
there are approximately seven sections
that we cover in devops we go from
general devops questions source code
management with tools such as git and
continuous integration and here will
focus on Jenkins continuous testing with
tools such as selenium and then you also
have the operations side of devops which
is your configuration management with
tools such as Chef puppet and ansible
containerization with Docker and then
continuous monitoring with tools such as
nagios the first video is going to zero
in on the left hand side of the screen
we're going to zero in on those general
devops questions source code management
continuous integration then continuous
testing our second video is going to get
into the operations tools and we'll get
into that and the next time you come
back above me there's probably a link
that will take you straight to that
video so let's just get into those
general devops questions so one of the
questions that you're going to be asked
is how is devops different from agile
and the reality is is that devops is a
cultural way of being able to deliver
Solutions that's different from agile if
we look at the evolution of deliver
delivery over the last five to ten years
we've gone from waterfall based delivery
to Agile delivery which is on Sprints to
where we are with continuous integration
and continuous delivery around devops
the whole concept of devops is
culturally very very different from
agile and the difference is is that
you're looking at being able to do
continuous releases what does that mean
the difference is is that you want to be
able to send out code continuously to
your production environment that means
the operations team the development team
have to be working together that means
that any code that gets created has to
be able to go to production very quickly
which means you need to be testing your
code continuously and then that
production environment must also be able
to be tested continuously and any
changes or any errors that come up have
to be communicated effectively and
efficiently back to the dev and op team
another area in which I see that devops
is different is really the response that
we have for how we engage with the
customer so the customer is coming to
your website to your mobile app to your
chat bot or any digital solution that
you have and has an expectation when
you're going through and actually doing
a devops paradigm the old model would be
that you would capture requirements from
the customer then you do your
development then you do your testing and
there'd be these barriers between each
of those as we move faster through from
waterfall to Agile what we saw is that
with agile we were able to respond much
faster to customer demand so instead of
it being weeks or months sometimes in
some cases years between releases of
software what we saw it would was a
transition to weeks and months for
releases on software now we see with
devops is that the release cycle has
shrunk even further with the goal of
continuously delighting the customer how
further has that release cycle shrunk to
there are companies that have gone from
having really releases of once a week or
once every two weeks or once a month to
now having multiple releases a day
indeed some companies have up to 50
releases a day this isn't something to
also bear in mind is that each of those
releases are tested and verified against
test records so that you can guarantee
that the code that's going to production
is going to be good continuous code so
what are the differences between the
different phases of devops so
effectively there are two main phases of
devops there's the planning and coding
phase and then there's the deploying
phase and you have a tool such as
Jenkins that allows you to integrate
between both environments some of the
core benefits that you may have to
devops are going to be some technical
benefits and some business benefits so
when somebody asks you what are the
benefits of devops you can reply that
from a technical point of view you're
able to use continuous software delivery
to constantly push out code that has
been tested and verified against scripts
that have been written and approved you
can be able to push out smaller chunks
of code so that when you have an issue
you're not having to go through massive
blocks of code or massive projects
you're going through just very small
micro services or small sections of code
and you're able to detect and correct
problems faster on the business side the
benefits are absolutely fantastic from a
customer that's coming to your website
and or to your mobile app they're going
to see it responses happening
continuously so that the customer is
always aware that you as a company are
listening to their demands and
responding appropriately you're able to
provide a more stable environment and
you're able to scale that environment to
the demands of the number of customers
that are using your services so how you
approach a project that needs to
implement devops so this is really an an
exciting area for you to be in so there
are effectively three stages when it
comes to actually working in a devops
the first stage is an assessment stage
and think of this as the back of the
napkin ideation stage this is where you
are sitting with a business leader and
they're giving you ideas of what they
would like to see from feedback that
they've had from their customers this is
Blue Sky opportunity this is thinking of
Big Ideas that second stage and this
often comes as a fast follow to stage
one is actually proving out that concept
so developing a proof of concept and a
proof of concept can actually be a
multiple different things so it could be
something as simple as a wireframe or it
could be something that is as complex as
a mini version of the file application
depending on the scope of the work that
you're delivering will really depend on
how complicated you want the POC to be
but with that in mind whatever other
choice you make you have to be able to
deliver enough in the POC so that when
you present this to a customer they're
able to respond to that creation that
you've developed and able to give you
feedback to be able to validate that you
are going with the right solution and
able to provide the right product to
your customers that third stage is where
you get into your devops stage and this
is just the exciting part this is where
the rubber hits the road and you start
releasing code based on a backlog of
features that are being requested for
the solution in contrast to doing agile
delivery where you just continuously
work through a backlog with devops what
you're also looking at is putting an
analytics and sensors to be able to
validate that you are being successful
with the solution that's being delivered
so that once you've actually start
delivering out code that customers can
interact with you want to be able to see
what are the pieces of the solution that
they are using what do they like what is
complicated where are the failure points
and you want to use that data and feed
that bag into your continuous
integration and have that as a means to
be able to backfill the demand of work
that gets completed in the bank log so
what is the difference between
continuous delivery and continuous
deployment so continuous delivery is
based on putting out code that can be
deployed safely to production and
ensures that your businesses and
functions are running as you would
expect them to be so it's going through
and completing the code that you would
actually see continuous deployment in
contrast is all about ensuring that
you're automating the deployment of a
production environment so you're able to
go through and scale up your environment
to meet the demands of both the solution
and the customer this makes software
development and release processes much
more fast faster and more robust so if
we look here we can actually see where
continuous integration and continuous
deployment come hand in hand so when you
actually start out with the initial
pushes of your code that's where you're
doing your continuous integration and
your continuous delivery and then at
some point you want to get very
comfortable with deploying the code that
you're creating so that it's being
pushed out to your production
environment so one of the things that's
great about working with the tools that
you use in a devops continuous
integration and continuous delivery
model is that the development tools that
you use the containerization tools the
testing tools should always reflect the
production environment and what this
means is that when you actually come to
deploying solutions to production there
are no surprises because your
development team have been working
against that exact same environment all
the way through so a question that
you'll also be ask is you know what is
the role of a configuration Management
in devops and so the role of
configuration management really has
three distinct areas I'm the first and
this is really obvious one and this is
the one where you probably already have
significant experiences has the ability
to manage and handle large changes to
multiple systems in seconds rather than
days hours or weeks as that may have
happened before the second area is that
you want to also demonstrate the
business reasons for having
configuration management and the
business reason here is that it allows
it and infrastructure to standardize on
resource configurations and this has a
benefit in that you're able to do more
with fewer people so instead of having a
large configuration team you can
actually have a smaller more highly
skilled team that's able to actually
manage an even larger operational
environment and thirdly you want to be
able to highlight the ability to scale
so if you have configuration management
tools you're able to manage a
significant number of servers and
domains that may have multiple servers
in it allows you to effectively manage
servers that are deployed on cloud or
private cloud and allow you to do this
with high accuracy so how does
continuous monitoring help and maintain
the entire architecture of the system so
when this question comes up you want to
dig in and show your knowledge on how
configuration and continuous monitoring
is able to control an entire environment
so the number one topic that you want to
bring up when it comes to continuous
monitoring is that with being able to
effectively monitor your entire network
24 7 for any changes as they happen
you're able to identify and Report those
thoughts or threats immediately and
respond immediately for your entire
network instead of having to wait as it
happens sometimes for a customer to
email or call you and say hey your
website's down nobody wants that that's
an embarrassing thing the other three
areas that you want to be able to
highlight are the ability to be able to
ensure that the right software and the
right services are running on the right
resources that's your number one
takeaway that you want to be able to
give of continuous monitoring the second
is to be able to monitor the status of
those servers continuously this is not
requiring manually monitoring but having
a agent that's monitoring those servers
continuously and then the third is that
by scripting out and continuously
monitoring your entire environment
you're creating a self-audit trail that
you can take back and demonstrate the
effectiveness of the operations
environment that you are providing so
one of the cloud companies that is a
strong advocate for devops is Amazon's
web services AWS and they have a really
five distinct areas them that you can
zero in on board services so when the
question comes up what is the role of
AWS in devops you want to really hold
out your hand and list of five areas of
focus using your thumb and finger so you
want to have flexible Services built for
scale automation secure and a large
partner ecosystem and having those five
areas will really be able to help
demonstrate why you believe that AWS and
and other Cloud providers for AWS are 70
the leader in this space are great for
being able to provide support for the
role of devops so one of the things that
we want to be able to do effectively
when we're releasing any kind of
solution is to be able to measure that
solution and so kpis are very important
so you will be asked for three important
Dev of kpis and so three that really
come to mind that are very effective the
first one is me mean time to failure
recovery and what this talks about is
what is the average time does it take to
recover from a failure and if you have
experience doing this then look at the
experience you have and use a specific
example where you were able to
demonstrate that mean time to failure
recovery the second is deployment
frequency and with deployment frequency
you want to be able to discuss how often
do you actually deploy Solutions and
what actually happens when you're
actually doing those deployments and
what does the impact to your network
look like when you're doing those
deployments and then the third one is
really tied to that deployment frequency
which is around what is the percentage
of failed deployments and so how many
times did you deploy to a server and
something happened where the server
itself failed what we're looking for
when you're going through and being
asked for these kpis is experience with
actually doing a devops deployment and
being able to understand what devops
looks like when you're pushing out your
in infrastructure and then the second is
being able to validate that
self-auditing ability and one word of
caution is don't go in there and say
that you have a hundred percent success
uh the reality is that servers do
degrade over time and you maybe want to
talk about a time when a server did
degrade in your environment and use that
as a story for how you're able to
successfully get over and solve that
degradation so one of the terms that is
very popular at the moment is
infrastructure as code and so you're
going to be asked to explain
infrastructure as code and really it's
it's something that actually becomes a
byproduct of the work you have to do
when you're actually putting together
your devops environment and
infrastructure's code really refers to
the writing of code to actually manage
your environment and you can go through
many of the other tools that we've
covered in this series but you'll see
that XML or Ruby or yaml are used as
language which is to describe the
configuration for your environment this
allows you to then create the rules and
instructions that can be read by the
machines that are actually setting up
the physical Hardware versus a
traditional model which is having
software and installing that software
directly onto the machine this is really
important when it comes to cloud
computing there really is a strong
emphasis of being able to explain
infrastructure as a service and
infrastructure as code is fundamental to
the foundation to infrastructure as
service and then finally allows you to
be able to talk about how you can use
scripted languages such as yaml to be
able to create a consistent experience
for your entire network all right so
let's now get into the next section
which is source code management and
we're going to focus specifically on git
the reason being is that get is really
the most popular source code management
solution right right now there are other
Technologies out there but for the types
of distributed environments that we have
uh source code management with Git is
probably the most effective so the first
question you'll be asked when it comes
to get is to talk about the difference
between centralized and distributed
Version Control and if we look at the
way that the two are set up older
Technologies such as older versions of
Team Foundation server though the
current version does actually have git
in it but older versions required a
centralized server for you to check in
and check out of code the developer in
the centralized system does not have all
the files for the application and if the
centralized server crashes then you
actually lose all of the history of your
code now in contrast a distributed model
actually we do check in our code to a
server however for you to be effective
and building out your Your solution you
actually check out all of the code for
the solution directly onto your local
development machine so you can actually
have a copy of the entire solution
running on your local machine this
allows you to be able to work
effectively offline it really allows for
scalability when it comes to building
out your team so if you have a team that
may be in Europe you can actually even
scale that team with people from Asia
from North America or South America very
easily and not have to worry about
whether or not they have the right code
or the wrong code and in addition to
that if the actual main server where
you're checking in your code does crash
it's not a big deal because you actually
have each person has a copy of the code
so as soon as the server comes back up
you have to check back in and
everybody's running back as if there was
nothing had happened at all so one of
the questions you'll be asked is to give
the answer to some of the commands you
use for working with Git so if you were
to be ask the question is what is the
git command that downloads any
repository from GitHub to your computer
on the screen we have four options we
have git push get Fork get clone and get
commit the answer in this instance would
be get clone now if you want to be able
to push code from your local system to a
GitHub repository using git then first
of all you want to be able to do is
connect the local repository to a remote
repository and in the example you may
want to talk about using the command get
remote ad origin and then the actual
path to a GitHub repository you could if
you wanted to actually at this point
also talk about other repositories such
as get lab that you can also work with
or a private get repository that would
be used just for the development team
once you've actually then added the
local repository into your local
computer then the second action you want
to use is a push which is to actually
push your local files out to the master
environment so you'd use the command git
push origin master so one question you
may be asked is what is the difference
between a bear repository and a standard
way of initializing a git repository so
let's look through what is the standard
way so the standard way using get init
allows you to create a working directory
using the command git in it and then the
folder that creates is the folder that
creates all the revision history related
to the work that you're doing in
contrast using the bear way you have a
different command for setting that up so
it would be git init dash dash there and
it does not contain any working or
checked out source files locally on your
machine in addition the revision history
is actually stored in the root folder
versus a subfolder that you'd have with
the normal get in it initialization so
which of the following CLI commands
would you be used to rename a file so we
have git RM get MV get RM Dash are or
none of the above well in this instance
it would be gate MV a question that
you'll be asked around commit is going
to be what is the process to revert a
commit that has already been pushed and
made public and there are two ways you
can address this the first is to
actually address the bad file in the new
commit and you can use the command git
commit Dash M and then put in a comment
for why that file is being removed the
second is to actually create a new
commit that actually undoes all the
changes that were made with the bad
commit and then to do that you would use
git revert and then the commit ID and
the commit ID and it could be something
such as
560e0938f but you'd have to find that
from the the commit that you had made
but that would allow you to revert any
bad files that you had submitted so the
two ways of being able to get files from
a get repository and you're going to be
asked to explain the difference between
git Fetch and get Paul so get fetch
allows you to fetch and download only
new data from a new repository it does
not integrate any of the new data into
your working files and it can be undone
at any time if you want to break out the
remote tracking branches in contrast get
pull updates the current head Branch
with the latest changes from the remote
server so you get all of the files and
downloaded it downloads new data and
integrates it with the current working
files you have on your system and it
tries to merge remote changes with your
local Wireless so one of the questions
you'll get asked about get is what is a
get stash so as a developer you will be
working on the con Branch within a
solution but what happens if you come up
with an idea where it's something that
will take a different amount of time for
you to be able to complete but you don't
want to interrupt the mainline Branch so
what you can actually do is you can
actually create a branch that allows you
to start working on your own work
outside of the mainline branch and this
is called git stash it allows you to be
able to modify your files without
interrupting the mainline Branch so you
once you start talking about branching
in git be prepared to answer and explain
the concept of branching so essentially
what it allows you to do is have a
Mainline Master branch that has all the
code that the team is checking in and
checking out against but allows you to
have an indefinite number of branches
that allows for new features to be built
in parallel to the mainline branch and
then at some point be reintroduced to
the mainline Branch to allow the team to
add in new features and so if we look
through the merge and get rebase these
are the two features that you would be
using continuously to be able to talk
about how you take a branch and merge it
back into the mainline Branch so on the
left hand side we have get merge which
allows you to take the code that you're
creating and merge it back into the
master on the right hand side what you
have is a slightly different approach
this is for projects where you reach a
point in a project where you go okay
we're going to effectively restart the
project at this point in time and we
want to ignore the complete history
that's happened before that and that's
called get rebase and that would allow
you to rewrite the project history by
creating a brand new Mainline branch
that ignores all other previous branches
that have happened before it you can if
you want to very quickly and easily find
out all the files that have been used to
make a particular commit so when
somebody asks you the question how do
you find a list of files that has been
changed in a particular commit you can
actually say that all you have to go is
find the command get div tree dash R and
then the hash that you'd use for the
commit and that would actually then give
you a breakdown of all the files that
have been made with that particular
commit a question that you'll be asked
when you're talking about merging files
is what is a merge conflict in git and
how can it be resolved so essentially a
merge conflict is when you have two or
more branches that are competing with
commits in git and you have to be able
to determine which is the appropriate
files that need to be submitted and this
is where you would go in and to actually
help resolve this issue you'd actually
go in and manually edit the conflicted
files to select the changes you want to
keep in the final merge so let me go
through the steps that you would take to
be able to illustrate this when you're
talking about this particular question
in your interview now there are
essentially four stages the first would
be under the repository name you want to
select a pull request and you want to be
able to show how that pull request would
be demonstrated inside of GitHub So
within the pull request there's going to
be a highlight of conflict markers and
you'll be able to select which conflicts
um you want to keep and which you want
to merge and which ones you want to
change so if it's a step through how you
would actually resolve a merge conflict
and the first step would be under GitHub
you want to be able to pull the
repository name and then the pull
request around that repository in the
pull request list click the pull request
with a merge conflict and that you'd
like to be able to resolve they'll pull
up a file they'll list out all of the
conflicts for you near the bottom of
that file will be a list of the requests
that need to be resolved and then if you
need to make a decision on which
branches you want to keep or which ones
you want to change that will have to be
something you have to put in
instructions inside of the file you'll
actually see that there are conflict
markers within the instructions which
are going to ask you which files you
want to change change which ones you
want to keep if you have more than one
merge conflict in your file scroll down
to the next set of conflict markers and
repeat steps four and five until you
resolve all of the conflicts you will
want to mark your file as resolve in
GitHub so that the repository knows that
you are having everything resolved if
you have more than one file with a
conflict then you want to go then on to
the next file and start working on those
files and just keep repeating the steps
we've done up to this point until you
have all the conflicts resolved and then
once you have all of the resolutions
created then you want to select the
button which is commit merge and then
merge all your files back into GitHub
and this will take care and manage the
resolution of the merge conflict within
GitHub so you can also do this through
command line and with the command line
you want to use a get bash and so you
want to as a first step open up get bash
and then navigate to the local git
repository and command line by using the
CD change directory and then list out
the actual folder where you actually are
putting all of your code and then you
want to be able to generate a list of
the files that are affected with the
merge conflict and in this instance here
you can actually see the file style
guide.md has a merge conflict in it and
as before with working with GitHub you
actually go through and use a text
editor and use any text editor but as
you go through and edit out what you
want to keep and what you want to manage
in your conflict so you actually have a
resolution that's been created so that
you'll be able to then once you you're
using the conflict markers you can
actually merge your files together so
that the solution itself will allow you
to incorporate your commits effectively
into the resolution once you've gone
through and applied your changes you're
able to then merge the conflicted
command minutes into a single commit and
able to push that up to your remote
repository all right let's talk about
the next section which is continuous
integration with Jenkins so the first
question you'll be asked about with
Jenkins is explain a Master Slave
architecture of Jenkins so the way that
Jenkins is set up is that the Jenkins
Master will pull code from your remote
git repository such as GitHub and we'll
check that repository every time there
is a code commit it will distribute the
workload of that code and the tests that
need to be applied to that code to all
of the Jenkins slaves and then on
request the Jenkins master and the
slaves will then carry out all the
builds and tests to be able to produce
test reports the next question you'll be
asked is what is a Jenkins file and
simply put a Jenkins file as a text file
that has a definition of the Jenkins
Pipeline and is checked into a source
code repository and this will really
allow announce for three distinct things
to happen one it allows for a code
review and iteration of the pipeline it
permits an audit Trail for that Pipeline
and also provides a single source of
troop for the pipeline which can be
viewed and edited so which of the
following commands runs Jenkins from the
command line is it Java Dash jar
jenkins.war Java Dash War jenkins.jar
Java dot dash jar Jenkins jar Java Dash
War Jenkins dot wall and the answer is a
Java Dash jar jenkins.war so when
working with Jenkins you're going to be
asked what are the key Concepts and
aspects of working with the Jenkins
Pipeline and you want to really hold out
your fingers here and go through four
key areas and that is pipeline node step
and stay so pipeline refers to the use
defined model of a CD continuous
delivery pipeline node are the machines
which is which are part of that Jenkins
environment within the pipeline step is
a single task that tells Jenkins what to
do at that particular point in time and
then finally stage defines a
conceptually distinct subset of tasks
performed through the entire Pipeline
and tasks could be build test and deploy
so which of the following file is used
to Define dependency in Maven and do we
have A build.xml B palm.xml C
dependency.xml or D version dot XML and
the answer is
palm.xml working with Jenkins you're
going to be asked to explain the two
types of pipeline used in Jenkins along
with the syntax and so a scripted
pipeline is based on groovy script as
the domain specific language for Jenkins
and there are one or more note blocks
that are used throughout the entire
Pipeline on the left hand side you can
actually see what the actual script
would look like and the right hand side
shows what the actual declaration for
each section of that script would be the
second time of Jenkins pipeline is a
declarative Pipeline and a declarative
pipeline provides a simple and a
friendly syntax to Define what the
pipeline should look like and then you
can actually at this point use an
example to actually break out how blocks
are used to define the work completed in
a decorative pipeline so how do you
create a copy and backup of Jenkins well
to create a backup periodically back up
Jenkins to your Jenkins home directory
and then create a copy of that directory
it's really as simple as that the
question you'll be asked as well is how
can you copy Jenkins from one server to
another well there are essentially there
are three ways to do that one is you can
move a job from one installation of
Jenkins to another by copying the
corresponding job directory the second
would be to create a copy of an existing
job directory and making a clone of that
job directory but with a different name
and the third is to rename an existing
job by renaming a directory so security
is fundamental to all the work that we
do within devops and Jenkins provides
the center core to all the work that
gets completed within a devops
environment there are three ways in
which you can apply security to
authenticate users effectively and when
you're asked about this question of
security within Jenkins the three
responses you want to be able to provide
is a Jenkins has its own internal
database that uses secured user data and
user credentials B is you can use a ldap
or lightweight directory access protocol
server to be able to authenticate
Jenkins users or C you can actually
configure Jenkins to authenticate by
using such as oauth which is a more
modern method of being able to
authenticate users you're going to be
asked how to deploy a custom build of a
core plugin within Jenkins and
essentially the four steps you want to
go through are first of all copying the
dot HPI plugin file into the Jenkins
home plugins sub directory you want to
remove the plugins development directory
if there is one you want to create an
empty file called
plugin.hpi dot pinned and once you've
completed these three steps restart
Jenkins and your custom build plugin
should be available how can you
temporarily turn off Jenkins security if
the administrative user has locked
themselves out of the admin console this
doesn't happen very often but when it
does it's good to know how you can
actually get into Jenkins and be able to
resolve the problems of authenticating
effectively into the system as an
administrator so when you want to be
able to get into a Jenkins environment
what you want to be able to do is locate
the config file you should see that it's
set to True which allows for security to
be enabled if you then change the user
security setting to false Security will
be disabled allow you to make your
administrative changes and will not be
re-enabled until the next time Jenkins
is restarted so what are the ways in
which a build can be scheduled and run
in Jenkins well there are four ways in
which you can identify the way a build
can be scheduled on running Jenkins the
first is when source code management
commits new code into the repository you
can run Jenkins at that point the second
can be the after the completion of other
builds so maybe you have multiple builds
in your project that are dependent on
each other and when so many other builds
have been executed then you can have
Jenkins run you can schedule builds to
run at a specified time so you may have
nightly bills of your code that
illustrate the changes in the solution
you're building and then finally you
also can manually build a environment on
request occasionally you will want to
also restart Jenkins and so it's good
that when a question around how do you
restart Jenkins manually comes up that
you have the answers and there are two
ways in which you can do it one is you
can force a restart without waking for
bills to complete by using the Jenkins
URL that you have for your environment
slash restart or you can allow all
running bills to complete before restart
are required in which case you would use
the command of the URL for your Jenkins
environment slash safe restart so let's
go into the fourth and final section of
this first video which talks about
continuous testing with selenium so the
first question you will be asked most
likely around selenium are what are the
four different selenium components and
again you will want to hold open your
fingers because there are four distinct
environments you have selenium
integrated development environment or
selenium IDE you have selenium remote
control or selenium RC you have selenium
Webdriver and then selenium grid you'll
be asked to explain each of those areas
in more detail but let's start off with
by looking at selenium driver what are
the different except actions in selenium
Webdriver so it's useful to remember
that an exception is an event that
occurs during the execution of a program
that disrupts the normal flow of that
program's instructions and so we have
four we have a timeout exception an
element not visible exception no such
element exception and a session not
found exception and each of those if we
step through them are the four different
types of exceptions that can be thrown
up when using the selenium Webdriver so
as we evolve in our digital world with
the different types of products that are
available for us to be able to build
Solutions onto multiple platforms you're
going to be asked can selenium and other
devops tools run in other environments
and so a good question around this is
cancellium test and application in an
Android web browser and the short answer
is absolutely yes it can we have to use
the Android driver for it to be able to
work so you want to be able to talk
about the three different types of
supported tests within selenium so when
the question comes up what are the
different test types supported by silent
you can answer that and there are three
different types of tests first is a
functional test second is a regression
test and third is a load testing test
the functional test is a kind of Black
Box testing in which test case are based
on a specific area or feature within the
software a regression test helps you
find any specific areas that functional
tests or non-functional areas of the
code wouldn't be able to detect the load
testing test allows you to monitor the
response of a solution as you increase
the volume of hits and how you're using
the code are put onto it an additional
question you'll be asked is how can you
get a text of a web element using
selenium but the get command is used to
retrieve tax of a specific web element
and it's important to remember how over
the the command does not return any
parameters but just returns a string
value so you want to be able to capture
that stream value and discuss about it a
question you'll be asked around selenium
is can you handle keyboard and mouse
actions using selenium and the answer is
yes you can but you have to make sure
that you're using the advanced user
interaction API and the advanced user
interaction API is something that can be
scripted into your test and it allows
you for capturing methods such as a
click and hold and drag and drop Mass
events and then keyboard down or
keyboard up a key release events so that
if you want to to capture say the use of
control shift or a specific function
button of the keyboard you'd be able to
capture those of the following four
elements which of these elements is not
a web element method a get text B size C
get tag name d send k keys and it's B
size you're going to be asked to explain
what is the difference for when we use
find element or find elements and so if
we look at find element find element
finds the first element in the current
web page that matches the specified
locator value in contrast find elements
finds all of the elements on the web
page that matches the specified value
when using Webdriver what are the driver
close and Driver quit commands and these
are two different methods used to close
a web browser session in selenium so
driver close will close the current web
browser which your focus is set and
Driver quick closes all the browser
windows and ends the web driver session
completely the final question that you
are likely to be asked in using selenium
is how can you submit a form using
selenium well in this instance that's
relatively easy the following lines of
code will let you submit a form in
selenium which would be web element El
equals driver dot find element and then
you put in the ID and the element ID and
then L submit open close parentheses
semicolon so let's just get into the
first section which is configuration
management so one of the questions that
you'll get asked right away is why do
you have SSL certificates used for Chef
really fundamentally your immediate
answer should be security SSL provides
for a very high level of private
security and private and public key
pairing this really is essential to
ensure that you have a secure
environment throughout your entire
network the second part should be that
if you're using SSL and you're using the
private public key model within SSL
you're able to guarantee the systems on
your network that the chef that you'll
be able to validate that the nodes
within your network the chef is
validating against actually are the real
nodes themselves not imposters so you
will also also be asked some questions
such as the following which is the
following commands would you use to stop
or disable the HTTP service when the
system boots and you'll typically get
four responses and there'll be hashtag
systemctl disable
httpd.service or is it system disable
http.service system disable httpd or the
final option which is systemctl disable
httpd.service your answer should be the
first one which is hashtag systemctl
disable
http.service so Chef comes with a series
of tools that allow it to function
effectively and one of the tools that
you're going to be asked about is what
is Test Kitchen and Test Kitchen is
essentially a command line tool that
allows you to be able to test out your
cookbook before you actually deploy it
to a real node so some of the commands
that you would use are for instance if
you want to create an instance of Test
Kitchen you would do kitchen create if
you want to destroy brilliant instance
after you created it you do kitchen
destroy and if you want to be able to
combine multiple instances you would do
kitchen converge so a question you'll
get is around Chef is how does Chef
apply differ from Chef client that's a
fundamentally the difference between
them is that Chef apply will validate
the recipe that you're working on
whereas Chef client looks to apply and
validate the entire cookbook that's run
in your server environment so one is
focused on the recipe and the other is
focus on the entire cookbook so there
are some differences when you're working
with different command lines so for
instance and when you're working with a
puppets and you're working with one
version of puppets and you want to do
what is the command to sign a requested
certificate the top example here is for
puppet version 2.7 whereas the lower
option here is for puppet version three
and that's something to bear in mind
when you're going through your interview
process is that at the tools that are
used within a continuous integration can
continuous delivery devops model do vary
and so you want to be able to talk
knowledgeably about the different
versions of the tools so that when
you're talking to your interviewer
you're able to show you the Deep
knowledge that you have which open
source or Community tools do you use to
make puppet more powerful and
essentially this question is going to be
asking you to look beyond the core
Foundation of puppet itself and so the
three options you have is being able to
track configurations with jira which you
should be doing anyway but it's a great
way to be able to clearly communicate
the work that's been done with puppet
our Version Control can be extended with
get and then the changes should be
passed through Jenkins so the three
tools you want to be looking at
integration with jira Git and Jenkins so
what are the resources in public well
fundamentally there are four the
resources are basic unit of any
configuration management tool they are
the features of the nodes they are the
written catalog and the execution of the
catalog on a node so as we dig deeper
into puppets one of the things that you
are likely to be asked regarding puppet
is what is a class and puppets and so a
class in public is really the name
blocks in your manifest that contain the
various configurations and this can
include Services files and packages we
have on the screen here example of what
a class would look like when you write
it out and you may want to memorize just
one class don't memorize just a whole
set of classes just memorize one the
person that's interviewing you is just
really looking for someone who has a
working knowledge they're not looking
for you to have memorized complete
massive classes but having one small
class to be able to illustrate the
experience you have is extremely
valuable to the interviewer and
particularly if it's a technical
interview so as we move into ansible one
of the things that you're going to be
asked around ansible is what is ansible
role so a role is an independent block
of tasks and variable files and
templates embedded with inside of the
Playbook so the example we have here on
the screen actually shows you one role
within a Playbook and in this role it is
to install Tomcat on a node again as
with previous question within puppet of
a class it's probably good to have
memorized just one or two roles so you
can talk knowledgeably about ansible
when you're having your interview so
when you're working with ansible when
should you be using the curly brackets
and so just as a frame of reference
there's often two different ways that
these kind of brackets are referred to
they're either referred to as French
brackets or curly brackets either way
and what you'll be wanting to ask is
when would you use these specific types
of brackets within ansible and really
the answer comes down to two things one
is that it makes it easier to
distinguish strings and undefined
variables and the second is for putting
together conditional statements when you
are actually using variables and the
example we have here is this prints the
value of and we have Foo and we have to
then put in the variable conditional
statement of Foo is defined as something
so what is the best way to make content
reusable and redistributable with
ansible and there's a really essentially
three the first is to include a sub
module or another file in your playbook
the second is to import an improvement
of an include which ensures that a file
is added only once and then the third is
roles to manage the tasks within the
Playbook so a question you will be asked
is provide a differences between ansible
and puppets so if we look at ansible
it's a very easy agentless installation
it's based on python you can configure
it with yaml and there are no support
for Windows in contrast puppets is is an
agent based installation it's written in
Ruby the configuration files are written
in DSL and it has support on all popular
operating systems so we dig deeper into
the actual architecture ansible it has a
much more simple architecture and it's
definitely a push only architecture in
contrast to puppet it's a more
complicated but more sophisticated
architecture where you're able to have a
complete environment managed by the
puppet architecture so let's get on to
our next section which is
containerization so let's go through and
you're going to be asked to explain what
the architecture of Docker is and Docker
really is the most popular
containerization environment so Docker
uses a client server architecture and
the docker client is a service which
runs in a command line and then the
docker Daemon which is run as a rest API
within the command line will accept the
requests and interacts with the
operating system in order to build the
docker images and run the docker
containers and then the docker image is
a template of instructions which is used
to create containers the docker
container is an executable package of
applications and its dependencies
together and then finally the docker
registry is a service to host and
distribute Docker images among other
users so you'll also be asked to provide
what are the advantages of Docker over a
virtual machine and and this is
something that comes up very
consistently in fact um you may want to
even extend it as having what are the
differences between having a dedicated
machine a virtual machine and a Docker
or docker-like environment and really
the arguments for Docker are just
absolutely fantastic you know first of
all Docker does contain an occupy Docker
containers occupy significantly less
space than a virtual machine or a
dedicated machine the boot up time on
Docker is significantly faster than a VM
containers have a much better
performance as they are hosted in a
single Docker image Docker is highly
efficient and very easy to scale
particularly when you start working with
kubernetes easily portable across
multiple platforms and then finally for
space allocation Docker data volumes can
be shared and reused among multiple
containers the argument against virtual
machines is significant and particularly
if you're going into an older
environment where a company is still
using actual dedicated hardware and
haven't moved to a cloud or cloud-like
environment your Arguments for Docker
are going to be very very persuasive be
very clear on what the advantages are
for Docker over a virtual machine
because you want to be able to
succinctly share them with your team and
this is something that's important when
you're going through the interview
process but also equally important
particularly if you're working with a
company that's transitioning or going
through a digital transformation where
they aren't used to working with the
tools like Docker you need to be able to
effectively share with that team what
the benefits are so how do we share
Docker containers with with different
nodes and in this instance what you want
to be able to do is Leverage The Power
of Docker swarm so Docker swarm is a
tool which allows the it administrators
and developers to create and manage
clusters of swarm nodes within the
darker platform and there are two
elements to the node there's the manager
node and then there's the the worker
node the manager node as you'd assume
matches the entire infrastructure and
the working node is actually the work of
the agent as it gets executed so what
are the commands to create a Docker
swarm and so here we have an example of
what a manager node would look like and
once you've created a swarm on your
imagine now you can now add working
nodes to that swarm and again when
you're stepping through this process be
very precise in the execution part that
needs to be taken to be able to
effectively create a swarm so start with
the manager node and then you create a
worker node and then finally when a node
is initializes a manager note it can
immediately create a token and that
token is used for the worker nodes and
associating the IP address with the
worker nodes question 17 how to run
multiple containers using a single
service it is possible to run multiple
containers as single service by using
Docker compose and docket compose will
actually run each of the services in
isolation so that they can interact with
each other the language used to write
out the compose files that allow you to
run the service is called yaml and yaml
stands for yet another markup language
so what is the use of a Docker file so a
Docker file actually is used for
creating Docker images using the build
command so let's go through and show on
the screen what that would look like and
this would be an opportunity where if
you're actually in a technical interview
you could potentially even ask hey can I
draw on a whiteboard and show you what
the architecture for using the build
command would look like and what the
process would look like and again when
you're going through an interview
process says as someone who interviews a
lot of people one of the things I really
like is when an interview candidate does
something that's slightly different and
in this instance this is a great example
of where you can stand up to the
Whiteboard and actually show what can
actually be done through actually
creating images on the Whiteboard very
quickly little square boxes where you
can actually show the flow for creating
a build environment as an architect this
should be something that you're
comfortable doing and by doing it in the
interview and suddenly you want to ask
permission before you actually do it but
doing this in the interview really helps
demonstrate your comfortable feelings of
working with these kind of architecture
drawings so back to the question of
creating a Docker file so we go through
and we have a Docker file that actually
then goes ahead and creates the docker
image which then in turns creates the
docker container and then we are able to
push that out up to a Docker Hub and
then share that Docker Hub with
everybody else as part of the docker
registry with the whole network so what
are the differences between Docker image
and Docker containers so let's go
through the docker image so the docker
images are templates of a Docker
container an image is built using a
Docker file and it stores that Docker
file in a Docker repository or a Docker
Hub and you can use Docker Hub as an
example and the image layer is a
read-only file system the docker
container is a collection of the runtime
instances of a Docker image and the
containers are created using Docker
images and they are stored in the docker
Daemon and every container is a layer is
a read write file system so you can't
replace the information you can only
append to it so while you can actually
use yaml for writing your so a question
you can be asked is instead of yaml what
can be an alternate file to build Docker
compose so yaml is the one that is the
default but you can also use Json so if
you are comfortable working with Json
and my that is something that should be
you get comfortable with is you want to
be able to use that to name your files
and it's a frame of reference Json is a
logical way of being able to do value
paired matching using a JavaScript like
syntax
so you're going to be asked to how to
create a Docker container so let's go
through what that would look and we'll
break it down task by task so the task
is going to be create a MySQL Docker
container so to do that you want to be
able to build a Docker image or pull
from an existing Docker image from a
Docker repository or Hub and then you
want to be able to then use Docker to
create a new container which has my
sequel from the existing Docker image
simultaneously the layer of read write
file system is also created on top of
that image now below at the bottom of
the screen we have what the command
lines look for that so what is the
difference between a registry and a
repository so let's go through that so
for the docker registry and repository
for the registry we have Docker registry
Is An Open Source server-side service
used for hosting and distributing Docker
images whereas in contrast for
repositories collection of multiple
versions of a Docker image in a registry
a user can distinguish between Docker
images with their tag names and then
finally on the the registry Docker also
has its own default registry called
Docker hub for the repository it is a
collection of multiple versions of
Docker images it is stored in a Docker
registry and it has two types of public
and private registry so you can actually
create your own Enterprise registry so
you're going to be asked you know what
other Cloud platforms that support
Docker really you know lifts them all
and we have listed here Amazon web
services Microsoft Azure Google Cloud
Rackspace but you could add in their IBM
bluemix you could put in Red Hat really
any of the cloud service providers out
there today do support Docker it's just
become an industry standard so what is
the purpose of Expose and publish
commands in Docker so if we go through
expose is an instruction used in Docker
file whereas publish is used in Docker
run command for expose it is used to
expose ports within a Docker Network
whereas with Publishers can be used as
side of a Docker environment for expose
it is a documenting instruction used at
the time of building an image and
running a container whereas with
published is used as to map a host port
to a running container port for expose
is the command used in Docker whereas
for publish we use the command Dash p
for when we're doing our command line
used in Docker and examples of these are
exposed 8080 or with Docker we would put
in all for publish we would do the
example Docker run Dash Dash p and then
0.0.0.80 a colon 80 as our command line
so let's look at continuous monitoring
so with continuous monitoring how does
nagos help in continuous monitoring of
systems applications and service and so
this is really just a high level
question of using nagias within your
environment and you should be able to
just come back very quickly and say
negus allows you to help manage the
servers and check if they've been
sufficient utilized and if there are any
task failures that need to be addressed
and so there are three areas of
utilization and risk that you want to be
able to manage this is being able to
verify the status and services of the
entire network the health of your
infrastructure as a whole and if
applications are working properly
together with our web services and apis
that are reachable
so the second question you'll be asked
is how does negus help in continuous
monitoring of systems applications and
services so it's able to do this by
having the initial negative process and
scheduler and the additional plugins
that you would use for your network
connect with the remote resources and
the negative web interface to be able to
run status checks on a predefined
schedule so what do you mean by nagius
remote plugin executor or the mpre of
Nag years so mpre allows you to execute
plugins on links Unix machines that
allow you to do additional monitoring
and machine metrics such as disk usage
CPU load Etc what are the ports used by
nagios for monitoring purposes in this
example there are three and they're easy
to remember so I would memorize these
three but they're essentially ports five
six six five six six seven and five six
six eight so the there are two types of
checks within that year so you will be
asked for what is an active and passive
check in nagios so an active check and
is initiated by the nagios process and
is run on a regular schedule a passive
check is initiated and formed by an
external application or process so this
may be where a system is failing and
checks our results are submitted to
their years for processing and to
continue with this for what is an active
and passive check active checks are
initiated by the check logic within the
nagios Daemon now yes we'll execute a
plug-in and pass information about what
needs to be checked plugin will then
check the operational state of the host
or servers and then process the results
of the host or service check and send
out notifications in contrast with the
passive check it is an external
application that initiates the check it
writes the results of the check to an
external command line file and I guess
reads the external command file and
places the results of all passive checks
in a queue for later processing so you
can go back and revalidate and then
negos may send out notifications log
alerts Etc depending on the results that
they get from checking the information
so you're going to be asked to explain
the main configuration file and its
location in Nigeria so the main
configuration file consists of a number
of directives that affect how many years
operate so consider this as the
configuration file that it's read by
both now us processor and the cgis so
this will allow you to be able to manage
the main configuration file that's
placed into your settings directory so
what is the netios network analyzer and
again hold out your four fingers because
there are four options here so the nagis
network analyzer R1 provides an in-depth
look at all your network traffic source
and security threats two allows system
admins to gather high level information
on the healthier Network Three provides
a central view of your network traffic
and bandwidth dictator and then four
allows you to proactive in resolving
outages abnormal behavior and threats
before they affect critical business
processes so what are the benefits of
HTTP and SSL certificate monitorings
with nag years so with HTTP certificate
monitoring it allows you to have
increased server and services and
application availability obviously very
important fast detection of network
outages and protocol failures and allows
web transaction and web service
performance monitoring the SSL
certificate monitoring allows you for
increased website availability frequent
application availability and provides
increased security so explain
virtualization with nagios so in
response to this the first thing you
should be able to talk about is how 9
years itself can run on many different
virtualization platforms including
Microsoft Visual PC VMware Zen Amazon
ec2 et cetera Etc so just make sure you
get that right off the bad and I guess
it was able to provide capabilities
tomorrow and so much of metrics on
different platforms it allows for ensure
for quick detection of service and
application failures and has the ability
to be able to monitor against many
metrics including CPU usage memory
networking and VM status so name the
three variables that affect recursion
inheritance in nagias and it is name use
and register so name is a template name
that can be referenced in other object
definitions use specifies the name of
the template object that you want to
inherit its properties and variables
from and register indicates whether or
not the object definition should be
registered to nagios and on the right
hand side of the screen we have an
example of what that script would look
like again you may want to be able to
memorize this as it's something that you
can actually write down and show someone
if you're going through a technical
interview so why is nagyo said to be
object r oriented and fundamentally
comes down to the object configuration
format that you can use in your object
definitions it allows you to inherit
properties from other object definitions
and this is typical object-oriented
development and is now applied for the
nagios environment so some of the
objects that you can inherit are
Services host commands and time periods
so finally explain what is State talking
in nagios and so there are really four
options here when you're talking about
State stalking so State stocking is used
for login purposes in nagios it allows
you to enable for a particular host or
service that nagos will watch over very
carefully it will lock any changes it
sees in the output of the check results
and then finally helps the analysis of
log files well this was all about devops
Advanced full course hope you
first give it a thumbs up and share it
with your friends also if you have any
questions then please feel free to ask
away in the comment section our team
will reach out to you as soon as
possible thanks for watching and keep
learning staying ahead in your career
requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in Cutting Edge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign