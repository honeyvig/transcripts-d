foreign
of data science where extraordinary
becomes possible and unimaginable
becomes real in this captivating Journey
we delve into the foundations of data
science exploring its boundless
potential and unraveling the secrets
behind its immense power whether you are
a curious beginner or a seasoned
professional this data science Basics
course video will take you on an
exhilarating ride through the Realms of
data science machine learning and
artificial intelligence discover the
fundamental principles that underpin
data science as we demystify complex
algorithms and shed lights on the inner
workings of neural networks witness how
data science revolutionizes Industries
empowering business to make smarter
decisions saving lives and propelling US
towards a brighter future it means
yourself in the captivating world of
machine learning where data science
systems evolve adapt and continuously
enhance their Performance Based on data
ethical considerations that surround
data science from privacy and bias to
the responsible users of this powerful
Technologies ensuring a fair and
inclusive future join us on this
extraordinary Quest as we unlock the
secrets of data science empowering you
to harness its potential and shape the
World of Tomorrow get ready to embark on
this transformative Journey welcome to
the data science Basics course today we
will be learning foundations of data
science through the following docket
below
first we will unravel what exactly data
science is moving ahead we will discuss
important reasons on why data science is
a perfect carrier for aspiring data
scientists in 2023 now once clear with
these goals let's start mapping our
journey by discussing the tried tested
and result oriented career path to data
science later we shall guide you with
the perfect courses to begin your
Learning Journey with the critical
aspects discussed we will get started
with our data science Basics by learning
mathematics and statistics for data
science then we'll get started with the
true programming languages also known as
the two Titans of data science which are
none other than Python and r
and last but not least we'll conclude
the session by discussing the most
important data science algorithms with
that having said if you are someone who
is interested in building a career in
data science and ml by graduating from
the best universities or a professional
who elicits to switch careers with data
science by learning from the experts
then try giving a shot at the simply
learn Sports graduate program and data
science from Purdue University in
collaboration with IBM the link in the
description box should navigate to the
home page where you can find a complete
overview of the program being offered
take action upscale get ahead let's take
a minute to hear from our Learners who
have experienced massive success in
their careers and if these are the types
of videos you would like to watch then
hit that subscribe button and click on
the Bell icon to never miss an update
from us I'm soon going to enjoy it
during the time Trading
any data is the most important factor to
be successful there I plan to use the
professional certificate program in data
science to make that happen
hey I'm Ali handy I live in Iran and I'm
a Civic engineer professional I run my
own business by the name of ARG CMO
my business deals with providing
consultancy in the construction sector
and selling artificial stones and black
components I juggle a few jobs just the
same my business I do some trading and I
teach at the local University as a
part-time civil engineering instructor
though I'm a scholar and I have
published many articles on civil
engineering now when I look at them I
feel that if I knew data science at that
time many things would be different I
wanted to enhance my knowledge for which
I decided to take course insane I chose
Simple Run scores because of his
Association before the university I
doubt it's not just a good opportunity
to learn data science but also gain a
certification from a highly reputed
University
simpler and also provide again with some
complementary courses in Python and
agile which I think will be really
helpful in the Line run I plan to use
these learnings to expand my business
and make it profitable at University we
have to deal with a lot of projects that
include a lot of data I think this
course will help me solve those problems
more rationally it will help me build my
personal brand as a data scientist too
right now they spend my time in trading
about five hours a day in scalping mode
with my own strategy but soon I will
enter into trading algorithms which are
in dire need of data analysis I'm happy
that I'm Paving the road the success and
removing obstacles in my way by
upskilling myself
are you one of the many who dreams of
becoming a data scientist keep watching
this video if you're passionate about
data science because we will tell you
how does it really work under the hood
Emma is a data scientist let's see how a
day in a life goes while she's working
on data science project well it is very
important to understand the business
problem first in our meeting with the
clients Emma asks relevant questions
understands and defines objectives for
the problem that needs to be tackled she
is a curious Soul who asks a lot of eyes
one of the many traits of a good data
scientist now she gears up for data
acquisition to gather and scrape data
from multiple sources like web servers
logs databases apis and online
repositories oh it seems like finding
the right data takes both time and
effort after the data is gathered comes
data preparation this step involves data
cleaning and data transformation data
cleaning is the most time consuming
process as it involves handling many
complex scenarios here Mr deals with
inconsistent data types misspelled
attributes missing values duplicate
values and whatnot then in data
transformation she modifies the data
based on defined mapping rules in a
project
ETL tools like talent and Informatica
are used to perform complex
Transformations that helps the team to
understand the data structure better
then understanding what you actually can
do with your data is very crucial for
that Emma does exploratory data analysis
with the help of Eda she defines and
refines the selection of feature
variables that will be used in the model
development but what if Emma skips this
step she might end up choosing the wrong
way tables which will produce an
inaccurate model thus exploratory data
analysis becomes the most important step
now she proceeds to the core activity of
a data science project which is data
modeling she repetitively applies Force
machine learning techniques like k n
decision tree knife base to the data to
identify the model that best fits the
business requirement she trains the
models on the training data set and test
them to select the best performing model
Emma prefers python for modeling the
data however it can also be done using R
and SAS well the trickiest part is not
yet over visualization and communication
Emma meets the clients again to
communicate the business findings in a
simple and effective manner to convince
the stakeholders she uses tools like
Tableau power bi and click view that can
help her in creating powerful reports
and dashboards and then finally she
deploys and maintains the model she
tests the selected model in a
pre-production environment before
deploying it in the production
environment which is the best practice
right after successfully deploying it
she uses reports and dashboards to get
real-time analytics further she also
monitors and maintains the Project's
performance well that's how Emma
completes the data science project we
have seen the daily routine of a data
scientist is a whole lot of fun has a
lot of interesting aspects and comes
with its own share of challenges now
let's see how data science is changing
the world data science techniques along
with genomic data provides a deeper
understanding of genetic issues in
reaction to particular drugs and
diseases logistic companies like DHL
FedEx have discovered the best routes to
ship the best suited time to deliver the
best mode of transport to choose thus
leading to cost efficiency with data
science it is possible to not only
predict employee attrition but to also
understand the key variables that
influence employee turnover also the
airline companies can now easily predict
flight delay and notify the passengers
beforehand to enhance their travel
experience well if you're wondering
there are various roles offered to a
data science like data analyst machine
learning engineer deep learning engineer
data engineer and of course data
scientist the median based salaries of a
data scientist can range from 95 000 to
165 000 so that was about the data
science are you ready to be a data
scientist if yes then start today the
world of data needs you technological
advancement has offered us seamless
connectivity and the advantage of
smartphones and connected devices
worldwide has elevated the gravitas at
which the data is generated every minute
zettabytes of data endless data lakes
and server Farms billion dollar
Investments stakeholders CMOS ctOS and
CEOs everyone is dependent on one
detailed report that data scientist
delivers a wise decision backed by data
research and Analysis can yield
prospering results still on the contrary
one wrong marketing decision one
misinterpreter detail could be a good
recipe for a catastrophic financial
disaster for an organization that's a
pivotal role a data scientist plays in
any organization a data scientist
caliber to device algorithm to crunch
numbers and extract precious information
is invaluable this is one good reason
why data science is reported as a top ID
profession in the current IT industry
and is projected to be on the top for a
good time welcome to Simply lens YouTube
channel today we will explain why you
should learn data science before we get
started take a moment to subscribe to us
and hit the Bell to get notified of our
content if you are watching this I am
sure you are interested in data science
looking for a flexible and online
training program that better suits you
like a glove and helps you get ahead in
your career well your search and save
simply learn is offering an online data
science bootcamp with Caltech in
collaboration with IBM here you will
mentored by real-time industry
professionals aligning your learnings to
the current industry standards to get a
better chance of planning your dream job
the course covers the most sought after
technical skills like exploratory data
analysis descriptive statistics
inferential statistics model building
and fine tuning supervised and
unsupervised learning and Sample
learning deep learning data visual
realization and the course will help you
get Hands-On with important tools like
numpy pandas python R and sci-fi what
are you waiting for hurry up for more
details check the link in the
description box below IIT firms are
venturing into data science recognizing
its potential to open the doors to wide
spectrum of opportunities it enables
Enterprises to Monitor and Report
performance metrics to facilitate High
turned and reinforced decision making
organizations can now analyze Trends to
make critical decisions to engage
clients better enhance company
performance boost productivity and
increase profitability the
organizational importance of data
science is increasing at a Relentless
Pace the global data science platforms
Market is poised to grow at a cagr of
16.43 percent more than any other
industry converging to it data science
facilitates it with its power to extract
information from larger volumes of data
information technology makes our life
easier by gathering and processing more
data quickly and efficiently to provide
results in minutes and hours unlike days
and weeks statistics and science have
become the answer to complex data data
science enables stepping and analyzing
massive amounts of untapped data from
many sources to leverage productivity
and profitability this is one good
reason why many Tech experiments find
data science to be the most interesting
job profiling apart from the it domain
data science has spread its roots into
other fields for instance the healthcare
sector is more inclined towards data
science and artificial intelligence to
diagnose and treat the most complicated
syndromes and robots to perform the most
tedious surgeries online stores and
shopping websites depends solely on data
science to manage the supply chain and
enhance the user experience by
recommending the right product data
science plays a pivotal role in
transportation and Logistics it provides
customized recommendations to best suit
customers comfort and manages unexpected
circumstances using Predictive Analytics
geographical experts weather reports and
many more employee data science to
predict seismic events Reservoir
Behavior climate interpretation voter
boys to enable navigation and more
construction companies are integrating
their process with data science to
enhance bloating constructing and
automating tasks to build better faster
and bigger projects in the most
efficient way possible in the
ever-evolving field of data science new
advancements and discoveries in research
accelerate rapidly you always get to
learn something new making every day's
work exciting new data science skills
can be acquired endlessly giving your
Competitive Edge with knowledge and
expertise the salary of a data
scientists in India ranges from 15 to 25
lakhs per annum and in the United States
of America the salaries range from 150
000 to 250 000 per annum depicting the
future of data science results in an
unmatched potential to settle in a
secured job role as the field expense
more jobs should become available as
more data scientists are needed for
analysis individuals wishing to pursue a
career in data science can look forward
to bright future with data science data
science has a huge scope across all
Industries the global market food data
science is expected to grow all the way
up to 230 30 billion dollars by early
2026 opening the gates for about 11
million data scientists job roles
another important aspect of the future
of data science is artificial
intelligence AI will likely to be
knowledging data scientists will have to
deal with in the future to put it
another way the future of data science
will align itself to make it better in
the long run to become a data scientist
the Professionals in the organizations
need to have healthy data literacy
powerful programming skills mathematics
statistics logic building skills strong
foundations in data cleaning and a
comfortable understanding of data
visualization and machine learning
algorithms handling all these are
nothing less than herding cats one scale
Gap would be the final nail in the
coffin if you are an aspiring data
scientist looking for online training
and certifications from prestigious
universities and in collaboration with
leading experts then search no more just
crap simply learns postgraduate program
in data science from Purdue University
in collaboration with IBM that should be
your right choice for more details use
the link given in the description box
below Google had gathered five exam
bytes of data between the beginning of
time 2003. this amount of data started
to be produced every two days in 2010
and every 40 minutes by 2021 the
responsibility of a data scientist is to
gather clean and present data and have a
keen business sense and analytical
abilities let us have a discussion about
it in the upcoming slides how can you
become a data scientist as a beginner I
guarantee you after watching this video
you will have a clear understanding on
how to drive your career as a data
scientist hey guys welcome to Simply
learn before proceeding please make sure
you subscribe to Simply learns YouTube
channel and press the Bell icon to never
miss any updates today we are going to
cover significance of data scientists in
Industries after that prerequisites and
Technologies required for a data
scientist and finally salary offer data
scientist I have a query for you which
technology is used by Google Maps to
predict traffic jams deep learning
machine learning natural language
processing data structure please leave
the answer in the comment section below
and stay tuned to get the answer
significance now now we will see how top
industries are involved in the field of
data science by 2025 the data science
Industry is anticipated to grow to a
value of 16 billion dollars there is an
abundance of data science jobs all over
the world now let's list out crucial
areas where data science is used media
and entertainment the major player in
the media and entertainment sector such
as YouTube Netflix hot star Etc have
begun to use data science to better
understand their audience and provide
them with recommendations that are both
relevant and personalized e-commerce
data science has aided retail companies
in better meeting their expectations as
they bring a unique combination of deep
data knowledge technology skills and
statistical experience data scientists
are in high demand in the retail
industry top recruiters are Amazon
Flipkart Walmart myntra Etc digital
marketing large volumes of data are
currently being fetched from its users
through search Pages social networks
online traffic display networks movies
web pages Etc a high level of business
intelligence is needed to analyze such
large amount of data and this can only
be done with the proper use of data
science approaches top recruiters are
Amazon Flipkart Facebook Google Etc
cyber security data science and AI are
now being used by the cyber security
industry to prevent the growing usage of
algorithms for harmful purposes top
recruiters includes IBM Microsoft
Accenture Cisco and many more before
moving forward what is the response to
the Google Map question that I asked
answer is machine learning
coming to prerequisites now that when no
significance of data science in
Industries let us explore the
prerequisites and Technologies required
for a data scientist seeing the demand
of data scientists in every industry it
is obvious that the scope of a data
scientist is very high so how to start
there is no necessity that you should be
knowing any technology or programming
language you can be a Layman too data
scientists typically have a variety of
educational and professional experiences
most should be proficient in four
crucial areas important skill is
mathematical expertise three concepts
like linear algebra multivariable
calculus and optimization technique are
crucial because they aid in our
understanding of numerous machine
learning algorithms that are crucial to
data science
similar to that knowing statistics is
crucial because they are used in data
analysis additionally important to
statistics probability is regarded as a
must of for mastering machine learning
next is computer science in the field of
computer science there is a lot to learn
but one of the key inquiries that arises
in relation to programming is r or
Python language there are many factors
to consider when deciding which language
to choose for data science because both
have a comprehensive collection of
libraries to implement complex machine
learning algorithms in addition to
learning a programming language you
should learn the following computer
science skill fundamentals of algorithm
and data structures distributed
computing machine learning deep learning
Linux SQL mongodb Etc domain expertise
most individuals wrongly believe that
domain expertise is not crucial to data
science yet it is consider the following
scenario if you are interested in
working as a data scientist dentist in
the banking Industries and you already
know a lot about it for instance you are
knowledgeable about stock trading
Finance Etc this will be very
advantageous for you and the bank itself
will favor you over other applicants and
finally communication skill it covers
both spoken and written Communication in
a data science project the project must
be explained to others when finding from
the analysis have been reached this can
occasionally be a report that you
provide to your team or employer at work
sometimes it might be a blog entry it is
frequently a presentation to a group of
co-workers regardless a data science
project always involves some form of
communication of the Project's findings
therefore having a good communication
skill is a requirement for being a data
scientist apart from all this practicing
is very important keep using different
tools also start reading blogs on data
science start building projects on data
science which can be added to your
resume also you can find many
interesting courses on data science by
simply learn salary reward is the result
of good work now we shall discuss
salaries that a data scientist will get
it should come as no surprise that data
scientists may add significantly to a
business every step of the process from
data processing to data cleansing
requires persistence a lot of arithmetic
and statistics as well as scattering of
engineering skills one of the most
important factors in a data scientist
salary is experience at the beginner
level a data scientist can make 95 000
annual the typical annual compensation
for a mid-level data scientist is
between 130 000 and 195 000 a seasoned
data scientist typically earns between
165 000 and 250 000 per year
in India at the beginner level a data
scientist can make 9 lakh forty thousand
rupees on average per year at mid level
data scientists will get 20 lakhs rupees
per annum and if you are at the advanced
level you will get paid an average of
rupees 25 lakhs annually this salary
will vary in different countries the top
hiring businesses in the US that provide
the highest salaries for data scientists
are apple with 180 000 per annum next is
Twitter with 170 dollars per annum meta
technology
170 dollars annually
LinkedIn 160 000 per annum crypto
technology provides 17 lakh 50 000 per
annum IBM provides 14 lakhs per annum
and Accenture will provide you with 19
lakhs per annum and finally American
Express will provide on average of 13
lakhs per annum no matter if you are
recently graduated have experience in
the it field or are seeking a career
change these courses in data science
will provide you with the necessary
Foundation to pursue a successful career
in data science obtaining an education
in data science can open doors to
various job opportunities such as data
analyst data engineer data architect or
data scientist your specific path in
data science will depend on your skills
and interests however all these career
paths require a certain envelope
Proficiency in programming data
visualization statistics and machine
learning data engineers and data
Architects primarily work with coding
databases and intricate queries while
data analysts and data scientists focus
on analyzing collecting and interpreting
large data sets to Aid in making
informed business decisions it is a
rapidly expanding and financially
rewarding career path with data
scientists reporting an average annual
salary of 130 000 in the United States
and in India it is 12 lakhs per annum in
in this simply land video on top courses
for data science we will explore some of
the top courses in data science
including what on Purdue IIT kanpur
Caltech and spjmir these courses are at
the Forefront of data Science Education
and Research their programs are designed
to equip students with the skills and
knowledge needed to succeed in this
exciting and rapidly evolving field but
before moving ahead do subscribe to our
YouTube channel and hit the Bell icon to
get all the updates from Simply learn so
let's get started first one on the list
is Caltech the California Institute of
Technology known as Caltech is a private
research University in Pasadena
California the University's division of
engineering and applied sciences offer
data science courses these data science
course curriculum emphasizes Hands-On
learning and encourage students to work
on projects that have real world
applications that's why simply learn has
collaborated with Caltech to create a
data science course that leverages the
superiority of caltex academic eminence
this data science course course critical
data science topics like Python
programming are programming machine
learning deep learning and data
visualization tools through an
Interactive Learning model with live
sessions by global practitioners and
practical labs this course also provides
a program certificate with up to 14 CU
credits from Caltech ctme and a course
completion certificate from caltex atme
you will also earn membership to the
Caltech ctme Circle and an online
convocation by the Caltech program
director and last but not least thanks
to an experience with tools like numpy
pandas are Python and sci-fi with 25
plus industrial relevant projects from
the likes of Walmart Amazon Uber
Mercedes-Benz and many more interested
in this course then do check out the
link to this course in the description
the second one on the list is Purdue
University Purdue is a public research
University located in West Lafayette
Indiana the University's Department of
computer science offers a data science
program covering data mining machine
learning and statistical modeling well
with simple and sports graduate program
in data science in partnership with
Purdue University and in collaboration
with IPM you can have the opportunity to
take the advantage of master classes by
Purdue faculty and IBM experts exclusive
hackathons and ask me anything sessions
by IBM this program in data science is
perfect for working professionals and it
focuses on essential job skills like R
and Python Programming machine learning
NLP and data visualization with Tableau
to prepare for the job market you will
learn all this through live sessions
with experts Hands-On Labs IBM
hackathons and corporate projects this
course will also provide you with the
data sensor you get from Purdue the
eight most Innovative University in the
US along with IBM certificates from IBM
courses with this you will also get a
chance to build your resume and prepare
your interviews with valuable insights
from industry experts and 25 plus
industry relevant projects from Amazon
Walmart Uber and Mercedes-Benz you can
find the link to this course in the
description box and the third one
is a global technology company that has
been a leader in the data science field
for decades in addition to offering a
range of data science Solutions and
tools Horton also provides training and
education in data science this AI
program in collaboration with vote on
impacts in-depth knowledge of Big Data
Ai and machine learning and how to apply
them across the three key business
functions marketing finance and people
management this program also covers the
risks and ethics of using Ai and how you
can design governance Frameworks for
Effective AI implementation apart from
this you will gain a certificate from
work and online the Wharton schools
digital learning platform and you will
learn how AI can be applied across
business functions such as marketing
Finance HR and you will also be having
live master classes from Top world and
faculty and get insights on how AI is
utilized across companies like
McDonald's visa and City Group this
program Ram is designed to equip
students with the skills and knowledge
needed to succeed in this fast growing
field and become a data scientist by
diving deep into the nuances of data
interpretation mastering Technologies
like machine learning and mastering
powerful programming skills to take your
career in data science to the next level
also the link for this course is
mentioned in the description box
following that we have IIT kanpur the
Indian Institute of Technology kanpur is
one of the top engineering schools in
India known for its strong focus on
research and Innovation the institute's
department of computer science and
engineering is one of the most popular
universities offering data science
courses taking advantage of search
resourceful alumni simply then provides
a professional certificate course in
data science in collaboration with IIT
kanpur mastering essential data science
skills through applied learning with
live classes by industry experts
asynchronous videos Hands-On labs and
master classes from distinguished IIT
kanpur faculty along with that you will
get hands-on experience on 25 plus
industry relevant projects from the
likes of Walmart Amazon Uber
Mercedes-Benz and many demo along with
career mentorship and profile building
sessions with valuable insights from
industry experts this course is designed
with a carefully curated curriculum that
includes critical topics like python
machine learning NLP and data
visualization delivered using Hands-On
labs and Industry relevant projects to
help you advance your career trajectory
this data science course in India aims
to domestify data science the link is
mentioned in the description box and the
last one is sp jmir spjn Institute of
Management and research is a leading
business schools located in Mumbai India
the institutes business intelligence and
analytics program is designed to provide
students with a strong foundation in
data science analytics and business
intelligence therefore simply learn has
collaborated with SPG Mir to design the
data science for business decisions
program that will help you learn this in
demand skill set and turn yourself into
a data-driven decision maker to optimize
your organization's performance this
program enables you to master the
concepts of machine learning statistics
data visualization Etc and create a
strategy to implement a data science
project in your organization this
program will help you drive digital
transformation in your organization by
understanding data privacy data
architecture and governance you will
also earn a program completion
certification from svjmaya a leading
School of Management in India and master
the concepts of data science through
live sessions delivered by SP Jamir
faculty you will also beg an opportunity
to Avail of spga my executive Alumni
network membership post program
completion Hanson experience on multiple
projects and a Capstone pertaining to
various industry domains the link is
mentioned Below in the description let's
begin this lesson by defining the term
statistics statistics is a mathematical
science pertaining to the collection
presentation analysis and interpretation
of data it's widely used to understand
the complex problems of the real world
and simplify them to make well-informed
decisions several statistical principles
functions and algorithms can be used to
analyze primary data build a statistical
model and predict the outcomes
an analysis of any situation can be done
in two ways statistical analysis or a
non-statistical analysis
statistical analysis is the science of
collecting exploring and presenting
large amounts of data to identify the
patterns and Trends statistical analysis
is also called quantitative analysis
non-statistical analysis provides
generic information and includes text
sound still images and moving images
non-statistical analysis is also called
qualitative analysis
although both forms of analysis provide
results statistical analysis gives more
insight and a clearer picture a feature
that makes it vital for businesses
there are two major categories of
Statistics descriptive statistics and
inferential statistics
descriptive statistics helps organize
data and focuses on the main
characteristics of the data it provides
a summary of the data numerically or
graphically numerical measures such as
average mode standard deviation or SD
and correlation are used to describe the
features of a data set
suppose you want to study the height of
students in a classroom in the
descriptive statistics you would record
the height of every person in the
classroom and then find out the maximum
height minimum height and average height
of the population
inferential statistics generalizes the
larger data set and applies probability
Theory to draw a conclusion it allows
you to infer population parameters based
on the sample statistics and to model
relationships within the data modeling
allows you to develop mathematical
equations which describe the inner
relationships between two or more
variables consider the same example of
calculating the height of students in
the classroom in inferential statistics
you would categorize height as tall
medium and small and then take only a
small sample from the population to
study the height of students in the
classroom
the field of Statistics touches our
lives in many ways from the daily
routines in our homes to the business of
making the greatest cities run the
effect of Statistics are everywhere
there are various statistical terms that
one should be aware of while dealing
with statistics
population sample variable quantitative
variable qualitative variable discrete
variable continuous variable
a population is the group from which
data is to be collected
a sample is a subset of a population
a variable is a feature that is
characteristic of any member of the
population differing in quality or
quantity from another member
a variable differing in quantity is
called a quantitative variable for
example the weight of a person number of
people in a car
a variable differing in quality is
called a qualitative variable or
attribute for example color the degree
of damage of a car in an accident
a discrete variable is one which no
value can be assumed between the two
given values for example the number of
children in a family
a continuous variable is one in which
any value can be assumed between the two
given values for example the time taken
for a 100 meter run
typically there are four types of
statistical measures used to describe
the data they are measures of frequency
measures of central tendency measures of
spread measures of position
let's learn each in detail
frequency of the data indicates the
number of times a particular data value
occurs in the given data set
the measures of frequency are number and
percentage
central tendency indicates whether the
data values tend to accumulate in the
middle of the distribution or toward the
end
the measures of central tendency are
mean median and mode
spread describes how similar or varied
the set of observed values are for a
particular variable
the measures of spread are standard
deviation variance and quartiles the
measure of spread are also called
measures of dispersion
position identifies the exact location
of a particular data value in the given
data set
the measures of position are percentiles
quartiles and standard scores
statistical analysis system or SAS
provides a list of procedures to perform
descriptive statistics
they are as follows
proc print proc contents proc means proc
frequency proc univariate proc G chart
proc box plot proc G plot
proc print it prints all the variables
in a SAS data set
proc contents it describes the structure
of a data set
proc means it provides data
summarization tools to compute
descriptive statistics for variables
across all observations and within the
groups of observations
proc frequency it produces one way to
in-way frequency and cross-tabulation
tables frequencies can also be an output
of a SAS data set
proc univariate it goes beyond what proc
means does and is useful in conducting
some basic statistical analyzes and
includes high resolution graphical
features
proc G chart the g-chart procedure
produces six types of charts block
charts horizontal vertical bar charts pi
and donut charts and star charts
these charts graphically represent the
value of a statistic calculated for one
or more variables in an input SAS data
set
the trig variables can be either numeric
or character
proc box plot the box plot procedure
creates side-by-side box and whisker
plots of measurements organized in
groups
a box and whisker plot displays the mean
quartiles and minimum and maximum
observations for a group
proc G plot G plot procedure creates
two-dimensional graphs including simple
Scatter Plots overlay plots in which
multiple sets of data points are
displayed on one set of axes plots
against the second vertical axis bubble
plots and logarithmic plots
in this demo you'll learn how to use
descriptive statistics to analyze the
mean from the electronic data set
let's import the electronic data set
into the SAS console
in the left plane right-click the
electronic.xlsx dataset and click import
data
the code to import the data generates
automatically copy the code and paste it
in the new window
the proc means procedure is used to
analyze the mean of the imported data
set
the keyword data identifies the input
data set in this demo the input data set
is electronic
the output obtained is shown on the
screen
note that the number of observations
mean standard deviation and maximum and
minimum values of the electronic data
set are obtained
this concludes the demo on how to use
descriptive statistics to analyze the
mean from the electronic data set so far
you've learned about descriptive
statistics let's now learn about
inferential statistics
hypothesis testing is an inferential
statistical technique to determine
whether there is enough evidence in a
data sample to infer that a certain
condition holds true for the entire
population
to understand the characteristics of the
general population we take a random
sample and analyze the properties of the
sample we then test whether or not the
identified conclusions correctly
represent the population as a whole
the population of hypothesis testing is
to choose between two competing
hypotheses about the value of a
population parameter
for example one hypothesis might claim
that the wages of men and women are
equal while the other might claim that
women make more than men
hypothesis testing is formulated in
terms of two hypotheses
null hypothesis which is referred to as
H null alternative hypothesis which is
referred to as H1
the null hypothesis is assumed to be
true unless there is strong evidence to
the contrary
the alternative hypothesis assumed to be
true when the null hypothesis is proven
false
let's understand the null hypothesis and
alternative hypothesis using a general
example
null hypothesis attempts to show that no
variation exists between variables and
alternative hypothesis is any hypothesis
other than the null
for example say a pharmaceutical company
has introduced a medicine in the market
for a particular disease and people have
been using it for a considerable period
of time and it's generally considered
safe
if the medicine is proved to be safe
then it is referred to as null
hypothesis
to reject null hypothesis we should
prove that the medicine is unsafe if the
null hypothesis is rejected then the
alternative hypothesis is used
before you perform any statistical tests
with variables it's significant to
recognize the nature of the variables
involved based on the nature of the
variables it's classified into four
types
they are categorical or nominal
variables ordinal variables interval
variables and ratio variables
nominal variables are ones which have
two or more categories and it's
impossible to order the values examples
of nominal variables include gender and
blood group
ordinal variables have values ordered
logically however the relative distance
between two data values is not clear
examples of ordinal variables include
considering the size of a coffee cup
large medium and small and considering
the ratings of a product bad good and
best
interval variables are similar to
ordinal variables except that the values
are measured in a way where their
differences are meaningful
with an interval scale equal differences
between scale values do have equal
quantitative meaning
for this reason an interval scale
provides more quantitative information
than the ordinal scale
the interval scale does not have a true
zero point a true zero point means that
a value of 0 on the scale represents
zero quantity of the construct being
assessed
examples of interval variables include
the Fahrenheit scale used to measure
temperature and distance between two
compartments in a train
ratio scales are similar to interval
scales and that equal differences
between scale values have equal
quantitative meaning
however ratio scales also have a true
zero point which give them an additional
property for example the system of
inches used with a common ruler is an
example of a ratio scale
there is a true zero point because zero
inches does in fact indicate a complete
absence of Link
in this demo you'll learn how to perform
the hypothesis testing using SAS
in this example let's check against the
length of certain observations from a
random sample
the keyword data identifies the input
data set
the input statement is used to declare
the Aging variable and cards to read
data into SAS
let's perform a t-test to check the null
hypothesis
let's assume that the null hypothesis to
be that the mean days to deliver a
product is six days
so null hypothesis equals six Alpha
value is the probability of making an
error which is five percent standard and
hence Alpha equals 0.05
the variable statement names the
variable to be used in the analysis
the output is shown on the screen
note that the p-value is greater than
the alpha value which is 0.05 therefore
we fail to reject the null hypothesis
this concludes the demo on how to
perform the hypothesis testing using SAS
let's now learn about hypothesis testing
procedures there are two types of
hypothesis testing procedures they are
parametric tests and non-parametric
tests
in statistical inference or hypothesis
testing the traditional tests such as
t-test and Anova are called parametric
tests they depend on the specification
of a probability distribution except for
a set of free parameters
in simple words you can say that if the
population information is known
completely by its parameter then it is
called a parametric test
if the population or parameter
information is not known and you are
still required to test the hypothesis of
the population then it's called a
non-parametric test
non-parametric tests do not require any
strict distributional assumptions there
are various parametric tests they are as
follows t-test Anova
chi-squared linear regression let's
understand them in detail
t-test
a t-test determines if two sets of data
are significantly different from each
other
the t-test is used in the following
situations
to test if the mean is significantly
different than a hypothesized value
to test if the mean for two independent
groups is significantly different
to test if the mean for two dependent or
paired groups is significantly different
for example
let's say you have to find out which
region spends the highest amount of
money on shopping
it's impractical to ask everyone in the
different regions about their shopping
expenditure
in this case you can calculate the
highest shopping expenditure by
collecting sample observations from each
region
with the help of the t-test you can
check if the difference between the
regions are significant or a statistical
fluke
Anova
Anova is a generalized version of the
t-test and used when the mean of the
interval dependent variable is different
to the categorical independent variable
when we want to check variance between
two or more groups we apply the Anova
test
for example let's look at the same
example of the t-test example now you
want to check how much people in various
regions spend every month on shopping in
this case there are four groups namely
East West North and South with the help
of the Anova test you can check if the
difference between the regions is
significant or a statistical fluke
chai square
chi-square is a statistical test used to
compare observed data with data you
would expect to obtain according to a
specific hypothesis
let's understand the chi-square test
through an example
you have a data set of mail Shoppers and
female Shoppers let's say you need to
assess whether the probability of
females purchasing items of 500 or more
is significantly different from the
probability of males purchasing items of
500 or more
linear regression
there are two types of linear regression
simple linear regression and multiple
linear regression
simple linear regression is used when
one wants to test how well a variable
predicts another variable
multiple linear regression allows one to
test how well multiple variables or
independent variables predict a variable
of interest
when using multiple linear regression We
additionally assume the predictor
variables are independent
for example finding relationship between
any two variables say sales and profit
is called Simple linear regression
finding relationship between any three
variables say sales cost telemarketing
is called multiple linear regression
some of the non-parametric tests are
wilcoxan rank sum test and kresco
Wallace h-test
will coxin rank some test the wilcoxan
signed rank test is a non-parametric
statistical hypothesis test used to
compare two related samples or matched
samples to assess whether or not their
population mean ranks differ in wilcoxon
rank some test you can test the null
hypothesis on the basis of the ranks of
the observations
kresco Wallace h test
Wallace h-test is a rank-based
non-parametric test used to compare
independent samples of equal or
different sample sizes in this test you
can test the null hypothesis on the
basis of the ranks of the independent
samples hello everyone welcome to this
video on R4 data science full course
today we have our experience instructors
Richard Ajay and Pete Ferrari who will
help us learn everything from Basics to
Advanced and master data science in r
we will start by learning data science
from an animated video you will then
learn the essential Concepts in data
science and understand the important
packages in R for data science such as
deployer and tidier
you will look at some of the widely used
data science algorithms such as linear
regression logistic regression decision
trees random forests including time
series analysis finally you will get an
idea about the salary structure skills
jobs and resume of a data scientist
are you one of the many who dreams of
becoming a data scientist keep watching
this video if you're passionate about
data science because we will tell you
how does it really work under the hood
Emma is a data scientist let's see how a
day in a life goes while she's working
on data science project well it is very
important to understand the business
problem first in our meeting with the
clients Emma asks relevant questions
understands and defines objectives for
the problem that needs to be tackled she
is a curious Soul who asks a lot of eyes
one of the many traits of a good data
scientist now she gears up for data
acquisition to gather and scrape data
from multiple sources like web servers
logs databases apis and online
repositories oh it seems like finding
the right data takes both time and
effort after the data is gathered comes
data preparation this step involves data
cleaning and data transformation data
data close time consuming process as it
involves handling many complex scenarios
here Mr deals with inconsistent data
types misspelled attributes missing
values duplicate values and what not
then in data transformation she modifies
the data based on defined mapping rules
in a project
ETL tools like talent and Informatica
are used to perform complex
Transformations that helps the team to
understand the data structure better
then understanding what you actually can
do with your data is very crucial for
that Mr does exploratory data analysis
with the help of Eda she defines and
refines the selection of feature
variables that will be used in the model
development but what if Emma skips this
step she might end up choosing the wrong
way tables which will produce an
inaccurate model thus exploratory data
analysis becomes the most important step
now she proceeds to the core activity of
a data science project which is data
modeling she repetitively applies
diverse machine learning techniques like
k n decision tree knife base to the data
to identify the model that best fits the
business requirement she trains the
models on the training data set and test
them to select the best performing model
Emma prefers python for modeling the
data however it can also be done using R
and SAS well the trickiest part is not
yet over visualization and communication
Emma meets the clients again to
communicate the business findings in a
simple and effective manner to convince
the stakeholders she uses tools like
Tableau power bi and click view that can
help her in creating powerful reports
and dashboards and then finally she
deploys and maintains the model she
tests the selected model in a
pre-production environment before
deploying it in the production
environment which is the best practice
right after successfully deploying it
she uses reports and dashboards to get
real-time analytics further she also
monitors and maintains the Project's
performance well that's how Emma
completes the data science project we
have seen the daily routine of a data
scientist is a whole lot of fun has a
lot of interesting aspects and comes
with its own share of challenges now
let's see how data science is changing
the world data science techniques along
with genomic data provides a deeper
understanding of genetic issues in
reaction to particular drugs and
diseases logistic companies like DHL
FedEx have discovered the best routes to
ship the best suited time to deliver the
best mode of transport to choose thus
leading to cost efficiency with data
science it is possible to not only
predict employee attrition but to also
understand the key variables that
influence employee turnover also the
airline companies can now easily predict
flight delay and notify the passengers
beforehand to enhance their travel
experience well if you're wondering
there are various roles offered to a
data scientist like data analyst machine
learning engineer deep learning engineer
data engineer and of course data
scientist the median base salaries of a
data scientist can range from ninety
five thousand dollars to 165 000 so that
was about the data science are you ready
to be a data scientist if yes then start
today the world of data needs you that's
all from my side today thank you for
watching comment below the next topic
that you want to learn and subscribe to
Simply learn to get the latest updates
on more such interesting videos data
science with r sponsored by simply learn
that's
www.simplylearn.com get certified get
ahead my name is Richard kirschner and
I'm with the simply learned team what's
in it for you we're going to go through
an introduction to our yr cran
comprehensive R archive Network and
cover installing R then we'll get into
simple linear regression using our line
of best fit using error summation
correlation analysis in R and then we'll
get into classification using R use case
predict the class of a flower let's
start with an introduction to R first
it's an open source R is completely free
and open source with active community
members extensible it offers various
statistical and graphical techniques
compatible R is compatible across all
platforms Linux Windows and Mac Library
R has an extensive library of packages
for machine Learning Easy integration it
can be easily integrated with popular
softwares like Tableau SQL Server Etc
this only touches a little bit on these
different aspects of R the fact that
it's a free it's very extensive it
probably has one of the most extensive
set of data analysis packages currently
out it has a compatibility is
continually growing so it's integrated
with everything from cluster Computing
to python integration which is now
coming out and its extensive libraries
allows you to import the different
libraries to use for whatever your needs
are makes it a very diverse and easy to
use coding source for analyzing data R
is more than just a programming language
as I just touched upon it it has a
worldwide repository system
comprehensive R archive Network and it
can be accessed at https colon slash
slash cran dot
rproject.org it provides up-to-date
versions of code and documentation for R
cran hosts around 10 000 packages of r
that is a huge repository focused on
just data analytics let's install R to
get started you can easily download the
executable file for R and install it
from cran website if you go to under the
downloads in this case since I'm on a
Windows machine we'll walk through it
from the r 3.5 version for Windows so
here we are at https colon slash cran
dot R Dash project dot org and you can
see down here we have the different
options for the downloads if we go under
Windows which is I'm on a Windows
machine if you're in a Mac machine you
do the Mac or Linux there's install R
for the first time just click on that
and open and run it the installation is
pretty simple follow the default options
to finish the installation after the
installation is complete you'll see the
r icon on your desktop alternatively
there are mirror sites and so we can go
into our studio this to take a quick
look at that if you're on the r Studio
website that's
www.rstudio.com you can go down under
products and rstudio download you'll see
a number of options here the first one
is the rstudio desktop open source
license that's the same thing you just
downloaded including Ubuntu you can
install R using as regular package
management tools and when you're using a
Linux system that is preferred because
then it registers it properly on the
setup system let's go ahead and open up
our R studio and take a look here we are
and I've opened this up in the r studio
version which automatically opens up
some extra windows which is nice let's
go and take a look at that we have our
console on the left this is your main
workspace so if I go in here and I do
click the mouse and I do a four plus
four I'll come up and say the answer is
eight and you have some environmental
information and over on the right you
have plots usually when you're working
in here let's do a script we'll go up
here into the plus sign in the upper
left hand corner and just add some
script in here in this case it showed up
on the top but you can move these
windows around and let's say I do y
equals three plus four x equals y plus 2
and then I'm going to do just X and
let's do a plot and throw a plot in
there c c is a notation that these are
going to be Cartesian points so we got 1
comma two comma three I'd be like your X
and then y 3 comma four comma 5 for just
a standard call scatter plot we don't
have to memorize this we'll go into some
of this later on and then I can take all
of this if I go under code since I'm
working in the code console and I go
down to run region and just run all it
takes this code and just runs it through
my console and you can see down here
it's executed y equals three plus four x
equals X plus 2 and if you add 3 plus 4
plus 2 you get 9 that's what the X does
and then I did it plot I threw the plot
in there I'm plotting one two three
three four five if you use just a
straight plot it's a scatter plot and
you can see that this appears on the
bottom right where you have your plots
coming in so it's a very quick way to
show data that's one of the wonderful
things about R is it's very easy and
quick to go through and do different
functions on the data and analyze it so
it's a very popular package before you
start programming an R you should
install packages and its dependencies
packages provide pre-assembled
collections of functions and objects
each package is hosted on the cran
repository not all packages are loaded
by default but they can be installed on
demand remember earlier we're talking
about all the different packages
available you don't want to install
everything from R it would just be a
huge waste of space you want to just
install those packages you need so to
install packages in our studio you go
under Tools and install packages when
you click on the install packages you'll
get a dialog box you'll see where it has
a repository current because there's
other repositories and you can even
download and install your own packages
you can build and then you'll pick out
the packages you want and separate
multiple with spaces or commas so you
can install numerous packages at the the
same time in this case we've installing
the forecast forecast package and you
can see down here I just type in
forecast all the install all the
dependencies uses other packages to
build on it has those then just click on
install and it's done before we cover
the basic linear regression model let's
just take a quick look at some of the
different parts of our or parts of
programming you'll need or scripting
first you know there are various data
structures in R we have vectors we have
matrixes we have arrays data frames and
lists vectors is the most basic data
structure if you remember vectors are a
location and a direction it's how
they're generally defined although a
vectors when you're talking about
scripting can contain numerous different
values you could have vectors with four
five six seven eight different values in
it for example a picture on the computer
might have the location of the pixel and
the number for the red Hue and the green
Hue and the blue hue so now you have
some thing with five different numbers
in it consisting Vector matrixes allow
you to move stuff around so you might
have a two by three Matrix that you
switch to a three by two we looked at an
X Y plot earlier it might be that you
have everything is you have 10 different
numbers and each one has two values x y
and you need to switch that Matrix so
then you have two rays of five numbers
arrays are just at a collection and you
can have arrays of arrays of arrays data
frames have labels on them which makes
them easier to use so usually we use a
lot of data frames when we're working
with data because they're just easy you
can have a column and you can have a row
so think of rows and columns when you
see the term data frames and then lists
are usually homogeneous groups so in R
you're usually looking at similar data
that's connected in the list so the
first thing we do with before we even
importing the data you should have the
data ready so we need to look at the
data and see what's going on you can
import data from various sources
including Excel minitab CSV table text
files CSV is usually a text file comma
separated variables tabs separated
variables there's all kinds of different
options here and importing the table is
very simple we have table read table
file equals data table header equals
true and let's just take a closer look
at what we're talking about here scratch
the closer look I didn't realize it was
just for example so importing a table
file is pretty simple you read.table
file equals data.table so whatever the
name of the file is comma header equals
true so if there's a header to it in
this quick flash they have name age
gender we'll actually do this in r with
another data in just a minute but you
can see it's very easy to read a lot of
people use or even in other programming
languages so I can quickly look at the
data before they even start analyzing it
csev file same thing read.csv file
equals data CSV header equals true
separation equals space so in this case
even though CSV stands for comma
separated variable this one is separated
by spaces and they have the header name
age gender so it's the same file saved
as a CSV file and there's also Excel
Excel has its own issues as far as
making sure you know what the tables are
and the headers are but you can see that
each one of these is easy to import so
and just like it's easy to import the
data you can also export tables in R so
you can see here right dot table my file
see my file.text comma separated and the
scoop T just means it's tab separated
instead of using tab files on there
example like Excel so you can write a
DOT XLS to my file in this case they did
a text separation equal scoop T so it's
a tab separated for Excel file CSV same
thing very easy to write a CSV file to
your computer once you've changed the
data or altered it depending on what
you're doing with it and once we have
our data imported and we can save it
afterwards and Export it uh graphing
visualization in R is very powerful and
it's quick love doing this before even
exploring the data sometimes you just
want to graph it to see what it is there
so you have an idea of what you're
looking for so graphics and R cover a
huge amount of different things our
includes powerful package of Graphics
that help in data visualization these
Graphics can be viewed on screen saved
in various formats including PDF PNG
JPEG wmf and PS can be customized
according to very graphic needs you can
copy and paste in word or PowerPoint
files R supports various types of
Graphics including bar chart pie chart
histogram kernel density plots line
chart box plot heat map word cloud
cure ones but these are the main ones
that most people use I know I use a lot
of heat Maps but word clouds are a lot
of fun if you're doing websites and data
and word analysis and histogram is very
popular all of these are very widely
used let's look at the box plots also
known as whisker diagrams box plots
display the distribution of data based
on minimum first quartile medium third
quartile and maximum so right off the
bat we can use a box plot to explore our
data with very little work to create a
box plot we simply give a box plot and
the data very straightforward so we
might have passenger numbers in the
thousands I guess this is exploring data
dealing with airplanes and you can see
here they just have a simple plot if you
break it down you have your if we go
back one you'll notice we have like a
minimum a maximum our medium first
quartile and third quartile this is
break that apart you can see the line on
the bottom is your minimum your line on
the top is the maximum gamer medium and
your first chord right there first
quartile and third quartile and the way
a lot of times you read this is things
that are above the Box are outliers and
things below the box or outliers and
what's in the middle of the box is use
the data you're looking at
regression now we have a few tools of
what R can do let's look at the theory
behind linear regression and how that
applies to what we're going to use in R
let's understand more concepts by
solving a few algorithms firstly linear
regression which can be applied in two
ways one we can estimate the
relationship between two variables does
body weight influence the blood
cholesterol level so we just want to
know if it's even a valid connection or
not will the size of the house affect
the house price so we can estimate the
relationship between two variables also
and then we can predict the value of one
variable dependent on the basis of other
independent variables so once we know
they're connected we want to use that to
go ahead and predict it and the cases
above maybe the body weight versus
cholesterol so the more the higher the
body weight is probably the higher
cholesterol so we first we explore is
there a connection and if there is can
we actually apply that to a linear
regression model where we can see those
how they're connected same thing with
house pricing the simplest form of a
simple linear regression equation with
one dependent and one independent
variable is represented by y equals m
times X plus c y being the dependent
variable X being the independent
variable M being the slope of the line
which you can see over here on our graph
equals Y2 minus y1 over X2 minus X1 and
C being The Intercept or the coefficient
of the line now you don't have to
memorize all the formulas we're going to
go in here because most of R does this
for you but you do need to know what's
going on in the back end so that you can
understand if it's working or it's not
working where the problem lies types of
linear regression there are simple
linear regression considers one
quantitative and independent variable X
to predict the other quantitative but
dependent variable Y and that's a
formula we just saw y equals m times X
plus C and then we have multiple linear
regression considers more than one
quantitative and independent variable so
instead of having just X you have X of 1
x of 2 x of 3 to predict the other
quantitative but dependent variable y so
instead of looking at just body mass we
might also look at height so you have
body mass and height would be two
independent variables housing prices
might have to do with the distance to a
certain area so location might be one of
the variables along with the size of the
house let's consider a sample data set
with fibros and find out how to draw the
regression line so in here you can see
we have independent variable and our
dependent variable or we've taken some
random data in this case we have one two
three four five for x and we have two
four six five eight for y and the first
thing we do is we might plot it if you
look at the plot you can see that it has
a nice line through it and just
eyeballing it you can guess that there's
probably a linear regression model that
will fit this data then we go ahead and
calculate the mean we count the mean of
X and the mean of Y and you can see that
we plotted that on there 3 comma 5 for
this particular data set and so drawing
the equation of the regression line
we'll go ahead and take a summation of x
a summation of Y take a summation of x
squared and a summation of Y squared and
then x times y this is why it's so nice
for the computer to do this work for you
because I don't know about you I hate
sitting there with a calculator and if
you have just five variables that would
be very long not a big deal but if
you're doing hundreds of variables
definitely much easier to let the
computer do it all so the intuition
behind the regression line breaks down
to the linear equation which is
represented as y equals m times X plus C
in this case m equals this very lengthy
formula where you have your summations
so all the values that we've already
computed fit right into this equation
and so you have n being the number of
variables that's what the n stands for
your x times y we computed the sum of X
sum of Y over the number of variables
times the sum of x squared minus the sum
of x squared and then of course we have
uh you put the numbers in and you get a
value of 1.3 and you find out that your
C is equal to 1.1 and once we have this
formula we can easily compute the Y
values y equals m times X plus C equals
1.3 times 3 plus 1.1 equals 5. and this
would be the example of x equals three
you could also put in x equals 6 an
unknown value let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equals 1.3 and C equals 1.1 so
here's our regression line and then you
can see if we put in all the numbers one
two three four five we don't get the
exact prediction because it's a line
through the data you'll see that it ever
so slightly off 2.4 3.75 6.3 7.6 but it
is a good estimate of what the data is
going to generate let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equals 1.3 and C equals 1.1 the
best fit line should have the least sum
of squares of these errors also known as
e-square the predicted y value residuals
of Errors the actual y values so one of
the things we're looking at when we put
this information together is we want
these distances to be minimized so it
has the smallest amount of error
possible let's find out the predicted
values of Y for corresponding values of
X using the linear equation where m
equals 1.3 and C equals 1.1 and here you
can see we've done the same kind of
chart we have our y predicted and then
we have the actual y minus the Y
predicted because we are looking for the
error squared or the E squared values
the sum of the squared errors for this
regression line is 3.9 we check this
error for each line and conclude the
best fit line having the least e-square
values minimizing the distance there are
lots of ways to minimize the distance
between the line and the data points
like sum of squared errors sum of
absolute errors root mean square error
Etc and no matter how you compute the
errors you can see here this is what
it's basically going through it's taking
the line and slowly adjust it until it
has a minimal error available the
minimal distance between the line and
the different data points
use case we'll start with linear
regression and then we'll jump in and
also do a decision tree so you can see
how they are same and how they differ in
the way they function and what they're
used for and while we're going through
the use case you can start linking the
theory behind it connected to the actual
use and as you connect those dots you
can see that by making changes in the
model based on the theory you can also
fine tune it we'll talk about that
briefly as we go through this for this
analysis we'll use the default cars data
set to find the correlation between
variables cars is a standard built-in
data set that makes it convenient to
show linear regression in a simple and
easy to understand fashion and we'll
start with head cars and string cars now
I've gone ahead and removed the script
on here to make this easier to see and
we don't have to do any kind of
importing of data because it's already
in there I'm just going to type in head
and cars and hit enter and what you're
going to notice is it's going to display
the first six rows one two three three
four five six and you'll see speed and
distance now this is a little bit like a
spreadsheet so if you're using an Excel
spreadsheet it should look the same and
this is a standard data frame kind of
setup in this case whenever you say head
it usually lists the first number of
rows depending on your package in R it
starts with one and then it does the
first six rows string when we do a
string of cars it's going to break it
down and show us that it's a data frame
it has 50 objects that's what the 50 OBS
of two variables and then we have speed
the first variable is going to be a
number is what nnuum stands for 4477 it
just starts listing the data there and
distance now I'm assuming the speed is
the speed of the engine otherwise if
it's the speed of the car then the
distance should always be the same and
that would be to be kind of silly but
this is the speed of the engine and
based on the speed of the engine can we
correlate that to the distance let us
visualize the data using scatter plot to
understand the relationship between
predictor and response and I want you to
notice that in this slide a line has
been
through the data now when we first start
with data you don't have that line so as
we plot it I want you to kind of
visualize that line there and just kind
of say hey does this stuff kind of
scatter around a line or not before we
actually create the model and we simply
do plot and I'm going to do a shorthand
here cars and I hit enter and it comes
over here to the right and plots our
data for us now because there's only two
variables I don't have to do anything
special I'm going to expand this over
environment so we can see it and you can
see you can just visualize a line right
through the data and it's all clumped
along that line which makes it really
nice for doing a linear regression model
now I did something a little tricky here
because I'm very lazy I typed in plot
cars a lot of times you don't want to do
that because you don't want to plot all
the data we could do plot in this case
cars dollar sign speed the first
variable cars dollar sign distance when
I do that I get the same plot all I'm
doing is telling it to use just these
two columns of data and although the
first one is quick and easy if you have
two columns of data the second one is
what you really want to do when you're
plotting data like this you want to
control which columns you're using and
then we want to do a correlation
correlation analysis studies the
strength of relationship between two
continuous variables it involves
Computing the correlation coefficient
between the two variables if one
variable consistently increases with
increasing value of the other then they
have a strong positive correlation of
value close to one and again I can just
type in correlation cars the short form
and we can see we have a nice
correlation going on there between the
two what we really want to do though is
we want to do we want to correlate cars
and then we're going to do the speed
comma cars and the distance and so when
you look at this and actually hit the
enter key in the middle of that which is
fine you can in R do it line by line so
if you're trying to do a bunch of
different columns you might want to do
that to make it easier to read and you
can see here I get the
0.806894 the closer this is to one the
more these correlates saying there's a
pretty good correlation a positive
correlation between the two variables
and if you look at it when I did just
cars which did the correlation over all
the variables you'll see that speed
correlates with speed 100 because
they're the same that's just the nature
of that but when you look at speed
versus distance we get the
0.806849 and if you do distance to
distance you get also one since they're
identical variables so now it's time to
build our linear regression model on the
entire data set to build the
coefficients let's just take a look and
see what that looks like back in our
console I'm going to type in linear mod
and in R we want to do the arrow kind of
like an arrow and a line or in this case
the less than minus sign that's the same
as assigning whatever we're going to put
after it to this variable so we're
creating the linear variable LM stands
for linear module and we're going to
look at speed and distance so we put the
little squiggly bracket between them
this lets R know we're going to deal
with these two columns comma data equals
cars and when I hit enter on here we've
generated our linear mod now we want to
go ahead and summarize it and we simply
do a summary and then brackets linear
mod and it generates all kinds of
information on our model so we can
explore just how well this model is
fitting the data we have right now now
if you've done linear regression and
other packages and scripts you're going
to see that this is so easy to explore
data in R so even somebody who's working
in say python or
spark Hadoop coming back to the r
console to do some basic exploration of
the data is very beneficial and if you
use just our R goes into all kinds of
different packages and you can even do
cluster computations through h2or
there's a lot of cool things this is
what makes ours so wonderful is how
easily we can summarize something like a
linear model and to explore this data
let's go back to the slide and we'll see
here where it says the residuals the fir
1q stands for first quadrant median
third quadrant Max coefficients and we
first want to do is look at the estimate
standard deviation on here and you'll
see The Intercept is at minus
17.5791 and speed
3.9324 so we can compute the distance
equals to The Intercept plus the beta
times the speed which just means that we
can assign a distance equal to minus
17.579 plus the 3.932 times the speed
the value of P should be less than 0.05
for the model to be statistically
significant and this is one of the
things I love about R you can see right
down here we punched in the numbers and
it even tells you the significant codes
so if you're trying to guess or remember
it tells you right away uh hey this 1.49
to the minus 12 is way below 0.001 on
there and even puts three stars next to
it so you can see that that is a very
high correlation and you can see the
0.0123 gets one star because it is
greater than the 0.05 or it's less than
the 0.05 so this shows that there's a
very high correlation statistically is
very significant so now we're going to
go ahead and create a training and a
test data set and the first thing we
want to do is we want to set the seed so
the sample can be recreated for future
use remember in all these different
regression models they usually have some
randomization going on in the back to
kind of fit it and guess where it's
going to go and if you want everything
to match you want to start with the same
seed in that randomizer that way it
recreates it identical every time you
run it on a different computer and just
like working with data in any package
we're going to create indices for the
training data and we're going to model
the training data and then we're also
going to do the test data and build our
model on the training data let's walk
through that and see what that looks
like in R and I'd actually miss type
that one let's type in set dot seed to
set our seed for the 100 let's go
training row index and that's our name
we're making up for our variable and
we're going to set that to sample Now
sample is a command in R so we're going
to take a random sample of some of the
data and then we're going to do one
colon in rows cars so this is the number
of rows in cars and we're going to start
with one and Sample them so Row one of
the data we're skipping any of the
titles on the top and then in the sample
it has two variables that go in the
first one is the in rows of cars and
then we want to do
0.8 times in rows of cars this basically
says that we're going to take 80 percent
of all the rows in the car sample we'll
hit enter to go ahead and run that and
we see here that I had an error and that
is because it's in row actually put an S
on there that's something I do a lot on
we'll just fix that real quick brings up
a point that if you use your up down
arrow you can quickly paste through the
last things you typed in and then you
can reuse them so there we go now I have
my training row index which is basically
a list of all the rows we're going to
use and now that we have a training row
index we'll go ahead and create our
training data variable and we'll assign
that cars and we'll put that as a
training row index and this is supposed
to be brackets because we're dealing
with a array type format we want to do
the training row index comma because we
just need the index of the row and as
far as the columns we want all the
columns so here we have our training
data and now we set up a training data
we also need our test data and with our
test data we'll assign that also to cars
and because we use the training row
index and we use the sample to create it
we can simply do a minus sign training
index comma so it's identical to the
training data but the test data is going
to be the ones the rows that were not
included in the random sample of 80
percent so it's going to be twenty
percent of the data and of course it's
very easy to do mistakes on here so
we'll come back into this bring back up
the error and then we have training the
training index training row index there
we go very easy quick error is in R so
now we have our training data and our
test data so we have 80 in training data
and 20 percent in test data and let's go
ahead and create our model we'll call it
LM mod and this is identical to what we
did earlier here's our linear model
we're going to have our two different
columns if you remember above we had to
tell out which columns we wanted I'm
going to do distance
[Music]
speed and then the data we're going to
set equal to training
data so as opposed to doing all of cars
we want to use just the training data on
here and now that we've created a model
off of the training data we need to go
ahead and run our prediction off of our
test data we want to see how the test
data which has nothing to do with the
model so far how well it fits and we'll
go ahead and call it distance predict
there's our assignment so we're going to
assign the value to it and with r we
type in predict and then we called our
model we go ahead and pull up our model
LM mod and so we're going to use that
model and then we want to put in the
test data here's our test data and so
now we're creating the distance predict
variable and we're going to put all the
information as far as predicting on our
tested data and we'll run that just hit
the enter key and then if you remember
from before we want to review the model
diagnostic measures and so we're going
to go in here and do a summary and
you'll see in here we have a couple
different things going on but let's go
ahead and take a summary of this and
just walk through what this means so
summary and then we call that LM mod and
we hit enter and it pulls it up and we
have a nice summary of our LM mod and
the first thing we're going to note is
that the data that's right now in our LM
mod that we're summarizing is our
training data and let's go back to the
slide and highlight this so the data
equals the training data that's where
the lmod comes in residuals we have our
minimum our first quarter medium third
quarter and the max or quadrant quarter
you'd think I was doing a business
analysis as opposed uh into the year
taxes in first quarter versus quadrant
we look at our coefficient we can see
where our intercept is and our speed is
and we note over here our significance
codes so they all match up so a simple
correlation between the actuals and the
predicted values can be used as a form
of accuracy measurement so we took a
look at the model we created and now we
need to take our predictions and just
see how accurate it is on the data that
we didn't use to program the linear
model so let's create a variable called
actuals underscore predicts and assign
it data frame this is how we create a
data frame as we just assign data.frame
and we'll do a c bind and in our C bind
we're going to take our data and we're
going to create a column called actuals
so this is our choice the name actuals
this is going to equal test data and
we're going to use a dollar sign and
distance
we're running out a little room there so
let me pull this over so we can see it
better there we go distance and then we
also want to take and compare that to
what we predicted for the our other data
and we'll call this column predicted and
that's going to equal our distance
predict and remember distance predict is
set to the predicted values of our test
data so what this is is we're saying hey
here's our test data the actual data and
the distance and then we have what we
predicted that distance to be and we'll
go ahead and assign that and we can do a
quick head
actuals predicts there we go remember
head shows this it's a data frame so
it's going to show us the labels and the
first set of data in this case you'll
notice that the rows have a different
count they're not one two three four
five six well we randomly picked 20 of
the data so this is the first six rows
of that random selection which comes out
as 1 4 8 20 26 31 and we have the
actuals so the actual value is 2 and the
predicted value in this first one is is
minus five so they're way off 22 7 26 20
26 37 54 to 42 50 to 50. so one of these
is actually pretty right on where a lot
of them are really off at the beginning
let's just see what that looks like
though because just eyeballing the first
six rows does not tell us what's really
going on so let's create another
variable correlation accuracy and we're
going to assign this correlation cor
remember we do a COR and we have our
actual predictions so let's just see how
our actual values versus the prediction
correlate with each other and we sign
that One X
score predicts there we go let it auto
finish for me and we can just go ahead
and type in correlation accuracy and see
what that looks like so we have an
actuals one a predicts 0.8 and you can
see how they kind of correlate we have
the 0.82 and if you remember from before
we're looking for either 0.001 0.01 0.05
0.1 that is a p-value which we're not
looking at here this is not based on the
p-value this is based on we looked at
earlier with correlation that the closer
to one the closer the value is r and so
0.82 gets upwards of one so you see that
there's a normal pretty much a
correlation in here but we want to dig a
Little Deeper because it doesn't really
tell us how accurate it is for that
we're going to use another tool for that
we're going to calculate the min max
accuracy and make and the min max
accuracy equals the mean value of the
minimal actuals and predicts over the
max value of the actuals and predicts
and and then the mean absolute
percentage error or mape as it's called
Equals the mean of the absolutes
predicted minus actuals over the actuals
that's a lot to follow as far as
remembering all these different math and
theory behind it cool thing though is
it's all done pretty much for you so you
can compute all the error matrixes in
one go using the regress eval function
in dmwr package since this is the first
time I've used this install I'm going to
have to install the dmwr package so we
can actually install it two different
ways if we remember we talked about
installing package earlier we can go
under Tools and install packages and I
certainly could type in
dmwr it even comes up and lets me know
that's one of the main packages up there
well that's a tool of the rstudio setup
you can also install this using the
console we simply type in install dot
you can even see here it comes up
install packages and then
dmwr and when I hit enter I should do
the same thing as the other one does it
goes through and installs the dmwr
package it takes just a moment to zip
through all the different package setup
and then the format for dmwr is DM WR
we're going to go colon colon we are
specifically looking at the regression
dot eval and then we're going to do
actuals underscore predicts and we'll go
ahead and put in the individual columns
on this one so we have actuals I love
the auto type typing and then again
actuals predicts and this time we want
to do it against the predicteds and
we'll go ahead and hit enter on here and
you'll see in here it comes up with our
Mae MSE rmsc and the mape values so we
looked at the linear regression model
let's go ahead and take a look at the
decision trees so let's go ahead and
talk about another algorithm decision
tree decision tree is a tree shaped
algorithm used to determine a course of
action each branch of the tree
represents a possible decision
occurrence or reaction there's a tree
which helps us by assisting Us in
decision making let's look at the basic
terminologies to understand decision
trees we have our root node we have a
splitting we have decision node decision
node and then those split into terminal
node decision node terminal node
terminal node and each decision node
continues to split until it ends in a
terminal node note a is the parent node
of B and C also note we call the
terminal nodes Leaf nodes you can also
see that we have a branch or a sub tree
so when you have a split everything
under that split under one side is
called a sub tree you cannot build your
decision tree without knowing entropy
and Information Gain entropy entropy is
the measure of randomness of impurity in
the data set so we have here A bunch of
fruit and you can see we have it looks
like apples oranges and bananas it's
very chaotic so it's a very random data
set has a very high entropy and if we
take out one group let's say we take out
the apples it's a little less random now
we only have bananas and oranges so it's
a less random data set and we have lower
amount of entropy entropy is a measure
of Randomness from Purity in the data
set so when you have a homogeneous data
set we will have entropy equals zero and
here you can see it's all oranges and
equal divided data set will have an
entropy equal to one so here we have
half our what is it looks like a couple
bananas a couple oranges and a couple
apples so everything is equal so we
haven't P of 1 on there since there's
two of everything Information Gain it is
a measure of decrease in entropy after
the data set is split also known as
entropy reduction so we look over here
we have an entropy equals E1 and the
information gain as we split the bananas
out the size becomes smaller and you'll
see that E1 is going to be greater than
E2 or we measure E2 of the apples and
oranges in this case if you love your
fruit it's probably getting you hungry
right about now and then the information
gain from our level 2 equals E2 minus E1
where E2 is greater than E3 as we come
down here we'll see that the E3 is now
the third level we split the oranges and
apples out and each time that entropy
becomes less and less and less until in
this case we have an entropy of zero
since they're all homogeneous in their
order all oranges all apples all bananas
so let's take a look at the use case
decision tree and actually put this into
code and see what that looks like as we
go from Theory to script and to predict
the class of flower based on the petal
length and width using R and you'll see
here we have these beautiful irises
probably the most popular data set for
beginning data analysis and statistics
we have the setosa the virginica and the
versic color let's install the packages
that will help us in the use case so
because we're doing decision tree we
have our part R part Dot Plot and then
we have the library R part and the
library are part Dot Plot let's go ahead
and take a look at that and if you
remember we have two ways of installing
it we can do the install packages we did
that earlier here we go install packages
using my up Arrow to get to it or we can
go up to tools install packages and I
can do R part and our part Dot Plot and
then go ahead and hit the install right
as it will be updated this are currently
loaded oh it looks like some of them
already loaded that's fine we'll just go
ahead and update those and once we have
those updated let's take a look and see
what we have going on here oh it's still
loading
let's go ahead and type in library our
part and Library R part Dot Plot so we
want to bring it into our console with
who we're working in so up and now we
just download the packages and now we
want those libraries available to what
we're doing so the IRS although it's
built into the r package download on the
basic download we actually have to
install the data or bring the data in so
we're going to do the data Iris and then
we're going to do the string Iris and
let's just flip back on over to our
rstudio and set up our data Iris
and then we're going to do string before
we do that let's do head head
Iris there we go and you can see the
head has simple length sepal widths
petal length pedal width species in this
case the top Parts all setosa and each
row gets a number and then we did the
string
address and that comes up with
information and if we flip back to our
slide you can see here the structure of
the database under string is it's a data
frame as 150 objects five variables each
so each one has five variables all
features are numerical except species
which is categorical as it is our Target
variable which we want to predict so we
want to predict whether it's going to be
a varicosa versus color setosa so you
can see here we want to predict all the
features are numerical except species
which is categorical it is our Target
variable which we want to predict how do
you remember that's a setosa the Versa
color the virginica the three different
categories we saw earlier in the
beautiful pictures of the flower and so
we're going to go ahead and use the set
seed to decide the starting point used
in the generation of sequence of random
numbers remember we set the seeds so
that if we ever want to reproduce what
we're doing it will reproduce the same
thing each time because using the same
randomizer seat bring slightly different
results but depending on what you need
it for and then we're going to Generate
random numbers using run if and the
question is why do we want to create
random numbers on here that's kind of an
unusual thing to do for inros Iris and
let's go back and just take a quick look
here actually we want to go back to our
R studio and you're going to notice when
I did the head of Iris what did I have I
had cytosis cytosis atosa cytosa cytosa
so the data is organized by species and
it's already grouped them together in
the data set well we want to randomize
that so that doesn't affect our output
and then we'll go ahead and take and
create our Iris Ran where we recreate
the data frame based on a random order
instead of setos all being grouped
together let's go ahead and put that
into our R studio and set dot seed and
we'll match their number
9850 this way we'll have identical
results to what they're doing on the
slides and we're going to set the
variable J G equal to run if and basis
on in row of Iris
and finally we're going to create Iris
underscore ran and we're going to assign
our variable we're creating Iris
underscore ran and we'll set that equal
to we can do that the original database
we're going to set that equal to Iris
and we need brackets because we're
assigning the valuables and we want to
change the order so there's an actual
command order we can do that's going to
be order G so this means all the rows in
the order that g is so we just
randomized all those rows and then we
want to keep all the columns so we'll
just put a comma and we'll leave it
blank for the columns so now Iris Ran
will have some randomized rows coming in
and if we do head let's just do this
real quick Iris Ran we can see there's
no longer cytosis cytosa we now have
virginica setosa versicolor setosa and
you can see the road numbers at the
beginning we have 103 20 63 17. so
they're randomized on there pretty well
now that we've done a little data
manipulation we can go ahead and build
our model using our part as follows and
one of the things I want you to notice
here is we're going to build our model
for our R part on the first hundred
because our data equals Iris Ran one to
a hundred there's 150 objects so we're
keeping a third of them off to the side
that way we have our training set and
our test set already pre-built because
we randomized the rows going in from the
beginning and then once we've done that
we want to go ahead and print our model
out and take a look at it let's go ahead
and put this into our our studio and
take a look and so we have our model
we're going to call it model that's our
variable name that we're giving it we're
going to assign that our part and in
Brackets we'll start with species and we
want to correlate species to all the
other columns so how do we do that and
the shorthand is just to put a period
there and then we're going to put a
comma and we want to set our data equal
to Iris Rand that's the data we put in
there and remember I said we're going to
do just the first 100 rows so we can
save the last 50 for test so we can do
that simply by doing Row 1 to 100 so in
R you start with one for your rows and
because we want all the columns we just
put a comma and that denotes that we're
going to use all the columns in here and
in the our part there's different
methodologies we're using the method
equals
class and you realize that we're doing a
classification between the three
different types of flowers and so this
is what we want to go ahead and set up
our model with and when we run that
we've now created a nice model for us we
can go and just type in model hit enter
and you'll see I'll print out a bunch of
information about our model so let us
know what's going on in there and let's
just break that down and see what that
looks like so we have our Target our
predictors that species and it's going
to relate to we used a period to denote
all the columns we're going to take the
first 100 rows to train the data and we
have the defining method to be used as
classification that's what the method
equals class is and we want to do next
is we're going to do a plot and take a
look and see what this actually looks
like a little visual here which is nice
and we'd simply do that with our plot
for the model and let's break this down
a little bit and see what that looks
like in the r setup and we do this our
part Dot Plot a little bit of a mouthful
there sometimes that's part of the art
part package or module so when you
imported this in it also included its
own plotting format and we're going to
send it our variable model and there are
actually a number of types of plots
listed for our part but we're just going
to do type equals four Fallen dot leaves
equals T and extra equals 104. let's go
ahead and hit enter on there and you'll
see right over here on the right hand
side it goes into our plots we just
enlarge it large that we have our Versa
color our setosa versicolor virginica
and let's go ahead and break that down
as you can see down here it represents
the tree as it splits it looks at it and
says hey if the pedal dot length is less
than 2.6 it's going to be a setosa so 34
of them fall into that category and if
the pedal length is greater than 2.6
then looks at that and says okay now I
have three different variables we're
going to split it again pedal width is
less than 1.7 is going to be a versa
color and if it's greater than 1.7 it's
a virginica and so you can see here it's
very simple tree it forms so I'm
levels you can see a branch off to the
right very nice depiction of what's
going on as far as the model now that's
created but once we create the model we
need to figure out is this model any
good so let me go back over here let's
go back to our slides and the first
thing we're going to do is we're going
to take our test data remember we saved
50 rows for the end to go ahead and run
a predict on there so let's go ahead and
run that prediction so we'll call it
model underscore predict and we're going
to hit a wrong button there to predict
and we're going to what are we going to
predict we're going to predict our
initial model and we need to send the
data so data equals and we don't
actually have to put the data part in we
just put in Iris underscore ran and then
we want to do just the last
50 rows and we denote that by 101 colon
150 and then comma and we'll leave that
part blank since we want all the columns
to show up and then finally this is a
classification so we need to let it know
that type equals class there we go and
let's just see what that looks like
model predict and so this is what the
data is put out as is it says whatever
this is 131 is a virginica number 10 is
a setosa line number 95 is a versa color
and so on all the way through what we
really want to know is how good our
model is so we're going to go ahead and
install a couple more packages we're
going to install the packages required
to use the confusion Matrix and to do
that we're going to install carrot and
e1071 and then of course we want to set
them to our project by using the library
command and so we can simply install
packages and we want carrot and we'll go
ahead and just do that right there
there's carrot installed
go ahead did the inside packages
e1071 as it goes through there's a
really quick install compared to the
carrot and once we've installed the
package
we need to load it into our workspace so
there's our Library carrot
and once that's loaded we'll go ahead
and also load the or set the library to
the e1071 so that we have both those
libraries available to us here
on every
e1071 and now these two libraries are
fully available to us and then we're
going to use these tools to create the
confusion Matrix
Matrix this is the iris RAM and then
we'll go ahead and put the brackets here
and we're only going to do 101 to 150
and then we only want the first five
columns not the last five column the
first five columns so 5 in there and
we'll set that to our model predict and
then when we hit enter it's going to
head and just print out all this
so right here we're looking at our
confusion Matrix to see how everything
kind of balances out let's go over that
and see a little closer what that
actually means so the first part we see
is a reference and in here we have our
prediction because we could call the
column if you remember
and we have it for setosa versicolor
virginica and then we have such a
versatile virginica going down each side
and the way you read this is if you look
at this we predicted
18 out of 16 cytosa
and then versacolor we predicted 13 of
the Versa color and then two of those we
predicted as virginica so we were wrong
on two of those and with the virginica
we predicted 17 correct and we had two
wrong that were Versa color so we look
at the overall statistics on here and we
have an accuracy of 0.92 and a 95 CI
that's pretty good as we're going
through here those are good deals on
that and there's a lot more information
as you go down this the one thing I want
to remind you on this as we're looking
at these predictions and this is also
true for the linear regression model we
looked at earlier when we talk about the
accuracy that is the accuracy on this
data and we say that as programmer to
make that a very clear distinction
because if you're in data analysis you
should cringe if I say ah this is a 95
accurate model without that put in there
because bad data in bad data out we
won't go into detail right now on that
but that is a very important to note
that whenever you quote any of the
accuracies on here or you discuss any of
these values that come up so what does
python have for data science
python isn't interpreted dynamically
typed language with a precise and
efficient syntax and is renowned for its
ease of use to analyze visualize and
present data python has an extensive
library with built-in modules providing
easy access to system functionalities
which not only improves accessibility
but also provides standardized solutions
for everyday programming challenges
by taking away platform-specific
requirements and replacing them with
platform neutral apis modules in Python
libraries encourage users to make
programs portable
why should a data scientist use Python
apart from being an open source
ecosystem Python and most of its
incredible libraries are platform
independent so you can run this notebook
on Windows Linux or Os 10 without a
change python is part of the winning
formula for productivity software
quality and maintainability at many
companies and institutions around the
world python is one of the leading
Technologies used widely in academic
circles and industries because of its
flat learning curve still not sold on
python then here's some proof
case study
forestwatch.com a weather report
accuracy rating service is a hundred
percent python solution using it in all
its components the front end back end as
well as the performance critical
portions python was chosen to develop
the solution as it comes with many
standard libraries useful in collecting
parsing and storing data from the web
python is impressive as an
object-oriented rapid application
development language one of Python's key
strengths lies in its ability to produce
results quickly without sacrificing
maintainability of the resulting code
because of the clean design of the
language refactoring the python code was
also much easier than in other languages
moving code around simply requires less
effort
forecastwatch.com was made possible
because of the ease of programming
complex tasks in Python the rapid
development that python allows the ease
to analyze and represent data in a
simple and Visually appealing Style
Market potential
so how does the future look for python
as the tool for data science
according to Katie nuggets 46 of all the
data science jobs list python as the
skill required second only to SQL
according to a survey by the Association
for computing Machinery ACM posted on
javaworld.com python has surpassed Java
as the top language used to introduce
U.S students to programming and computer
science
projected skill Gap there is also going
to be a huge demand for data scientists
who know python McKinsey and Company a
trusted advisor and counselor for many
of the world's most influential
businesses and institutions projects
that by 2018 the demand for data
scientists who know python May surpass
Supply by perhaps 60 making python a
must know skill for data scientists
forbes.com mentions that the demand for
python programmers in Big Data related
positions increased by 96.9 percent in
12 months now let's talk about python
for doing data science we need some kind
of a programming language or a tool and
so on so this session will be about
python there are other tools like for
example R and we will probably do a
separate video on that but this session
is on Python and you must have already
heard python is really becoming very
popular everybody is talking about
python not only data science in iot and
Ai and many other places so it's a very
popular it's getting very popular so if
you are not yet familiar with python
this may be a good time to get started
with it so why do we want to use Python
so basically python is used as a
programming language because it is for
data science because it has some rich
tools from a mathematics and from a
statistical perspective it has some rich
tools so that is one of the reasons why
we use Python and if you see some of the
trends if you're probably tracking some
of the trends you will see that over the
last few years python has become
programming language of choice and
especially for data Sciences was earlier
one of the most popular tools but now
increasingly python is being used for
doing data science and of course as well
as r one of the reasons of course is
that Python and R are open source
compared to SAS which is a commercial
product so that could definitely be one
explanation but beyond that I think it
is the ease of understanding this
language the ease of using this language
which is also making it very popular in
addition to the availability of
fantastic libraries for performing data
science what are the other factors there
are speed then there are availability of
number of packages and then of course
the design goal all right so what are
each of these design goal primarily the
syntax rules in Python are relatively
intuitive and easy to understand thereby
it helps in building applications with
the con size and readable code base so
with the few lines of code you can
really achieve a lot of stuff and then
there are are a lot of packages that are
available that have been developed by
other people which can be reused so we
don't have to reinvent the wheel and
last but not least the speed so python
is relatively faster language of course
it is not as fast as let's say CRC plus
plus but then relatively it is still
faster so these are the three factors
which make python the programming
language of choice so if you want to get
started with python the first thing
obviously is to install python so there
is some documentation there are some
steps that you need to follow so we will
try to briefly touch upon that otherwise
of course there are a lot of material
available on how to install Python and
so on you can always look around but
this is one of the again there are
different ways in which you can also
install python so we will use the
Anaconda path there is a packaging tool
called Anaconda so we will use that path
you can also directly install python but
in our session we will use the Anaconda
route so the first thing you need to do
is download and conduct and this is the
path for that and once you click on this
you will come to a page somewhat like
this and download you can do the
corresponding download Based on whether
you have a Windows or Ubuntu there is a
also a download possible for our package
available for Ubuntu if you are doing
something on Ubuntu So based on which
operating system in fact this page will
automatically detect which operating
system you are having and it will
actually suggest so for example you see
here if you're running Mac OS then it
will automatically detect that you have
Max and the corresponding installers
will be displayed here similarly if you
are on some flavor of Linux like Ubuntu
or any other then you will get the
corresponding download links here and
then beyond that you can also select
which version of python you want to
install of course the latest version is
in the three point x range at the time
of recording this 3.6 is one of the
latest versions but some of you may want
to do or start with the earlier version
which is python to 2.7 to point x and
you can download that as well if you
don't have anything installed then my
suggestion is start with python 3.6 all
right so once you do that you will be
able to install Python and you will be
able to run Jupiter notebook okay so now
that you know how to install Python and
if you have installed a python let's
take a look at what are the various
libraries that are available so python
is a very easy language to learn and
there are some basic stuff that you can
do for example adding or printing hello
world statement and so on without
importing any specific libraries but if
you want to perform data analysis you
need to include or import some specific
Library so we are going to talk about
those as we move forward so pandas for
example is used for structured data
operations so if you let's say are
performing something on a CSV file you
import a CSV file create a data frame
and then you can do a lot of stuffs like
data munging and data preparation before
you do any other stuff like for example
machine learning or so on so that's
pandas sci-fi as the name suggests it is
kind of it provides more scientific
capabilities like for example it has
linear algebra it has Fourier transform
and so on and so forth then you have
numpy which is a very powerful library
for performing n-dimensional or creating
and dimensional arrays and it also has
some of the stuff that is there in
sci-fi like for example linear algebra
and Fourier transform and so on and so
forth then you have matplotlib which is
primarily for visualization purpose it
has again very powerful features for
visualizing your data for doing the
initial what is known as exploratory
data analysis for doing univariate
analysis by variate analysis so this is
extremely useful for visualizing the
data and then scikit-learn is used for
performing all the machine learning
activities if you want to do anything
like linear regression classification or
any of this stuff then the scikit-learn
library will be extremely helpful in
addition to that there are a few other
libraries for example networks and I
graph then of course a very important
one is tensorflow so if you are
interested in doing some deep learning
or AI related stuff then it would be a
good idea to learn about tensorflow and
tensorflow is one of the libraries there
is a separate video on tensorflow you
can look for that and this is one of the
libraries created by Google open source
Library so once you're familiar with
machine learning data analysis machine
learning then that may be the next step
to go to deep learning and AI so that's
where tensorflow will be used then you
have beautiful soup which is primarily
used for web scraping and then you take
the data and then analyze and so on then
OS library is a very common Library as
the name suggests it is for operating
system so if you want to do something on
creating directories or folders and
things like that that's when you would
use OS all right so moving on let's talk
in a little bit more detail about each
of these libraries so sci-fi as the name
suggests is a scientific library and it
very specifically it has some special
functions of our integration and for
ordinary differential equations so as
you can see these are mathematical
operations or mathematical functions so
these are readily available in this
library and it has linear algebra
modules and it is built on top of numpy
so we will see what is there in numpy so
this is a again as the name suggests the
num comes from number so it is a
mathematical library and one of its key
features is availability of
n-dimensional array object that is a
very powerful object and we will see how
to use this and then of course you can
create other let's say objects and so on
and it has tools for integrating with CC
plus plus and also Fortran code and then
of course also has linear algebra and
Fourier transformation and so on all
these scientific capabilities okay what
else pan does is another very powerful
Library primarily for data manipulation
so if you're importing any files you
will want to create it like a table so
you will create what is known as data
frames these are very powerful data
structures that are used in Python
Programming so pandas Library provides
this capability and once you import a
data import the data into Data frame you
can pretty much do whatever you're doing
like in a regular database so people who
are coming from a database background or
SQL background would really like this
because it is very they will feel very
much at home because it feels like
you're using you're viewing a table or
using a table and you can do a lot of
stuff using the pandas library now there
are two important terms or components in
pandas series and the data frame I was
just talking about the data frame so
let's take a look at what our series and
what is a data frame So within pandas we
have series and data frame so series is
primarily some of you may also be
knowing this as let's say an array so
it's a one-dimensional structure data
structure if you will so in some other
languages you may call it as an array or
maybe some others probably an equivalent
of a list in R perhaps I'm not very sure
on that aspect but yes so this is like a
one-dimensional storage of information
so that is what is series whereas data
frame is like a table so you have a
two-dimensional structure you have rows
and you have columns and this is where
people as I said who are familiar with
SQL and databases will be able to relate
to this very quickly so you have like a
table you have rows and columns and then
you can manipulate the data so if you
want to create a series this is how you
would create a code snippet and as you
can see the programming in Python is
very simple there are no major overheads
you just need to import some libraries
which are essential and then start
creating objects so you don't have to do
additional Declaration of variables and
things like that so that is I think one
key difference between Python and other
programming languages and what does this
series contain it has to contain these
numbers 6346 and X is my object
consisting of the series so if you
display if you just say x it will
display the contents of X and you will
see here that it creates a default index
then you have data frame so if you want
to create a data frame as you can see
the series is like a one-dimensional
structure there is just like a row one
row of items whereas a data frame looks
somewhat like this it is a two
dimensional structure so you have
columns in one dimension and then you
have rows in the other dimension how do
you create a data frame you need to
create you need to rather import pandas
and then you import in this case we are
basically creating our own data so
that's the reason we are importing numpy
which is one of the libraries we just
refer to a little bit uh before so we
are using one of the functionalities
within numpy to create some random
numbers otherwise this is not really
mandatory you probably will be importing
the data from outside maybe some CSV
file and import into the data frame so
that's what we are doing here so in this
case we are creating our own test data
that's the reason we are importing numpy
as NP and then I create a data frame
saying PD dot data frame so this is the
keyword here similarly here in this case
while creating series we set PD dot
series and then you pass the value
similarly here using pd.dataframe now in
order to create the data frame it needs
the values in each of these cells what
are the values in the rows and what are
the values in the column so that in our
example we are providing using this
random number generator so NP speed or
random is like a class or a method that
is available in numpy and then you are
saying okay generate some random numbers
in the form of a four by three Matrix or
4x3 data frame the 4 here indicates the
number of rows and the three here
indicates the number of columns so these
are the columns 0 1 2 are the columns
and these are the rows here 0 this is
one this is two this is three okay and
once again it will when you display DF
it will give us a default index there
are ways to Omit that but at this point
we will just keep it simple so it will
display the default index and then the
actual values in each of these rows and
columns so this is the way you create a
data frame so now that we have learned
some of the basics of pandas let's take
a quick look at how we use this in real
life so let's assume we have a situation
where we have some customer data and we
want to kind of predict whether the
customer's loan will be approved or not
so we have some historical data about
the loans and about the customers and
using that we will try to come up with a
way to maybe predict whether loan will
be approved or not so let's see how we
can do that so this is a part of
exploratory analysis so we will first
start with exploratory analysis we will
try to see how the data is looking so
what kind of data so we will of course
I'll take you into the jupyter notebook
and give you a quick live demo but
before that let's quickly walk through
some of the pieces of this program in
slides and then I will take you actually
into the actual code and do a demo of
that so the Python program structure
looks somewhat like this the first step
is to import your all the required
libraries now of course it is not
necessary that you have to import all
your libraries right at the top of the
code but it is a good practice so you if
you know you are going to need a certain
set of libraries it may be a good idea
to put from a readability perspective
it's a good practice to put all the
libraries that you're importing at the
beginning of your call however it is not
mandatory so in the middle of the code
somewhere if you feel that you need a
particular Library you can import that
library and then start using it in the
middle of the code so that's also
perfectly fine it will not give any
errors or anything however as I said
it's not such a good practice so we will
import all the required libraries in
this case we are importing pandas numpy
and matplotlib and in addition if we
include this piece of code percentage
matplotlib inline what will happen is
all the graphs that we are going to
create the visualizations that we are
going to create will be displayed within
the notebook so if you want to have that
kind of a provision you need to have
this line so it's always a good idea
when you're starting off I think it's a
good idea to just include this line so
that your graphs are shown in line okay
so these are the four we will start with
these four lines of then the next step
is to import your data so in our case
there is a training data for loans by
the name loan PE underscore train.csv
and we are reading this data so in this
case you see here unlike the previous
example where we created a data frame
with some data that we created ourselves
here we are actually creating a data
frame using some external data and it's
the method is very very straightforward
so you use the read underscore CSV
method and it is a very intuitive
function name and you say APD dot read
underscore CSV and give the path of the
file CSV file that's about it and then
that is read into the data frame DF this
can be any name we are calling it DF you
can call XYZ anything there's a name
just name of the object so head is one
of the methods within the data frame and
it will give us the first five so this
is just to take a quick look now you
have imported the data you want to
initially have a quick look how your
data is looking what are the values in
some of the columns and so on and so
forth right so typically you would do a
head DF dot head to get the sample of
let's say the first few lines of your
data so that's what has happened here so
it displays the first few lines and then
you can see what are the columns within
that and what are the values in each of
these cells and so on and so forth you
can also typically you would like to see
if there are any null values or are
there any is the data for whatever
reason is invalid or looking dirty for
whatever reason some unnecessary
character so this will give a quick view
of that so in this case pretty much
everything looks okay then the next step
is to understand the data a little bit
overall for each of the columns what is
the information so the describe function
will basically give us a summary of the
data what else can we do pandas also
allows us to visualize the data and this
is more like a power of what we call it
as univariate analysis that means each
and every column you can take and do
some plots and visualization to
understand data in each of the columns
so for example here the loan amount
column we can take and then the hist
basically hist method will create a
histogram so you take all the values
from one column which is loan amount and
you create a histogram to see how the
data is distributed right so that's what
is happening here and as you can see
there are some extreme values so this is
again to identify do we have to do some
data preparation because if the data is
in a completely haphazard way the
analysis may be difficult so we these
are the initial or exploratory data
analysis is primarily done to understand
that and see if we need to do some data
preparation before we get into the other
steps like machine learning and
statistical modeling and so on so in
this case we will see that here by
plotting this histogram we see that
there are some extreme values so there
are some values a lot of it is around
100 range but there is also something
one or two observations in the 700 range
so it's pretty scattered in that sense
or they're not really scattered
distributedly scattered but it is
randomly scattered so the range is
really huge so what can we do about this
so there are some steps that we need to
do normalization and so on so we'll see
that in a bit so this is for one of the
columns let's take another column which
is applicant income similar kind of
similar situation you have most of your
observations in this range but there are
also some which are far off from where
most of the observations are so this is
also pretty this also has some extreme
values so we'll have to see what can be
done grid history is a binary value so
some people have a zero value and some
will have credit history of one this is
just like a flag so this basically is
telling us how many people have one and
how many people have zero so looks like
majority of them have a value of one and
a few about 100 of them have a value of
zero okay what else can we do so we now
understood a little bit about the data
so we need to do some data wrangling or
data munging and see if we can some
bring in some kind of normalization of
all this data and we will kind of try to
understand what is data wrangling and
before we actually go into it okay so
data wrangling is nothing but a process
of cleaning the data if let's say there
are there are multiple things that can
happen in this particular example there
were no missing values but typically
when you get some data very often it
will so happen that a lot of values are
missing either they are null values or
there are a lot of zeros now you cannot
use such data as it is to perform some
let's say predictive analysis or perform
some machine learning activities and so
on so that is one part of it so you need
to clean the data the other is unifying
the data now if these ranges of this
data are very huge some of them are
going from some columns are going from
zero to a hundred thousand and some
columns are just between 10 to 20 and so
on these will affect the accuracy of the
analysis so we need to do some kind of
unifying the data and so on so that is
what wrangling data wrangling is all
about so before we actually perform any
analysis we need to bring the data so to
some kind of a shape so that we can
perform additional analysis actual
analysis on this and get some insights
now how do we deal with missing values
is a very common issue when we take data
or when we get data from the business
when a data scientist gets the data from
the business so we should never assume
that all our data will be clean and all
the values filled up and so on because
in real life very often there will be
the data will be dirty so data wrangling
is the process where you kind of clean
up the data first of all identify
whether the data is dirty and then clean
up so how do we find some data is
missing so there are a few ways you can
write a small piece of code which will
identify if for a given column or for
given row any of the observations are
null primarily so this line of code for
example is doing that it is trying to
identify how many null values or missing
values are there for each of the columns
so this is a Lambda function and what we
are saying is find out if a value is
null and then you add all of them how
many observations are there where this
particular column is null so it does
that for all the columns so here you
will see that for loan ID obviously it's
an ID so there are no null values or
missing values gender has about 13
observations where the values are
missing similarly marital status has
three and so on and so forth so we'll
see here for example loan amount has 21
observations where the values are
missing loan amount term has 14
observations and so on so we'll see how
to handle this missing values so there
are multiple ways in which you you can
handle missing values if the number of
observations are very small compared to
the total number of observations then
sometimes one of the easy ways is to
completely remove that data so or delete
that record exclude that record so that
is one way of doing it so if there are
let's say a million records and maybe 10
records are having missing values it may
not be worth doing something to fill up
those values it may be better off to get
rid of those observations right so that
is the missing values are
proportionately very small but if there
are relatively large number of missing
values if you exclude those observations
then your accuracy may not be that very
good so there the other way of doing it
is we can take a mean value or for a
particular column and fill up wherever
there are missing values fill up those
observations or cells with the mean
value so that way what happens is you
don't get give some value which is too
high or too low and it somehow fits
within the range of the observations
that we are seeing so this is one
technique again there are it can be case
to Case and you may have to take a call
based on your specific situation but
these are some of the common method if
you see in the previous case loan amount
had 21 and now we went ahead and filled
all of those with the mean value so now
there are zero with missing values okay
so this is one part of a data wrangling
activity so what else you can do you can
also check what are the types of the
data so DF dot d types will give us what
are the various data types so all right
so you can also perform some basic
mathematical observations we have
already seen that mean we found out so
similarly if you do call the mean method
for the data frame object it will
actually perform or display or calculate
the mean for pretty much all the
numerical columns that are available in
this right so for example here applicant
income application income and all these
are numerical values so it will display
the mean values of all of those now
another thing that you can do is you can
actually also combine data frames so
let's say you import data from one CSV
file into one data frame and another CSV
file into another data frame and then
you want to merge these because you want
to do an analysis on the entire data
okay one example could be that you have
data in the form of CSV files one CSV
file for each month of the year January
February March each of these are in a
different so you can import them into
let's say 12 data frames and then you
can merge them together as a single data
frame and then you perform your analysis
on the entire data frame of the entire
data for the year so that is one example
so how do we do that this is how we do
again in this case we are not importing
any data we are just creating some
random values using some random values
so let's assume I have a data frame
which is by the name one and I assign
some random values here which is a 5x4
format so there are five rows and four
columns and this is how my data frame
one looks and then I create another data
frame which is data Frame 2 again random
numbers of the format five by four and I
have something like this now I want to
combine these two how do I combine these
two I can use the concatenate or concat
method and I can combine these two so PD
Dot concat and it takes the the data
frames one and two if you have more of
them you can provide them and it will
just simply add all of them merge all of
them or concatenate whatever you call
whichever term you call it will so of
course we have to make sure that the
structure Remains the Same like I said
this could be let's say sales data
coming for 12 different months but each
of the files has the same structure so
now you can combine all of them merge
all of them by using the concat method
if we have let's say structure is not
identical then what what will happen
let's say we have these two data frames
one has a column by the name key and the
second column is L well and a second
data frame which has a column by the
name key but the second column by the
name r well not L valve so you see here
the structure is not identical so you
can still combine them but then the way
they get combined or merged is somewhat
like this so it takes the key as a
common parameter between them some
common column has to be there otherwise
this will not work and then we have to
use merge instead of concatenate and
when we do a merge then we get the
result will be in this format what it
does is it uses the key as the Common
Thread between them and then it kind of
populates the values accordingly so if
you see here the first one had four and
bar for key and then it had L values of
1 and 2 right so if we go back 4 and bar
had one and two L values so that's what
we see here 1 and 2 whereas in the right
data frame we had Foo bar and bar as a
second time and then R values are 3 4
and 5. so what it has done for Foo it
has put for the existing right for 4 is
already existing because it has come
from left so it will just put the value
of r value here which is three similarly
it will put 4 here because for bar if
you go back for bar it is the value is 4
and since it has one more value of bar
it will go and add this 5 as well the
only thing here is that this one had for
example left had only two values and
only one value for bar but since we are
appending or merging and there are two
key values with the bar therefore it
will kind of repeat the value of L Val
here so that's what we are seeing in
this case right so L value appears twice
the number 2 appears twice but that is
because R value that are two of them
okay all right so that is how when you
don't have identical structure that's of
you merge now we will talk a little bit
about psychic learn so scikit-learn is a
library which is used for doing machine
learning of work for performing machine
learning activities so if you want to do
linear regression logistic regression
and so on there are easily usable apis
that you can call and that's the
advantage of scikitlan and it provides a
bunch of algorithms so I think that is
the good part about this Library so if
you want to use scikit-learn obviously
you need to import these modules and
also there are some sub modules you may
have to import based on what you're
trying to use like for example if we
know if we want to use logistic
regression again people who are probably
not very familiar with machine learning
that is a separate module for machine
learning you may want to take a look at
that but we will just touch upon the
basics here so machine learning has some
algorithms like linear regression
logistic regression and random Forest
classification and so on so that is what
we are talking about here so those
algorithms are available and again if
you want to use some of them you need to
import them and from the scikit learn
Library so psychic learn is the top
level Library which is basically SQL
learn right and then it has a kind of
sub parts in it you need to import those
based on what exactly you will be or
which algorithm you will be using so
let's take an example as we move and we
will see that whenever we perform some
machine learning activity those of you
who are familiar with machine learning
will already know this we split our
labeled data into two parts training and
test now there are multiple ways of
splitting this data how do we either
some people do it like 50 50 some people
do it 80 20 which is training is 80 and
test it is 20 and so on so it is an
individual preference there are no hard
and fast Rules by and large we have seen
that training data set is larger than
the test data set and again we will
probably not go into the details of why
do we do this at this point but that's
one of the steps in machine learning so
scikit-learn offers a readily available
method to do this which is train test
split all right so in this example let's
say we are taking the values X and Y are
our values X is the independent
variables and Y is our dependent
variable okay and we are using these two
and then I want to split this into train
and test data so what do we do we import
the train test split sub module from
within scikit-learn which is SK learn
right so within that we import train
test split and then you call this train
test split method or function or
whatever you call it and pass the data
so X is the all the values of the
independent variables and Y is our
labels so you pass X and Y and then you
specify what should be your size of the
test data so only one you need to
specify so if you say test size is 0.25
it is understood that train size will be
0.75 so you're telling what should be
the ratio of the split so technically it
doesn't nothing prevents you from giving
whatever you like here so you can give
test as 80 and train as 20 so whichever
way but then there's normal practices
you will have the training data set
would be larger than the test data set
and typically it would be 80 20 75 25 or
65 35 something like that right so that
is the second parameter and this is just
to say that you know the data has to be
randomly split so it shouldn't be like
you take the first 75 percent and put it
in training and then the next 25 percent
and put it in test so that so such a
thing shouldn't happen so we first set
the state random state so that the the
splitting is done in a very random way
so if they're randomly picked up the
data and then put it into training and
test and then this is results in these
four data frames so explain and X test
and white rain and white test okay so
that is basically the result it will now
that the splitting is done let's see how
to implement or execute logistic
regression so in logistic regression
what we try to do is try to develop a
model which will classify the data
logistic regression is an algorithm for
supervised learning for performing
classification so logistic regression is
for classification and usually it is
binary classification so binary
classification means there are two
classes so either like a yes no or for
example customer will buy or will not
buy so that is a binary classification
so that's where we use logistic
regression so let's take a look at the
code how to implement something like
that using scikit-learn so the first
thing is to import this logistic
regression submodule or subclass
whatever you call it and then create an
instance of that so our object is
classifier so we are creating an object
by the name this is a name by the way
you can give any name in our case we are
saying classifier we say classifier is
equal to logistic regression so we are
creating an instance of the logistic
regression variable or class or whatever
okay and you can pass a variable or a
parameter rather which is the random
state is equal to zero and once you
create the object which in our case is
named classifier you can then train the
object by calling the method fit so this
is important to note we don't call any
there is no method like train here but
we call what is known as there is a
method called fit so you are basically
by calling the fit method you are
training this model and in order to
train the model you need to pass the
training data set so X underscore train
is your independent variables the set of
independent variables and Y underscore
train is your dependent variable or the
label so you pass both of these and and
call the fit function or fit method
which will actually result in the
training of this model classifier now
this is basically showing what are the
possible parameters that can be passed
or initiated when we are calling the
logistic or the instance of logistic
regression so this is but you can also
look up the help file if you have
installed python so some of these are
very intuitive but some you may want to
take a look at the details of what
exactly they do all right so moving on
once we train the model by calling fit
then the next step is to test our model
so this is where we will use the test
data you need to pay attention here here
I am calling so there are two things one
is in order to test our data we have to
actually call what is known as the
method known as predict right so here
this is where so the training is done
now is the time for inference isn't it
so we have the model now we want to
check whether our model is working
correctly or not so what do you do you
have your test data remember we split it
25 percent of our data was stored here
right we split it into test and training
so that 25 percent of the data we pass
to and call the method predict so that
the model will now predict the values
for y right so that's why here we are
calling it as y underscore predict and
um if we display here as I said this is
the logistic regression which is
basically binary classification so it
gives us the results like yes or no in
this particular case and then you can so
this is what the model has predicted or
model has classified now but we also
know we already have the labels for this
so we need to compare with the existing
labels with the known labels whether
this classification is correct or not so
that is where is the next step which is
basically calculating the accuracy and
so on will come into play okay so in
this case the first thing most important
thing to note is we do the prediction
using predict and here we are passing X
underscore test and not train right in
this case we did x n and whiter so again
one more point to be noted here in case
of training we will pass both the
independent variables and also the
dependent variables because the system
has to internally it has to verify that
is what is the training process so what
it will do it will take the X values it
will try to come up with the Y value and
compare with the actual y value right so
that is what is the Training Method so
that's why we have to pass both X as
well as y whereas in case of predict we
don't pass both we only pass because we
are pretending as if this is the actual
data so in actual data you will not have
the labels isn't it so we are just
passing the independent variables and
the system will then come up with the Y
values which we will then okay remember
we also know the actual value so we will
compare this with the actual values and
we will find out whether how accurate
the model is so how do we do that we use
what is known as a confusion Matrix so
this is also readily available in the
python Library so so we import this
confusion Matrix and some of you who
already know machine learning will find
this familiar but those who are new to
machine learning this confusion Matrix
is nothing but this Matrix this kind of
a matrix which basically tells how many
of them are correctly predicted and how
many of them are incorrectly predicted
so the some of the characteristics let's
quickly spend some time on this
confusion Matrix itself this the total
numbers out here these are just the
numbers these are like number of
observations then the accuracy is
considered to be highest when the the
numbers or the sum of the numbers across
the diagonals is maximum okay and the
numbers outside of the diagonal should
be minimum so which means that if this
model was 100 accurate then the sum of
these two there would have been only
numbers in these two along the diagonal
this would have been 0 and this would
have been zero okay so that is like a
hundred percent accurate model that is
very rare but just that you are aware so
just to give an idea okay all right so
once you have the confusion Matrix you
then try to calculate the accuracy which
is in a percentage so there are two
things that we can do from a confusion
Matrix so that we can calculate from a
confusion Matrix one is the accuracy and
the other is the Precision what is the
accuracy accuracy is basically a measure
of how many of the observations have
been correctly predicted okay so let's
say this is a little bit more detailed
view of the confusion Matrix it looks
very similar like as we saw in this case
right so this is a two by two Matrix
that's what we are seeing here 18 27
2103 so 18 27 2103 now but what are
these values that is what is kind of the
labels are shown here in this so there
are altogether 150 observations so as I
said the sum of all these four right 18
plus 27 plus one zero three plus two is
equal to 150 that's the first thing we
have to observe the sum of all these
values will be equal to the sum of test
observation number of test observations
we have 150 test observations because
remember we had about 500 we split that
into 2575 so that is why we have 150
here and I think 350 in the training
data set okay so that we get the number
correct so that's the first thing now
this next thing is let's take a look at
the actual values this view is the
actual view so there are actually right
in the actual data we have labels yes
and no so as per the actual data there
are 45 observations tagged as no and
similarly there are 105 observations
that are tagged as yes or labeled as yes
okay now I know for the first time when
you're seeing this it may be a little
confusing but just stay with me okay so
this is the actual part of it and this
side tells us the predicted part of it
so our model is predicted and it has
totally predicted 20 of them as no right
so that is what this is totally 20 of
them as predicted as no and it has
predicted 130 of them as yes okay I hope
this part is clear so before we go into
the middle part let us first understand
what exactly are these numbers so
actually tag does no there are 45 total
actually tagged as yes there are 105 and
predicted no there are 20 predicted as
yes there are 130. this is the result
from our model okay this is the result
from our model and this is the actual
value which we already know because this
is our label data that's the first thing
now now let us take a look at each of
these individually okay now what are the
options we have once again okay so now
what is happening here let us look at
these these values so this 18 says that
these are actually tagged as no and the
model is also predicted as no which
means this is what is known as a true
positive right or true negative sorry
right which means that our model is
predicted is it correctly it is negative
because it says no so and it has also
predicted no so it is known as what is
known as true negative okay now let's
come to this side of it that way we are
talking about the diagonal remember I
said most of the values should be in the
diagonal okay so that means these 18 are
correctly tagged they are labeled as no
and our model is predicted as no so
these are correctly tagged and these are
known as true negative okay similarly if
we come diagonal it down there are 103
observations which are labeled as yes
actual value is s and our model is also
predicted as yes and these are known as
true positive values positive because of
this yes okay right so what is important
is this is true this is also true so we
have to make sure that the maximum
number of values are in the true section
okay true positive and true negative
that's the reason I said the Sum along
the diagonal should be maximum now let's
say if your model was 100 accurate this
sum in this case it is only 103 plus 103
plus 18 which is 121 but if our model
was accurate the sum of these two would
have been 150 that means it's a perfect
model okay all right now what else since
we covered these two let's also cover
these two so here this says that 27 of
them were actually labeled no but our
model is predicted as yes that means
this is wrong right similarly these are
two of them where the actual value is
yes but our model is predicted as no
that means it's a wrong prediction so
you get the point so therefore along the
diagonals are the correct values whereas
in other places it is all wrong values
or wrong predictions okay now how do we
calculate accuracy from this information
so the way to calculate accuracy is so
we say okay there are total observations
are 150 and what are the correctly
predicted values these are the correctly
predicted values which is 18 plus 103 so
this will give us our accuracy so 103
plus 18 which is 121 divided by our
total observations which is 150 is our
accuracy which is 0.8 or we can say it
is 80 percent okay now there is another
concept called Precision so Precision is
given by the formula true positives
divided by the predicted positives
totally predicted positives okay what do
we mean by that which are the true
positives here remember which are the
true positives we just recall we just
talked in the previous slide which are
the true positives you see here so this
hundred and three are the true positives
which means that the value is positive
actual value is positive predicted value
is also positive so that's why it's
called a true positive so 103 divided by
so that is our true positive 103 divided
by totally predicted as yes now what is
totally predicted is yes remember 130 of
them I have all together been predicted
as yes not that they are correctly
predicted only 103 have been correctly
predicted but 130 of them have been
predicted as yes so Precision is
basically the ratio of these two out of
the totally predicted how many of them
are actually true that ratio so 103 by
130 which is again about 80 percent is
the Precision that's how you calculate
Precision so this is just a simple
formula in the term that you need to
remember so accuracy is you need to take
total of true positive and true negative
divided by the total number of
observations whereas Precision is true
positives divided by the totally
predicted positives okay so that is our
accuracy and precision now what we did
the accuracy calculation was manual but
we can also use some libraries which are
already existing and the functions
within that Library so a psychic learn
provides one such method so for example
accuracy underscore score is one such
method so if you use that and pass your
test and predicted values only the y u
need to pass right the dependent
variable values so if you pass that it
will calculate it for you so in this
case again as you can see it still
calculates the same which is 80 percent
which we have seen here as well okay so
this can be done using the method great
so that's pretty much what we have done
here before we conclude let me take you
into the code and show you how it
actually looks okay so this is our code
let me run it okay one by one we've
already seen most of the steps in the
slides so I will but I will run this in
the actual jupyter notebook some of you
if you are not yet familiar with jupyter
notebook again there are other videos we
created on how to install Jupiter
notebook and how to set up jupyter
notebook and so on in this tutorial also
we there was one slide on how to install
Python and jupyter notebook if you have
not yet done please do that so that then
you can actually walk through this code
while you're watching this okay so what
are we doing here we are importing the
library's required libraries recall here
we have pandas we have numpy and for uh
visualization we have matplotlib and
this line is basically reading the CSV
file so we have the CSV file locally on
our local drive and this is where I'm
checking the data just so that I'm
starting with my exploratory analysis
how the data is looking so it looks good
I don't know major missing values or
anything like that so it will display
all the columns and it will show me the
first five rows if when I'm using this
head function and then I want to see a
kind of a summary of all the each of the
numerical columns so that's what I'm
doing here so these are the numerical
columns and it gives a summary like how
many observations are there what is the
mean standard deviation minimum maximum
and so on and so forth for each of them
and then you can do some visualization
so this is the visualization for this us
okay the next step is to view the data
data visualization and we will do that
using a histogram for a couple of these
columns so in this case I'm taking a
look at the loan amount and if I create
a histogram it displays the data here in
the form of a histogram one thing that
we gather from this as I mentioned in
the slides as well is how the data is
kind of scattered so while most of the
values are in this range 0 to 300 range
there are a few extreme values around
the 700 range so that is one information
we get from this histogram similarly for
the applicant income if we draw a
histogram something similar we can see
that while most of the values are in
this range 0 to 20 000 range there are a
few in the range of 80 000 and probably
65 000 and so on okay so the next step
is to perform data wrangling where we
will check if any data is missing and
how to fill those missing values and so
on so in this case we will just check
for all the columns how many data or how
many entries are there with missing
values so this is the result so loan ID
has all the columns or all the cells
filled gender has 13 missing values
marital status has three missing values
and so on and so forth loan amount as 21
and this is what we are going to show
you how to remove these missing values
so when you have missing values as I
mentioned in the during the slides there
are a couple of ways of handling that
one is you can completely remove those
or you fill in with some meaningful
values so in this case we will fill the
missing values with the mean value of
the loan amount so let's go ahead and do
that and now if we check here now loan
amount number of missing values is 0
because what we did was for all these 21
cells where the values were missing we
filled with the mean value of the low
number so now there are no more missing
values for loan amount we can do this
for other columns as well but this was
just one example so we have shown it
here okay so we we will run this for
credit history and loan amount term as
well and then if we calculate the mean
of pretty much all the numerical columns
that's the method call so DF dot mean
will give us the mean of all the
numerical values and another thing that
we can do is we if you want to find out
what are the data types of each of these
columns so you can call DF dot d types
and get the data types of course it may
not be that very useful most of the
cases is an object but for example this
one it shows as int64 and there are
float64 and so on and so forth now in
addition to doing the exploratory data
analysis we can do some machine learning
activity as well so in this case we are
going to do logistic regression so this
is the example that I have shown you in
the slides as well this is the actual
code for that all right so the first
step here is to import the libraries and
then the next step is to separate the
independent variables and the dependent
variables so X is our independent
variable and Y is our dependent variable
so we separate the data into two parts
and this will be our Target as well
right so that's how we separate it now
we have to split the data into training
and test data sets as I mentioned in the
during the slides we use the train test
split method and when we call this and
pass the independent variables and the
dependent variables and we specify the
test size to be 0.25 which means the
training size will be 0.75 which is
nothing but you split the data into
training data set which is 75 percent
and test data set in which is 25 percent
okay so once you split that you will
have all your independent variables data
in X strain the training data which is
75 percent of it similarly independent
variables for test will be in X
underscore test and dependent variable
strain
underscore train and dependent variable
test will be wired score test once we do
this we have to do a small exercise for
scaling remember we had some data which
was kind of very scattered there were
some extreme values and so on so this
will take care of that so that the data
is normalized so that before we pass to
our algorithm the data is normalized so
that the performance will be much better
the next step is to create the instance
of logistic regression object so that's
what we are doing here so classifier is
our logistic regression instance right
classifier is equal to logistic
regression we are saying so one instance
of plastic regression is created and
then we call the Training Method the
name of the method actually is fit but
what it is doing is it is taking the
training data X is the training data or
the independent variables and Y is the
dependent variables so we are taking
both of these and the model gets trained
so the method for calling the training
is fit okay so it gives us the output
and then once we are done with the
training we do the testing and once
again just to recall in the slides when
I was showing you the slides also I
mentioned we don't pass y here while we
are testing while for training we do
pass y but right so for fit we are
passing X and Y but for test we are only
passing X something you need to observe
because y will be calculated by the
model and we will then compare that with
the known value of y to measure the
accuracy so that's what we will do here
and the method that is called here is
predict so this will basically create or
predict the values of Y now we have in
this case a binary classification so the
outputs are yes or no y indicates yes
and then indicates no so y or n is the
output now how do we measure the
accuracy as we have seen earlier I
described how confusion Matrix works and
how we can use confusion Matrix for
calculating the accuracy that's what we
are seeing here so this is the confusion
Matrix and and you want to do the
measure the accuracy you can directly
use this method and we find that it is
80 so we in the slides we have seen when
we calculate manually as well we get an
accuracy of 80 so we will be learning
the data science algorithms today by
going through the following docket at
first we will learn the logistic
regression in R then we will move ahead
and know what is a decision tree
followed by that we will understand what
is clustering next we will learn
divisive clustering later the support
Vector machine advancing ahead we will
have the cayman's clustering and lastly
we will wind up the session by Learning
Time series analysis I hope I made
myself clear with the agenda now
escalating over to our training experts
hello and welcome to logistic regression
in R my name is Pete Ferrari and I am
with the simply learned team and today
we're going to cover logistic regression
in the r programming language logistic
regression is kind of a misnomer in that
one most people think of regression they
think of linear regression which is a
machine learning algorithm for
continuous variables however logistic
regression is a classification algorithm
not a continuous variable prediction
algorithm logistic regression is also
sometimes called logic regression in
this video we are going to learn why we
would use regression as a predictive
algorithm what is regression the
different types of regression as I've
already mentioned some regression
algorithms are classification algorithms
and some are continuous variable
algorithms why use logistic regression
what is logistic regression and then
we'll look at a use case a College
admission using logistic regression so
why would we use regression well let's
say we had a website and our Revenue was
based on the traffic that we could drive
to that website whether through r d or
marketing and we wanted to predict the
revenue based on site traffic and the
website traffic would be related to the
revenue we could generate the more
traffic driven to our website then the
higher our revenue or at least that's
what we would intuitively assume and so
we need to predict the revenue based on
our website traffic here we have the
plot of Revenue versus website traffic
traffic would be considered the
independent variable and revenue would
be the dependent variable often the
independent variable or variables if we
had more than one could be called the
explanatory variables and the dependent
variable would be called the response
variable however we typically refer to
them as independent and dependent
variables so our intuition tells us that
the independent variable drives the
dependent variable and if there is some
relationship between the two variables
and we would say that there is a
correlation between the two variables
and then we may be able to use the
independent variable to make predictions
about the dependent variable if you look
at the chart on the right you'll see
there is a clear Trend between website
traffic and revenue as website traffic
increases the revenue increases and we
could draw a line to show that
relationship and then we could use that
line as a predictor line so for example
what will Revenue be if our traffic is
4.5 K we see that when the traffic is
around 4 500 the revenue is around 13
000 and if we could draw a perpendicular
line from 4.5 K on the axis on the
x-axis the traffic axis up to the orange
regression line sometimes called the
line of best fit then we could draw
another line over to the y-axis the
revenue axis and we could see where it
lands and that would be the prediction
so in this example when the traffic is
around 45
000 hits the predicted revenue is around
13
000. normally we wouldn't actually draw
those lines we would generate an
equation and we would call that equation
a model and we could plug the in
independent variable into the equation
to generate the dependent variable
output which we would call our
prediction so what is regression
regression is a statistical relationship
between two or more variables where a
change in the independent variable is
associated with a change in the
dependent variable it's important to
note that at all variables are related
to each other for example a person's
favorite color may not be related to
revenue from a website here the change
in one variable height is closely
associated with the change in the other
variable age this makes intuitive sense
as you get older from when you're born
you get taller and it seems to make
sense if we plot that data we would see
those green points on the graph up to
some certain age where growth would
taper off and the plot in the middle of
the screen we see the clear linear
relationship between age and height
which is indicated by the solid red line
we sometimes call that line a trend line
or a regress Russian line or the line of
best fit and we see that the height is
the dependent variable and age is the
independent variable of course you might
ask doesn't height depend on other
factors of course it does but here we're
looking at the relationship between two
variables one independent and one
dependent age and height there are
various types of regression linear
regression logistic regression multiple
linear regression polynomial regression
and there are others decision tree
regression random Forest regression but
linear regression is probably the most
well known by definition when there is a
linear relationship between a dependent
variable which is continuous and an
independent variable which is continuous
or discrete we would use linear
regression when the Y value in the graph
is categorical such as yes or no true or
false they voted they did not vote they
purchased something they did not
purchase something it depends on the X
variable the logistic regression is when
the Y value in the graph is categorical
in nature for example yes no purchased
don't purchase voted did not vote and it
depends on the X variable notice the
trend line for linear regression and the
line for logistic regression they're
different more on that later and there's
polynomial regression as well when the
relation between the dependent variable
Y and the independent variable X is in
the nth degree of X sometimes we say an
nth degree polynomial of X in this
picture you can see that the
relationship is not linear there's a
curve to that best fit trend line so why
would we use logistic regression and we
need to understand why we would use
logistic regression and that linear
regression picking the machine learning
algorithm for your problem is no small
task and it really behooves us to
understand the difference between these
machine learning algorithms linear
regression answers the question how much
so in our earlier example if website
traffic grows then how much would
Revenue grow or as logistic regression
would answer what will happen or not
happen linear regression in general is
used to predict a continuous variable
like height and weight logistic
regression is used when a response
variable only has two outcomes yes or no
true or false often we refer to logistic
regression as a binary classifier since
there are only two outcomes so let's try
to understand this with an example let's
say we have a startup company and we are
trying to figure out whether the startup
will be profitable or not that's binary
with two possible outcomes profitable or
not profitable and let's use initial
funding to be the independent variable
here we have a graph that shows funding
versus profit and it appears linear once
again our intuition tells us the more
funding a startup has the more
profitable it will be but if course data
science doesn't depend on intuition it
depends on data but this graph does not
tell whether the startup will be
profitable yes or no it only states that
with an increase in funding The Profit
also increases that's not binary if we
wanted to predict how much profit then
linear regression would be useful but
that's not what we are being asked hence
we need to make use of logistic
regression which has two outcomes in our
case profitable and not profitable so
let's draw a graph again the x-axis will
be our independent variable funding and
the y-axis will no longer be the
dependent variable profit but it will be
the probability of profit for example if
you look at a company with a funding of
say 40 then the probability that that
company will be profitable is around 0.8
or 80 percent based on the best fit line
called a sigmaid curve in the example we
plotted several companies some with 10
15 20 some with 50 60 or 70 or 65 and we
indicated whether they were zero not
profitable or a one profitable on the
graph and this is how we should think of
logistic regression in this example
given the amount of funding we calculate
the probability that a company will be
profitable or not profitable and if we
use the threshold line of 50 then we
have our classifier if the probability
is 50 or higher the company is
profitable if the probability is lower
than 50 percent it's not profitable so
what is logistic regression this is a
linear regression graph so let's compare
linear regression to logistic regression
and take a look at the trend line that
describes the model it's a straight line
which is why we call it linear
regression but using linear regression
we can't really divide the output into
two distinct categories yes or no to
divide our results into two categories
we would have to clip the line between 0
and 1. if you recall probabilities can
only be between zero and one and if
we're going to use probability ability
on the y-axis then we can't have
anything that is below zero or above one
thus we would have to clip the line and
once we clip the line we see that the
resulting curve cannot be represented in
a linear equation so for logistic
regression we will make use of a sigmoid
function and the sigmoid curve as the
line of best fit notice that it's not
linear but it does satisfy our
requirement of using a single line that
does not need to be clipped for linear
regression we would use an equation of a
straight line Y equals B sub zero plus b
sub 1 times x x being the independent
variable y being the dependent variable
but as we've said we cannot use a linear
equation for binary predictions so we
need to use the sigmoid function which
is represented by the equation P equals
one divided by the quantity one plus e
to the minus y e being the base of the
natural logs then taking the log of both
sides and solving we get our sigmoid
function and graphing it we get our
logistic regression line of best fit the
applications for logistic regression are
endless here we might have a couple of
gardeners or farmers and one farmer says
I am planning to grow plants in my
backyard and I want to know whether my
trees will get infested with bugs which
I'm sure is a huge problem and these
days the fewer pesticides we use the
better off we are for health reasons the
young lady says I can use logistic
regression in a model to predict it for
you and the other farmer says well how
is logistic regression going to help but
we know that healthy versus not healthy
is a binary classification and we can
use a binary classifier like logistic
regression to solve this problem which
is what the young lady has in mind if we
have the data we can model the data and
generate a sigmoid function that best
fits our data and then use the
probability of a healthy tree versus a
not healthy tree or an infested tree
once again we see our sigmoid line of
best fit and some points that represent
healthy trees and not healthy trees once
again we plot our sigmoid curve scratch
that and once again once our sigmoid
curve is plotted and our threshold is
set almost always at 50 percent we can
use the sigmoid curve and calculate the
probabilities and hear the independent
variable of width which I assume is an
application of pesticides or no
pesticides to determine whether or not
the probability of healthy is greater
than 50 percent the threshold or less
than 50 percent the threshold and now
let's take a use case of a College
admission problem of course we can't
solve anything using data science if we
don't know what the problem is so we
need a problem statement and here the
problem statement is simple we are given
a data set and we need to predict
whether a candidate will get admission
in a desired college or not based on
their GPA and college rank so the very
first thing we need to do is imp support
the data set and if it's a small enough
data set and it can fit into our
computer memory then we are golden
otherwise we have some work to do the
data set that we were given is in CSV
format comma separated values CSV files
are easily imported into our environment
once we import the data we next need to
select and import the libraries that we
will need although R is a great
programming language with a lot of
built-in functions it is easily and
powerfully extended by the use of
libraries and packages then we need to
split the data set into a training set
and a test set it's important to note
that the data set that we've imported
that we were given has the GPS and
college ranks for several students but
it also has a column that indicates
whether those students were admitted or
not so what we're saying is that the
data set has the answers and if you
think about it it must how could we
possibly train a model have a model
learn if we didn't know the answer is
already that's why it's called machine
learning so we train the model and then
we test the model once we split the data
into training and test sets we will
apply our regression on the two
independent variables GPA and rank
generate the model and then run the test
set through the model once that is
complete we will validate the model to
see how well it performed so now it's
demo time so here we have our studio my
favorite IDE or integrated development
environment and I have a script already
composed to run our logistic regression
on the problem at hand which is our
College admissions problem so let's just
walk down the script before I run it and
talk about the various points in the
script so first Define the problem while
the problem was defined and you really
can't do anything in data science
without a clear problem definition once
the problem is defined you can load your
libraries acquire your data set the
working directory in my case I'm on
Windows explore the data Munch the data
if necessary and then prepare the data
scale the data if necessary split the
data into train and test data sets then
train the model using the training data
and run the test data through the model
and then validate the model for accuracy
and precision Etc so here we are going
to load our libraries I'm going to use a
package called CA tools and now that the
library is loaded I'm going to set my
working directory and if I come over
here to the files tab you'll see that
there is my working directory and
networking directory there's a file
called binary.csv and that's the CSV the
comma separated value file that the
college gave me I'm going to ingest and
then look at that data and as you can
see it has four columns GRE GPA rank and
then the answer column admit whether or
not someone was admitted which would be
a one or not admitted which would be a
zero and if we come over to the right
and we can look in see some of the first
few values of each of those four columns
now let's split the data we're going to
take this my data frame and split it
into two a training set and a test set
and the ratio we're going to use is 80
20. so 80 percent of the data will go
into the training set and 20 will go
into the test set now that ratio could
be 60 40 it could be 70 30. typically it
really depends on the size of your data
how much data you have but this is for
our example and for our purposes 80 20
is perfect next we'll do a little data
munging in general you munch the data
early on after ingestion and you really
have to be careful in this case we don't
have any missing values so we don't have
any real outliers our data was pretty
clean when we got it and ingested it but
in general that's not the case and a lot
of work and a lot of attention needs to
be paid to the munging process here
we're just going to take our data the
admission column and the rank column
them and convert them to categorical
variables right now they're integers as
you can see on the right hand side and
once I run these two lines they will be
converted to factors and all the fun
stuff we're going to use the glm
function the general linear model
function to train our logistic
regression model and the dependent
variable is admit and the independent
variables are GPA and rank and the
little tilde sign here says the
dependent variable will be a function of
GPA and rank the two independent
variables the data will be the training
set and the family will be binomial and
binomial indicates that it's a binary
classifier it's a logistic regression
problem there it is we ran our model and
there's a summary of our model you can
see that there is some statistical
significance in GPA and in Rank by the
coefficients and output of the model so
next let's run the test data through the
model
and once we have done all that we can
now set up a confusion Matrix and look
at our predictions versus the actual
values again this is important we had
the answers and now we took and we
predicted some answers so hopefully our
predicted answers match up with the
actual answers we'll run a confusion
Matrix as you can see the predicted
values versus the actual values and it's
important here to note that if it was
predicted faults and it was actually
false and we see 189 or if it was
predicted true and it was actually true
which is 27 we did well on those but
there are a few that were predicted
incorrectly some were predicted true but
were actually false some were predicted
false that were actually true and now
we'll confirm what our percentage
accuracy is 72 percent not bad but we
could probably do better how we'll leave
that to another video what is it
decision tree and here we have kind of a
new looking front panel of a car driving
down the street and we have left or
right so you're coming to an
intersection you got to decide which way
to go you're going to go left are you
going to go right away or go so do you
go right away we have a green light so
probably want to go right away after you
check for cross traffic make sure no one
is asleep at the wheel or talking on
their cell phone decision tree is a tree
shaped algorithm used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so another
example is we have a shopkeeper very
nondescript shopkeeper kind of a little
spooky without the eyes so we have a
shopkeeper and he wants to organize his
stall so I must organize my stall just
looking around trying to figure out what
the best way to stack all his goods are
and he has some broccoli some oranges
and some carrots this is kind of a nice
example because we can see where some of
the colors and shapes overlap or others
don't and so you can create a tree out
of this very easily and say is it
colored orange if it's not what goes
into one stack well that happens to be
all their broccoli and if it is colored
orange then it goes into another stack
and you're like well that's still kind
of chaotic you know people are looking
at carrots and oranges a very strange
combination to put in a box so the next
question might be is it round and if it
is then yes you put the oranges into one
box and no that means it's carrots we
put it in the other box and you can see
that a lot of stuff we do in life easily
can fall into like a decision tree how
do you decide something you know does it
go left does it go right yes or no and
even with this we even have a number of
different end choices broccoli carrots
and oranges so what kind of problems
does this solve what problems can be
solved by using a decision tree
classification we're dealing with the
classification identifying to which set
an object belongs and from our example
we just looked at a character's orange
well broccoli is green I guess it's
nondescript day
without the faces I love it because a
lot of times you're looking at data you
really don't want to know what exactly
the data is but you're sorting it so you
know like this kind of represents data
that you really don't know exactly where
it's coming from just kind of a fun
thing regression regression is another
problem the biggest can be solved
regression problems have continuous or
numerical valued output variables
example predicting the profits of a
company so we have two different primary
problems we solve for we solve for
classification is it green or is it
orange and we solve for regression which
is a continual number so you know if
you're doing stocks this would be a
dollar value that ranges anywhere from
zero to thousands of dollars how does a
decision tree work terms you must know
first so before we dive into the
decision tree let's look at some of the
terminology for the decision tree we
have notes each internal node in a
decision tree is a test which splits the
objects into different categories and
continuing with our vegetable stand
example we have broccoli no it's not
orange and oranges and carrots are and
so on and so you can see right there
where it splits that's a note where the
Orange is split and the carrot split
into each of their own groups and each
one of the splits is a node the very top
node of the decision tree is called the
root node so the very first one we
started off where we split it is a
colored orange and you can see that
pointed nice and neatly there this is a
note at the top each external node in a
decision tree is called the leaf node
the leaf node is the output and you can
see down here we have our broccoli and
we have our carrots and our oranges each
one of those is a leaf node so to
quickly rehash our terms we have
anywhere there is a split that's a node
and if it's at the very top of the tree
that's the root node and if you're at
the very bottom of the tree where you
have a final answer that is the leaf
node so you have a root note a leaf node
and then everything else is just a
generic node so one of the most
important terms are Concepts that powers
a decision tree is entropy entropy is a
measure of the messiness of your data
collection the Messier or more random
your data higher will be the entropy so
we can see here we have broccoli oranges
and carrots are all mixed together and
it's very chaotic and you could say this
collection has high entropy and if we
sort out the carrots out of here in I
guess just one of the broccoli I don't
know where the other broccoli went you
could save it if you have just the
oranges and the broccoli this is a lower
entropy they say this collection has low
entropy but really it's higher and lower
one is higher or lower than the other
one and from entropy we get Information
Gain information is the decrease
obtained in entropy by splitting the
data set u based on some conditioning so
we have our information one which we can
actually there's actual computations and
so we have our E1 which is put in there
for our entropy value at the base node
or root node and we have two A2 which is
a leaf node and so we can actually
calculate there's calculations for an
entropy and we actually look at those
two values the entropy and we take that
we have E1 is greater than E2 therefore
the Information Gain equals E1 minus E2
so here we have another example and
we're actually going to attach some
values to this one in just a moment so
we're going to move away from the fruit
and we're going to go to everybody's
favorite thing organizing a child's
closet and here we have a young lady
waving at us hi my cupboard is a mess I
must organize my stuff and we're going
to classify the objects based on their
attribute set using a decision tree we
can look at it we have a shape a size
and a label and a number and I'm going
to go ahead and look at the labels
because that's really what's key on this
one is we have a couple balls in here
some books cards and blocks and if you
look at that when you have something
that's a ball that's round so it's a
very different shape the book card and
blocks are all rectangular but the books
are a different size so the books have
like a size six I guess you think of
them as very large cards a size four so
they're not quite as big and the blocks
or little Lego blocks it looks like kind
of fun to stack on there and then we can
actually quantify this and we can
actually say hey we got we have
different number we have two balls two
books one card four blocks and we split
at each level based on certain
conditions on the attributes so
splitting aims at reducing the entropy
and we have a formula our first actual
formula you'll see happens in the
background when we're running these
different modules in R it looks a little
complicated but we can break that up and
simplify that for you so we see this
formula the negative sum of I of x
equals the probability of one value of x
times the log of two probability value
of x the term probability value of x
it's probably easiest to understand if
we go ahead and put the numbers in there
so you can actually see what we're
looking at and so we plug the numbers in
there we look at the probability of the
value of x well if we do the number of
balls there's two balls and the total
number of items is nine so we have a
probability of any one object being two
out of nine so that's pretty
straightforward math when you look at
the numbers you can actually see that
very easily and so we can calculate our
entropy of two over nine times the
squared log of 2 over 9 plus 1 9 the log
squared of one over nine plus four over
nine times the log square root of four
over nine so our whole group is the sum
of all the entropies and so when we look
at that we have the entropy of the balls
which is the two over nine rectangles
two over nine are mean books two over
nine cards one over nine and the blocks
four over nine and we just plug those
numbers in there and we get an actual
value of
1.8282 so what the heck does a module do
with that number well now we must find
the conditions for our split every spit
must give us the maximum achievable
Information Gain and so we have our
first entropy value 1.8282 and it turns
out when you compare all the different
individual entropy values for the balls
for the books for the cards for the
blocks rectangles you find out that the
biggest split is going to to be in the
shape and it's going to be our first
split will be on the shape that will
directly segregate the balls out so
they'll split up a huge group from one
from the other and so we compute that we
end up with E2 which is all of our
rectangles and then we have a leaf node
which is the balls and so with our E2 we
see that 1.3784 is much less than the
1.8282 and so our second split will be
on the size as that will directly
segregate the books and so when we do
that we can see that if its size is
greater than 5 all the books go to one
side while the cards and the blocks go
to the other and again we have our E3
which equals 0.716 which is considerably
less than 1.3784 so our entropy is
dropping each time we do a split and of
course our third split we look at and
we'll see that we're left with size
again and we'd split it with four and
two in this case if it's greater than 3
it's going to be a block if it's less
than three it's going to be a card and
at this point each of our object is
looped into similarities so they all
look the same and we end up with an
entropy of zero and so you can see as we
go down the entropy continues to get
less and because it hits zero we can say
all our objects are now classified with
a hundred percent accuracy if it was
only that easy to clean the kids closets
and you can see we have a nice drawing
that shows you where all the leaf nodes
are in this in which case we have the
balls the books the blocks and the cars
are part of the leaf note and the root
node is our first split and we have all
the objects and we're splitting them by
a rectangle by shape so we're going to
dive into the use case and in this case
we're going to get a little morbid there
I am behind my desk that's definitely
not me I don't have that big of a chin
but I do usually have a cup of coffee
and it looks kind of like that it's
sitting next to me on my desk so the use
case is going to be survival prediction
in R and let's Implement a
classification of data set based on
Information Gain this is going to use
the ID3 algorithm don't let that scare
you that's the most common algorithm
they use to calculate the decision tree
there's a couple different ways they can
calculate the entropy but the formula is
all the same behind it so ID3 is most
common in R and then we'll be using the
r studio for our interface for our IDE
so our studio is free to download
There's the free version of course
there's an Enterprise version and
there's all kinds of different choices
in there but one of the wonderful things
about R is that it's been an open source
for a very long time and is very well
developed it's also very quick and easy
to use on a lot of things so this is
going to be in the rstudio IDE there's
some other interfaces but this is I
would suggest using the r Studio it's
the most common one and if y'all
remember I said this is going to be a
little morbid because we're talking
about how many people survived and died
on these ships so in this case we have a
nice vacation cruise ship going on there
or I'm not sure what that is with the
tugboat so we have a ship and it had 20
lifeboats the lifeboats were distributed
based on the class gender and age of the
passengers we will develop a model that
recognizes the relationship between
these factors and predicts the survival
of a passenger accordingly so we want to
predict whether the person is going to
survive or die in a shipwreck and we'll
be using a data set which specifies if a
passenger on a ship survived its wreck
or not so we're going to look at this
data and if you open it up into a
spreadsheet and don't forget you can
always put a note on the YouTube video
down below and we will be monitoring
that you can request the data set sent
to you or more correspondence you can
also go to
www.simplylearn.com and request this
data set too and it's just a simple CSV
file we'll send out and you can see
right here we have survived so one
indicates a person survived the wreck
zero they didn't make it they were under
the water and the luxury class of the
cabin so if they were in you know what
what value is like one two three
depending on what luxury class they were
and then we have this is kind of
interesting how many siblings they had
so it's kind of an interesting added
statistic in there number of parents on
board word and disembark location so
where the destination is
so here we've opened up our studio and
if this is your first time in our studio
you'll notice that it opens up
standardly in three panes the upper left
is for text documents you're bringing in
and then the bottom left is the actual
commands as they're processed
your environment set up on there on
there I've zoomed in with my font a
little oversized in fact we can even
take that up one more size and see if we
can get a larger setup view here we go
so I've set the font for 18 probably a
lot larger than you need to work on a
project and the first thing I'm going to
do is on the right hand panel we're
going to be working with plots so I'm
going to click on the little plot tab up
there and I'm just going to collapse
that over to the side for right now and
then I'm going to open up the bottom one
and you'll see on the top I actually
have two different tabs team in the back
was really nice to set up the whole
package for what we're working on here
and we'll go through this line by line
and then you'll see as we execute the
different lines they're going to appear
down below so I'll be flipping between
these two left panels and let me go
backwards just for a second here we're
going to do this part so let's take a
look at the installs
and if you look at the installs and I'm
not going to actually run the installs
because I've already run them on my
computer so these are already installed
in my R packages and I don't have to
rerun them we have a number of packages
we're bringing in to our because that's
how R functions and the first number of
packages are all part of our decision
tree so our F selector our R part our
carrot with the dependencies equal true
and our dplyr and our R part plot oh
that's a mouthful you'll need to import
all of these in your setup
and the first one which is your F
selector compute your chi-squared your
information gain your entropy all of
those formulas are a part of the F
selector so when you bring it in that's
what that package is the r part is your
partitioning of your decision tree so
these are your primary two decision tree
modules that we're bringing in
the carrot is apart for actually
splitting apart the data because we
always want to do a test set a train set
so they built a package carrot to kind
of handle all those you'll see carrot
used a lot of other machine learning
packages when you're doing the r the
dply-r is for mutating your data so you
can sum it and classify it you can
filter it that's all part of the DPL yr
and then our part has another piece
called plot and that's where helping us
plot the data as we sort out the tree
because the art part creates a tree but
you still want to be able to plot it and
then we come down here and we have some
additional tools we're going to work
with a spreadsheet this is a Microsoft
Access spreadsheet so we're going to use
the xlsx package to import it and you
might think that's a little outdated
because I know when I'm doing any new
projects I try to save my data into a
comma separated variable file or
something more standardized but a lot of
times you'll end up with these old
databases where they're exporting data
and their export comes in an Excel
spreadsheet and guess what this is a
Time Saver instead of opening up the
spreadsheet re-saving it as a comma
separated variable file you can just
open it right up in R which is really
nice and then we have our data tree
which goes right along with displaying
the data so we have our rpart.plot and
we're going to put that into the data
tree so have a nice visual and finally
you need to install the package C8 tools
and CA tools is another package used for
General manipulation of your data and
your vectors so it also has a split
function for training data I know we
have another package for that but both
those packages fit in there as far as
the working with the data so a lot of
times you'll see these loaded in there
together and you'll see in a moment
we'll import the elementary stat learn
package also which is very generic
working with data setup as far as
statistics and data processing now
because my Java setup is a little
different because I use a developer
package well the latest update of the
developer package and the r are
struggling to see each other so I need
to go ahead and add this in chances are
you don't need to worry about this but
if you do get an R Java error you'll
need to go ahead and add an align to set
your environment and I'm just going to
set the Java home I went ahead and
looked up the last solid JRE folder and
you'll see under program files Java you
can look them all up to make sure it's
the JRE and not the jdk the difference
between those is one is a standard
executable that's a JRE folder that is a
folder not an actual file and the jdk
deals more with developer side on things
and I'm going to go ahead and run this
and if you're in R you can run these
lines individually by holding your
control key down and entering the enter
button so I've now loaded my workspace
and set my environment here and we can
actually look and just make sure a
system Dot
git environment is the opposite one and
I can just look real quick and say hey
what's my Java home set to
and if I run that
whoops I forgot to put brackets around
it
there we go
and if I do that you'll see that it's
set to the jdk18025
and again we're looking at the split
screen and let me just move this up here
a little bit and let me hit the enter
key a bunch of times it comes up on the
screen
but when I run it and I'm running it
line by line the Run text then appears
down here and this is what's actually
being executed so I could just as easily
type it in down here as putting it up
here on the top part
that's just a little bit of rstudio and
working with rstudio
so I've shown you the installs and again
I've already installed it on my computer
so it takes a minute or two to install
it
you need to go ahead and then bring
those libraries in to work with so
basically we're going through all the
different libraries we're using elements
that learn is a standard R Library so it
should already be installed you don't
have to install that one and I can just
go right down them and you'll see as I
install them if they show up on the
bottom and every time you go to run a
new setup
you need to go ahead and install these
pull the libraries in you got to tell
the tell your r or Studio these are the
libraries you're using
and you can see right down here that
as it executes them
it has all the different things one of
them had like a couple different parts
to it so loaded the lattice and the
ggplot2
really nothing to worry too much unless
you see errors down here so if you start
getting error codes then you might have
to go troubleshoot and find out why it's
not installing correctly or did it
install correctly
but it's pretty straightforward we've
covered all the libraries
and again if you want a copy of this
code simply add a comment Down Below on
the YouTube video or visit
www.simplylearn.com and they'll be happy
to send you a copy of the base code
the next step is we want to go ahead and
load the Excel file
after setting the working directory
so let's go ahead and set the working
directory scwwd is a command for that in
r
and then this is wherever you have
downloaded your data to in your files
too A lot of times if you get the zipped
framework sent to you it will unzip all
into one location and then you got to
let that location up and put that in
here a lot of times it automatically
sets that for you if you all if you're
working in the same directory in this
case you can see I've pasted in
very lengthy
path because I have this as part of my
documents my business docs and simply
learn and marketing videos and so on
I'll go ahead and do the control enter
and run that so now my working directory
is set and you can see down below it's
executed this on the console down here
and if I open up that working directory
you can see I have a ship XLS file
that's the data we're going to look at
and we looked at that on the slide you
could also open this up and I will want
to go ahead and open it up with a
notepad
by default it'll open up as a
spreadsheet I hope it is an Excel file
so I would have to open it up as an
actual spreadsheet wasn't thinking that
one through this is one of the problems
with saving stuff as a proprietary ship
file even to something as widely used as
Microsoft Excel spreadsheet
but when we open up the spreadsheet you
can see here the same data we looked at
before
this was coming up in this file
and so I take this file we can go ahead
and let's um create a variable we'll
call it add
and we're going to assign it
that's what the lesser than Dash means
if you've never used R before and we're
just going to assign it a ship XLS
and then I'll do my control enter and
you'll see it run down here ship add is
assigned ship.xls our Excel spreadsheet
on there
and then let's have another variable
we'll call it DF for data frame and
we're going to assign this read
xlsx and that's from the library SLX so
we just import it
and we want to read the file add
because we just created that variable
with the ship.xls
and I didn't catch this right away but
you'll see when we open it up in a
spreadsheet
it might be hard to see in your video
screen it has a the first sheet is
called ship one so this data is on the
page ship one and the Excel spreadsheet
and we got to make sure that we tell the
reader to look for that
and so we can simply do that in our
command line
sheet name
equals
and it was ship one on there
so we go ahead and hit the control enter
and run this line and it brings it down
below and executes it it's now reading
in that data from that spreadsheet
that's what's going on and it takes you
just a moment to go ahead and read it in
and then with any data processing we
need to select the meaningful columns
for the prediction so this is important
that we sort through our data and set it
correctly so that whatever we're feeding
it into can read it correctly
we can do this with DF
we're going to reassign it and we're
going to select
and from there we're going to select DF
as our main data frame going in
and then list the columns that we want
and if we look at the data we have
survived we have a class we have a name
sex and age
and then there's siblings and parents we
really don't want to know the name
because we don't need the name to run
this on here and then we got to decide
which one of these we want to work with
and the guys in the back played with the
data a little bit and they came up with
the following columns
survived which we need definitely
because that's what we're actually
trying to figure out is whether we're
going to survive or not what class there
which which class are the boat they're
writing in their first class second
class
sex in h we went ahead and dropped the
rest of them at least for this example
and so I can go ahead and do my control
enter and you'll see it execute down
below
and now DF equals these three columns or
four columns plus our answer is one of
those so I always think of it as three
columns plus one but it is four columns
and then for the next step we want to
make sure the data is in the right
format so again I'm going to resign the
DF and we're going to mutate the data
and that means we're going to take each
one of these columns and we're going to
let it know what kind of data that is
so our first one is I'm going to put the
database in or the data frame in there
DF
and then we want to go ahead and do
survived equals a factor of survived so
this is a yes no it's letting us know
that this is a zero or a one on here
and then for the class and the age those
are as numeric
there's other ways to process class
since it is a very restricted number on
there but for this we're just going to
do it as a numeric value same thing with
age and you see we didn't do anything
with sex we're just going to leave that
as male or female and that's fine for
this because it'll pick up on that when
we run the decision tree
and we'll do our control enter so now
I've assigned the new DF and you can see
it comes down below as we run it on
there so this is now loaded in DF now as
a mutated data frame that we started
with and we've set it up with our
specific columns we want to work with in
this case survived class sex and age and
we've formatted three of those columns
to a new setup so one of them is Factor
that's what we use for survive so it
knows it's a zero and one and then we
switched class and age to make sure that
knows that those are numeric values
now that we have our data formatted we
need to go ahead and split the data into
a training and testing data set and by
now if you've done any of our other
machine learning tutorials you know this
is standard you need to have your data
that we train it with and then you got
to make sure it works on data that it
hasn't trained on so we'll go ahead and
do the splitting let's take a look and
see what that looks like
and when you're dealing with splitting
the data we're doing a little Randomness
so we'll just go ahead and set the seed
to one two three you can set that to
whatever number you want that's just
setting the random variable seed and
we'll go ahead and hit enter on there
and to do this we're going to create
sample and we can go equals
sample Dot split
so we're splitting up the data this is
actually creating a whole new column
trues and falses that's all stored in
Sample and we want our data frame
a dollar sign to denote that we're going
to be working with
one of the different columns
a nice thing you can do in r
and it actually shows my survived so DF
survived
and then our split
ratio split ratio I want to add a period
in there which it doesn't have our split
ratio is going to be equal to and we'll
do 0.70 so we're going to take 70
percent of the data of those that have
survived and we're going to just
randomly select which one of those we're
going to use for training and which one
we're going to use for testing it out
later
we'll hit our enter and load that sample
in there
and then we'll go ahead and create our
train and our test data set so I'm going
to set train equal to
this is where it kind of gets
interesting because we're going to do
subset
and then we're going to do a subset of
our DF
where sample
and then we're going to use equals since
this is a logic gate so wherever it sets
sample equal to true we're going to use
that line in DF
and this is of course R so it's all
capitalized for true
so here's our trained data frame we're
creating so when I execute this and it
shows up down below it goes up and says
hey this sample created a whole bunch of
trues and false and we're going to take
those trues and falses and wherever
we've created a true which is 70 of the
data randomly selected that's what we're
going to train the data with and then we
need our test set
and that's a subset just the same thing
we just did
we need our data frame and this is where
the sample equals
false and again when I hit Ctrl enter it
goes down this line of code and it runs
it down here in our terminal window
so now we've nicely split our data up we
have our training data set and our test
data set
and this is of course where we get to
the point of what we're actually trying
to do which is create our decision tree
and we're going to do that by training
the decision tree as a classifier
this always gets me you spend all this
work sorting through the data
reformatting it in this case mutating it
and filtering it and creating a train
set and a test set
so we're going to go ahead and create
our tree and we're going to assign to it
the r part that's the module we imported
and for the our part we need to go ahead
and
do What's called the survived and this
is kind of a little complicated to look
at so let's just take a quick look at
this I'm going to go backward on it
first part is we're importing the data
so data equals train
and then we want to know is who survived
so the first part we put in here is the
column we're looking for is survived and
you'll see that we have added the
squiggly Mark in the period because
we're looking just for this prediction
we just want to know who survived on
these boats for programming our decision
tree and I'll go ahead and hit Ctrl
enter
and we've now created the tree that tree
with all those values is now in there
and if you remember from before we've
taken from when we did the closet full
of clutter we've done the same thing
where we have our entropy and that one
line computes the entropy and the
increase in information and it brings
the entropy down to zero so it's the
same thing but instead of applying it
here as we did visually to the closet
it's now applied it to the data on the
ship it's now looked at the survival of
zero or one based on female male what
age they are or the best split of the
age is and what class they were in you
can see we have first class if we go
down there I think there's
a third yeah third second so I guess
it's class one two and three but you can
see that's how it's split it up so let's
use these different variables to
minimize the entropy and maximize the
information gain and that's all done in
that one line of code
boy we worked on all that theory and uh
computer doesn't appear in one line of
code it's important to understand the
theory behind it so you know what's
happening because as they make changes
to these or if you add special settings
on it you have to know what they're
doing
but in this case you can see that we
once we get it formatted the computer
does all the heavy lifting for us
and it never ceases to amaze me just how
advanced machine learning has become
these days and how easy it is for people
to get into it and learn the basic
setups and be able to compute your
different machine learning models
once we've computed our decision tree
and we built it on there we want to go
ahead and start doing predictions with
it
and what are we going to predict well
you remember that we created our data
with the train and test so we have our
training data which we program the tree
for now we want to see what it does with
the test data so let's see what that
looks like and we can wrap that into
tree Dot
survived dot predicted
we'll assign that to our prediction
this is nice because we have the command
predict from one of the modules the r
part module that we brought in
I can simply put in here that we're
going to use the tree
we have our test data
and type
this happens to be classification or
class
and this would also be if you were to
distribute this tree you could save the
tree and send it someplace else to be
used on new data this would be the
command line we would use to predict on
here predict you have your tree saved
you have whatever data you're putting
through and you let it know it's doing a
classification on the tree and we'll go
ahead and run that so now that's loaded
up
and now that we've gone through we've
trained our tree we run a prediction on
our test data remember the tree had
never seen the test data till now so
we've already built the tree and we're
just testing it out
and to do that we're going to use a
confusion Matrix for evaluating the
model and let's see what that looks like
and that is simply confusion
Matrix so once we have the confusion
Matrix which came in from our carrot we
need to compare the two different pieces
of data and the first one is going to be
the value we generated up here we have
tree.survived.predictit
so I'm just going to copy that and paste
it down here
and then the second one is the tree
we're comparing that to
let's see survived
but it's not the tree it's easy to get
confused sometimes when you're moving
these things around
we're going to compare it to test the
test group of data specifically
survived call
s go ahead and run this and see what
that looks
that look so let's go ahead and scroll
up a little bit here to where it first
executed the confusion Matrix
and the most important part of this is
that we have our prediction
and our reference and so zero means they
didn't survive one means they survived
we had our prediction on here
versus the actual and that's what these
two columns mean
and so when you read this little chart
we have zero zero so both the prediction
and the actual reference said we agreed
on 218 values for people who did not
survive a little Grim there for data
remember talking this is morbid data
talking about death rates of people on
boats it says that they both agreed on
93 so 93 of them where we predicted the
value that they would survive and the
reference came up the same likewise the
predicted were predicted one
and the reference said no these people
didn't survive well our prediction got
25 wrong and where it says they didn't
survive and they actually did it got 57
wrong
and come on down with the um
confusion Matrix and we could look at
the accuracy comes up with the
0.7913 or 95 CI
so yeah you're looking at pretty high
accuracy
information ratio p-value when you're in
statistics of p-value is one of the
things we look at remember that there
are certain values if it's over 0.95
is considered highly valuable variable
0.5 means maybe anything less than that
means it's probably not connected
you have to look up the p-value a little
bit more on that and then they just have
more information down here
which you can go through and
which you can go through but the
important part is that we have a very
high accuracy that's what we're looking
at this is a very accurate model and
again always remember the domain you're
working with certainly this is an
example and this is a good little piece
of data you can play with but if I was
say betting money and my accuracy came
up as 79 or over 95 percent CI I'd be
like yes let's bet on that that's a good
bet but if I was betting on a life or
death situation I might kind of rethink
that a little bit if that is if it's
going to affect my life that way you can
also look at this as if you're trying to
figure out where the boats go in this
particular example and where to put the
lifeboats that's actually pretty good
you'll be able to say hey we can
increase the chances of people surviving
this accident by putting the lifeboats
in the appropriate places
and then this is always fun let's
visualize the decision tree one of the
cool things you can do inside of R real
easily and if you remember one of the
modules we brought in has a r part Dot
Plot we're going to take that and we're
going to dump that into the PRP for the
tree
and let's go ahead and hit enter
and this is fun let me move this over so
you can get a nice view you can see how
it actually split it up and let me just
rerun that again another expanded the
right you'll see right here where one of
the biggest splits was whether the
person was male or female I didn't know
that was probably one of the biggest
breaks as far as lowering the entropy
and increasing the Information Gain
and then once you know sex is male I
came down here and it said you're not
going to survive I guess the men were
throwing themselves overboard on the
boat I don't know maybe they're being
very gallant and letting the women and
children survive which is what I hope
that means so once you know that it's
not a melon it's a female passenger if
you were in class three or above you had
a high chance of surviving if you're
less in class three in class one or two
then it was based on age and so you can
see where age was greater than 19. so
anybody over 19 had a high chance of
surviving and then if they were or less
than 19 I'm sorry if the age was a
little confusing on that one but if the
age was one side of 19 they probably
survived if not then they said hey if
your age is less than 60 you probably
survived and if not
people who are older probably didn't
make it out of the boat in Time so
elderly have a much higher risk which
you would expect you'd expect that to be
the case that elderly would have a hard
time getting off into the lifeboats and
they would probably step aside for the
younger generation I know I would and
you know so that might be more of a
social Norm statistic as opposed to
survival rate let's go and put this back
on over here
and see what else we can do with this
data
if you're working with the shareholders
it's good to have a lot of tools as far
as how to display you know they'll want
to see this number here where you say
hey this is what the confusion Matrix
looks like and we got a 0.79 accuracy
that would be like the number one thing
you'd talk to the shareholders about
other data scientists care about the
p-value and the information rate and
information like that but this is really
the core right here this is how much
data and this also shows them how much
data was looked at so you have a you can
actually just add these numbers together
and you can see how many different
people were in the this particular data
set I mean lines in the data set but we
go ahead and visualize the 2D decision
boundary so let's take a look at that
that's kind of fun it's a little bit
more complicated so I'm not going to go
into it in too much detail today now the
guys in the back put this code together
for me which is great so I'm just going
to paste that in because like I said
we're not going to go into it in too
much detail I want to just show you that
there's some other ways to display the
data that is not just this nice little
tree over here which gives a little bit
more maybe view of what's going on as
you dig deeper into Data that becomes
very important it might not be something
so simple where you can look at in this
case one two three four notes so they
have four major nodes that they split
the data in
so we're looking at this we're going to
go ahead and create a set test equal to
test comma c241
we're just assigning color column to go
with the data okay that's what that is
take set and have it equal to test
and we'll do X1
set that equal to minimum set
of one minus one Max set of one plus one
by equals .01
they had to play with these numbers a
little bit to get this to look right so
we're creating a sequence on here from
our the set we created so our test set
and then X2 is also creating a sequence
based on different values
and then we're going to put that all
into a grid so let's go ahead and run
that through all the way down to the
grid
and they'll make a little more sense
when we add the column names to the grid
okay
so we'll set our test we'll create our
X1 sequence and our X2 sequence remember
a sequence is just like in this case
starting with 0.1 it's like
0.01.02.03.04 so we're going to create a
grid with all these values separated by
0.01 so that's all we're doing right now
let's go ahead and set that up there's
our grid
and then we want to go ahead and create
some column names and for our grid set
we're going to have our C class and age
so we're just doing the class and the
age on here remember we did 6 also
before so it's just class and age on
this chart that we're pulling out
and we're going to create a y grid and
the Y grid is going to equal tree
survived predict it
okay so here's our y grid and our
predicted values on there
now we're actually going to do something
we're going to plot and we're going to
plot the Y grid oops hold on let me see
we're sitting here
and let me go ahead and just run the
plot it's got to be the
there we go so let's take a look at this
and see we can make a little sense of
this code and what's going on here
because there's a couple things going on
what we've done is we've plotted a
decision tree classification data set
we've done our plot and we have our set
so our set's equal to our test data and
then we have our main we went ahead and
give it a name decision tree
and then we have our x-lab
and our X limit and we've set those up
as class with our y lab equals age and
our X limit equals range X1 comma y
limit equals range of X2
and when you look over here you can see
that we have age on the left and class
on the bottom
and So based on age and class
and so once we put in this nice graph
here we want to go ahead and add our
Contours and our points so let's go
ahead and run that and I'm going to run
all three of these so we can see all the
colors come in on the graph and this is
actually going to take a little bit
because remember you can make it go a
little faster by changing a couple of
the notes up here well we did 0.01 from
the Min to Max by changing that to 0.1
or other values you can actually it'll
run a little quicker and we're going to
create a nice little
decision tree classification test set
just to see what that looks like and how
that data displays
now that was taking so long to run with
.01 I went ahead and just did a 0.1 so
instead of filling the whole page with
different colors for a really pretty
view now I just have these different
colored lines coming across it gives us
another model that we can show for
whoever you're working with in this case
your shareholders of the company or the
individuals who you're posting to the
web for and you can see where we have
the green and the red and we've set
those up to identify from the decision
who's going to make it and who's not
based on age and class now my favorite
one on this and I'm going to go back and
just highlight this really quick was let
me just rerun that go back to PRP tree
where we display the tree and that's
where we split it in part because this
information gives us some keys that we
can then use in other models so it's
good to know that sometimes what you're
taking comes out and maybe you create a
bucket for people under 19 between 19
and 60 and over 60 and different things
like that so you can actually put people
in buckets and a class age split just a
quick note on there again you know lots
of different things from which you can
display the data and you can see how we
use the decision tree to figure out
chances of somebody surviving based on
their class and their age what is a
random forest and welcome to Raphael's
Vineyard and I don't even know how to
pronounce that in Alsace France the
largest wine production region in the
world so they have sentries of wine
production and data they've collected on
how to make the best how to make their
grapes and grow them and how to properly
get the right grapes and ferment them
like Raphael wine makers around the
world are facing a grave problem so
let's meet the man himself and try
helping him out what is a random Forest
we're going to come back to the vineyard
but we're going to look at some other
examples along the way too open up the
door and of course here comes Rafael
that probably I'd probably look closer
to Raphael than I do to the young lad
with the beard hey Raphael so tell us
how it's going not as good as before not
many people work in this profession
anymore so every stage of production has
gone slower if only we could somehow
speed up the process while looking for
you I'm a machine learning engineer how
about we automate the quality prediction
process for wine let me Begin by telling
you what machine learning and random
Forest is machine learning is a science
of applying algorithms that allow a
computer to predict the outcome without
being explicitly programmed random force
is an ensemble machine learning
algorithm so we're bringing in different
pieces and then putting those pieces
together it operates by building
multiple decision trees remember I told
you we don't want just a lone tree out
there in the we want a forest they work
for both classification if you remember
from part one we had our vendor with his
broccoli and oranges I think he also had
carrots and it also the output given by
the majority of its decision trees is
the forest final output so if you have a
number of decision trees you have five
decision trees and four of them say it's
broccoli then it's probably broccoli and
you can also use it as a regression
model the output given by the majority
of decision trees is a forced final
output just like you do in
classification you can also come up with
actual values and numbers as a
regression model could you explain how a
random Force Works in layman terms it's
always a good thing to explore the
information on something a little bit
easier to understand and see sure let's
consider the case of Sam a high school
school student this so reminds me of my
own high school student who's just
started his first year in college asking
all his friends so Sam is confused on
which course to take up so he's trying
to figure out what his next class is
going to be at school should he take
music lessons dance lessons math science
maybe social studies so he decides to
ask a couple of his friends for
suggestions and Sam approaches Jane
first so he comes up to Jane and says
hey which of these classes should I take
and Jane asked Sam a few questions based
on which she can suggest theoretical do
you like Theory no he doesn't like the
theory part calculative do you like
calculative he goes yes I like doing
calculations and so Jane based on a
couple questions goes she has a little
decision tree there going she goes
mathematics sounds good so James forms a
decision tree based on Sam's response
and gives her suggestion and of course
this is based on Jane's views and her
history and the data that she's
collected over her years in high school
next time approach is Bella Bella
awesome hey do you like the field of
science
and Sam goes yes and then Bella goes do
you like I knew in Industry he like knew
things coming up in industry and Sam
goes yes and Bella goes you know you
should similar to Jane Bella also forms
a decision tree based on Sam's response
and gives her suggestion and says hey
how about artificial intelligence sounds
like oh artificial intelligence so Sab
next asked Terry for his suggestion and
Sam goes do you like scoring things
he goes yes and he goes do you like
theoretical and he goes no and he comes
up and says mathematics so you have two
people saying mathematics and one person
suggesting artificial intelligence says
two out of three friends suggested math
Sam decides to take mathematics and
there's a lot of subtleties in this
example it's actually a really good
example some of the things we run into
in data science is when we collect data
and we collect from different sources we
might have data from one source that
measures one thing or one set of
instruments and we might have data from
another source that measures something
else we saw that with each of his
friends are measuring different things
and asking different questions that's a
little bit beyond the scope of this
particular tutorial so let's go back to
our Wine Cellar so you see more the
number of decision trees more accurate
will be the prediction random forests
have a number of applications in Market
already if you've been watching any of
our other tutorials you'll will see that
these are very common we have in banking
it can be used to predict fraudulent
customers is used to analyze these
symptoms of patients detecting the
disease in e-commerce the
recommendations are predicted based on
customers activity stock market trends
could be analyzed to predict profit or
loss most recently I was watching a
review on weather here in the United
States and the data they pull in for
that and one of the top weather places
actually uses five different models
random forest was one of them I believe
these five different models to predict
the weather and depending on what the
models come up with and what's going on
some of the models do better than others
I thought that was really interesting
it's important to note so when you see
us repeat the same things over and over
and you're looking at the different
machine learning tools you realize that
we use multiple tools to solve the same
problem and we can find the best one
that fits for that particular situation
so let's see how that fits back in our
wine production to help speed up the
process of wine production we will
automate the prediction of wine quality
suppose our random Forest builds three
decision trees we have the first
decision tree which is going to divide
everything by chlorides less than 0.08
and it's either yes or no and then it's
going to look at alcohol greater than
six yes or no quality equals high
quality not so good and if the chlorides
were greater than 0.08 quality equals
low so there's our first treat and then
our second tree is going to split it on
sulfates less than 0.6 and pH pH less
than 3.5 and again we have it coming
down here that if the pH is low and the
sulfites are low the quality equals high
if the pH is a little higher but the
sulfides are low it's not so good and if
the sulfides are greater than 0.6 we
have a quality equal below and then
we'll do a third decision tree we'll
split it based on ph and we can see
below how that tree looks like with our
sugar less than 2.5 if not it's a quad
equals low it's real low quality and if
it is less than 2.5 then the pH decide
whether it's going to be a quality five
or if not so good and if we track one of
our wines down the list we'll look at it
we'll say oh two out of three decision
trees indicate the quality of our wine
to be five the forest predicts the same
so this sounds promising so let's go
ahead and dive in and see what that
actually looks like so this is the data
set that holds all the attribute values
required to predict the Wines quality
and I'll pull this up in a spreadsheet
in just a moment but you can see here we
have fixed acidity volatile acidity
citric acid all the way to Quality and
then quality is like a four five six or
seven I believe and this will be done in
R so let's go ahead and get started in
our coding we'll be using R Studio
before we dive into our studio I always
like to open up the data in the
spreadsheet and look at the actual data
that I have and they've called it wine
quality Dash Red Dot XLS so it's a
spreadsheet a Microsoft spreadsheet
and when we open it up we'll see that it
matches what we had before as far as the
slide was showing us with the we have 12
different columns the first 11 are our
features our fixed acidity volatile acid
citric acid things you always want to
look at when you're looking at your data
and we've done this before in the part
one but just a reminder we want to note
that we have decimal places here float
values and a lot of these things
especially the first number of columns
free sulfur dioxide looks to be an
integer but it varies a lot so we have
you know I see some values all the way
up to 51 7 4
. so we'd probably keep this as a linear
model kind of thing same thing with the
sulfur dioxides densities again decimal
places sulfides alcohol
quality I want you to note in the
quality that we only have a few
qualities here this isn't like a spread
between 0 and 100 it looks to be three
four five six and seven
and so when we start looking at the
processes we can have the option of
either doing this as a
a series of integers or float values
going from
three to seven or we can convert that to
a categorical three four five six and
seven being the categories but we'll
look at that in the coding here in just
a minute and finally you'd want to go
ahead and scroll down through the data
and just look for missing data or
anything weird obviously working with
big data you wouldn't be able to do that
if this was spread across numerous
computers and you had millions of rows
we have what 1687 rows here so it's a
nice size spreadsheet still considered a
small data set let's go ahead and switch
over and let's take a look at this in
the R so here we are in our R studio and
if you remember it comes up
automatically in three panels I have my
font on fairly large to help out
the upper left panel is a file that's
open you can have multiple files and
tabs open as you can see on mine right
now I'll probably close most of those in
just a minute
and this is where you can put in your
commands then you can execute these
commands by either clicking on the Run
box or control enter is the hotkeys for
that and all that does is then puts that
down in the console and executes that
line of command you could just paste
these directly in the console I've
certainly been known to do that if I'm
loading up some data and I just want to
take a quick look at it in R because I'm
doing the coding in something else or if
I just want to do a quick explore before
I dive in and figure out what needs to
be done that's one of the tricks you can
do and then on the right hand side we
have our environment history connections
we mainly are going to just be sitting
on plots and I'm just going to minimize
this for right now out of the way when
we dive in if you haven't yet you do
need to go ahead and install the package
random Forest I'm not actually going to
do that because I've already run that we
don't need to rerun that but I am going
to go ahead and import it into our
project with the library and then random
forest and we'll do the control enter
you can always hit the Run button up
there too
and then now the library has been
uploaded in there and you can see down
below it takes this and just copies it
and pastes it down below and runs it in
our console and then just a quick
troubleshooting thing my Java happens to
be the developer's job the latest
release of the developers Java from last
week is clashing with my R and rerouting
my Java files so your chances are you
don't need to to look this up or find
out where your Java is but there's a JRE
folder if you do have an error that
comes up next in the next Library you
look that up it's usually under program
files Java and you'll see jdk depending
on what version you have on there and
then find that JRE folder and I'm just
going to set the environment on there
you have to do that every time you run
the script so with r there is a place in
R where you can set it permanently but
generally it's set to whatever your
system variables are set to so you don't
want to mess with that so you just want
to put this in your code if you're
having problems and you can always take
it out then we need to go ahead and
bring in another Library
and we're going to be importing a the
spreadsheet so it's an X the xlsx since
it's a Microsoft spreadsheet then I'll
go ahead and run that
let's go ahead and create a variable
path we're going to assign to variable
path the location of our folder so this
is a very lengthy one you'll have to
copy and paste it off your computer my
name is Richard and you can see under
it's under C column users Richer's
documents docs business simply learn
marketing
random forest and then the actual file
which is wine quality Dash Red Dot XLS
so I put in the whole path in here and
you could also set the computer path to
whatever folder you have it in and then
just use just the name of the file but I
went ahead and did this because it a lot
of times it's good to be where your
paths are going where you're pulling
your data from and again I went to
control enter so I could you'll see that
that pushed the line down below and ran
that line so our path is now set
and then let's create a variable called
Data
and we'll assign that to a read
and we're going to be reading let's see
where is it there it is and spreadsheet
and before I put in we need the path
but we also need one more thing on the
spreadsheet because it is an Excel
spreadsheet we need to know which page
we're on so let's go look that up
and in the spreadsheets it might be a
little hard to see because it's a very
tiny font down there but you'll see the
page tabs on the bottom this actually
has two different pages and we want page
one for this project so let's flip back
over and put that in our code and the
term they use is sheet index I always
have to look that stuff up
and it was sheet index of one we'll go
ahead and execute this
so at this point we've now loaded the
data into the variable data
and it's reading those and I think there
was like a couple thousand rows of data
in there so it takes just a moment to
pull it in
once I have my data in there I always
like to do something like this data
and in I always forget an R it's a head
data
and we can run that and you'll see it
prints out that what we looked at in the
spreadsheet so we have our fixed acidity
volatile acidity citric acid residual
sugar chlorides free sulfur all the
different things we're actually looking
for Quality so we want to keep that in
mind that quality is the final setting
is it a three four five six what is the
quality on there
and the first thing we need to do is
we're actually going to take the data
quality
and let me see quality there it is
in R the dollar sign means that you're
looking at one of the columns in here or
features as it is so we see data dollar
sign quality in r that means that's what
we're going to be editing and what we
want to do is we want to change the
quality just a little bit
and so we're going to set it equal to
now we're not assigning a variable we're
altering it so data dollar sign quality
equals and we want to do as is one of
the functions in R and we're going to do
as a factor
and then data
quality because that's what we're
actually editing on there and this stuff
just for a second to look at this
Factor takes this
and looks at the data as a
categorically so instead of it being on
a continual scale of you know zero to a
hundred or something like that it
actually looks at it as category of five
category of six category of a category
of cat category of dog there are yes or
no categories versus a continual now
there's a lot of different
things in here that we could do the same
thing with
but all of these really are linear
numbers 7.4.70
sulfur dioxide 3467 so we really don't
we need to mess with any of the other
variables in there but we do want to go
ahead and set this up and we'll come up
here and we'll do our control enter and
we'll run it down below and you'll see
data quality equals as Factor data
quality so we've now turned that into a
factor or into a categorical data
and so let me just bring this down a
little bit the guys in the back did the
rest of this next piece of code there
are so many different ways and if you
have been looking at any of our
tutorials this should look kind of
familiar this is one way to do it you'll
see we have our training data and our
testing data and so we've done here is
we've got it we have a data set size
equals
and we're just going to do by the row
and so we want 0.8 or 80 percent of the
data is going to be our training set to
train our forest and if you train it
forest or any machine learning model of
course want to test it so we're going to
save 20 of the data for testing and when
you look at this we have the index
which we set at 80 percent comma and
then a negative index so that's the
remaining setup so it goes from that
index on this is our notation for
sorting out that data and then we create
the index and we use the term sample
and this is just going to go in there
and set our different samples for in row
size data set size and everything so we
have an index now so the data is indexed
and we're just going to pull off the
first set of index and the end of the
index for our data and we can run all of
this in one shot we'll go into code
and we'll run selected lines and so now
we have our data in here we have our
training set and our testing set that's
what we wanted to get to
so once we've formatted all the data
then we get to the next real easy step
which is a single line to bring in our
random Forest
so let's go ahead and we'll use the
variable RF standing for random forest
and we're going to assign
if you remember correctly we brought in
our library random
if you wonder why I go up here all the
time is because I'll forget which
letters are capitalized in some of these
because I switch between packages so
much but it's random with a capital f
for 4S random Forest so the RF is going
to equal random Forest
and of course the we start by telling it
which column we're trying to predict
and of course we can always go down here
and double check we're running this on
the quality there we go quality and
we're using just that column all the way
to the end
and then we've added our team in the
back has gone in and played with this a
little bit and we'll go a little bit
over what they did but I'm going to put
in the values that they have set for the
different options in the random Forest
before I do that it would help if I tell
it what data to use data is going to
equal our training set
and then we have the variables that they
put together and let's just talk a
little bit about what these different
variables mean
so M try and entry these are some of the
variables you play with with the random
for us the M try not so much
when they first put it out I think mtry
was very common four is a good value I
can't remember what the default is but
it's probably three four or five or
something like that depending I think it
depends on how many features you have
going in there so you could even just
leave this as a default without too much
troubles
the entry this is where it gets
important and I'm going to make this
2001
one of the suggestions is to always make
this odd like a tie breaker just in case
the default is like 500 or 501.
and then finally we have importance
equals true I can't remember what it
defaults to but generally you want to
set the importance equals to true it
actually looks at the correlation
between the different features to make
sure that there is some kind of
correlation as it's doing its
calculations so our one line of code is
going to assign a random forest and go
ahead and train a random for us all in
one execution so this whole thing has
been centered around getting to RF set
to the random forest and let's go ahead
and execute this variable or this line
of code and you'll see it takes a little
bit down here to execute as it goes
through and sets it up
and you'll see it probably takes a
minute or so to execute the code
depending on the size of the data and
how good your machine is
and then with all of our data we can
start looking at the answers and we'll
just do RF we'll execute that
and you'll see that the random forest
does a nice job you can see right here
it has our data equals training it
basically goes over everything we talked
about
and you come down here and it has a
confusion Matrix which is nice we always
like our confusion Matrix when we're
talking to our shareholders and
explaining what the data comes out with
and how it trains
and we can go ahead and plot our RF
let's see what that looks like
and
as the number of force grows you
remember we set it to 2001 Forest you
can also see how the error comes in how
the different error on the different
variables
this kind of a nice graph that gives you
an idea it might help you adjust whether
you really want to do 2000 different for
us you might actually truncate this off
at around maybe 800 because you notice
the lines kind of continue on again this
is just still on the plot
in the course the next step is we need
to go ahead and predict remember we
split our data into two groups so let's
go ahead and get a result we'll call it
result and we'll assign this one
there we go
we'll set this to a data frame
there we go there's our data frame
and then oh let's call this we have our
test our test
quality because that's what we're we're
setting up on our test group with the
quality on there
then we want to have a predict a nice
little keyword there to go and run our
random for us
and then we tell it that we're going to
predict using our RF our random Forest
our testing group
1 to 11 is what we're using type equals
response so let's go ahead and execute
it that's a mouthful on there
somewhere I left out a comma in there so
it gave me an error but you can see
right here we've said in there now we
have a result and just real quick we
could do the head of the result
comma 5 and just take a look at the
first five lines and you'll see we have
the actual value and what we predicted
over here
and we could also just do all of the
result and just see what that looks like
oops
and print that out and so it has all the
different data we're going through on
there
and finally let's just go ahead and plot
there we go we'll put plot up here plot
and let's plot the result
and just see what that looks like
maybe not the best one to show to the
shareholders but it does give you an
idea on the left on the y-axis you'll
see that it predicted eight
and this tells you that it a lot of the
eight and the eight scale was pretty
probably gave us about 50 percent came
up there at eight
really maybe 50 percent it didn't guess
right but it did come up there and say
that it was at least a four five or six
even where it was considered an eight
and so on you can see these different
blocks coming in so that if you line up
the three with the three four with the
four and you kind of cross index them a
heat map would probably do better but
for this example we'll just do a quick
plot of the data as far as how it works
and you can again see the results that
we predicted over here and you can look
those over
of course back in The Wine Cellar we're
talking to our Vineyard owner and if you
remember from before we looked at this
chart right here the error rate is 29.05
percent therefore our accuracy is 70.95
if you're doing a Vineyard that's
probably pretty good accuracy and always
check your domain when you're sharing
this stuff because if you're doing
something that's life or death maybe 70
accuracy might not be so good but if
you're looking at where you're going to
distribute the bottles to for the wine
and who's going to drink it you know 70
accuracy is pretty good for that that's
probably a lot better than having no
accuracy whatsoever and not even knowing
what Wine's going out to who so we have
automated the process of predicting the
quality of wine I myself prefer to
predict it by sampling it that's great
thank you I like to visit this guy and
Sample his wine and find out just how
good his different Vineyards are what is
clustering so we'll have a group of
people here hanging out at a table or
local coffee shop I'm guessing coffee
shop since they all have the same mugs
and one of them is trying to figure out
what they're going to do I have 20
places to cover in four days I guess
he's going traveling that sounds fun one
of those Breeze through Costa Rica or
visiting the U.S or going to to Europe
to go visit all the highlights she's got
20 places he wants to go and he wants to
hit him in four days very ambitious and
how will I manage to cover all of them
that's the question that he's coming up
how am I going to get to all these
different places in the short time I
have maybe he's a sales team so he has
20 places he's got to do demos for you
can make use of clustering by grouping
the places into four clusters each of
these clusters will have places which
are close by so we're going to Cluster
them together by what is closest to the
other one then each day you can visit
one cluster and cover all the places in
the cluster great that's a great idea
you know if you're going to hit four
different places maybe you're going to
Cluster whatever streets are closest
together so you don't have to travel
spend all your time traveling the method
of dividing the objects into clusters
which are similar between between them
and are dissimilar to the objects
belonging to another cluster so this is
a formal definition of clustering and we
have here what we call hierarchical
clustering and partial clustering and
we're going to go more into detail on
hierarchical clustering and you'll see
the difference between the two where
partial clustering doesn't have the
descending graph it just has groups of
things and under hierarchical clustering
we're going to go over agglomerative and
divisive and you can think of
agglomerative of bringing things
together and divisive as dividing them
apart and then partial clustering the
two most common ones are K means that's
probably the most common used for
partial clustering and there's also
fuzzy c means and there's other ways of
clustering and other algorithms out
there but these are the two big ones
k-means really being the most common one
used but we're not going to dig too deep
into partial clustering because we're
studying hierarchical clustering today
so what is clustering well the
applications of clustering are pretty
numerous we have things is like customer
segmentation what people kind of Belong
Together how do we group them together
we have social network analysis so
social networks we might look at
sentiments what group of people like
something what group of people don't
like something and then we can use that
sentiments to suggest new cells for them
you know this whole group is into
Harley-Davidson motorcycles so we cater
to them to sell them Harley-Davidson
motorcycle parts and we find out that
people in that group also like leather
jackets they also like motorcycle boots
and they like bandanas my brother's into
Harley-Davidson so that's why I picked
that kind of funny example but you can
kind of see we look for similarities
between these people and then we group
them together accordingly and that's a
good sentimental clustering is very
common in so many things today we want
to know the sentiments on stocks who is
interested in stocks who's not
sentimental analysis we talked about
customer segmentation in cells well we
want to know what kind of sentimental
view people have on in different
restaurants if we're going to open up a
new restaurant or a new putting stores
in and of course city planning a lot of
this I just kind of aimed at the city
planning side sitting planning is Big
with clustering we want to Cluster
things together so that they work we
don't want to put a industrial Zone in
the middle of somebody's neighborhood
where they're not going to enjoy it or
have a commercial zone right in the
middle of the industrial Zone where no
one's going to want to go next to a
factory to go eat a high-end meal or
dinner so it's very big in City Planning
but it's also very big in just
pre-processing data into other models so
when we're exploring data being able to
Cluster things together reveal things in
the data we never thought about and then
once we have it clustered we can put
that into another model to help us
figure out what it is we want for our
solution so let's go a little deeper
into hierarchical clustering let's
consider we have a set of cars and we
have a group similar we went to group
similar ones together so below we have
you'll see four different cars down
there and we get two clusters of car
types an SUV so if you're just looking
at it you can probably think oh yeah
we'll put the sedans together and the
SUVs together and then at last we can
group everything into one cluster so we
just have just cars so you can see as we
have this we make a nice little tree
this is very common we see anybody talks
about hierarchical clustering this is
usually what you see and what comes out
of it we terminate when we are left with
only one cluster so we have as you can
see we bring them all together we have
one cluster we can't bring it together
anymore because they're all together
hierarchical clustering is separating
data into different groups based on some
measure of similarity so we have to find
a way to measure what makes them alike
and what makes them different
agglomerative clustering is known as a
bottom up approach remember I said think
of that as bringing things together so
you see I think the Latin term aglo is
together because you have your
agglomerate rocks where all the
different pieces of rocks are in there
so we want to bring everything together
that's a bottom up and then divisive is
we're going to go from the top down so
we take one huge cluster and we start
dividing it up into two clusters into
three four five and so on digging even
deeper into how hierarchical clustering
Works let's consider we have a few
points on a plane so or this plane is 2D
so we have an X Y coordinates kind of
makes it easy we're going to start with
measure the distance so we want to
figure a way to compute the distance
between each point each data point is a
cluster of its own remember if we're
going from the bottom up a glomerative
then we have each point being its own
cluster we try to find the least
distance between two data points to form
a cluster and then once we find those
with the least distance between them we
start grouping them together so we start
forming clusters of multiple points this
is represented in a tree-like structure
called dendogram so there's another key
word dendogram and you can see it is
it's just a branch we've looked at
before and we do the second group the
same so it gets its own dendrogram and
the third gets its own denigram and then
we might group two groups together so
now those two groups are all under one
dendogram because they're closer
together other than the P1 and P2 and
then we terminate when we are left with
one cluster so we finally bring it all
together you can see on the right how
we've come up all the way up to the top
whoops and we have the gray hierarchical
box coming in there and connecting them
so we have just one cluster and that's a
good place to terminate because there is
no way we can bring them together any
further so how do we measure
in the data points
really where it starts getting
interesting up until now you can kind of
eyeball it and say hey these look
together but when you have thousands of
data points how are we going to measure
those distances and there is a lot of
ways to get the distance measure so
let's go and take a look at that
distance measures will determine the
similarity between two elements and it
will influence the shape of the Clusters
and we have euclidean distance measure
we have squared euclidean distance
measure which is almost the same thing
but with less computations and we have
the Manhattan distance measure which
will give you slightly different results
and we have the cosine distance measure
which again is very similar to the
euclidean playing with triangles and
sometimes it can compute faster
depending on what kind of data you're
looking at so let's start with the
clitian distance measure the most common
is we want to know the distance between
the two points so if we have Point p and
point Q the euclidean distance is the
ordinary straight line it is the
distance between the two points in
euclidean space and you should recognize
D equals in this case we're going to sum
all the points so if there was more than
one point we could figure out the
distance to the not more than one points
this is the sum of more than two
Dimensions so we can have the distance
between each of the different dimensions
squared and that will give us and then
take the square root of that and that
gives us the actual distance between
them and they should look familiar from
euclidean geometry maybe you haven't
played too much with multiple Dimensions
so the summation symbol might not look
familiar to you but it's pretty
straightforward as you add the distance
between each of the two different points
squared so if your y difference was 2
minus 1 squared would be two and then
you take the difference between the X
again squared and if there was a z
coordinates it would be you know Z1
minus Z2 squared and then take the
square root of that and sum it all or
sum it all together and then take the
square root of it so to make it compute
faster since the difference in distances
whether one is farther apart or closer
together than the other we can do What's
called the squared euclidean distance
measurement this is ident technical to
the euclidean measurement but we don't
take the square root at the end there's
no reason to it certainly gives us the
exact distance but as far as doing
calculations as to which one's bigger or
smaller than the other one it won't make
a difference so we'll just go with the
so we just get rid of that that final
square root computes faster and it gives
us the pretty much the euclidean squared
distance on there now the Manhattan
distance measurement is a simple sum of
horizontal and vertical components or
the distance between two points measured
along axes at right angles now this is
different because you're not looking at
the direct line between them and in
certain cases the individual distances
measured will give you a better result
now generally that's not true most times
you go with euclidean squared method
because that's very fast and easy to see
but the Manhattan distance is you
measure just the Y value and you take
the absolute value of it and you measure
just the X difference you take the
absolute value of it and just the Z and
if you had more you know know different
dimensions in their a b c d e f however
many dimensions you would just take the
absolute value of the difference of
those dimensions and then we have the
cosine distance similarity measures that
angle between the two vectors and as you
can see as the two vectors get further
and further apart the cosine distance
gets larger so it's another way to
measure the distance very similar to the
euclidean so you're still looking at the
same kind of measurement so should have
a similar result as the first two but
keep in mind the Manhattan will have a
very different result and you can end up
with a bias with the Manhattan if your
data is very skewed if one set of values
is very large and another set of values
is very small but that's a little bit
beyond the scope of this it's just
important to know that about the
Manhattan distance so let's dig into the
agglomerative clustering and a glimmer
trick clustering begins with each
element as a separate cluster and then
we merge them into a larger cluster how
do we represent a cluster of more than
one point so we're going to kind of fix
the distance together with the actual
agglomerative and see what that looks
like and we're actually going to have
three key questions that are going to be
answered here so how do we represent a
cluster of more than one point so we
look at the math what it looks like
mathematically and geometrically how do
we determine The Nearness of clusters
when to stop combining clusters always
important to have your computer script
or your whatever you're working on have
a termination point so it's not going on
eternally we've all done that if you do
any kind of computer programming or
writing a script let's assume that we
have six data points in a euclidean
space so again we're dealing with X Y
and Z in this case just X and Y so how
do we represent a cluster of more than
one point let's take a look at that and
first we're going to make use of
centroids very common terminology in a
lot of machine learning languages when
we're grouping things together so we're
going to make use of centroids which is
the average of its points and and you
can see here we're going to take the 1 2
and the 2 1 and we're going to group
them together because they're close I
mean if we were looking at all the
points we'd look for those that are
closest and start with those and we're
going to take those two we're going to
compute a point in the middle and we'll
give that point 1 5 1.5 1.5 and that's
going to be the centroid of those two
points and next we start measuring like
another group of points we got 4 1 5 0.
when they're pretty close together so
we'll go ahead and set up a centroid of
those two points in this case it would
be the 4.5 and 0.5 would be the
measurements on those two points and
once we have the centroid of the two
groups we find out that the next closest
point to a centroid is over on the left
and so we're going to take this and say
oh 0 0 is closest to the 1.5 1.5
centroid so let's go ahead and group
that together and we compute a new
centroid based on those three points so
now we have a centroid of 1.1 or 1 comma
1. and then we also do this again with
the last point the 5 3 and it computes
into the first group and you can see our
dendogram on the right is growing so we
have each of these points are become
connected and we start grouping them
together and finally we get a centroid
of that group two and then finally the
last thing we want to do is combine the
two groups by their centroids and you
can see here we end up with one large
group and you'll have its own centroid
although usually they don't compute the
last centroid we just put them all
together so when do we stop combining
clusters well hopefully it's pretty
obvious to you in this case when they
all got to be one but there are actually
many approaches to it so first pick a
number of clusters K up front and this
is done in the fact that we don't want
to look at 200 in clusters we only want
to look at the top five clusters or
something like that so we decide the
number of clusters required in the
beginning and we terminate when we reach
the value K so if you looked back on our
clustering let me just go back a couple
screens you'll see how we clustered
these all together and we might want
just the two clusters and so we look at
just the top two or maybe we only want
three clusters and so we would compute
which one of these has a wider spread to
it or something like that there's other
computations to know how to connect them
and we'll look at that in just a minute
but to note that when we pick the K
value we want to limit the information
that's coming in so that can be very
important especially if you're feeding
it into another algorithm that requires
three values or you set it to four
values and you need to know that value
coming in so we might take the
clustering and say okay only three
clusters that's all we want for K so the
possible challenges this only makes
sense when we know the data well so when
you're clustering with K clusters you
might already know that domain and know
that that makes sense but if you're
exploring brand new data you might have
no idea how many clusters you really
need to explore that data with let's
consider the value of K to be 2. so in
this case in our previous example we
stop and we are left with two clusters
and you can see here that this is where
they came together the best while still
keeping separate the data the second
approach is stop when the next merge
would create a cluster with low cohesion
so we keep clustering till the next
merge of clusters creates a bad cluster
low cohesion setup on there that means
the point is so close to being between
two clusters it doesn't make sense to
bring them together but how is cohesion
defined oh let's dig a little deeper
into cohesion the diameter of a cluster
so we're looking at the actual diameter
of our cluster and the diameter is the
maximum distance between any pair of
points in the cluster we terminate when
the diameter of a new cluster exceeds
the threshold so as that diameter gets
bigger and bigger we don't want the two
circles or clusters to overlap and we
have radius of a cluster radius is the
maximum distance of a point from
centroid we terminate when the radius of
a new cluster exceeds the threshold
again we're not we don't want things to
overlap so when it crosses that
threshold and it's overlapping with
other data we stop so let's look at
divisive clustering remember we went
from the bottom up now we want to go
from the top down divisive clustering
approach begins with a whole set and
proceeds to divide it into smaller
clusters so we start with a single
cluster composed of all the data points
we split it into different clusters this
can be done using monothetic divisive
methods what is a monothetic divisive
method and we'll go backwards and let's
consider the example we took in the
agglomerative clustering to understand
this so we consider a space with six
points in it just like we did before
same points we had before and we name
each point in the cluster so we have in
this case we just gave it a letter value
a b c d e f since we follow top-down
approach in divisive clustering obtain
all possible splits into two columns so
we want to know where you could split it
here and we could do like a a b split
and a cdef split we could do BCE ADF and
you can see this starts generating a
huge amount of data a b c t e EF and so
for each split we can compute cluster
sum of squares and we can see here the
we actually have the formula out for us
B J 12 equals N1 of absolute value of x
minus absolute value of x squared so
again we're Computing all the different
distances in there squared back to your
kind of euclidean distances on that and
so we can actually compute B A J between
clusters one and two and we have the
mean of the cluster and the grand mean
depending on the number of members in
the cluster and we select the cluster
with the largest sum of squares let's
assume that the sum of squared distance
is largest for the third split we had up
above and that's where we split the a b
c out and if we split the ABC out we're
left with the def on the other side we
again find the sum of squared distances
and split it into clusters so we go from
ABC we might find that the a is splits
into b c and d into e f and again you
start to see that hierarchical dendogram
coming down as we start splitting
everything apart and finally we might
have a splits in b and c and then each
one gets their own d e f and it
continues to divide until we get little
nodes at the end and every data has its
own point or until we get to K if we
have set a k value so we've kind of
learned a little bit about the
background and some of the math in
hierarchical clustering let's go ahead
and dive into a demo and our demo today
is going to be for the problem statement
we're going to look at U.S oil so a U.S
oil organization needs to know its cells
in various States in U.S and cluster of
the states based on the cells so what
are the steps involved in setting this
problem up so the steps we're going to
look at and this is really useful for
just about any processing of data
although I believe we're going to be
doing this in R today we're going to
import the data set so we'll explore our
data a little bit there create a scatter
plot it's always good to have a visual
if you can so once you have a visual you
can know if you're really far off in the
model you choose to Cluster the data and
any splits you need and then we're going
to normalize the data so we're going to
fix the data up so it processes
correctly we'll talk more detail about
normalization when we get there and then
calculate the euclidean distance and
finally we'll create our dendogram so it
looks nice and pretty and we have
something we can show to our
shareholders so that they have something
to go on and know why they gave us all
that money and salary for the year so we
go ahead and open up R and we're
actually using our studio which is the
really has some nice features in it it
automatically sets up the three Windows
where we have our script file on the
upper left
and then we can execute that script and
it'll come down and put it into the
console bottom left and execute it and
then we have our plots off to the right
and I've got it zoomed in hopefully not
too large a font but large enough that
you can see it
and let's just go ahead and take a look
at some of the script going in here
it's clustering analysis and we're going
to work we'll call it my data and we're
going to assign it in R this is a symbol
for assigning and we're going to go read
CSV
read CSV file
and we'll put that in Brackets and let's
before we go any further let's just look
at the data outside of R it's always
nice to do if you can
and the file is going to be called
utilities.csv this would also be the
time to get the full path so you have
the right path to your file and remember
that you can always Post in the comments
down below
and when you post down there just let us
know you want to connect up with simply
learn so that they can get you this file
so you can get the same file we're
working on and you can repeat it and see
how this works this is utilities.csv
It's a comma separated variable file so
it's pretty straightforward and you can
see here they have the city fixed charge
and a number of different features to
the data and so we have our RoR cost low
demand cells
nuclear fuel cost on here and then going
down the other side we have U.S cities
Arizona Boston they have Central U.S I
guess they're grouping a number of areas
together the Commonwealth area you can
see down here in Nevada New England
Northern U.S Oklahoma the Pacific region
and so on so we have a nice little chart
of different data they brought in
and so I'm going to take that complete
path that ends in the utilities.csv
and we're going to import that file let
me just enlarge this all the way upside
an extra set of brackets here somehow or
maybe I missed a set of brackets
this can happen if you're not careful
you can get brackets on one side and not
the other or in this case I got double
brackets on each side there we go and
then the magic hotkeys in this case are
your control enter which will let me go
ahead and run the script
and so I've now loaded the data and as
you can see I went ahead and shrunk the
plot since we're going to be looking at
the window down below
and we can simply convert the data to a
string
now all of us do this automatically the
first time we say hey just print it all
out as a string and then we get this
huge mess of stuff
that doesn't make a whole lot of sense
so and you can see here they have you
can probably kind of pull it together as
looking at it but let's go ahead and
just do the head
now we'll do the head of my data
there we go and control enter on that
and the head shows the first five rows
you'll see this in a lot of different
scripts in R it's you type in head and
then in Brackets you put your data and
it comes through and lists the first
five rows as you can see below
and it shows Arizona Boston Central and
it has the same columns we just looked
at so we have the fixed charge the RoR
the cost the load the D demand I'm not
an expert in oil so I'm not even sure
what D demand is sells I'm guessing
nuclear how much of it's supplied by
nuclear and the actual fuel cost
and then the different states that it's
in or different areas
and one of the wonders of R is all these
cool easy to use tools that are so quick
so we'll do pairs
and pairs creates a nice graph so let me
go ahead and run this
uh whoops the reason it gave me an error
is because I forgot to resize it so let
me bring my plot way out so we can see
it and let's run that again
and you'll see here that we have a nice
graph of the different data and how it
plots together how the different points
kind of come together
this is neat because if you look at this
I would look at this and say hey this is
a great candidate for some kind of
clustering and the reason is is when I
look at any two pairs let's go down to
say cells and fuel costs towards the
bottom right
and when you look at them where they
cross over you sort of see things how
they group together but it's a little
confusing you can't really pinpoint how
they group together you could probably
look at these two and say yeah there's
pretty good commonalities there and if
you look at any of the other pairs
you'll start seeing some patterns there
also and so we really want to know what
are the patterns on all of them put
together not just any two of them but
the whole setup
let me go ahead and Shrink my this down
for just a second just a notch here
and let's create a scatter plot oops
and this is simply just use the term
plot in Brackets and then which values
do we want to plot and if we remember
when we looked at the data earlier let
me just go back this way
in this case let's go ahead and compare
two just two values to see how they look
and we'll do fuel costs and cells
and it's in my data so we gotta let it
know which two columns we're looking at
next to each other and it will open up
our plot thing and then go ahead and
execute that
and we can see on those close-up what
we're just looking at in the pairs
and if I was eyeballing this I would say
oh look there's a kind of a cluster up
here of five items and this one is hard
to tell which cluster to be with but
maybe it's six you go in the top one and
you have a middle cluster and a bottom
cluster maybe two different clusters so
you can sort of group them together fuel
cost and the sales and see how they
connect with each other
and again that's only two different
values we're looking at so in the long
run we want to look at all of them
and then the people in the back
they sent me this grip so we can go
ahead and add labels so with my data
text fuel cost cells the labels equal
City
position four these are numbers you can
kind of play with till they look nice
and oops again I forgot to resize my
plot it doesn't like having it too small
and we'll run that
I must type something in here
oh I did a lowercase dnated capital D
there we go
so now we can go in here and do this
with my data and you can see a little
hard to see on my screen with all the
the different things in there plus the
actual cities so we can now see where
all the cities are in connection with in
this case fuel cost and sales so you
have a nice label to go with the graph
and then we can also go ahead and plot
in this case let's do oh the RoR oops
and we'll do that
also with the cells and do my data
remember to leave it lowercase this time
so we plot those two
we'll come over here and it's going to
I'm surprised it can give me an error
and then we'll also add in the with
statement so we put some nice labels in
there
and it's going to be the same as before
except instead of doing the fuel cost
cells we want the r cells and we'll
execute that
oops and of course it gives me an error
because I shrunk it down so let's redo
those two again
there we go and we can see now we have
the RoR with cells and we'd probably get
a slightly different clustering here if
I was looking at these cities they're
probably different than we had before
but you could probably look at this and
say ah these kind of go together and
those kind of go together but again
we're going to be looking at all of them
instead of just one or two
and so at this point we want to dive
into the next step where we're going to
start looking at a little bit of coding
or scripting here
this is very important because we're
going to be looking at normalization
we put that in there normalization and
if you've done any of the other machine
learning skills and setup dishes start
to look very normal in your
pre-processing of data whether you're in
r or python or any of these scripts we
really want to make sure you normalize
your data so it's not biased
remember we're dealing with distances
and if I have let's say the RoR is even
look at this graph here on the right
you can see where my RoR varies between
8 and 14. that's a very small variable
and our cells varies between four
thousand and sixteen thousand so you can
imagine the distance between four
thousand and eight thousand which is the
distance of four thousand versus eight
to ten versus two the cells is going to
dominate so if we do any kind of special
work on this it's going to look at cells
and it's going to Cluster them just by
the cells alone and then RoR might have
a little tiny effect of two versus four
thousand we want to level the playing
field
turns out there's actually a number of
ways in script to normalize
so I'm just going to put in the code
that they put together in the back for
me and let's talk about it a little bit
so we have Z we're going to assign it to
my data
and let's go ahead and
we're going to do a little reshaping
across all rows
or I mean across all columns so each of
the rows is going to have a little
reshaping there
and then we're going to get M which
stands for means
and we're going to apply it to my data
so again we want to go ahead and create
a the most common variable in there
and then s is going to be SD stands for
standard deviation
so instead of just doing a lot of times
what they do with normalization of data
is we just reshape the data everything
between 0 and 1. so that if the lower
end is eight that now becomes 0 and the
upper end is 14 that now becomes 1.
that doesn't help if it is not a linear
set of data so with this we're going to
look for the means and the standard
deviation for reshaping the data
and that way the most common values now
become the kind of like the center point
and then the standard deviation is how
big the spread so we want the standard
deviation to be equal amongst all of
them and then finally we go ahead and
take Z and with the Z we're going to
reassign it and we're going to scale the
original my data which we re-shaped
based on M and based on the standard
deviation
and the 2 in here that just means we're
looking at everything in kind of a x y
kind of plot
and we can quickly run these control
enter control enter control enter
control enter so now we have Z which is
a scaled version of my data
and now we can go ahead and calculate
the euclidean distance
oops calcu
there we go
and in R this is so easy once you've
gotten to here we've done all that
pre-data processing
we'll call it distance
and we'll assign this to
dist so d-i-s-t
is the computation for getting the
euclidean distance and we can just put Z
in there because we've already
reformatted and scaled Z to fit what we
want let me go ahead and just hit enter
on that
and I'm going to widen my left hand side
again
I'm always curious what does this data
look like so let's just type in distance
which will print the variable down below
oops
you have to hit Ctrl enter
and this prints out a huge amount of
information
as you can see just kind of streams down
there
and let's go ahead and enlarge this
and I don't know about you but when I
look at something like this it doesn't
mean a whole lot to me other than I see
two three four five six
and then you kind of have the top part 6
17 18
so I imagine this is like a huge chart
is what we're looking at
and we can go ahead and use print oh
distance
digits
equal three and let's run that
oops I keep forgetting that it has to go
through the graph on the right and we
see a different slightly different
output in here let me just open this up
so we can see we're looking at and by
cutting down the distance you can start
to see the patterns here if it's looking
at the different distances
so if I go to the top we have the
distance between one and two one and
three one and four one and five one and
six and then two and three and so on
obviously the distance between itself is
zero and it doesn't repeat the data so
we don't care to see two versus one
again because we already know the
distance between one and two
since we have a nice view all the
distances in the chart and that's what
we're looking at right here a little
easier to read that's why we did the
print statement up here to do digits
equals three make it a little bit
smaller we could even just do digits
well let's just do two see what that
looks like we might lose some data on
this one if it's uh if something's way
off
but we have a nice setup and we see the
different distances and that's what we
were computed here between each of the
points
and then the whole reason we're doing
this is to get ourselves a nice
dendogram going a nice clustering
dendogram we'll do a couple of these
looking at different things
we'll take a variable hc.l and we're
going to assign it
H cluster
and then distance
that easy we've already computed the
distances so the age clustering does all
the work for us
and let me hit enter on there so now we
have our HCL which is assigned the H
clustering computation based on
distances and apart and then I'm going
to expand my graph because we would like
to go ahead and see what this looks like
we can simply plot that
and hit the control enter so it runs
and look at that we have a really nice
clustering dendrogram except when I look
at it the first thing I notice is it
really shows like numbers down below
now if you were a shareholder in some
data scientist came up to you and said
look at this this is what it means you'd
be looking at that going what the heck
does 3 9 14 19 1 18 mean so let's give
it some words there
so let's do the same plot with our HCL
HC there we go
and let's add in labels and this is just
one of the commands and plots
so we have labels equals my data
and then under my data we want to know
the city
and we'll have it hang minus one that's
just the instructions to make it pretty
so we'll run that
oops I accidentally ran just to hang my
one let me try that again
there we go okay so now you can see what
it's done in the hangman One turns it
sideways that way we can see Central
which is Central us and Kentucky
and we start to actually get some
information off our clustering setup and
the information you start looking at is
that when we put all the information
together you probably want to look at
Central America and Kentucky together
Oklahoma and Texas has a lot of
commonality as does Arizona and Southern
us and you can even group all five of
those Florida Oklahoma Texas Arizona and
Southern us these regions for some
reason share a lot of similarities and
so we want to start asking what those
similarities are
but this gives us a place to look it
says hey these things really go together
you should be grouping these together as
far as your cells and what's what you're
doing
and then one of the things you might
want to do is there's also we can do the
dendrogram average
this changes how we do the clustering
so it looks very similar like we had
before and those are HCL we're going to
assign it we're going to do an H cluster
we're still doing it on distance oops
distance
and this time we're going to set the
method to average so we can change the
methodology in which it computes the
values
and before if you remember correctly we
did median median is a little bit
different than means we did the most
common one and then we want the average
of the median
and let's go ahead and run that
and then we can plot
and here's our HCL
oops there we go here's our HCL and I
can run that plot and you can see this
changed a little bit so our way it
computes and groups things looks a
little different than it did before and
let's go and put the cities back in
there and do the hang
control copy let me just real quickly
copy that down here
because we want the same labels
and again you can see Nevada Idaho pujet
I remember we were looking at um
Southern U.S and Arizona Texas and
Oklahoma Florida so the grouping really
hasn't changed too much so we still have
a very similar grouping
it's almost kind of flipped it as far as
the distance based on average has
but this is something you could actually
take to the shareholders and say hey
look these things are connected and at
which point you want to explore a little
deeper as to why they're connected
because they're going to ask you okay
how are they connected and why do we
care
that's a little bit beyond the actual
scope of what we're working on today
but we are going to cover membership
what's called a clustering membership on
there
and let's create a member we'll just
call it Member One oops they are member
dot one and we're going to assign to
this
we're going to do cut tree
and
cut tree
it limits it so what that means is I
take my HC Dot
l in here
oops there we go dot l and so I'm taking
the cluster I just built and we want to
take that cluster and we want to limit
it to just a depth of three so we go
ahead and do that and run that one oops
let me go run
there we go so now I've created my
Member One
and then whoops let me just move this
out of the way we're going to do an
Aggregate and we're going to use Z
remember Z from above and we're going to
turn Member One into a list and then
we're going to aggregate that together
based on the mean let me go ahead and
enter run that
Oops I did remember L it's actually
number one
there we go
and if we take a look at this
we now have our group one fixed charge
and then all your different
columns listed there and most of them
should come up kind of looking between 0
and 1 but you'll see a lot of variation
because we're varying it based on the
means so it's a means the standard
deviation not just forcing it in between
0 and 1.
which is a much better way usually to
normalize your data than just doing the
zero one setup
and finally we can actually look at the
actual values
and the same chart we just did
oops
I made a mistake on there with my data
there we go okay
and again we now have our actual data
instead of looking at just the if you
looked up here it's all between 0 and 1
and when we look down here
we now have some actual connections and
how far distance this different data is
again because more of a domain issue and
understanding the oil company and what
these different values means and you can
look at these as being the distances
between different items
so a little bit different View and you
have to really dig deep into this data
we really want you to take away from
this is the dendigram and the charts
that we did earlier
and that is the cluster output and our
nice dendogram so this would be stage
one in data analysis of the cells again
you'd have to have a lot more domain
experience to find out what all the
individual numbers we looked at mean and
what the distance is and what's
difference between Central America and
Kentucky and why they're similar and why
it groups all of Central Kentucky
Florida Oklahoma Texas Arizona and
Southern us together into one larger
group so it'd be Way Beyond the scope of
this but you can see how we start to
explore data and we start to see things
in here where things are grouped
together and ways we might not have seen
before and this is a good start for
understanding and giving advice for
cells and marketing maybe Logistics City
development there's all kinds of things
that kind of come together in the
hierarchical clustering as you begin to
explore data and we just want to point
out that we get three clusters of
regions with the highest cells region
with average cells region with the
lowest cells again those are some of the
things that they clustered it around and
you could actually see where things are
going on or lacking you know in this
case if you're the lowest cells no one
wants to be in the region of the lowest
sales support Vector machine is a binary
classifier a supervised machine learning
algorithm and in this video we will
learn what all that means machine
learning algorithms come in three major
flavors supervised learning algorithms
unsupervised learning algorithms and
reinforcement learning and today we'll
cover classification under supervised
learning supervised meaning the data set
has known outcomes if it were
unsupervised it would not have known
outcomes and we wouldn't have the
categories or classes necessary for the
machine to learn but under supervised
learning we do have the known outcomes
in the data set and under supervised
learning there are two major types of
machine learning algorithms
classification which we'll cover today
support Vector machine and regression
classification you predict categories
and in regression you generally predict
values so while we look at supervised
learning keep in mind that
classification can really be considered
multi-dimensional in the sense that
sometimes you only have two classes yes
or no true or false and sometimes you
have more than two classes sometimes you
have say under risk management or risk
modeling low risk medium risk or high
risk and svm is a binary classifier it
is a classifier used for those true
false yes no types of classification
problems and in supervised learning
features are important if there are a
lot of features svm may be the better
classification algorithm Choice as
opposed to say logistic regression so if
with supervised learning the computer is
presented with example inputs and their
desired outputs those known outcomes and
the goal is to learn a general rule that
Maps inputs to those outputs for example
bug detection or customer term or stock
price prediction not the value of the
stock price but whether or not it will
rise or fall and weather prediction
Sunny not Sunny rain no rain no
classification algorithms generally take
past data data that we have those known
outcomes train the model take new data
once the model is trained ingest it into
the model and create predictions is it a
truck or is it a car and keep in mind
that I said features are important and
the number of features whether or not
those features are applicable to your
problem is a whole different discussion
but in some sense you know probably
intuitively that the color of a car
really isn't indicative of whether or
not it's a car or a truck but the length
of the vehicle or the number of axles or
the weight or the amount of seating may
be more applicable people to the
algorithm and to training the model so
what is support Vector machine support
Vector machine or svm is a type of
classification algorithm which
classifies data based on its features
and svm will classify any new element
into one of those two classes so we'll
give it some inputs the algorithm will
segregate and classify the data and then
create the outputs and next when we
ingest new data an unknown fruit
variable in this example the algorithm
will correctly classify the fruit apple
versus orange so now let's understand
what an svm is and if you want to
understand support Vector machine you
really need a good example and I know
that Cricut is a wonderful game I don't
know a lot about it but I know it's very
popular around the world and we're going
to use Cricut as an example of svm so
we'll classify qriket players into
batsman or Bowlers using the runs to
wicket ratio so a player with more runs
would be considered a batsman and a
player with more wickets would be
considered a bowler and if we could take
a data set say of people cricket players
with runs and wickets in columns next to
their names we could create a plot a
two-dimensional plot in this example
clear separation between qriket players
considered Bowlers and those cricket
players considered batsmen and you don't
always have a data set this clean where
there is a clear segregation of data
Bowlers versus batsman but for the sake
of understanding svm this example does
well so before we do any separation
before we apply any high-level
mathematics let's just take a look at an
unknown value a new data coming into our
data set and that datum we don't know
what class it belongs to that datum is a
bowler or a batsman we know that much
but we don't know should we consider
consider that qriket player a batsman or
a bowler and so we want to draw a
decision boundary some type of line
separating the two classes so that we
can use that decision boundary to
classify the new data point of course we
could draw lines there's a yellow line a
green line a blue line there's actually
not just three lines but an infinite
number of lines that we could draw
between those two classes so which one
do we pick and this is kind of
reminiscent of simple linear regression
where we find the line of best fit we
want the line of best separation we want
a line that clearly separates those two
groups as much as possible once we have
the correct line we would be able to
classify the new data point but in this
example you can see if we pick yellow
the new data point would be a bowler but
if we picked green or blue the new data
point would be a batsman so we need the
one that best separates the data what
line best separates the data well fine
find the best line by Computing the
maximum margin from equidistant support
vectors now support vectors in this
context simply means the two points one
from each class that are closest
together but that maximize the distance
between them or the margin so the word
Vector here you may think well he really
means points data points well in two
dimensional space yes and maybe even in
three-dimensional space but once you get
into higher dimensional spaces when you
get more and more features in your data
set you have to consider these as
vectors you can no longer really
describe them as points so we call them
vectors and the reason they're support
vectors is because the two vectors that
are closest together that maximize the
distance between the two groups support
the algorithm so that's why we call them
support vectors so if you look at our
example there are a couple of points at
the top that are pretty close to one
another and a couple of points at the
bottom of that graph that are pretty
close to each other and I'm not really
sure looking at the graph which two are
closest but clearly those points are the
ones we need to consider the rest of the
other points are too far away from the
rest of the other points in other words
the bowler points are far to the right
and the batsman data points are far to
the left so we'll look at these four
points to begin with mathematically we
would calculate the distance among all
of these points and we would minimize
that distance once we pick the support
vectors we'll draw a dividing line and
then measure the distance from each
support Vector to the line the best line
will always have the greatest margin or
distance between the support vectors if
we consider the yellow line as a
decision boundary the player with the
new data point would be considered a
bowler the margins don't appear to be
maximum though so maybe we can come up
with a better line so let's take two
other support vectors and we'll draw the
decision boundary between those and then
we will calculate the margin and notice
now that the unknown data point the new
value would be considered a batsman we
would continue doing this and obviously
a computer does it much quicker than a
human being over and over and over again
until we found the correct decision
boundary with the greatest margin and in
this case if you look at the green
decision boundary the green line appears
to have the maximum margin compared to
the other two and so let's consider this
the boundary of greatest margin and now
classify our unknown data value and now
clearly it belongs in the batsman's
class the green line divides the data
perfectly because it has the greatest
margin or the maximum margin between the
support vectors and now we can have
confidence in our classification that
the new data point is indeed a batsman
technically this dividing line is called
a hyperplane and the reason it's called
a hyperplane will become a little bit
more evident in a few minutes generally
in two-dimensional space we consider
those lines but in three-dimensional
space and higher dimensional spaces
they're considered planes or hyperplanes
so we always tend to refer to them as
hyperplanes and the hyperplane that has
the maximum distance from the support
vectors is the one we want so sometimes
called the positive hyperplane D plus is
the shortest distance to the closest
positive point and D minus sometimes
called a negative hyperplane as the
closest shortest distance to the closest
negative point and the sum of D positive
and D negative is called the distance
margin so if you calculate those two
distances and add them up that's the
distance margin and we always want to
maximize that if we don't maximize it we
can have a misclassification and you can
see the yellow margin region is much
smaller than the green margin so this
problem set is two-dimensional because
the classification is only between two
classes and so we would call this a
linear svm now we're going to take a
look at kernel svm and if you notice in
this picture this is a great depiction
of a plane not a line This is
three-dimensional space so that's the
plane that separates those two classes
so what if our two-dimensional data
looks like this what if there's no clear
linear separation between the two
classes and machine learning parlance we
would say that these are not linearly
separable how could we get support
Vector machine to work on data that
looks like this since we can't separate
it into two classes using a line what
we're going to do and clearly there's no
line I mean convince yourself there's no
line that goes and separates those two
classes so what we would do is we would
apply some type of trans information to
a higher Dimension we would apply a
function called a kernel function to the
data set such that the data set would be
transformed into a higher Dimension a
dimension high enough where we could
clearly separate the two groups the two
classes in this case with a plane much
the same way you saw the picture a few
slides ago where the plane was
separating the two classes here we
clearly could draw some planes between
the green dots and the red dots and of
course we could draw an infinite number
of planes separating those two classes
but we would draw the one that would
maximize the margin so if we let R be
the number of dimensions then the kernel
function would convert a given
two-dimensional space to a
three-dimensional space and as I said
once the data is separated in three
dimensions we can apply svm and separate
the two groups using a two-dimensional
plane and this is analogous in the
higher dimensions and that last picture
on the right hand side would be some
type of depiction of a higher Dimension
hyperplane and there are more than one
type of Kernel function there's gaussian
RBF kernel sigmoid kernel polynomial
kernel depending on the dimension and
how you want to transform the data there
are more than one choice for kernel
functions so now let's go through a use
case let's take a look at horses and
mules and see if we can use svm to
classify some new data so the problem
statement is classifying horses and
mules and we're going to use height and
weight as the two features and obviously
horses and mules typically in general
tend to weigh differently and tend to
stand taller so we'll take a data set
we'll import the data set we'll make
sure we have our libraries the
e1071 library has support Vector machine
algorithms built in we'll compute the
support vectors using the library once
the data is used to train the algorithm
will plot the hyperplane get a visual
sense of how the data is separated and
if it's two-dimensional or
three-dimensional that's great remember
if it's higher dimensional it's tough to
plot those and then we'll use the new
model the trained model to classify new
values in general we would have a
training set a test set and then ingest
the new data but for our example we're
just going to use the whole data set to
train the algorithm and then see how it
performs and once we see how it performs
we'll see did we get a horse did we
predict a horse when we had a horse did
we predict a mule when we had a mule so
here's the r code of course if you want
to run this our code you'll have to
install R and then our studio then
you'll have to ingest the data and then
you'll have to create an indicator
variable to help us with our plots we'll
install our libraries install the
package if you don't already have it
installed I do and then call the library
up into the r session that you're
working we'll create the data frame from
the data that was ingested we will will
then view the data frame and of course
we have to explore the data frame hoping
that we don't have missing values or we
don't have outliers if we do have
missing values or outliers we'll have to
either impute those values or delete
those values but in this case let's hope
we don't have any and now for a quick
demo on support Vector machines so we
have a working directory where my data
is stored and I'm going to set the
working directory to that data source
and read the data file into R and now
I'm going to create a vector that really
is just an indicator variable it's the
negative ones and the ones that indicate
whether it's a mule or a horse negative
one for mule positive one for a horse so
I'll create that vector and svm there's
a great package
e1071 Great Library that has the support
Vector machine functions that we're
going to use and now I'm going to create
the data frame I'm going to take my data
and I'm going to add to it the is Force
vector
and now let's just take a peek and as
you can see there's the height weight
and whether or not the animal was a mule
or a horse so remember machine learning
we take a data source with known
outcomes and we apply our algorithm to
that data source in this case a data
frame of heights weights and the
indicator variable and then we use the
machine learning algorithm to learn from
that data with those known results so
now let's plot the data the height
versus weight and as you can see the
horses tend to be a lot taller and weigh
a lot more than the mules so now let's
run our algorithm our svm model against
the data and the key here is that it's a
linear model and we've run it and great
now we have a model let's look at the
summary of the model and as you can see
there's the formula that it ran and some
of the information about the vectors in
the classes and there are two support
vectors so now let's find those two
support vectors on our our graph and
this might be a little hard to see the
orange outline around this point and
this point indicate that those are the
two vectors that will support this model
and thus we call them the support
vectors now let's get the parameters of
the hyperplane from the model and let's
plot a line that will be our hyperplane
there's our hyperplane line and
therefore to the left and below are
mules and to the right and above our
horses above that line let's take a few
new observations and I've encoded these
so that it would be illustrative we'll
take these new observations and we'll
plot them and I'm going to generate a
new plot red being the horses black dots
being the mules I'll create an X and Y
axis I'm going to plot Three Dots and
you can see one of them the Green Dot
the first one kind of Falls near the
mules the blue one kind of Falls near
the horses and visually we might
conclude hey one's a mule one's a horse
but what about the orange dot well for
the orange dot we really need our
hyperplane we really need something to
clearly indicate which side it's on and
of course by looking at this data
visualization you can tell that the
orange dot is probably a mule and let's
verify those results by sticking those
observations into our model and predict
some outcomes and down below you can see
that for the three data points one two
and three the first one the green one
was negative one that's a mule the
second one was a positive one that's
blue that's the blue point which was a
horse and the third one which we weren't
sure about but clearly the svm model
correctly predicted that that third
point would be a negative one would be a
mule and we can visually see that by the
hyperplane on the graph so that's a
quick example of svm in general support
Vector machines are binary classifiers
we don't use them for higher level
classification if you had three or more
classes you might want to use something
like random Forest what applications you
can use it for face detection text
categorization image classification and
bioinformatics really anywhere that you
have the need to classify things into
two groups today we're going to cover
referred to as k n n and KN is really a
fundamental place to start in the
machine learning it's the basis of a lot
of other things and just the logic
behind it is easy to understand and
Incorporated in other forms of machine
learning so today what's in it for you
why do we need k n in what is k n how do
we choose the factor k
when do we use k n n
how does k n algorithm work and then
we'll dive in to my favorite part the
use case predict whether a person will
have diabetes or not that is a very
common and popular used data set as far
as testing out models and learning how
to use the different models in machine
learning by now we all know machine
learning models make predictions by
learning from the past data available so
we have our input values our machine
learning model Builds on those inputs of
what we already know and then we use
that to create a predicted output is
that a dog a little kid looking over
there and watching the black cat cross
their path no dear you can differentiate
between a cat and a dog based on their
characteristics cats
cats have sharp claws uses to climb
smaller length of ears meows and purrs
doesn't love to play around dogs they
have dull claws bigger length of ears
barks loves to run around you usually
don't see a cat running around people
although I do have a cat that does that
where dogs do and we can look at these
we can say we can evaluate their
sharpness of the claws how sharp are
their claws and we can evaluate the
length of the ears and we can usually
sort out cats from dogs based on even
those two characteristics now tell me if
it is a cat or a dog an odd question
usually little kids no cats and dogs by
now unless you live in a place where
there's not many cats or dogs so if we
look at the sharpness of the claws the
length of the ears and we can see that
the cat has a smaller ears and sharper
claws than the other animals its
features are more like cats it must be a
cat sharp claws length of ears and goes
in the cat group because KNN is based on
feature similarity we can do
classification using kn and classifier
so we have our input value the picture
of the black cat it goes into our
trained model and it predicts that this
is a cat coming out so what is KNN what
is the k n algorithm K nearest neighbors
is what that stands for it's one of the
simplest supervised machine learning
algorithms mostly used for
classification so we want to know is
this a dog or is not a dog is it a cat
or not a cat it classifies a data point
based on how his neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we've gone
from cats and dogs right into wine
another favorite of mine k n stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus a chloride level and then
the different wines they've tested and
where they fall on that graph based on
how much sulfur dioxide and how much
chloride K and K N is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equals
five we'll talk about K in just a minute
a day point is classified by the
majority of votes from its five nearest
neighbors here the unknown point would
be classified as red since four out of
five neighbors are read so how do we
choose K how do we know k equals five I
mean this was the value we put in there
so we're going to talk about it how do
we choose the factor K and N algorithm
is based on feature similarity choosing
the right value of K is a process called
parameter tuning and is important for
better accuracy so at k equals three we
can classify we have a question mark in
the middle as either a as a square or
not is it a square or is it in this case
a triangle and so if we set k equals to
3 we're going to look at the three
nearest neighbors we're going to say
this as a square and if we put k equals
to 7 we classify as a triangle depending
on what the other data is around it you
can see as the K changes depending on
where that point is that drastically
changes your answer and we jump here we
go how do we choose the factor of K
you'll find this in all machine learning
choosing these facts factors that's the
face you get he's like oh my gosh you
say choose the right K did I set it
right my values in whatever machine
learning tool you're looking at so that
you don't have a huge bias in One
Direction or the other and in terms of
knnn the number of K if you choose it
too low the bias is based on it's just
too noisy it's it's right next to a
couple things and it's going to pick
those things and you might get a skewed
answer and if your K is too big then
it's going to take forever to process so
you're going to run into processing
issues and resource issues so what we do
the most common use and there's other
options for choosing K is to use the
square root of n so N is a total number
of values you have you take the square
root of it in most cases you also if
it's an even number so if you're using
uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of n and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use knnn we can use K N when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where you might
get into a gig of data if it's really
clean it doesn't have a lot of noise
because K N is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the k n but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the knnn and also just
using for smaller data sets k n works
really good how does a k n algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false are either
normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using k n n so
if we have new data coming in this says
57 kilograms and 177 centimeters is that
going to be normal or underweight to
find the nearest neighbors we'll
calculate the euclidean distance
according to the euclidean distance
formula the distance between two points
in the plane with the coordinates x y
and a b is given by distance D equals
the square root of x minus a squared
plus y minus B squared and you can
remember that from the two edges of a
triangle we're Computing the third Edge
since we know the X side and the Y side
let's calculate it to understand clearly
so we have our unknown point and we
placed it there in red and we have our
other points where the data is scattered
around the distance D1 is the square
root of 170 minus 167 squared plus 57
minus 51 squared which is about 6.7 and
distance 2 is about 13 and distance 3 is
about 13.4 similarly we will calculate
the euclidean distance of unknown data
point from all the points in the data
set and because we're dealing with small
amount of data that's not that hard to
do and it's actually pretty quick for a
computer and it's not a really
complicated Mass you can just see how
close is the data based on the euclidean
distance hence we have calculated the
euclidean distance of unknown data point
from all the points as shown where X1
and y1 equal 57 and 170 whose class we
have to classify so now we're looking at
that we're saying well here's the
euclidean distance who's going to be
their closest name neighbors now let's
calculate the nearest neighbor at k
equals three and we can see the three
closest neighbors puts them at normal
and that's pretty self-evident when you
look at this graph it's pretty easy to
say okay what you know we're just voting
normal normal three votes for normal
this is going to be a normal weight so
majority of neighbors are pointing
towards normal hence as per k n
algorithm the class of 57 170 should be
normal so a recap of knnn positive
integer K is specified along with a new
sample we select the K entries in our
database which are closest to the new
sample we find the most common
classification of these entries this is
the classification we give to the new
sample so as you can see it's pretty
straightforward we're just looking for
the closest things that match what we
got so let's take a look and see what
that looks like in a use case in Python
so let's dive into the predict diabetes
use case so use case predict diabetes
the objective predict whether a person
will be diagnosed with diabetes or not
we have a data set of 768 people who
were or were not diagnosed with diabetes
and let's go ahead and open that file
and just take a look at that data and
this is in a simple spreadsheet format
the data itself is comma separated very
common set of data and it's also a very
common way to get the data and you can
see here we have columns a through I
that's what one two three four five six
seven eight eight columns with a
particular attribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes that's why
they're taking it and that could cause
issue in some of the machine learning
packages but for a very basic setup this
works fine for doing the KNN and the
next thing you notice is it didn't take
very much to open it up I can scroll
down to the bottom of the data there's
768. it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer computer I
can look at it I can manipulate it and
it's not going to really tax just a
regular desktop computer you don't even
need an Enterprise version to run a lot
of this so let's start with importing
all the tools we need and before that of
course we need to discuss what IDE I'm
using certainly can use any particular
editor for python but I like to use for
doing very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python36 I have a couple
different versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where you can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see I have
a number of windows open up at the top
the one we're working in and since we're
working on the k n predict whether a
person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
Cell actually we want to go ahead and
first insert a cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run this python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice minus what we're working on and by
now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy is NP
pandas is the pandas data frame and
numpy is a number array very powerful
tools to use in here so we have our
Imports so even brought in our pandas
are numpy our two general python tools
and then you can see over here we have
our train test split by now you should
be familiar with splitting the data we
want to split part of it for training
our thing and then training our
particular model and then we want to go
ahead and test the remaining data to see
how good it is pre-processing a standard
scalar preprocessor so we don't have a
bias of really large numbers remember in
the data we had like number of
pregnancies isn't going to get very
large where the amount of insulin they
take and get up to 256 so 256 versus 6
that will skew results so we want to go
ahead and change that so they're all
uniform between
-1 and 1. and then the actual tool this
is the K neighbors classifier we're
going to use
and finally the last three are three
tools to test all about testing our
model how good is it we just put down
test on there and we have our confusion
Matrix our F1 score and our accuracy so
we have our two general python modules
we're importing and then we have our six
modules specific from the SK learn setup
and then we do need to go ahead and run
this so these are actually imported
there we go and then move on to the next
step and so in this set we're going to
go ahead and load the database we're
going to use pandas remember pandas is
PD and we'll take a look at the data in
Python we looked at it in a simple
spreadsheet but usually I like to also
pull it up so we can see what we're
doing so here's our data set equals
pd.read CSV that's a pandas command and
the diabetes folder I just put in the
same folder where my IPython script is
if you put in a different folder you
need the full length on there we can
also do a quick Links of the data set
that is a simple python on command Len
for length we might even let's go ahead
and print that we'll go print and if you
do it on its own line link that data set
in the jupyter notebook it'll
automatically print it but when you're
in most of your different setups you
want to do the print in front of there
and then we want to take a look at the
actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I usually always
like to keep the print statement in
there but because most projects only use
one data frame pandas data frame doing
it this way it doesn't really matter the
other way works just fine and you can
see when we hit the Run button we have
the 768 lines which we knew and we have
our pregnancies it's automatically given
a label on the left remember the head
only shows the first five lines so we
have zero through four and just a quick
look at the data you can see it matches
what we looked at before we have
pregnancy glucose blood pressure sure
all the way to age and then the outcome
on the end and we're going to do a
couple things in this next step we're
going to create a list of columns where
we can't have zero there's no such thing
as zero skin thickness or zero blood
pressure zero glucose any of those you'd
be dead so not a really good Factor if
they don't if they have a zero in there
because they didn't have the data and
we'll take a look at that because we're
going to start replacing that
information with a couple of different
things and let's see what that looks
like so first we create a nice list as
you can see we have the values talked
about glucose blood pressure skin
thickness and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on a very common thing to
do and then for this particular setup we
certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the N A but we're
going to go ahead and do it as a data
set column equals datasetc column dot
replace this is this is still pandas you
can do a direct there's also that's that
you look for your nan a lot of different
options in here but the Nan num pnan is
what that stands for is none it doesn't
exist so the first thing we're doing
here is we're replacing the zero with a
numpy none there's no data there that's
what that says that's what this is
saying right here so put the 0 in and
we're going to play zeros with no data
so if it's a zero that means a person's
well hopefully not dead hope they just
didn't get the data the next thing we
want to do is we're going to create the
mean which is the integer from the data
set from the column dot mean where we
skip in A's we can do that and that is a
pandas command there the skip n a so
we're going to figure out the mean of
that data set and then we're going to
take that data set column and we're
going to replace all the npnan with the
means why did we do that and we could
have actually just taken this step and
gone right down here and just replace 0
and Skip anything where except you can
actually there's a way to skip zeros and
then just replace all the zeros but in
this case we want to go ahead and do it
this way so you can see that we're
switching this to a non-existent value
then we're going to create the mean well
this is the average person so if we
don't know what it is if they did not
get the data and the data is missing one
of the tricks is you replace it with the
average what is the most common data for
that this way you can still use the rest
of those values to do your computation
and it kind of just brings that
particular value of those missing values
out of the equation let's go ahead and
take this and we'll go ahead and run it
doesn't actually do anything so we're
still preparing our data if you want to
see what that looks like we don't have
anything in the first few lines just not
going to show up but we certainly could
look at a row let's do that let's go
into our data set with the printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the one shows you can see I skipped a
bunch in the middle because that's what
it does we have too many lines in
Jupiter notebook it'll skip a few and go
on to the next in a data set let me go
and remove this and we'll just zero out
that and of course before we do any
processing before proceeding any further
we need to split the data set into our
train and testing data that way we have
something to train it with and something
to test it on and you're going to notice
we did a little something here with the
pandas database code there we go my
drawing tool we've added in this right
here off the data set and what this says
is that the first one in pandas this is
from the PD pandas it's going to say
within the data set we want to look at
the eye location and it is all rows
that's what that says so we're going to
keep all the rows but we're only looking
at 0 columns 0 To 8. Remember column
nine here it is right up here we printed
it in here is outcome well that's not
part of the training data that's part of
the answer yes column nine but it's
listed as eight number eight so zero to
eight is nine columns so eight is the
value and when you see it in here 0 this
is actually zero to seven it doesn't
include the last one and then we go down
here to Y which is our answer and we
want just the last one just column eight
and you can do it this way with this
particular notation and then if you
remember we imported the train test
split that's part of the SK learn right
there and we simply put in our X and our
y we're going to do random State equals
zero you don't have to necessarily seed
it that's a seed number I think the
default is one one when you seated I
have to look that up and then the test
size test size is 0.2 that simply means
we're going to take 20 percent of the
data and put it aside so that we can
test it later that's all that is and
again we're going to run it not very
exciting so far we haven't had any
printout other than to look at the data
but that is a lot of this is prepping
this data once you prep it the actual
lines of code are quick and easy and
we're almost there with the actual
running of our k n n we need to go ahead
and do a scale the data if you remember
correctly we're fitting the data in a
standard scalar which means instead of
the data being from you know 5 to 303 in
one column and the next column is one to
six we're going to set that all so that
all the data is between minus one and
one that's what that standard scalar
does keeps it standardized and we only
want to fit the scalar with the training
set but we want to make sure the testing
set is the X test going in is also
transformed so it's processing it the
same so here we go with our standard
scalar we're going to call it SC
underscore X for the scalar and we're
going to import the standard scalar into
this variable and then our X train
equals SC underscore x dot fit transform
so we're creating the scalar on the X
train variable and then our X test we're
also going to transform it so we've
trained and transformed the X train and
then the X test isn't part of that
training it isn't part of the of
training the Transformer it just gets
transformed that's all it does and again
we're going to run this and if you look
at this we've now gone through these
steps all three of them we've taken care
of replacing our zeros for key columns
it shouldn't be zero and we replace that
with the means of those columns that way
that they fit right in with our data
models we've come down here we split the
data so now we have our test data and
our training data and then we've taken
and we've scaled the data so all of our
data going in now no we don't TR we
don't train the Y part the y train and Y
test that never has to be trained it's
only the data going in that's what we
want to train in there then Define the
model using K Nabors classifier and fit
the train data in the model so we do all
that data prep and you can see down here
we're only going to have a couple lines
of code where we're actually building
our model and training it that's one of
the cool things about Python and how far
we've come it's such an exciting time to
be in machine learning because there's
so many automated tools let's see before
we do this let's do a quick Links of and
let's do y we want yeah let's just do
length of Y and we get 768 and if we
import math we do math dot square root
let's do y train there we go it's
actually supposed to be X train before
we do this let's go ahead and do import
math and do math square root length of Y
test and when I run that we get 12.409 I
want to see show you where this number
comes from we're about to to use 12 is
an even number so if you know if you're
ever voting on things remember the
neighbors all vote don't want to have an
even number of neighbors voting so we
want to do something odd and let's just
take one away we'll make it 11. let me
delete this out of here that's one of
the reasons I love Jupiter notebook
because you can flip around and do all
kinds of things on the fly so we'll go
ahead and put in our classifier we're
creating our classifier now and it's
going to be the K neighbors classifier
and neighbors equal 11. remember we did
12 minus 1 for 11. we have an odd number
of neighbors P equals 2 because we're
looking for is it are they diabetic or
not and we're using the euclidean metric
there are other means of measuring the
distance you could do like square square
means value there's all kinds of measure
this but the euclidean is the most
common one and it works quite well it's
important to evaluate the model let's
use the confusion Matrix to do that and
we're going to use the confusion Matrix
wonderful tool and then we'll jump into
the F1 score
and finally accuracy score which is
probably the most commonly used quoted
number when you go into a meeting or
something like that so let's go ahead
and paste that in there and we'll set
the cm equal to confusion Matrix why
test why predict so those are the two
values we're going to put in there and
let me go ahead and run that and print
it out and the way you interpret this is
you have the Y predicted which would be
your title up here we could do let's
just do p r e d
predicted across the top and actual
going down actual
it's always hard to write in here actual
that means that this column here down
the middle that's the important column
and it means that our prediction said 94
and prediction in the actual agreed on
94 and 32. this number here the 13 and
the 15 those are what was wrong so you
could have like three different if
you're looking at this across three
different variables instead of just two
you'd end up with the third row down
here in the column going down the middle
so in the first case we have the the and
I believe the zero has a 94 people who
don't have diabetes the prediction said
that 13 of those people did have
diabetes and were at high risk and the
32 that had diabetes it had correct but
our prediction said another 15 out of
that 15 it classified as incorrect so
you can see where that classification
comes in and how that works on the
confusion Matrix then we're going to go
ahead and print the F1 score let me just
run that and you see we get a 0.69 in
our F1 score the F1 takes into account
both sides of the balance of false
positives where if we go ahead and just
do the accuracy account and that's what
most people think of is it looks at just
how many we got right out of how many we
got wrong so a lot of people when you're
a data scientist and you're talking to
other data scientists they're going to
ask you what the F1 score the F score is
if you're talking to the general public
or the decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82 percent not
too bad for a quick flash look at
people's different statistics and
running an sklearn and running the knnn
the K nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very whether
they should go get a checkup and have
their glucose checked regularly or not
the print accuracy score we got the
0.818 was pretty close to what we got
and we can pretty much round that off
and just say we have an accuracy of 80
percent tells us it is a pretty fair fit
in the model introducing naive Bayes
classifier have you ever wondered how
your mail provider implements spam
filtering or how online news channels
perform news text classification or how
companies perform sentimental analysis
of Their audience on social media all of
this and more is done through a machine
learning algorithm called naive Bayes
classifier we'll start with what is
naive Bayes a basic overview of how it
works we'll get into naive Bayes and
machine learning where it fits in with
our other machine learning tools why do
we need naive Bayes and understanding
naive Bayes classifier a much more in
depth of how the math works in the
background finally we'll get into the
advantages of the Nave Bayes classifier
in the machine learning setup and then
we'll roll up our sleeves and do my
favorite part we'll actually do some
python coding and do some text
classification using the naive base what
is naive Bayes let's start with a basic
introduction to the Bayes theorem named
after Thomas Bayes from the 1700s who
first coined this in the western
literature naive Bayes classifier works
on the principle of conditional
probability as given by the Bayes
theorem before we move ahead let us go
through some of the simple Concepts in
the probability that we will be using
let us consider the following example of
tossing two coins here we have two
quarters and if we look at all the
different possibilities of what they can
come up as we get that they can come up
as head heads they come up as head tell
tell head and Telltale when doing the
math on probability we usually denote
probability as a p a capital P so the
probability of getting two heads equals
one-fourth you can see in our data set
we have two heads and this occurs once
out of the four possibilities and then
the probability of at least one tail
occurs three quarters of the time you'll
see on three of the coin tosses we have
tails and and now to 4 that's 3 4. and
then the probability of the second coin
being head given the first coin is tell
is one-half and the probability of
getting two heads given the first coin
is a head is one half we'll demonstrate
that in just a minute and show you how
that math works now when we're doing it
with two coins it's easy to see but when
you have something more complex you can
see where these Pro these formulas
really come in and work so the Bayes
theorem gives us a conditional
probability of an event a given another
event B has occurred in this case the
first coin toss will be B and the second
coin toss a this could be confusing
because we've actually reversed the
order of them and go from B to a instead
of a to B you'll see this a lot when you
work in probabilities the reason is
we're looking for event a we want to
know what that is so we're going to
label that a since that's our focus and
then given another event B has occurred
in the Bayes theorem as you can see on
the left the probability of a occurring
given B has occurred equals the
probability of B occurring given a has
occurred times the probability of a over
the probability of B this simple formula
can be moved around just like any
algebra formula and we could do the
probability of a after a given B times
probability of b equals the probability
of B given a times probability of a you
can easily move that around and multiply
it and divide it out let us apply a
Bayes theorem to our example here we
have our two quarters and we'll notice
that the first two probabilities of
getting two heads and at least one tail
we compute directly off the data so you
can easily see that we have one example
HH out of four one fourth and we have
three with tails in them giving us three
quarters or three-fourths seventy-five
percent the second condition the second
set three and four we're gonna explore a
little bit more in detail now we stick
to a simple example with two coins
because you can easily understand the
math the probability of throwing a tail
doesn't matter what comes before it and
the same with the heads so still going
to be fifty percent or one half but when
they come when that probability gets
more complicated let's say you have a D6
dice or some other instance then this
formula really comes in handy but let's
stick to the simple example for now in
this sample space let a be the event
that the second coin is head and B be
the event that the first coin is tells
again we reversed it because we want to
know what the second event is going to
be so we're going to be focusing on a
and we write that out as a probability
of a given B and we know this from our
formula that that equals the probability
of B given a times the probability of a
over the probability of B and when we
plug that in we plug in the probability
of the first coin being tells given the
second coin is heads and the probability
of the second coin being heads given the
first coin being over the probability of
the first coin being Tails when we plug
that data in and we have the probability
of the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have one
half times one-half over one half F or
one-half equals 0.5 or 1 4. so the Bayes
theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understand standing naive
bays and machine learning like with any
of our other machine learning tools it's
important to understand where the naive
Bayes fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also a
reward system This falls under the
supervised learning and then under the
supervised learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease fees or
other elements and news classification
when you look at the Google news and it
says well is this political or is this
world news or a lot of that's all done
with the naive Bayes understanding naive
Bayes classifier now we already went
through a basic understanding with the
coins and the two heads and two tells
and head tail tail heads Etc we're going
to do just a quick review on that and
remind you that the naive Bayes
classifier is based on the Bayes theorem
which gives a conditional probability of
event a given event B and that's where
the probability of a given b equals the
probability of B given a times
probability of a over probability of B
remember this is an algebraic function
so we can move these different entities
around we could multiply by the
probability of B so it goes to the left
hand side and then we could divide by
the probability of a given B and just as
easy come up with a new formula for the
probability of B to me staring at these
algebraic functions kind of gives me a
slight headache it it's a lot better to
see if we can actually understand how
this data fits together in a table and
let's go ahead and start applying it to
some actual data so you can see what
that looks like so we're going to start
with the shopping demo problem statement
and remember we're going to solve this
first in table form so you can see what
the math looks like and then we're going
to solve it in Python and in here we
want to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the person
is going to buy based on these traits so
we can maximize them and find out the
best system for getting somebody to come
in and purchase our goods and products
from our store now having a nice visual
is great but we do need to dig into the
data so let's go ahead and take a look
at the data set we have a small sample
data set of 30 rows we're showing you
the first 15 of those rows for this demo
now the actual data file you can request
just type in below under the comments on
the YouTube video and we'll send you
some more information and send you that
file as you can see here the file is
very simple columns and rows we have the
day the discount the free delivery and
did the person purchase or not and then
we have under the day whether it was a
weekday a holiday was it the weekend
this is a pretty simple set of data and
long before computers people used to
look at this data and calculate this all
by hand so let's go ahead and walk
through this and see what that looks
like when we put that into tables also
note in today's world we're not usually
looking at three different variables in
30 rows nowadays because we're able to
collect data so much we're usually
looking at 27 30 variables across
hundreds of rows the first thing we want
to do is we're going to take this data
and based on the data set containing our
three inputs Day discount and free
delivery we're going to go ahead and
populate that to frequency tables for
each attribute so we want to know if
they had a discount how many people buy
and did not buy did they have a discount
yes or no do we have a free delivery yes
or no on those dates how many people
made a purchase how many people didn't
and the same with the three days of the
week was it a weekday a weekend a
holiday and did they buy yes or no as we
dig in deeper to this table for our
Bayes theorem let the event buy ba now
remember we looked at the coins I said
we really want to know what the outcome
is did the person buy or not and that's
usually event a is what you're looking
for and the independent variables
discount free delivery and day BB so
we'll call that probability of B now let
us calculate the likelihood table for
one of the variables let's start with
day which includes weekday weekend and
holiday and let us start by summing all
of our our rows so we have the weekday
row and out of the weekdays there's nine
plus two so it's 11 weekdays there's
eight weekend days and eleven holidays
that's a lot of holidays and then we
want to sum up the total number of days
so we're looking at a total of 30 days
let's start pulling some information
from our chart and see where that takes
us and when we fill in the chart on the
right you can see that 9 out of 24
purchases are made on the weekday 7 out
of 24 purchases on the weekend and 8 out
of 24 purchases on a holiday and out of
all the people who come in 24 out of 30
purchase you can also see how many
people do not purchase on the weekdays
two out of six didn't purchase and so on
and so on we can also look at the totals
and you'll see on the right we put
together some of the formulas the
probability of making a purchase on the
weekend comes out at 11 out of 30. so
out of the 30 people who came into the
store throughout the weekend weekday and
holiday 11 of those purchases were made
on the weekday and then you can also see
the probability of them not making a
purchase and this is done for doesn't
matter which day of the week so we call
that probability of no buy would be 6
over 30 or 0.2 so there's a 20 percent
chance that they're not going to make a
purchase no matter what day of the week
it is and finally we look at the
probability of B of A in this case we're
going to look at the probability of the
weekday and not buying two of the no
buys were done on the weekend out of the
six people who did not make purchases so
when we look at that probability of the
week day without a purchase is going to
be 0.33 or 33 percent let's take a look
at this at different probabilities and
based on this likelihood table let's go
ahead and calculate conditional
probabilities as below the first three
we just did the probability of making a
purchase on the weekday is 11 out of 30
or roughly 36 or 37 percent 0.367 the
probability of not making a purchase at
all doesn't matter what day of the week
is roughly 0.2 or 20 percent and the
probability of a weekday no purchase is
roughly two out of six so two out of six
of our no purchases were made on the
weekday and then finally we take our P
of a b if you look we've kept the
symbols up there we got P of probability
of B probability of a probability of B
if a we should remember that the
probability of a if B is equal to the
first one times the probability of no
buys over the probability of the weekday
so we could calculate it both off the
table we created we can also calculate
this by the formula and we get the 0.36
7 which equals or 0.33 times 0.2 over
0.367 which equals 0.179 or roughly 17
to 18 percent and that'd be the
probability of no purchase done on the
weekday and this is important because we
can look at this and say as the
probability of buying on the weekday is
more than the probability of not buying
on the weekday we can conclude that
customers will most likely buy the
product on a weekday now we've kept our
chart simple and we're only looking at
one aspect so you should be able to look
at the table and come up with the same
information or the same conclusion that
should be kind of intuitive at this
point next we can take the same setup we
have the frequency tables of all three
independent variables now we can
construct the likelihood tables for all
three of the variables we're working
with we can take our day like we did
before we have weekday weekend and
holiday we filled in this table and then
we can come in and also do that for the
discount yes or no did they buy yes or
no and we fill in that full tape table
so now we have our probabilities for a
discount and whether the discount leads
to a purchase or not and the probability
for free delivery does that lead to a
purchase or not and this is where it
starts getting really exciting let us
use these three likelihood tables to
calculate whether a customer will
purchase a product on a specific
combination of Day discount and free
delivery or not purchase here let us
take a combination of these factors day
equals holiday discount equals yes free
delivery equals yes
let's dig deeper into the math and
actually see what this looks like and
we're going to start with looking for
the probability of them not purchasing
on the following combinations of days
we're actually looking for the
probability of a equal no by no purchase
and our probability of B we're going to
set equal to is it a holiday did they
get a discount yes and was it a free
delivery yes before we go further let's
look at the original equation the
probability of a if B equals a
probability of B given the condition a
and the probability times the
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that in a
second in the formula time is the full
probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is the day equal holiday
those are our three variables of the
probability of a if B and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six or a no
purchase on a free delivery day and
three out of six or a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the six out of 30. we divide that
out by the probability of each category
over the total number so we get the 20
out of 30 had a discount 23 out of 30
had a yes for free delivery and 11 out
of 30 were on a holiday we plug all
those numbers in we get
0.178 so in our probability math we have
a 0.178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times the day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our
0.986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.986 we have a
probability of no purchase equals
0.178 so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals 0.98686 plus 0.178 and that
equals the 1.16 six four if we divide
each probability by the sum we get the
percentage and so the likelihood of a
purchase is 84.71 percent and the
likelihood of no purchase is 15.29
percent given these three different
variables so it's if it's on a holiday
if it's a with a discount and has free
delivery then there's an 84.71 percent
chance that the customer is going to
come in and make a purchase hooray they
purchased our stuff we're making money
if you're owning a shop that's like is
the bottom line is you want to make some
money so you keep your shop open and
have a living now I promised you that we
were going to be finishing up the math
here with a few pages so we're going to
move on and we're going to do two steps
the first step is I want you to
understand
why you want to under why you want to
use the naive Bayes what are the
advantages of naive Bayes and then once
we understand those advantages we just
look at that briefly then we're going to
dive in and do some python coding
advantages of naive Bayes classifier so
let's take a look at the six advantages
of the naive Bayes classifier and we're
going to walk around this lovely wheel
looks like an origami folded paper the
first one is very simple and easy to
implement certainly you could walk
through the tables and do this by hand
you got to be a little careful because
the notations can get confusing you have
all these different probabilities and I
certainly mess those up as I put them on
you know is it on the top of the bottom
got to really pay close attention to
that when you put it into python it's
really nice because you don't have to
worry about any of that you let the
python handle that the python module but
understanding it you can put it on a
table and you can easily see how it
works and it's a simple algebraic
function it needs less training data so
if you have smaller amounts of data this
is great powerful tool for that handles
both continuous and discrete data it's
highly scalable with number of
predictors and data points so as you can
see just keep multiplying different
probabilities in there and you can cover
not just three different variables or
sets you can now expand this to even
more categories number five it's fast it
can be used in real time prediction ends
this is so important this is why it's
used in a lot of our predictions on
online shopping carts referrals spam
filters is because there's no time delay
as it has to go through and figure out a
neural network or one of the other mini
setups where you're doing classification
and certainly there's a lot of other
tools out there in the machine learning
that can handle these but most of them
are not as fast as the naive Bayes and
then finally it's not sensitive to
irrelevant features so it picks up on
your different probabilities and if
you're short on data on one probability
you can kind of it automatically adjust
for that those formulas are very
automatic and so you can still get a
very solid predictability even if you're
missing data or you have overlapping
data for two completely different areas
we see that a lot in doing census and
studying of people and habits where they
might have one study that covers one
aspect another one that overlaps and
because of two overlap they can then
predict the unknowns for the group that
they haven't done the second study on or
vice versa so it's very powerful in that
it is not sensitive to the irrelevant
features and in fact you can use it to
help predict features that aren't even
in there so now we're down to my
favorite part we're going to roll up our
sleeves and do some actual programming
we're going to do the use case text
classification now I would challenge you
to go back and send us a note on the
notes below underneath the video and
request the data for the shopping cart
so you can plug that into python code
and do that on your own time so you can
walk through it since we walk through
all the information on it but we're
going to do a python code doing text
classification very popular for doing
the naive Bayes so we're going to use
our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our new use
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've traded it and we've shown you a
graph of what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever interface IDE you
want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you you have it set up correctly
should also work in a lot of the 2x you
just have to make sure all of the
versions of the modules match your
python version and in here you'll notice
the first line is your percentage
matplot library in line now three of
these lines of code are all about
plotting the graph this one lets the
notebook notes and this is an inline
setup that we want the graphs to show up
on this page without it in a notebook
like this which is an Explorer interface
it won't show up now a lot of Ides don't
require that a lot of them like on if
I'm working on one of my other setups it
just has a pop-up and the graph pops up
on there so you have that set up also
but for this we want the matplot library
in line and then we're going to import
numpy as NP that's number python which
has a lot of different formulas in it
that we use for both of our sklearn
module and we also use it for any of the
upper math functions in Python and it's
very common to see that as NP numpy is
NP then next two lines are all about our
graphing remember I said three of these
were about graphing well we need our
matplot library.pi plot as PLT and
you'll see that PLT is a very common
setup as is the SNS and just like the NP
and we're going to import Seaborn as S
and S and we're going to do the sns.set
now Seaborn sits on top of Pi plot and
it just makes a really nice heat map
it's really good for heat maps and if
you're not familiar with heat maps that
just means we give it a color scale the
term comes from the brighter red it is
the hotter it is in some form of data
and you can set it to whatever you want
and we'll see that later on so those
you'll see that those three lines of
code here are just importing the graph
function so we can graph it and as a
data science test you always want to
graph your data and have some kind of
visual it's really hard just to shove
numbers in front of people and they look
at it and it doesn't mean anything and
then from the
sklearn.data sets we're going to import
the fetch 20 news groups very common one
for analyzing tokenizing where words and
setting them up and exploring how the
words work and how do you categorize
different things when you're dealing
with documents and then we set our data
equal to fetch 20 news groups so our
data variable will have the data in it
and we're going to go ahead and just
print the target names data.target names
and let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp OS Ms
windows.miscellaneous and it goes all
the way down to talk
politics.miscellaneous talk
religion.miscellaneous these are the
categories we've already assigned to
this news group and it's called Fetch 20
because you'll see there's I believe
there's 20 different Topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually getting here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does
first we're going to set our categories
now if you noticed up here I could have
just as easily set this equal to
data.target underscore names because
it's the same thing but we want to kind
of spell it out for you so you can see
the different categories it kind of
makes it more visual so you can see what
your data is looking like in the
background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 News Group subset
equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates
train.data and we're just going to look
at data piece number five and let's go
ahead and run that and see what that
looks like and you can see when I print
train dot data number five under train
it prints out one of the Articles this
is article number five you can go
through and read it on there and we can
also go in here and change this to test
which should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number 5 is a different article but it's
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
Links of train dot data and if we run
that you'll see that the training data
has 11
314 articles so we're not going to go
through all those articles that's a lot
of articles but we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs
vtt line 58 lines 58 in article
Etc and you can scroll all the way down
and see all the different parts to there
now we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bayes and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we
have the print module from the earlier
one I didn't know why that was in there
so we're going to start by importing our
necessary packages and from the sklearn
features extraction dot text we're going
to import TF IDF vectorizer I told you
we're going to throw a module at you we
can't go too much into the math behind
this or how it works you can look it up
the notation for the math is usually
tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times are used in a document
how many times or how many documents are
used in and it's a well used formula
it's been around for a while it's a
little confusing to put this in here but
let's let them know that it just goes in
there and waits the different words in
the document for us that way we don't
have to wait and if you put a weight on
it if you remember I was talking about
that up here earlier if these are all
emails they probably all have the word
from in them from probably has a very
low weight it has very little value in
telling you what this document's about
same with words like in an article in
articles in cost of on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the article's doing well how
do you figure out all those weights in
the different articles that's what this
module does that's what the TF IDF
vectorizer is going to do for us and
then we're going to import our
sklearn.naive Bays and that's our
multinomial in B multinomial naive base
pretty easy to understand that where
that comes from and then finally we have
the skylearn pipeline import make
pipeline now the make pipeline is just a
cool piece of code because we're going
to take the information we get from the
TF IDF vectorizer and we're going to
pump that into the multinomial in B so a
pipeline is just a way of organizing how
things flow it's used commonly you
probably already guess what it is if
you've done any businesses they talk
about the sales pipeline if you're on a
work crew or project manager you have
your pipeline of information that's
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model dot fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train dot
Target is what category they already
categorized that that particular article
as and what's Happening Here is the
trained data is going into the tfid
vectorizer so when you have one of these
articles it goes in there it waits all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bayes we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're going to set that equal
to model dot predict most of the sklearn
use the term dot predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already read this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that take a walk
through it and see what that looks like
so back to our Jupiter notebook I'm
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so let me get a little bit
bigger there we go no reason not to use
the whole screen too big so we have here
from sklearnmetrics import confusion
Matrix
and that's just going to generate a set
of data that says I the prediction was
such the actual truth was either agreed
with it or is something different and
it's going to add up those numbers so we
can take a look and just see how well it
worked and we're going to set a variable
matte equal to confusion Matrix we have
our test Target our test data that was
not part of the training very important
in data science we always keep our test
data separate otherwise it's not a valid
model if we can't properly test it with
new data and this is the labels we
created from that test data these are
the ones that we predict it's going to
be so we go in and we create our SN heat
map the SNS is our Seaborn which sits on
top of the pi plot so we create a
sns.heat map we take our confusion
Matrix and it's going to be
met.t and do we have other variables
that go into the sns.heat map we're not
going to go into detail what all the
variables mean The annotation equals
true that's what tells it to put the
numbers here so you have the 166 the one
the zero zero zero one format d and c
bar equals false have to do with the
format if you take those out you'll see
that some things disappear and then the
X tick labels and the y t labels those
are our Target names and you can see
right here that's the alt atheism comp
graphics composms windows.miscellaneous
and then finally we have our plt.x label
remember the SNS or the Seaborn sits on
top of our matplot library our PLT and
so we want to just tell it X label
equals a true is is true the labels are
true and then the Y label is prediction
label so when we say a true this is what
it actually is and the prediction is
what we predicted and let's look at this
graph because that's probably a little
confusing the way we rattled through it
and what I'm going to do is I'm going to
go ahead and flip back to the slides
because they have a black background
they put in there that helps it shine a
little bit better so you can see the
graph a little bit easier so in reading
this graph what we want to look at is
how the color scheme has come out and
you'll see a line right down the middle
diagonally from upper left to bottom
right what that is is if you look at the
labels we have our predicted label on
the the left and our true label on the
right those are the numbers where the
prediction and the true come together
and this is what we want to see is we
want to see those lit up that's what
that heat map does is you can see that
it did a good job of finding those data
and you'll notice that there's a couple
of red spots on there where it missed
you know it's a little confused we talk
about talk religion miscellaneous versus
talk politics miscellaneous social
religion Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics you could understand
why it might mislabel them but overall
it did a pretty good job if we're going
to create these models we want to go
ahead and be able to use them so let's
see what that looks like to do this
let's go ahead and create a definition a
function to run and we're going to call
this function let me just expand that
just a notch here there we go I like
mine in big letters predict categories
we want to predict the category we're
going to send it as a string and then
we're sending it train equals train we
have our training model and then we had
our pipeline model equals model this way
we don't have to resend these variables
each time the definition knows that
because I said train equals train and I
put the equal for model and then we're
going to set the prediction equal to the
model dot predict s so it's going to
send whatever string we send to it it's
going to push that string through the
pipeline the model pipeline it's going
to go through and tokenize it and put it
through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
I'll put that through our naive Bayes
and from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train.target namespredict of zero and
remember that the train.target names
that's just categories I could have just
as easily put categories in there dot
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from I don't know
what the actual number numbers are let's
say 0 equals alt atheism so we're going
to convert that 0 to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going to go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out as returning the
variable train.target underscore names
it'll automatically print that for you
in your own IDE you might have to put in
print let's see where else we can take
this and maybe you're a space science
buff so how about sending load to
International
Space Station
and if we run that we get science space
or maybe you're a automobile buff and
let's do um oh they were going to tell
me Audi is better than BMW but I'm going
to do BMW is better than an Audi so
maybe you're a car buff and we run that
and you'll see it says recreational I'm
assuming that's what Rec stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous
so when we take our definition or our
function and we run all these things
through Kudos we made it we were able to
correctly classify text into different
groups based on which category they
belong to using the naive Bayes
classifier now we did throw in the
pipeline the TF IDF vectorizer we threw
in the graphs those are all things that
you don't necessarily have to know to
understand the naive Bayes setup or
classifier but they're important to know
one of the main uses for the naive Bayes
is with the TF IDF tokenizer vectorizer
where tokenizes the word and as labels
and we use the pipeline because you need
to push all that data through and it
makes it really easy and fast you don't
have to know those to understand naive
Bayes but they certainly help for
understanding the industry in data
science and we can see their categorizer
our naive Bayes classifier we were able
to predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our
trained model and with that we have
reached the end of this data science
Basics course if you have any questions
please feel free to let us know in the
comments below and we'll have it
answered for you at the earliest until
next time thank you stay safe keep
learning and get ahead
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here