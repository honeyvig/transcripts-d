welcome to the captivating world of data
analysis and this comprehensive course
we will embark on an enlightening
Journey to the fundamentals of data
analysis Unleash the Power of
information as we dive deep into the
essential Concepts techniques and tools
that form the backbone of this field
prepare to master the art of extracting
valuable insights from raw data as we
export data collection data cleaning and
wrangling game Proficiency in
statistical analysis and visualization
by unlocking the ability to unravel
hidden patterns and prints we will then
delve into the realm of hypothesis
testing regulation analysis predictive
modeling equipping you with the skills
to make informed decisions and accurate
predictions this video course will
provide Hands-On examples and real world
applications to solidify your
understanding by the end you will
possess a robust foundation in data
analysis enabling you to confidently
navigate the vast sea of data and
transform it into actionable knowledge
so let's bhagwan this captivating
journey and unlock the immense power of
data analysis together having said that
if you're an aspiring data analyst
looking for an online training and
certification from prestigious
universities and in collaboration with
leading experts then search no more
simply learn Sports graduate program in
data analytics from Purdue University in
collaboration with IBM should be a right
choice for more details use the link in
the description box below with that in
mind over to our training experts hello
everyone we have an interesting topic
for today and that is data analytics
jobs career and salary I will run you
through the top 6 data analytics job
roles so before I dive deep into the
various job rules let's quickly
understand how important a career in
data analytics is and what the future
holds for Professionals in this domain
let's take a look at the growth of data
so back in the early 2000s there is
relatively less data generated but with
a rapid rise in Technologies and with
the increase in the number of various
social media platforms and multinational
companies across the globe the
generation of data has increased by
Leaps and Bounds did you know that
according to the IDC the total volume of
data is expected to reach 175 zettabytes
in 2025
now that's a lot of data let's take a
look at how organizations leverage all
of this data
as you know there are zillions of
companies across the world these
companies generate loads of data on a
daily basis when I say data here it
simply refers to business information
customer Data customer feedback product
Innovations sales reports and profit
loss reports to name a few
companies utilize all of this data in a
wise way they use all of this
information to make crucial decisions
that can either hamper or boost their
businesses you might have heard of the
term data is the new oil well it
definitely is but only if organizations
analyze all the available data very well
then this oil is definitely valuable
and for that we have data analytics
organizations take the help of data
analytics to convert the available raw
data into meaningful insights
so what is data analytics technically
you can say it is a process wherein data
is collected from various sources then
cleaned which involves removing
irrelevant information and then finally
transformed into some meaningful
information that can be interpreted by
humans
various Technologies tools and
Frameworks are used in the analysis
process
as you might have heard of the term data
never sleeps well it surely doesn't
every millisecond some of the other data
is generated and this is a constant
process this process is only going to
increase in the near future with the
Advent of newer Technologies the data
analytics domain holds Paramount
importance in every sector
companies want to leverage on all the
generated big data and boost their
businesses
they need professionals who can play
with data and convert them into crucial
insights organizations are constantly on
the lookout for such candidates and this
opportunity will only increase as data
is only going to grow every second
so if you want to start your career in
this field or if you want to switch your
job role into a role in the data
analytics domain then we have a set of
job profiles that you can look at
weave a look into six job roles in the
data analytics field and learn what each
job role is all about the
responsibilities of a professional
working in that particular role the
skills required to get that particular
job the average annual salary of a
professional working in that Troll and
finally the company is hiring for that
role so let's start off
first we had the job role of a data
analyst
a data analyst is a person who collects
processes and performs statistical
analysis of large data sets
every business generates and collects
data be it marketing research sales
figures Logistics or Transportation
costs a data analyst will take this data
and figure out a variety of measures
such as how to price new materials how
to reduce Transportation costs or how to
deal with issues that cost the company
money they deal with data handling data
modeling and Reporting
now talking about their responsibilities
data analysts recognize and understand
the organization's goal they collaborate
with different team members such as
programmers business analysts engineers
and data scientists to identify
opportunities for solving business
problems
data analysts write complex SQL queries
scripts and store procedures to gather
and extract information from multiple
databases
they filter and clean data using
different modern tools and techniques
and make it ready for analysis they also
perform data mining from primary and
secondary data sources
data analysts identify analyze and
interpret Trends in complex data sets
this is done using statistical tools
such as R and SAS
another key responsibility of a data
analyst is to create summary reports and
build various data visualizations for
decision making and presenting it to the
stakeholders
next let us discuss the important skills
that you need to know to become a data
analyst
firstly you should have a bachelor's
degree in computer science or
information technology a master's degree
in computer applications or statistics
is also preferable
you must have a good understanding of
programming languages like R python
JavaScript and also understand SQL
in addition to that it is beneficial if
you have hands-on experience with
statistical and data analytics tools
such as SAS Miner Microsoft Excel and
ssas
basic understanding of machine learning
and its algorithms would be an advantage
acquaint yourself with descriptive
predictive prescriptive and inferential
statistics
most importantly you need to have a good
working knowledge of various data
visualization software along with
presentation skills this will help you
pitch in your ideas and viewpoints to
the clients and stakeholders better
now talking about their salaries a data
analyst earns nearly 5 lakhs 23 000
rupees per annum in India while in the
United States they earn around 62 000
453 dollars per annum
let's now look at a few of the companies
hiring data analysts so as you can see
we have the American e-commerce giant
Amazon then we have Microsoft the
American online payments company PayPal
then we have Walmart Bloomberg and
Capital One so that was all about data
analyst
the next job role is of a business
analyst
business analysts help guide businesses
in improving products services and
software through data-driven solutions
they are responsible for Bridging the
Gap between it and business using data
analytics to evaluate processes
determine requirements and deliver
data-driven recommendations and reports
to Executives and stakeholders
business analysts are responsible for
creating new models that support
business decisions and come up with
initiatives and strategies to optimize
costs
now let us look at the various
responsibilities of a business analyst
business analysts have a good
understanding of the requirements for
business their vital role is to work in
accordance with relevant project
stakeholders to understand their
requirements and translate them into
details which the developers can
understand
they frequently interact with developers
and come up with a plan to design the
layout of a software application
they also run meetings with stakeholders
and other authorities they engage with
Business Leaders and users to understand
how data-driven changes to products
Services software and Hardware can
improve efficiencies and add value
they ensured that the project is running
smoothly as per the requirements and the
design planned through user acceptance
and validation testing
they make sure all the features are
being incorporated into the application
Bas rely on different software to write
documentation and design visualization
to explain all the findings it is
extremely critical for any ba to
effectively document the findings where
each requirement of the client is
mentioned in detail
now let us look at the skills required
for a ba
a bachelor's degree in the field of
science engineering or statistics or any
related domain will suffice
knowledge of programming languages such
as Python and Java is beneficial you
should be really good at writing complex
SQL queries and you should also have
knowledge of various business process
models
along with knowledge of programming
languages ideas about statistical
analysis and predictive modeling is
necessary
decision making strong analytical and
problem solving skills are necessary to
solve software and business issues you
also need to have excellent presentation
and communication skills both oral and
written
moving on to their salary a business
analyst is expected to earn around 7
lakh rupees per annum in India in the US
they earn nearly 68 346 dollars per
annum
iqia Dell Phillips Honeywell the famous
American messaging platform WhatsApp the
UK based company Ernest and Young a few
of the companies hiring for business
analysts
up next we have the job role of a
database administrator
a database administrator is a
specialized computer systems
administrator who maintains a successful
database environment by directing or
performing all related activities to
keep the organization's data secure
they are responsible for storing
organizing and retrieving data from
several databases and data warehouses
their top responsibility is to maintain
data Integrity this means that database
administrator will ensure that the data
is secure from unauthorized taxes
moving on to their responsibilities
a database administrator develops
designs and maintains a database to
ensure that the data in it is properly
stored organized and managed well
they maintain data Integrity by avoiding
unauthorized taxes and they keep
databases up to date
they run tests and modify the existing
databases to ensure that they operate
reliably they also inform end users of
changes in databases and train them to
utilize systems
they need to cooperate with programmers
data analysts and the IT staffs to
ensure smooth running and maintenance of
databases
database administrators are responsible
for taking system backups in case of
power outages and other disasters so
they should have an efficient Disaster
Recovery plan
now let's have a look at their skills
to become a database administrator you
should have a bachelor's degree in
computer science or information
technology
knowledge of programming languages such
as python Java and Scala is important
you need to carry at least three to five
years of experience in data management
you need to have an understanding of
different databases such as Oracle DB
mongodb MySQL server and postgresql also
they should have an idea about database
design and writing SQL queries
finally you need to have a good
understanding of operating systems such
as Windows Mac OS and Linux along with
storage Technologies
talking about their salary a database
administrator in India can earn up to 4
lakh 97 000 rupees per annum in the US
they earn around 78 000 per annum
let's have a look at the companies
hiring for database administrators
so as you see here we have bookmyshow
Oracle the American MNC Intel Amazon
Robert Half and the New York Times to
name a few
fourth in the list of job roles we have
data engineer a data engineer is someone
who's involved in preparing data for
analytical and operational uses a data
engineer transforms data into useful
format for analysis they build and test
scalable Big Data ecosystems for
businesses a data engineer is an
intermediary between a data analyst and
a data scientist
now let's jump into their
responsibilities
data Engineers develop test and maintain
architectures they are responsible for
managing optimizing and monitoring data
retrievals storage and distribution
throughout the organization
they discover opportunities for data
acquisition fine Trends in data sets and
develop algorithms to help make raw data
more useful to the Enterprise
data engineers build large data
warehouses using ETL for storing and
retrieving data
they also recommend ways to improve data
quality and efficiency along with
building algorithms to help give easier
access to Raw data
data Engineers often work with big data
and submit their reports to data
scientists for analysis purpose they
need to recommend and sometimes
Implement ways to improve data
reliability efficiency and quality
moving on to the skills of a data
engineer
a data engineer should hold a bachelor's
degree in computer science or
information technology
they should have good hands-on
experience with python R and Java
also data Engineers should be well
versed with big data Technologies such
as Hadoop Apache spark Scala Cassandra
and mongodb
data warehousing and detail experience
are essential to this position along
with in-depth knowledge of SQL and other
database Solutions
basic knowledge of statistical analysis
will be an advantage along with idea
about operating systems
here is what a data engineer can earn so
in India a data engineer can earn up to
8 lakhs 85 000 rupees per annum while
they can earn around 103 000 dollars a
year in the USA
we have capgemini shortest talk the
American provider of stock photography
Spotify Accenture genpact and Facebook
hiring data engineers
the next exciting job role is of a data
scientist
a data scientist is a professional who
uses statistical methods data analysis
techniques machine learning and related
Concepts in order to understand and
analyze data to draw business
conclusions they make sense to messy and
unstructured data and bring value out of
it they employ techniques and theories
drawn from many fields within the
context of mathematics statistics
computer science and information science
a data scientist understands the
challenges in business and comes up with
the best Solutions using modern tools
and techniques to analyze visualize and
build prediction models to make business
decisions
let us now look at their
responsibilities in the industries
data scientists clean process and
manipulate data using several data
analytics tools they perform ad hoc data
mining collect large sets of structured
and unstructured data from disparate
sources
they design and evaluate Advanced
statistical models to work on Big Data
they also create automated anomaly
detection systems and keep constant
track of their performance
data scientists interpret the analysis
of big data to discover Solutions and
opportunities
a data scientist takes input from data
analysts and Engineers to formulate the
results
they use visualization packages and
tools to create reports and dashboards
for Relevant stakeholders they also
adopt new business models and approaches
apart from this they regularly built
predictive models and machine learning
algorithms
now moving on to the skills of a data
scientist
a bachelor's degree in computer science
or information technology will be fine
but a master's degree in the field of
data science will hold a major advantage
you also need to have a good experience
in the analytics domain
you should be proficient in programming
languages such as python Java and C plus
knowledge of Perl will also be an
advantage
familiarity with Apache Hive big and
Apache spark is necessary along with the
knowledge of Hadoop
in addition to knowing programming
languages you also need to know SQL
machine learning and deep learning
data visualization and Bs skills are
necessary for creating reports and
dashboards you should also be able to
communicate and present information and
ideas properly
now talking about their salary a data
scientist in India can expect an annual
salary of 10 lakhs 47 000 rupees per
year
meanwhile in the US they can earn up to
113 000 per annum that's a lot of money
from the many companies hiring for data
scientists here we have a few companies
named they are yet again Amazon Citibank
Apple Google the Japanese electronic
Commerce and online retailing company
Rakuten and Facebook
and finally we have machine learning
engineer machine learning Engineers are
professionals who develop intelligent
machines that can learn from vast
amounts of data and apply knowledge
without human intervention
they use different algorithms and
statistical modeling to make sense of
data they design and develop machine
learning and deep learning algorithms
their main goal is to create
self-running software
let's have a look at the
responsibilities of a machine learning
engineer
machine learning Engineers research
design and develop machine Learning
Systems they use exceptional
mathematical skills in order to perform
faster computations and work with
algorithms to create sophisticated
models
they perform a b testing and use data
modeling to fine-tune the results
they use data modeling and evaluation
strategy to find hidden patterns and
predict unseen instances
machine learning Engineers work closely
with data Engineers to build data
pipelines and interact with stakeholders
to get a Clarity on the requirements
most importantly they analyze complex
data sets to verify data quality but for
model tests and experiments choose to
implement the right machine learning
algorithm and select the right training
data sets
moving on to their skills
a machine learning engineer should have
a degree in computer science and
information technology they should have
an advanced degree in computer science
or maths
in addition to this they should also
have experience in the same domain
they should be proficient in programming
languages such as python RC plus and
Java
knowledge of Statistics probability and
linear algebra is necessary as all the
machine learning algorithms have been
derived from mathematics also having an
idea of signal processing would be
beneficial
machine learning Engineers need to have
a good understanding of data
manipulation and machine learning
libraries such as numpy Panda
scikit-learn Etc
they should have good oral and written
communication skills
let us now have a look at their salary
structure a machine learning engineer
earns 8 lakh rupees per annum in India
while in the US they can earn around 114
000 a year now that's a whopping amount
isn't it
let's have a look at the company's
hiring machine learning engineers
so as you see we have Amazon Microsoft
Oracle Salesforce Rapido and Accenture
to name a few
that was all about the job role of a
machine learning engineer
now that we have seen the different job
roles in the field of data analytics
let's also go ahead and see how an ideal
resume of a data analyst should look
like
seen on your screens is a sample
resuming of a data analyst you can grab
some ideas from this and incorporate
them in your resume
nowadays it's quite common to have a
professional photograph of yours on the
resume you can go ahead and have that
then your name in bold followed by your
contact details like email ID and phone
number
then moving on you would have to write
your summary briefly explain your
current job role and what you're looking
for in the future having a LinkedIn
profile link works well these days
employers can just go ahead and look at
your profile and gauge you well
make sure to have an active LinkedIn
profile
in addition to LinkedIn profile it's
also good to have a GitHub profile link
which can show your coding or other
technical skills
if it's impressive enough then a lot of
times the rest of your resume is just
secondary
as I mentioned this is a resume of a
data analyst so as you can see in the
summary here we have just spoken about
the basic responsibilities of a data
analyst
moving on to the experience part you
have to write the job title and below
that you can mention the company and the
tenure accordingly here you would have
to give a brief description of
achievements in the organization any
relevant accomplishments related to the
job you're applying for the tools and
the various Technologies you have worked
with
so in the sample you can see we have
spoken about data visualization using R
in Tableau next we have spoken about how
the candidate has worked with other
teams for a Better Business outcome
most of the data analysts use SQL and
Excel to handle data for reporting and
database maintenance and we have
mentioned that here as well
do make sure that you always specify the
tools you use
then you can also mention if you have
worked on improving data delivery for
example here we have spoken about
developing and optimizing SQL queries
data aggregations and ETL to improve
data delivery
finally you can speak a bit about your
reporting skills and if needed elaborate
on it
usually professionals would have worked
in a similar domain before becoming a
data analyst here we have taken the role
of a statistical assistant as the first
job since it's easier for a candidate
with this job role to shift into the
data analytics field nevertheless you
all can still mention your prior
experience here be it in any domain
under the responsibilities for this job
role we have given Basics such as coding
data prior to compute entry compiling
statistics from various reports
Computing and analyzing data and find me
some visualization and Reporting
moving to the education here you can
mention the name of your degree and the
university name if you have a post
graduation well in good you can list
both the degrees here also if you have
any certifications you can mention them
here under the education category
now moving to the skills depending on
your skills and your choice you can
either shift this part to the beginning
of the resume or have it here
as you see on your screens this is just
a different way of displaying your skill
sets you can have all the five stars
colored if you are excellent in that
particular tool or language
as you see it's crystal clear as to what
the candidates strong areas are
you can have various categories like
shown
for example under software development
you can list the languages that you know
and how proficient you are in those
particular languages
it's clear that the candidate knows
python better than JavaScript here so
the employer gets a clear idea about the
skills you possess and the depth of it
similarly you can mention the databases
as well the few mentioned here are more
or less a requirement to become a data
analyst at least SQL is a must
not to forget data visualization is also
very important when it comes to the job
role of a data analyst mention the tools
you know here and similarly give
yourself a rating out of five five stars
shaded being the highest
here we have mentioned Tableau and Excel
which are more than sufficient to become
a data analyst
moving to the non-technical skills you
can mention the languages you know here
here we have taken English and German in
addition to the languages you can also
feel free to mention the extracurricular
activities that you are good at
so this is how an ideal resume of a data
analyst should look like you can alter
it according to your achievements skills
and experience a study from new Vantage
Partners suggests 97.2 percent of
companies are now investing in data and
its analysis nowadays every company
needs a data expert do you also want to
become a part of this Market if yes how
to become a part of it what are the
essential skills one should have
this video will answer all these
questions but before watching this video
please subscribe to Simply learns
YouTube channel and press the Bell icon
to never miss any updates first of all
we are going to discuss who is a data
analyst working of a data analyst skills
required to become a data analyst two is
required and companies hiring and
finally we are going to discuss about
salary of a data analyst let me tell you
how simply learn can help you in your
journey to become a data analyst check
out the codes on data analytics in
collaboration with IBM with real time
project and business case studies you
will learn tools like SCI pipe pandas
and programming languages like python or
R2 enroll Now link is in the description
box below question for you which one of
the following is not a python library
for visualization
notebook please leave the answer in the
comment section below moving on who is a
data analyst a data analyst collects
analyzes and interprets data a data
analyst will convert raw data into
useful information data analysts are in
high demand because every industry uses
data analysis work of a data analyst as
a data analyst you will work closely
with the raw data and generate valuable
insights to help companies decide their
future goal if you like thinking out of
the box you are the perfect fit for this
domain data analysts help maximize
output when it comes to generating
Revenue working closely with both
business and data nevertheless this
field boasts handsome salaries for all
levels of expertise can become a data
analyst without prior experience yes
anyone can become a data analyst if they
enjoy solving real world problems have a
strong background in statistics and have
a creative mind if you feel you don't
have it you can definitely develop it so
let us know the skills in detail what
are the basic skill sets required for a
data analyst
data analyst must know basic mathematics
and statistics programming skills
machine learning and also data
visualizations tools so let us know what
are the basics that you need to learn as
a data analyst
mathematics it is always better to know
basic mathematics like linear algebra
and probability fundamentals linear
algebra is used in data preprocessing
and transformation which is the critical
process of every data analyst
statistics a branch of mathematics that
deals with collection analysis
presentation and implementation
probability we know that probability is
the study of How likely something will
happen which is essential for concluding
both probability and statistics are the
backbones of data analysis it is
feasible to become a data analyst with
only a basic understanding of these
three areas of mathematics but in order
to remain relevant and grow as a data
analyst once mathematical knowledge
should not be restricted compulsorily
use some of the tools as a data
analystic what are that
first is Microsoft Excel it is the most
well-known spreadsheet software in the
world it also has computation and
graphic features that are excellent for
data analysts no matter your area of
expertise or additional software you
might want Excel is a standard in the
industry it's useful built-in features
include form design tools and pivot
table it also generate a wide range of
additional features that help simplify
data manipulation
as a programming language every data
analyst should know python it is easy to
learn and has a simple syntax python is
quite adaptable and includes a vast
variety of resource libraries that are
appropriate for a wide range of diverse
data analytics activities these
libraries help in numerical and data
computation the pandas and numpy
libraries for instance are excellent for
supporting standard data processing and
stabilizing highly computational
operations you can also choose between
Python or RR is a well-known open source
programming language much like python
data visualization tool as we previously
mentioned data visualization tool is
also necessary to become a data analyst
power bi is a user-friendly interface
makes building interactive visual
reports and dashboard simple its most
vital selling point is its superb data
integration it works flawlessly with
Cloud sources like Google and Facebook
analytics as well as text files SQL
servers and Excel
Tableau is one of the best commercial
data analysis tool available it handles
huge amounts of data better than many
other bi tools and S is effortless it
has a visual drag and drop interface
however because it has no scripting
layer there is a limit to what Tableau
can do for example it could be better
for pre-processing data or building more
complex calculations
you might have heard about MySQL a lot
of time it is a standard language for
interacting with databases and it is
very helpful when working with
structured data SQL creates
user-friendly dashboards that may
present in various data ways and since
it is so simple to send complex commands
to databases and change data in seconds
it has commands like add edit delete
data in addition SQL is an excellent
tool for creating data warehouses
because of its Simplicity Clarity and
interactivity
overall I would suggest that to become a
data analyst you should work on
programming languages like python or R
plus MySQL to work on databases adding
to that Excel plus visualization tools
like Tableau or power bi you now know
what are the skills are and how it is
used what are you up to in an
organization as a data analyst
to create and evaluate the report using
automated tools like Tableau or power bi
to troubleshoot the reporting database
environment and reports data analyst you
will use statistical method to analyze
data sets and spot any valuable trends
that may develop over time
evaluate companies functional and
non-functional requirements data analyst
assist data warehousing in inspecting
and Reporting needs these are all the
responsibility of a data analyst in an
organization coming to companies hiring
a data analyst IBM Accenture Capital
mini TCS Facebook Amazon Flipkart meta
these are the top companies hiring a
data analyst but data suggests that
every small and medium-sized company
needs a data analyst therefore demand of
a data analyst is in every company so
there is no network job and salary of a
data analyst this is the final part the
salary of a data analyst is high all
over the world when it comes to the USA
the average salary for a data analyst as
a beginner is going as high as 70 000
plus dollars per annum for experienced
professional it is going as high as 120
000 per annum in India for a fresher it
is going as high as 8 lakh per annum and
for experienced professionals it is 20
lakh plus per annum such as the demand
for data analyst now that we have
covered every important skill it's time
for you to start working on it
certifications have become an essential
Benchmark for professional growth and
recognition in the dynamic field of data
analytics they not only validate your
expertise but also open doors to
exciting new career opportunities
and when it comes to Top Notch
certifications simply learns has
established itself as a trusted provider
of choice so let's embark on a journey
discover the best data analyst
certifications available for you are you
eager to step into the exciting world of
data analytics look no further than
simply learns postgraduate program in
data analytics
this comprehensive program is designed
to equip you with the essential skills
and knowledge needed to thrive in this
rapidly evolving field let's look into
the extraordinary features and benefits
that make this program truly stand out
from the crowd
first and foremost simply learns Career
Services is here to ensure that your
talent are noticed by top hiring
companies giving you a Competitive Edge
in the job market upon successfully
completion of the program you will
receive a prestigious postgraduate
program certificate powerful Testament
to your commitment and expertise in the
field of data analytics this certificate
holds a value that can open several
career opportunities to further enhance
your learning experience we have
introduced an exclusive partnership with
industry giant IBM through this
collaboration we bring you unpalid
opportunities such as exclusive
hackathons and ask me anything sessions
with IBM experts this unique expertise
allows you to engage directly with
industry professionals gain invaluable
insights and Tackle real world
challenges faced by data analyst this
collaboration ensures that you receive
Cutting Edge knowledge and stay ahead of
the curve and simply learn we believe in
the power of Interactive Learning that's
why our live online classes offer an
astonishing 8X higher level of live
interaction compared to other programs
delivered by industry experts these
engaging sessions provide you with a
platform to ask questions participate in
discussions and gain practical insights
that will deepen your understanding of
the subject matter one of the defining
feature of our program is the Hands-On
approach to the learning through our
Innovative applied learning model you
will have the remarkable opportunity to
work on a variety of real world projects
using industry data sets for reputable
sources such as Google Play Store and
the World Bank with over 14 data
analytics project about you will gain
valuable practical experience and
develop an impressive portfolio that
showcases your skills to potential
employers to enrich your Learning
Journey even further we present Master
Class delivered by a steam facility from
Purdue and IBM experts their immense
knowledge and expertise will provide you
with the valuable insights and unique
perspective ensuring that you receive a
well rounded in in data analytics now
you may be wondering if this program is
suitable for you the answer is yes our
postgraduate program in data analytics
is thoughtfully designed to the need of
all working professionals whether you
are an experienced data analyst looking
to upskill or someone new to this field
this program is perfect fit no prior
programming knowledge is required as we
cover everything from the fundamentals
to Advanced Technologies throughout the
program you will acquire a wide range of
skills essential for Success career in
data analytics they includes data
analytics statical analytics using Excel
data analysis with python and R data
visualization using Tableau and power bi
model on linear and logitic recursion
clustering using k mean
and supervised learning Technologies our
comprehensive curriculum ensures that
you are equipped with the necessary
tools to excel in this rapidly growing
field
so why wait any longer join simply
learns post graduate program in data
analytics and unlock the world of
opportunities prepare to dive deep into
the realm of data analytics collaborate
with industry experts and gain hands-on
experience with real world projects your
journey to become a data analytics
professional starts right here so the
link of this program will be in the
description box below do check it out
and enroll now next we have data
analytics boot camp
stay ahead of the data Revolution with
the Caltech data analytics bootcamp a
collaborative program offered by Caltech
designed specifically for working
professionals and students of the US
this bootcamp adapts an applied learning
approach providing integrated labs and
real-world projects for the business
environment experience academic
Excellence with the Kel-Tec data
analytics boot camp while also enjoying
the benefits of Caltech Campus Connect
the program offers highly Interactive
Learning ensuring active participation
and engagement this provides ample
hands-on experience to reform your
skills the Caltech data analytics
bootcamp caters to individuals from
diverse backgrounds offering substantial
benefits to working professionals gain a
safe search as Excel proficiency data
driven presentation Technologies data
manipulation with SQL data analytics
we're using Python and data
visualization using Tableau Additionally
you will learn data analytics with the
tools like AWS and other industry
relevant Technologies covering a
comprehensive curriculum the Caltech
data analytics bootcamp dive into key
Concepts including data analytics with
Excel python-based data analytics
database management with SQL W for data
visualization and data analytics on AWS
by completing this bootcamp you will be
equipped with the range of tools and
Technologies by choosing this bootcamp
you open doors to exciting career
opportunities with renowned companies
such as Microsoft Google Amazon IBM
apple and many more the Caltech data
analytics bootcamp has successfully
empowered numerous experience data
analysts and their testimonials are
available on the course page accessible
throughout the link in the description
box now if you aspire to become a data
analyst and acquire job ready skills
don't miss out on this intensive
training program join the Caltech data
analyst bootcamp and impact on your
journey to excel in the world of data
analysts the link for this bootcamp will
be in the description box do check it
out now we have another professional
certification course in data analytics
provided by the IIT kanpur for Indian
professionals and students are you ready
to unlock the secrets hidden within vast
amount of data and gain a comprehensive
Edge in today's Cutthroat business world
simply learns data analyst course
delivered in collaboration with IIT Khan
pool will provide you with extensive
expertise in the booming field of data
analytics this course is designed to
provide a deep understanding of the
principles Technologies and applications
of data analytics empowering you to
efficiently analyze interpret and
extract the actionable insights from the
data the course follows and structured
learning path that covers various
aspects of data analytics including
business analytics using Excel SQL
programming Foundation using python data
analytics with r programming and taboo
training some of the key features of
this program includes master classes
delivered by distinguished IIT kanpur
facility Hands-On lab experience to help
you master 14 plus tools and Frameworks
and Industry ready projects designed to
advance your career trajectory simply
learns job assistance Services is here
to help you get noticed by top hiring
companies increasing your chances to
secure a rewarding position with
renowned companies like Microsoft Google
Amazon IBM Goldman snacks and many more
upon successful completion of the
program you will receive a professional
certificate from IIT kanpur adding
immensely value to your credentials so
what are you waiting for roll now and
embark on this transformative Journey
with IIT kanpur's data analytics course
unlock the boundless potential of data
countless inspiration data analysts have
benefited from the data analytics
program you can find their testimonial
by following the link in the course page
in the description box below hi guys
this is Raul from Simply learn and today
we're going to look at three very
important data related roles in the
field of data science and then we're
going to pit them against each other so
welcome to data scientist versus data
analyst versus data engineer now let's
have a look at what's in store for you
firstly we'll talk about the job
descriptions skill sets required for
each role the salary the roles and
responsibilities and the companies
hiring for these positions so now let's
have a look at each of these roles in
detail first off let's have a look at
data scientist now a data scientist is
able to create machine learning based
tools or processes within the company
now they use Advanced Data techniques
such as clustering division trees neural
networks and so on so that they can
derive business conclusions they are the
senior most member in the team which
involves a day data engineer as well as
a data analyst now they need to have
in-depth knowledge of Statistics data
handling and machine learning they also
take inputs from data Engineers as well
as analysts so that they can formulate
actionable insights for the business now
data scientist also needs to have the
same skills as a data analyst and an
engineer but needs to have a lot more
in-depth knowledge and expertise with
these skills next up we have data
analyst now a data analyst is someone
who's able to translate numeric data
into a form that everyone in the
organization can understand now this is
an entry level position in the data
analytics team he or she needs to have
technical skills in programming
languages such as Python and have
knowledge of tools like Excel and
understand the basics of data handling
modeling and Reporting now in due time
they can move up the ranks by taking up
roles of data engineer and data
scientists with some experience that
they can accumulate over the years and
finally we have data engineer now data
engineer is someone who's involved with
pairing data Who's involved with
preparing data for analytical or
operational purposes now they are the
intermediary between the data analyst
and the data scientist he or she needs
to have a lot of experience when it
comes to developing constructing and
maintaining architectures now they do
generally work on big data and submit
their reports to the data scientists so
that they can be analyzed now let's have
a look at the skills that's required for
each of these roles first off we have
data scientists now since this role is a
little more coding oriented you need to
know a great deal when it comes to
programming languages programming
languages such as python or SQL SAS Java
and so on now you also need to be well
versed with Frameworks in relations to
Big Data such as Pig spark and do
peaking of Hadoop if you want to learn
more about how it works I suggest you
click on the top right corner and watch
our video on water sudo coming back data
scientists also need to be well versed
with machine learning deep learning and
other similar Technologies next up we
have data analyst now this role is much
less technical as compared to a data
scientist as well as a data engineer
considering how it's entry level here
knowing Pro programming languages is a
great bonus so an idea about programming
languages such as python R SQL
JavaScript ssas and so on is a great
benefit at the same time you do need to
be well versed with tools such as SAS
Miner Microsoft Excel ssas SPSS and so
on and finally we have data engineer now
being a data engineer requires you to be
well versed with a bunch of programming
languages as well as Frameworks now you
need to know about programming languages
such as python R SQL SAS Java and so on
while having expertise in Frameworks
such as Hadoop mapreduce Hive big Apache
spark data streaming nosql and so on now
let's talk about money or the salary
each of these roles get firstly we have
the data scientists who earns a whopping
137 000 US dollars per annum then we
have the data analyst who earns 67 000
per annum which is a pretty high salary
when you consider that it's only an
entry level job and a data engineer
which is in the median with 116 000 US
dollars per annum now let's talk about
roles and responsibilities firstly we
have the data scientist now a data
scientist gets to work with a lot of
unstructured data so they need to mine
and clean the data so that it's usable
they need to be able to design machine
learning models to work on the big data
they need to infer and interpret the
analysis on Big Data to be able to lead
an entire team to achieve the goals of
the organization and deliver conclusions
that have a direct business impact now
let's have a look at the roles and
responsibilities of a data analyst they
need to use queries to gather
information from a database they need to
process the data and provide summary
reports they need to use basic
algorithms for their work such as linear
regression logistic regression and so on
and have core skills in statistics data
munging data visualization and
exploratory data analysis and finally we
have data engineer now they need to mine
through the data so that they can gain
insights from it they need to convert
erroneous data into a usable form so
that they can be further analyzed they
need to write queries on data they need
to maintain the design as well as
architecture of the data and create
large data warehouses using ETL or
extract transform load now let's have a
look at some of the companies hiring for
this role firstly for data scientists
you have Citibank Facebook Schneider
Intel Amazon and so on for data analysts
you have Infosys Oracle Visa Capital One
Walmart and so on and for data engineer
you have Google Cisco flocaskend Apple
Spotify and much much more having said
that if you're an aspiring data analyst
looking for an online training and
certification from prestigious
universities and in collaboration with
leading experts then search no more
simply learn Sports graduate program in
data analytics from Purdue University in
collaboration with IBM should be a right
choice for more details use the link in
the description box below now that we
have looked at the various steps
involved in data analytics let's now see
the different tools that can be used to
perform the above steps
so as you can see we have seven tools
including a few programming languages
that will help you perform analytics
better now let's discuss them one by one
first we have python
python is an object oriented open source
programming language that supports a
range of libraries for data manipulation
data visualization and data modeling
python programmers have developed tons
of free and open source libraries that
you can use
you can find many of them via the python
package index which is py pi the
repository of python software
python provides the default package
installer called pip or pip
e python has libraries such as numpy for
numerical computation of data pandas to
manipulate data on numerical tables and
time series then you have sci-fi for
Technical and scientific computations it
also provides scikit-learn which is a
machine learning library for creating
classification regression and clustering
algorithms and finally it also has pi
torch and tensorflow for deep learning
up next we have r
R is an open source programming language
majorly used for numerical and
statistical analysis
it provides a range of libraries for
data analysis and visualization some of
these libraries are ggplot Thai divorce
plotly deployer and carrot
then we have tableau
Tableau is a popular data visualization
and analytics tools that helps you
create a range of visualizations to
interactively present the data build
reports and dashboards to Showcase
insights and Trends it can connect with
multiple data sources and give hidden
business insights and patterns
then we have a competitor of Tableau
which is power bi
power bi is a business intelligence tool
developed by Microsoft that has an easy
drag and draw functionality and supports
multiple data sources with features that
make data visually appealing
power bi supports features that help you
ask questions to your data and get
immediate insights you can also forecast
your data for predicting future trends
so the next tool is Click View
click view provides interactive
analytics with in-memory storage
technology to analyze vast volumes of
data and use data discoveries to support
decision making
it provides social media Discovery and
interactive guided Analytics
it can manipulate huge data sets
instantly with accuracy
up next we have Apache Spark
Apache spark is an open source data
analytics engine to process data in real
time and Carry Out complex analytics
using SQL queries and machine learning
algorithms
it supports spark streaming for
real-time analytics and Spark SQL for
writing SQL queries
it also has spark ml lib which is a
library that has a repository of machine
learning algorithms and then it has
graphics for graphical computation
and finally we have SAS
SAS is a statistical analysis software
that can help you perform analytics
visualize your data write SQL queries
perform statistical analysis and build
machine learning models to make future
predictions SAS empowers our customers
to move the world forward by
transforming data into intelligence
SAS is investing a lot to drive software
Innovation for Analytics Gartner has
positioned SAS as a magic quadrant
leader for data science and machine
learning
moving on to the applications of data
analytics
data analytics has been used in almost
every sector of business these days
let's discuss a few of them
first we have retail
customers expect retailers to understand
exactly what they need and when they
need it
data analytics helps retailers meet
those demands
retailers not only have an in-depth
understanding of their customers but
they can also predict Trends recommend
new products and boost profitability
retailers create assortments based on
customer preferences invoke the most
relevant engagement strategy for each
customer optimize supply chain and
Retail operations at every step of the
customer Journey
the second application is on Healthcare
Healthcare Industries analyze patient
data to provide life-saving diagnosis
and treatment options
they also deal with healthcare plans and
insurance information to drive key
insights using analytics they can
discover new drugs and come up with new
drug development methods
Advanced analytics allows healthcare
companies to improve patient outcomes
and experience
cancer cells and diabetic retinopathy
can be discovered using Medical Imaging
at number three we have Manufacturing
for manufacturers solving problem is
nothing new
they fight with difficult problems and
situations on a daily basis
from complex Supply chains to motion
applications to labor constraints and
Equipment breakdowns they deal with such
problems on a regular basis
using data analytics manufacturing
sectors can discover new cost saving and
revenue opportunities
the fourth application is related to the
banking sector
Banking and financial institutions
collect vast volumes of structured and
unstructured data to derive analytical
insights and make sound financial
decisions using analytics they can find
out probable Loan defaulters customer
churnout rate and detect fraudulent
transactions immediately
the final application is based on
Logistics
logistics companies use data analytics
to develop new business models that can
ease their business and improve
productivity
they can optimize routes to ensure
delivery reaches on time in a cost
efficient manner they also focus on
improving order processing capabilities
as well as Performance Management
with that
now let's look at the companies using
data analytics on a daily basis
so if we have the e-commerce giant
Amazon
then we have Accenture followed by the
American healthcare service organization
Cigna
then we have the American supplier of
Health Information Technology Solutions
Services devices and Hardware Cerner
followed by Target and Antivirus company
McCafe
next we have Rapido which is an Indian
bike rental company based in Bangalore
after that we have Flipkart and the
world's largest retail company Walmart
with that let's understand a case study
from Walmart and how it uses data
analytics to grow its business and serve
its customers better
Walmart is an American multinational
retail company
that has over 11 500 stores in 27
countries worldwide and it has
e-commerce websites in 10 different
countries
it has more than 5900 retail units
operating outside the United States with
55 banners in 26 countries with more
than 7 lakh Associates serving more than
100 million customers every week
it has over 2.2 million employees around
the world and 1.5 million employees in
the United States alone
Walmart's e-commerce Branch alone
employs more than 3000 Technologies from
Silicon Valley to India England and
South America
more than 240 million customers shop at
Walmart each week online and at its
Banner stores walmart.com sees up to 100
million unique visitors a month
according to comscore and is growing
every year
format collects over 2.5 petabytes of
data from 1 million customers every hour
that's really huge
now to make sense of all this
information Walmart has created data
Cafe a state-of-the-art analytics Hub
located within its Bentonville Arkansas
headquarters
here over 200 streams of internal and
external data including 40 petabytes of
recent transactional data can be modeled
manipulated and visualized
teams from any part of the business are
invited to bring their problems to the
analytics experts and then see a
solution appear before the rise on the
nerve centers touchscreen smart ports
Walmart also constantly analyzes over
100 million keywords to know what people
near each store are saying on social
media to understand the customer
behavior on what they like and dislike
Walmart uses modern tools and
Technologies to derive business insights
and improve customer satisfaction some
of these tools include python SAS nosql
databases such as Cassandra and Hadoop
Now using all these Technologies and
data analysis techniques Walmart can
better manage its supply chain optimize
product assortment personalize the
shopping experience give relevant
product recommendations and finally
optimize and analyze Transportation
lanes and routes for its Fleet of trucks
with that let's jump into our use case
demo where we will predict the sales
based on Advertising expenditure using
the linear regression model in r
the advertising expenditure has been
made via different mediums such as radio
television and newspaper
we will use the r programming software
to implement the demo
so why are
well R is a free and open source
software that can be downloaded from the
rcran website it is easy to learn and
use
our language is built specifically for
performing statistical analysis data
manipulation and data mining using
packages such as plier deep plier tidier
and lubricate
R supports data visualization with the
help of packages such as ggplot Google
waves are color viewer leaflet and GG
map
and finally the r software can be used
in a wide range of analytical modeling
including classical statistical tests
linear and non-linear modeling data
clustering time series analysis and more
now let's have a look at our data that
we will be using for this demo
here is our advertising CSV data set
which has four columns
you can see this
tv ads expenditure the next column is
for radio ads then we have the newspaper
ads and the last column is our Target
column that is the sales
so
the data set has
in total 200 rows
now
to understand the data let me give an
example
so consider the second row
so suppose you spend around 230 dollars
on TV ads
then 37.8 dollars on radio ads and 69.2
dollars newspaper ads
you can expect to sell nearly 22 units
of a particular product
similarly
if you are spending 44.5 dollars in TV
advertising
39.3 dollars in radio ads and 45 dollars
in newspaper ads you can sell around 10
units of certain item
we will analyze this data using linear
regression so linear regression is a
supervised learning algorithm which
means the data has labeled columns and
is used to predict numeric continuous
variables so our sales column here is
the target column and it has continuous
numeric variables
now let me go to the art studio and
start with the demo
so first
I'll create a new file then I'll select
our script
the next step is to install all the
necessary packages that we need for this
demo
if you already have the packages
installed in your rstudio you need not
do it again you can just call these
packages using the library function and
pass the package names
so first
I will install the D player package
which is used for data manipulation
I'll be using install.packages function
and I'll give the package name
so I'll type install dot packages if you
hit tab it will autocomplete
then under
quotations
I'll write deplier
I'm not going to run this because I
already have it installed in my R Studio
The Next Step I'll write I'll call this
package using Library function I'll give
the package name deplier
I'll run this
then I'll install the broom package it
takes the messy output of built-in
functions in R such as linear model or
LM then t-test and turns them into a
tidy data frame
so I'll copy the above code
I just paste it again
I'll change it to broom
here also I'll change it to broom
I'll run this
okay
then I'll be installing the ca tools
package which will help us build our
linear regression model
replace the same code and I'll take CA
tools
and I'll call that using the library
function
I'll run this
now sometimes people face issues with
installing this particular package if
you also face this problem do visit the
r Studio Community page now let me show
it to you
so
this is the rstudio Community page
and here they have the solution you can
just go through this two pages
all right
after this I will install the ggplot2
package which is a very popular package
in R for data visualization
I am not running installed packages
because I have already installed all
these packages before
if you have not so you have to run
install.packages first and then call the
library function
with that let's now load the data set
for this I will use the read.csv
function and provide the path location
where my data is located
followed by the data set name and the
extension
I'll assign the loaded data set to a
variable
let me now go ahead and show you where
my data set is located
so here is my advertising csb data set
and this is the location I'll copy this
location
I'll move back to our studio
and let me comment this line
load the data set
so I'll
take a variable name adds
and then using
read.csv function
I'll pass the path location where the
data set is present
now one thing to
notice we have to change all the
backslash to forward slash otherwise R
won't accept it
and finally I'll give the
data set name which is
advertising
dot CSV
let me run it
okay we have successfully loaded our
data set
now let us look at how our data set
looks like using the head function
so I'll give a comment display
the head of the data set
I'll be using the head function and I'll
pass
ads
run it
so
you can see the head function has
displayed the first six rows from the
advertising data set
let me now check the dimensions of the
data set so I'll use the dim function
it will give you the total rows and
columns present in the data set
give a comment check the
dimensions
I'll use the dim function and I'll pass
in
the ads variable
you can see
it has given the number of rows which is
200 and the total columns which is 4.
now if you want to get a summary of the
data set you can use the summary
function
so I'll directly type in summary
and I'll give ads
let me expand this
so actually summary function gives you
information about a few statistics for
each of the columns
so you can see the minimum value for
each column the maximum value for each
column the mean the median
first quartile and the third quartile
values
the first quartile or lower quartile is
the value that cuts off the first 25
percent of the data when it is sorted in
ascending order
the second quartile is the median which
has the value that cuts off the first 50
percent
and the third quartile
or the upper quartile is the value that
cuts off the first 75 percent of data
moving ahead let's do some data
visualization now to visualize our data
since our data has only numeric values
using Scatter Plots would be the best
option
so we will visualize our sales against
each of the independent variables for
that I will use the plot function and
give sales in my x-axis and the
independent variable names in the y-axis
let me now do that so I'll give a
comment
data visualization
first
I'll use the plot function
and then
in x axis using the
dollar symbol
I'll give sales
then in the y-axis
I'll give
my independent variable
you can see R is automatically giving
you the suggestions I'll select TV
then
I'll take type
is equal to
undercodes I'll give P which stands for
points
and I'll take the color as
red
so you can see under plots we have our
scatter plot
if I zoom in
you can see the red dots are pretty much
aligned in One Direction which means
if you are increasing the expenditure on
TV ads the units sold are also
increasing equally
so the more you spend on TV ads the more
sales you can expect
close it
now let's look at how sales vary based
on radio advertising expenditure
I'll copy this
paste it and under y-axis I'll change it
to
radio
and I'll take the color now as let's say
blue color
I'll run it
now file
to zoom in
now
if you look at the blue dots it is not
that linear compared to our previous
graph
you can see there are a few data points
like this
that show the sales were not good Even
after spending decent money on radio ads
but still you can expect a decent amount
of sales if you are willing to spend on
radio advertising
close it
let's now look at how sales will vary
based on the newspaper advertising
expenditure
I'll change the radio to
newspaper column
and this time I'll take color as
green
I'll run it
measurement
you can see the plots are very
haphazardly present the data is
completely non-linear and there seems to
be a low correlation between the sales
and newspaper advertising expenditure
now if you want to look at these plots
at a time you can use the pairs function
so I'll type Pairs and then pass in my
variable name which is ads
I'll run it
let me zoom in
so this is our plot and you can see
this has all the
visualizations so you can see the TV
sales
now you can see the sales that were made
with radio expenditure and with the
newspaper expenditure as well
I'll close it
moving ahead
let's check the correlation between the
variables and see what inside we can get
we will use the core function or cor
function and build a correlation Matrix
first let me go ahead and install the
core plot package so I'll give a comment
correlation analysis
for this I will have to install
the code plot package
I have already got it installed
then I'll call this function using
Library
run it
you can see code plot the version has
been uploaded
now
I'll tell you how you can grab only the
numeric columns
now our data only has numeric columns
but still let me tell you how you can do
it since correlations are based on
numeric columns only this can be done
using the S apply function
so for that we have already installed
the D player Library
I'll give a
variable name as
num dot calls which is
numeric columns
then I'll
pass in the supply function
I'll give the ads variable and I'll
check if the variable is numeric or not
so I'll use is dot numeric
let me run it
and now let's display what's there in
num.calls
you can see
it says TB it's true which means DB has
numeric values even radio has numeric
values similarly for newspaper and sales
also
then
I'll use the correlation function which
is cor to display the correlations
between the variables
so I'll give my variable name as cor dot
data and then
I'll take the core function
passing the ads variable
and I'll only filter out
the numeric columns
so common numeric columns means we need
all the rows and the selected columns
let me run it
and now to display let me call cor dot
data again
so this is our correlation output
as you can see the correlation values
are all above 0 which means there is a
positive correlation between the
variables and the change in one of the
independent variables will have a
positive impact on the sales numbers
tv ads
have the maximum correlation with sales
and the value is around 0.78
then there is radio advertising which
has correlation of about 0.57
with sales and newspaper ads have the
lowest correlation compared to the other
two which is at 0.22
now you can also build a correlation
Matrix using the correlation plot method
this will give you a visual
representation of the correlation
between the variables
so let's see how we can do that
I'll type
core plot
and I'll give
code.data
and I'll pass a method
as color
if I zoom in
you can see
this is our correlation Matrix
on the right
you can see the scale so -1
is for negative correlation
then there's light red 0 which is almost
white color then there's light blue and
finally dark blue for the maximum
positive correlation
the diagonals
are dark blue which represents the same
variables as in rows and in columns
so it's dark blue
tv ads
and radio ads have the next highest
correlation by newspaper ads have the
lowest correlation with sales
with that let's jump into the most
important part of this analysis which is
building our regression model
first we will look at a simple linear
regression model where we will take one
input variable that is tv ads
I'll be using the LM function or the
linear model function
to build the model
so I'll give a comment simple linear
regression
I'll take a variable name as model
underscore simple
and then using LM function
I'll give my target variable which is
sales and
using tilde
I'll give my independent variable which
is TB and data
as adds
I'll run it
now that we have built a linear
regression model let's check the summary
click summary function and I'll pass in
model underscore simple
let me run it
now if I expand this
you can see our intercept estimate is
around 7.03
so when the TV advertising budget is
zero we can expect sales to be around
7030 or 7030
also remember we are operating in units
of thousand
and for every
thousand dollar increase in the TV
advertising budget we can expect the
average increase in sales to be around
47 units
now the same summary can be checked
using the Tidy function
present in the broom package
so if I call
tidy and I'll give the
modeling which is model underscore
simple
I'll run it
so there you go this gives us a tidy
representation of the summary figures
now
let's build a regression model with more
than one input variable
so we'll build a multiple linear
regression model
I'll take my variable name as model
underscore multiple this time
and
I'll use the same LM function
I'll pass in the sales
and using
till
I'll take all the column names
TV then I'll use an addition operator
newspaper
followed by
the radio column
and then I'll take my data as ads
let's run it
I'll follow the same drill Let Me Now
call the summary function over this
newly created model
so I'll write summary and I'll select
my model name as model underscore
multiple
let me run it
so the interpretation of our
coefficients is the same as in simple
linear regression model
first we see that our coefficients for
TV and radio advertising budget are
statistically significant
since our P value is less than 0.05
while the coefficient of newspaper is
not which is around 0.86
thus changes in the newspaper budget
does not appear to have any relationship
with changes in sales
however for TV ads are coefficients
suggests that for every thousand dollar
increase in TV advertising budget
holding any other predictors constant we
can expect an increase in sales of
45 units
on average
similarly the radio coefficient suggests
that for every thousand dollar increase
in radio advertising holding all the
other predictors constant
we can expect an increase of 188 sales
units on an average
now you can also call the Tidy function
over this multiple linear regression
model so let me do that
I'll call tidy and I'll pass in
model underscore multiple
you can see it has given the output
now you can also find the coefficients
of the model using another method it's
called the coefficient Matrix
here is how you can do that so
take a variable name
and
I'll use the summary function
I'll pass in model underscore multiple
and
using the dollar symbol I'll take the
parameter as coefficient
let me call C coefficient now
so these are the coefficients of
different variables
let me now show you another example of
how you can train a linear regression
model using the ca tools Library
first I'll take a seed value a random
seed value of say
101
next I will split the data into training
and testing sets I'll take 70 for
training the data and thirty percent for
testing the data
so I'll use a variable sample
then
I'll call sample dot split
take ads and then
then I'll use another parameter called
split ratio
and I'll take the split ratio as 0.7
which is 70 percent
run it
and then
I'll use another variable called train
and
take the subset of the sample
pass in my ads variable
and I'll select sample is equal to equal
to true
similarly
I'll take another variable called test
I'll
use my subset function and
given the same parameters but this time
I'll take sample
is equal to equal to
false
which means
the test sample data set won't have any
values that are present in train data
set
I'll run it
now
we will use the same LM function to
create our model
so I'll take model
as my variable
and assign it to
LM function
so I'll assign the linear model to the
model variable I'll take sales as my
target column
use the tilde
followed by a DOT which means I am
taking all the variables
in terms of the independent variables
and then
I'll select my train data set
with that let's check the summary as
well
so this is the summary of our
newly created model
now you can also check the residual
collector from the trained model using
the residuals function
let me go ahead and assign a variable
called res for residual and I'll use the
residuals function
pass in my model
then
I'll convert the residuals into a data
frame so
I'll use the as dot data frame
function and pass in res
you can check the residuals
so these are the residual values
now it's time to make our predictions
using the test data set I'll use the
predict function for this
let me take another variable called
sales dot predictions
and I'll use the predict function
pass in my model followed by the test
data set now
I'll run it
then let me call sales.prediction to
display the values
as you can see
these are my predicted sales values
now let me combine this predicted sales
values to our original sales for the
test data
for that I'll use the C bind function
and pass the column names
I'll take another variable called
results
and use the C bind function
I'll take sales dot predictions
and I'll consider
the sales column from the test data
let me check the values now
so you have the predicted sales values
and the original values of sales
but you can see the columns don't have
any name assigned to them so let me go
ahead and assign the column names using
the call names function and convert it
into a data frame to make it look better
so I'll use the call names
function and
and pass in my results variable
then
I'll take a vector and
give the column names as spread for
predicted values and let's say real for
the original values
we run it
now I'll convert this into a data frame
so I'll use as dot data dot frame
and give my results variable
now if I display results
you can see
go on top
you can see
The Columns have been assigned
successfully so on the left you have the
predicted values and on the right you
have the real values
so we have successfully built a linear
regression model and predicted the sales
values using linear regression in r
you can also go ahead and find the
accuracy of this model
to know how good your model is we won't
be covering that as part of this
tutorial I'll leave it for you and
encourage you to do some research on how
you can find the accuracy of a linear
regression model
you will come across terms such as mean
squared error root mean squared error
and r squared value
if you are able to find the accuracy
please post the results in the comment
section or if you face any issues with
it please post your queries we'll be
happy to help you having said that if
you're an aspiring data analyst looking
for an online training and certification
from prestigious universities and in
collaboration with leading experts then
search no more simply learn Sports
graduate program in data analytics from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with our first question what
is the difference between Data Mining
and data profiling
it's real important to note that data
mining is a process of finding relevant
information which has not been found
before it is a way in which raw data is
turned into valuable information you can
think of this as anything from the sales
stats and from their SQL Server all the
way to web scraping and Census Bureau
information where the heck do you mine
it from where do you get all this data
and information
we look at data profiling is usually
done to assess a data set for its
uniqueness consistency and logic it
cannot identify incorrect or inaccurate
data values so if somebody has a
statistical analysis on one side and
they're doing their you might in the
wrong data to then program your data set
up so you got to be aware that when
you're talking about data mining you
need to look at the Integrity of what
you're bringing in where it's coming
from data profiling is looking at it and
saying hey how is this going to work
what's the logic what's the consistency
is it related to what I'm working with
find the term data wrangling and data
analytics data wrangling is a process of
cleaning structuring and enriching the
raw data into a desired usable format
for better decision making
and you can see a nice chart here with
our Discover it we just structure the
data how we want it we clean it up get
rid of all those null values we enrich
it so we might take and reformat some of
the settings instead of having five
different terms for height of somebody
you know in American English or whatever
clean some of that up and we might do a
calculation and bring some of them
together
and validate I was just talking about
that in the last one need to validate
your data make sure you have a solid
data source and then of course it goes
into the analysis very important to
notice here in data wrangling 80 percent
of data analytics is usually in this
whole part of wrangling the data getting
it to fit correctly and don't confuse
that with data cooking which is actually
when you're going into neural networks
cooking the data so it's all between
zero and one values
what are common problems that data
analysts encounter during analysis
handling duplicate and missing values
collecting the meaningful write data the
right time making data secure and
dealing with compliance issues handling
data purging and storage problems again
we're talking about data wrangling here
eighty percent of most jobs are in
wrangling that data and getting it in
the right format and making sure it's
good data to use
number four what are the various steps
involved in any analytics project
understand the problem we may spend 80
percent doing wrangling but you better
be ready to understand the problem
because if you can't you're going to
spend all your time in the wrong
direction this is probably the most
important part of the process everything
after it falls in and then you can come
back to it two data collection data
cleaning number three four data
exploration analysis and five interpret
the results
number five is a close second for being
the most important if you can't
interpret what you bring to the table to
your clients you're in trouble
so when this question comes up you
probably want to focus on those two
noting that the rest of it does eighty
percent of the work is in two three and
four well one and five are the most
important parts
which technical tools have you used for
analysis and presentation purposes
being a data analyst you are expected to
have knowledge of the below tools for
analysis and presentation purposes
there's a wide variety out there SQL
Server MySQL you have your Excel your
SPSS which is the IBM platform tab blue
python you have all these different
Tools in here now certainly a lot of
jobs are going to be narrowed in on just
a few of these tools like you're not
going to have a Microsoft SQL Server
MySQL server but you better understand
how to do basic SQL polls and also
understanding Excel and how the
different formats from column and how to
get those set up
number six what are the best practices
for data cleaning this is really
important to remember to go through this
in detail these always come up because
80 percent of most data analysis is in
cleaning the data make a data cleaning
plan by understanding where the common
errors take place and keep
Communications open identify and remove
duplicates before working with the data
this will lead to an effective data
analysis process focus on the accuracy
of the data maintain the value types of
data provide mandatory constraints and
set Cross Field validation
standardize the data at the point of
entry so that it is less chaotic and you
will be able to ensure that all the
information is standardized leading to
fewer errors on Entry
number seven how can you handle missing
values in a data set list wise deletion
in listwise deletion method entire
record is excluded from analysis if any
single value is missing sometimes we're
talking about records remember this
could be a single line in a database so
if you have your SQL comes back and you
have 15 different columns every one of
those has a missing value you might just
drop it just to make it easy because you
already have enough data to do the
processing average imputation use the
average value of the responses from the
other participants to fill in the
missing value this is really useful and
they'll ask you why these are useful I
guarantee it if you have a whole group
of data that's collected and it doesn't
have that information in it at that
point you might average it in there
regression substitution you can use
multiple regression analysis to estimate
a missing value that kind of goes with
the average imputation input regression
model means you're just going to get
you're going to actually generate
generate a prediction as to what you
think that value should be for those
people based on the ones you do have
multiple imputation so we talk about
multiple inputs it creates plausible
values based on the correlations for the
missing data and then average the
simulated data sets by incorporating
random errors in your predictions
what do you understand by the term
normal distribution and the second you
hear the word normal distribution should
be you think in a bell curve like we see
here normal distribution is a type of
continuous probability distribution that
is symmetric about the mean and in the
graph normal distribution will appear as
a bell curve the mean median and mode
are equal that's a quick way to know if
you have normal distribution is you can
compute mean median and mode all of them
are located at the center of the
distribution 68 of the data lies within
one standard deviation of the mean 95
percent of the data Falls within two
standard deviations of the mean 99.7 of
the data lies within three standard
deviations of the mean
what is time series analysis
time series analysis is a statistical
method that deals with ordered sequence
of values of a variable of equally
spaced time intervals time series data
on a covid-19 cases and you can see
we're looking at by day so our space is
of days and each day goes by if we take
a graph it you can see the time series
graph always looks really nice if you
have like two different in this case we
have what the United States going over
there I'd have to look at the other
setup in there but they picked a couple
different countries and it is it's time
sensitive you know the next result is
based on what the last one was Cove is
an excellent example of this anytime you
do any word analytics where you're
figuring out what someone's saying what
they said before makes a huge difference
is what they're going to say next
another form of Time series analysis
10. how is joining different from
blending in tableau so now we're going
to jump into the table blue package data
joining data joining can only be done
when the data comes from the same Source
combining two tables from the same
database or two or more worksheets from
the same Excel file all the combined
tables or sheets contains common set of
dimensions and measures
data blending data blending is used when
the data is from two or more different
sources combining the Oracle table with
the SQL server or two sheets from Excel
or combining Excel sheet and Oracle
table in data blending each data source
contains its own set of dimensions and
measures
how is overfitting different from
underfitting
how he's a good one overfitting probably
the biggest danger in data analytics
today is overfitting model trains from
the data too well using the training set
the performance drops significantly over
the test set happens when the model
learns the noise and random fluctuations
in the training data set in detail and
again the performance drops way below
what the test set has
the model neither trains the data well
nor can generalize to new data performs
poorly both on train and the test set
happens when there is less data to build
and an accurate model and also when we
try to build a linear model with a
non-linear data
in Microsoft Excel a numeric value can
be treated as a text value if it
proceeds with an apostrophe definitely
not an exclamation if you're used to
programming in Python you'll look for
that hash code and not an Amber sign
and we can see here if you enter the
value 10 into a fill but you put the
apostrophe in front of it it will read
that as a text not as a number
what is the difference between count
count a count blank and count if in
Excel
we can see here when we run in just
count D1 through D 23 we get 19 and
you'll notice that there is 19 numbers
coming down here
and so it doesn't count the cost of each
which is a top bracket it doesn't count
the blank spaces either with the
straight count
when you do a count a you'll get the
answer is 20. so now when you do count a
it counts all of them even the title
cost of each
when you do count blank we'll get three
why there's three blank fields
and finally the count if if we do count
F of e 1 to e23 is greater than 10
there's 11 values in there basic
counting of whatever is in your column
pretty solid on the table there
explain how vlookup Works in Excel
vlookup is used when you need to find
things in a table or a range by row
the syntax has four different parts to
it we have our lookup value that's the
value you want to look up we have our
table array
the range where the lookup value is
located
column index number the column number
and range that contains the return value
and the range lookup specify true if you
want an approximate match or false if
you want an exact match of the return
value
so here we see vlookup F3 A2 to C8 2
comma zero for prints now they don't
show the F3 F3 is the actual cell that
prints is in that's what we're looking
at is F3 so there's your prints he pulls
in from F3 A2 to C8 is the the data
we're looking into and then number two
is a column in that data so in this case
we're looking for uh age and we count
name as one age is two keep in mind this
is Excel versus a lot of your Python and
programming languages where you start at
zero in Excel we always look at the
cells as one two three so two represents
the h
0 is false for having an exact match up
versus one we don't actually need to
worry about that too much in this 0 or 1
would work with this example and you can
see with the Angela lookup again her
name would be in the F column number
four that's what the F4 stands for is
where they pulled Angela from and then
you have A1 to C8 and then we're looking
at number three so number three is
height name being one H2 and then height
three and you'll see here pulls in her
height 5.8
so we're going to run jump over to SQL
how do you subset or filter data in SQL
to subset or filter data in SQL we use
where and having clause and you can see
we have a nice table on the left where
we have the title the director the year
the duration we want to filter the table
for movies that were directed by Brad
Bird why just because we want to know
who what Brad Bird did so we're going to
do select star you should know that the
star refers to all in this case we're
what are we going to return we're going
to return all title directory year and
duration that's what you mean by all for
movies movies being our table where
director equals Brad Bird and you can
see he comes back and he did the
incredible on Ratatouille
two subsetter filter data SQL we can
also use the where and having Clause so
we're going to take a closer look at the
different ways we can filter here filter
the table for directors whose movies
have an average duration greater than
115 minutes so there's a lot of really
cool things into this SQL query and
these SQL queries can get pretty crazy
select director sum duration as total
duration average duration as average
duration from movies Group by director
having average duration greater than
115.
uh so again what are we going to return
we're going to return whatever we put in
our select which in this case is
director we're going to have total
duration and that's going to be the sum
of the duration we're going to have the
average duration average underscore
duration which is going to be the
average duration on there and then we of
course go ahead and group by director
and we want to make sure we group them
by anyone that has an having an average
duration greater than 115. these SQL
queries are so important I don't know
how many times you the SQL comes up and
there's so many different other
languages not just MySQL not Microsoft
SQL but in addition to that where the
SQL language comes in especially with
Hadoop in other areas so you really
should know your basic SQL doesn't hurt
to get that little cheat sheet and
glance over it and double check some of
the different features in SQL
what is the difference between where and
having clause in SQL where where Clause
works on row data in where Clause a
filter occurs before any groupings are
made aggregate Junctions cannot be used
so the syntax is select your columns
from table where what the condition is
having Clause works on aggregated data
having is used to filter values from a
group aggregate functions can be used in
the syntaxes select column names from
table where the condition is grouped by
having a condition ordered by column
names
what is the correct Syntax for reshape
function in numpy so we're going to jump
to the numpy array program
and what you come up with is you have in
this case it'd be numpy dot reshape a
lot of times you do an import numpy as
NP
reshape and then your array and the new
shape
and you can see here as we as the actual
example comes in the reshape is a and
we're going to reshape it in two comma
five setups and you can see the printout
in there that prints in two rows with
five values in each one
what are the different ways to create a
data frame in pandas
well we can do it by initializing a list
so you can Port your pandas as PD very
common data equals Tom 30 jerry20 Angela
35 we'll go ahead and create the data
frame and we'll say
pd.dataframe is the data columns equals
name and age so you can designate your
columns you can also it is a index in
there you should always remember that
the index in this case maybe you want
the index instead of one two to be the
date they signed up or who knows you
know whatever and you can see right
there it just generates a nice pandas
data frame with Tom Jerry and Angela
another way you can initialize a data
frame is from dictionary you can see
here we have a dictionary where the date
equals name Tom Jerry Angela Mary ages
20 21 1918 and if we do a DF PD dot data
frame on the data you'll get a nice the
same kind of setup you get your name age
Tom Jerry Angela and Mary
write the python code to create an
employee's data frame from the
emp.csv file and display the head and
summary of it to create a data frame in
Python you need to import the pandas
library and use the read CSV function to
load the CSV file and here you can see
we have import pandas is PD employees or
the data frame employees equals pd.read
CSV and then you have your path to that
CSV file there's a number of settings in
the read CSV where you can tell it how
many rows are the top index you can set
the columns in there
you can have skip rows there's all kinds
of things you can also go in there and
double check with your read CSV but the
most basic one is just to read a basic
CSV
how will you select the department and
age columns from an employee's data
frame
so we have import pandas is PD you can
see we have created our data we will go
ahead and create our employees PD data
frame on the left
and then on the right to select
department and age from the data frame
we just do employees you put the
brackets around it now if you're just
doing one column you could do just
department but if you're doing multiple
columns you've got to have those in a
second set of brackets it's got to be a
reference with a list within the
reference
what is the criteria to say whether a
developed data model is good or not a
good model should be intuitive
insightful and self-explanatory follow
the old saying kiss keep it simple
the model develops should be able to
easily consumed by the clients for
actionable and profitable results
so if they can't read it what good is it
a good model should easily adapt to
changes according to business
requirements we live in quite a dynamic
world nowadays so it's pretty
self-evident and if the data gets
updated the model should be able to
scale accordingly to the new data so you
have a nice data pipeline going where
when something when you get new data
coming in you don't have to go and
rewrite the whole code
what is the significance of exploratory
data analysis
exploratory data analysis is an
important step in any data analysis
process exploratory data analysis Eda
helps to understand the data better it
helps you obtain confidence in your data
to a point where you're ready to engage
a machine learning algorithm it allows
you to refine your selection of feature
variables that will be used later for
model building you can discover hidden
Trends and insights from the data
how do you treat outliers in a data set
an outlier is a data point that is
distant from other similar points they
may be due to variability in the
measurement or may indicate experimental
errors
uh one you can drop the outlier records
pretty straightforward you can cap your
outliers data so it doesn't go past a
certain value you can assign it a new
value you can also try a new
transformation to see if those outliers
come in if you transform it slightly
differently
explain descriptive predictive and
prescriptive analytics descriptive
provides insights into the past to
answer what has happened uses data
aggregation and data mining techniques
examples an ice cream company can
analyze how much ice cream was sold
which flavors were sold and whether more
or less ice cream was sold than before
predictive understands the future to the
answer what could happen use the
statistical models and forecasting
techniques
the example predicts a sale of ice
creams during the summer spring and
rainy days so this is always interesting
because you have your descriptive which
comes in and your businesses are always
looking to know what happened hey did we
have good sales last uh quarter what are
we expecting next quarter in sales and
we have a huge jump when we do uh
prescriptive suggest various courses of
action to answer what should you do uses
optimization and simulation algorithms
to advise possible outcomes example
lower prices to increase sell of ice
creams produce more or less quantities
of certain flavor of ice cream and we
can certainly uh today's world with the
covid virus because we had that on our
earlier graph you could see that as a
descriptive what's happened how many
people have been infected how many
people have died in an area predictive
where do we predict that to go
um do we see it going to get worse is it
going to get better what do we predict
that we're going to need in hospital
beds and prescriptive what can we change
in our setup to have a better outcome
maybe if we did more social distancing
if we tracked the virus
how do these different things directly
affect the end and can we create a
better ending by changing some
underlying criteria
what are the different types of sampling
techniques used by data analysis
sampling is a statistical method to
select a subset of data from an entire
data set population to estimate the
characteristics of the whole population
one we can do a simple random sampling
so we can just pick out 500 random
people in the United States to sample
them
they call it a population in regular
data we also call that a population just
because that's where it came from was
mainly from doing census
systematic sampling
cluster sampling
dratified sampling and judgment or
purposive sampling
then we have our systematic sampling
that's where you're doing like using one
five ten fifteen twenty use a very
systematic approach for pulling samples
from the setup cluster sampling that's
where we look at it and we say hey some
of these things just naturally group
together if you were talking about
population which is a really a nice way
of looking at this cluster sampling
would be maybe by a zip code we're going
to do everybody's zip code and just
naturally cluster it that way
stratified sampling would be more
looking for shared things the group has
like income so if you're studying
something on poverty you might look at
their naturally group People based on
income to begin with and then study
those individuals in the income to find
out what kind of traits they have
and then judgmental that is where the
researcher very carefully selects each
member of their own group
so it's very much based on their
personal knowledge
jumping on the 26th what are the
different types of hypothesis testing
hypothesis testing is a procedure used
by estheticians and scientists to accept
or reject statistical hypothesis we
start with a hypothesis testing we have
null hypothesis and alternative
hypothesis
on the null hypothesis it states that
there is no relation between the
predictor and the outcome variables in
the population it is denoted by H naught
example there is no association between
patients BMI and diabetes
alternative hypothesis it states there
is some relation between the predictor
and outcome variables in the population
it is denoted by H1 example there could
be an association between patients BMI
and diabetes
and that's the body mass index if you
didn't catch the BMI and you're not in
medical
describe univariate bivariate and
multivariate Analysis
a univariate analysis it is the simplest
form of data analysis where the data
being analyzed contains only one
variable an example is studying the
heights of players in the NBA because
it's so simple it can be described using
Central Tendencies dispersion quartiles
bar charts histograms pie charts
frequency distribution tables
the bivariate analysis it involves
analysis of two variables to find causes
relationships and correlations between
the variables example analyzing sale of
ice creams based on the temperature
outside
bivariate analysis can be explained
using correlation coefficients linear
regression logistic regression Scatter
Plots and box plots
and multivariate Analysis it involves
analysis of three or more variables to
understand the relationship of each
variable with the other variables
example analyzing Revenue based on
expenditure so if we have our TV ads we
have our newspaper ads our social media
ads and our Revenue we can now compare
all those together
the multiverted analysis can be
performed using multiple regression
factor analysis classification and
regression trees cluster analysis
principle component analysis clustering
bar chart dual axis chart
what function would you use to get the
current date and time in Excel
in Excel you can use the today and now
function to get the current date and
time and you can see down here with the
two examples are just equals today or
equals now
using the sumifs function in Excel find
the total quantity sold by cells
Representatives whose names start with a
and the cost of each item they have sold
is greater than 10.
and you can see here on the left we have
our actual table and then we want to go
ahead and sumifs so we want the E2
through E20 B2 through B20 greater than
10. and this basically is just saying
hey we're going to take everything in
the E column
and we're going to sum it up but only
those objects where the D column is
greater than 10 that's what that means
there
is the below query correct if not how
will you Rectify it
select customer ID year order date as
order Year from order where order year
is greater than or equal to 2016.
and hopefully you caught it right there
uh it's in the devils in the details we
can't not use the Alias name while
filtering data using the where Clause so
the correct format is all the same
except for where it says where the year
order date is greater than or equal to
16 versus using the order year which we
assigned under the select setup
how are union intersect and accept used
in SQL the union operator is used to
combine the results of two or more
select statements
and you can see here we have select star
from region one and we're going to make
a union with select star from region two
and it basically takes both these SQL
tables and combines them to form a full
new table on there
so that's your union as we bring
everything together
when we look at the intersect operator
Returns the common records that are the
result of the two or more select
statements
so you can see here we select star from
region one intersect select star from
region two
and we come up with only those records
that are shared that have the same data
in them and hopefully you jumped ahead
to the accept the accept operator
returns The Uncommon records that are
the result of two or more select
statements so these are the two records
or the records that are not shared
between the two databases
using the product price table write an
SQL query to find the record with the
fourth highest market price
so here we have a little bit of a brain
teaser they're always fun
and the first thing we want to do is
we're going to go ahead and I'm going to
if you look at the script on the left we
really want the fourth one down so we're
going to select the top four from
product price but we're going to order
it by Marketplace descending SP order by
market price ascending
so we do is we take the top four of the
market price ascending and that's going
to give us the four greatest values
and then we're going to reverse that
order and do descending and we're going
to take the top one of that which is
going to give us the lowest value which
would be the fourth greatest one in the
list
from the product price table find the
total and average market price for each
currency where the average market price
is greater than 100 and currency is in
the INR or the AUD
so INR or AUD India Rupal or Australia
dollar
you can see over here the SQL query if
you had trouble putting this together
you might actually do some of it in
reverse
and you can see right here where the
average market price is greater than 50.
remember we use having not where at the
end because it's part of the group so
Group by currency because we want those
two currencies and we want the currency
India the INR or the AUD
and as you keep going backwards we're
actually going to be selecting the
currency the sum of the market price as
total price and the average Marketplace
as average price so there's our select
it's going to come from the product
price which is just our table over there
and then we have where our currency is
in uh and like I said you can put
together however you want but hopefully
you got to the end there
so this question will test your
knowledge in Tableau exploring the
different features of Tableau and
creating a suitable graph to solve a
business problem
and of course Tableau is very visual in
its use so it's very hard to test it
without actually just getting your hands
on and if you can't visualize some of
this and how to do it then you should go
back and refresh yourself
using the sample Superstore data set
Create A View to analyze the cells
profits and quantities sold across
different subcategories of items present
under each category so the first step is
to go ahead and load the sample
Superstore data set
so make sure you know how to load the
sample the superstore data set that's
underneath either the connect button in
the upper left or the Tableau icon up
there and be able to pull in the data
set and then once you've done that you
just drag the category and subcategory
on rows and salaries onto columns it
will result in a horizontal bar chart
so in this one we're just going to drag
profit onto color and quantity onto
label sort the sales axes in descending
order of sum and cells within each sub
category
and if you're at home doing this you'll
see the chairs in their Furniture
category have the highest sales and
profit while tables had the lowest
profit for office supplies subcategory
binders made the highest profit even
though storage had the highest sales
under technology category copiers made
the highest profit though it was the
least amount of sales
to create a dual axis chart in Tableau
to present cells and profits across
different years using sample Superstore
data set
load the orders sheet from the sample
Superstore data set
drag the ordered data field from the
dimensions onto columns and converted
into continuous months
drag cells onto rows and profits to the
right corner of the view until you see a
light green rectangle one of those
things if you haven't done this Hands-On
you don't know what you're doing you're
right into a buying so you can be just
kind of dropping it and wondering what
happened synchronize the right axes by
right clicking on the profit axis
and then let's finalize it by going
under the marks card change some cells
to bar and some profit to line and
adjust the size
and then we have a nice display that we
can either print out or save and send
off to the shareholders
let's go and do one more Tableau design
a view in Tableau to show Statewide
cells and profits using the sample
Superstore data set
and here you go ahead and drag the
country field onto the view section and
expand it to see the states drag the
states field onto size and profit onto
color
increase the size of the Bubbles at a
border and a Halo color States like
Washington California and New York have
the highest sales and profits while
Texas Pennsylvania and Ohio have a good
amount of sales but the least amount of
profits
we'll go ahead and Skip back to python
numpy Suppose there is an array number
equals NP or numpy if you're using numpy
depending on how you set it up dot array
and we just have one to nine broken up
into three groups extract the value 8
using 2D indexing so you can see on the
left we have our import numpy is in p
number equals our NP array if we print
the number we have one two three four
five six seven eight nine
since the value 8 is present in the
second row and First Column we use the
same index position and pass it to the
array and you just have number two comma
1 and you get eight and remember we're
in Python so you start at zero not one
like you do in Excel
always gets me if I'm working between
Excel and python where I just kind of
flip and usually it's the Excel that
messes up because I do a lot more
programming
Suppose there is an array that has value
0 1 all the way up to nine how will you
display the following values from the
array one three five seven nine
uh so first of all we go ahead and
create the array NP dot a range of 10
which goes from 0 to 9 because there's
ten numbers in it but we don't include
the 10. we print it out the first thing
you want to do is what's going on here
with one three five seven nine well if
we divide by two there's going to be a
remainder equal to one and then from
python we remember if you use the
percentage sign you get the remainder on
there so the remainder is 1 and then you
have the your numpy array and then we
just want to do a logical statement of
all values that have a remainder of 1
and that generates our nice one three
five seven nine
thank you there are two arrays A and B
stack the arrays A and B horizontally
boy these horizontal vertical questions
will get you every time
and in numpy we go ahead and we've
created two different arrays over here A
and B the first one is your concatenate
NP dot concatenate
A and B on axes equal one
that is the same as H stack and in the
back end they're still identical they
run the same that's all each stack is a
concatenate and axes equals one
how can you add a column to a panda's
data frame
suppose there's an imp data frame that
has information about few employees
let's add address column to that data
frame and you can see on the left we
have our basic data frame you should
know your data frames very well
basically looks like an Excel
spreadsheet as you come over here it's
really simple you just do DF of address
equals the address once you've assigned
values to the address
using the below given data create a
pivot table to find the total cells made
by each cells represented for each item
display the cells as a percentage of the
grand total
so we're back in Tableau select the
entire table range click on insert Tab
and choose pivot table
select the table range and the worksheet
where you want to place the pivot table
it'll return a pivot table where you can
analyze your data
drag the cell total on the values and
sales rep and item onto row labels it'll
give the sum of the cells made by each
representative for each item they have
sold
and finally right click on sum of cell
total and expand show values as to
select percentage of grand total
real important just understand what a
pivot table is we're just pivoting it
from rows and columns and switching this
direction on there
and finally we have our final pivot
table and you can see the values rules
and sum of total sale
so we're going to go ahead and take a
product table this is off of an SQL so
we're going to do some SQL here
and we're going to use the product and
sales order detail table find the
products that have total units sold
greater than 1.5 million and here's our
sales order detail table so we have a
product table and a sales order detail
table two separate tables in the
database
and we're going to do is put together
the SQL query we want to select PP name
sum sod unit price as cells and then we
have our p p dot product ID from
production product as PP inner join
cells.sales order detail as sod on PP
product ID equals sod.product ID Group
by pp.name comma pp.product ID having a
sum of sod.uniprice greater than the 150
million there that's a mouthful and
again these SQL queries they start
looking really crazy until you just
break them apart and do them step by
step
and what we're looking for is the inner
join and how did you do the group by
this really when they know how do you do
this inner join this comes up so much in
SQL how do you pull in the ID from one
chart and the information from another
chart and the sum totals on that chart
how do you write a stored procedure in
SQL let's create a storage procedure to
find the sum the squares are the first n
natural numbers so here we have our
formula n times n plus 1 times 2N plus 1
over 6. and you can see from the command
prompt or the setup you have depending
on what your login is the command is
create procedure Square sum1
declare our variable at n of integer as
begin then we're going to declare the
sum of integer set sum equals n times n
plus 1 plus 2 times n plus one over six
and then of course we can go ahead and
print those out print First cast
member sign in or our variable as a
variable character 20 natural numbers
parenthesis sum of the square is cast
the at sum as a variable character 40
int then we do the output display the
sum of the square for first four natural
numbers we have execute Square sum 1 and
then we're going to put in 4 and you can
see here where it brings up the first
four natural numbers sum of square is
30.
write a store procedure to find the
total even number between two user given
numbers
couple things to note here first we go
and create our procedure you have your
two different variables the N1 N2 and we
go ahead and begin we're going to
declare our variable count as an integer
we're going to set count equal to zero
and then we have while n is less than N2
we're going to begin and if N1 remainder
2 equals 0 so we're going to divide it
by two even number begin we're going to
set the count equal to count plus one
we're going to print even number plus
cast n as a variable character 10 for
printing count is plus cast variable
count as variable character 10 end else
print odd number plus cast variable
number one as variable character ten and
then we go ahead and set the increment
our variable one up one so it'll go from
in one all the way to N2 and I'll print
the total number of even numbers
and you can see here we went ahead and
executed it we're going to count the
even numbers between 30 and 45 and you
can see it goes all the way down to
eight
what is the difference between tree maps
and heat maps in tableau
now if you've worked in Python other
programmings you should automatically
know what a heat map is but a tree map
are used to display data in nested
rectangles use Dimensions to define the
structure of the tree map and measure to
define the size or color of individual
rectangles
tree maps are relatively simple data
visualization that can provide Insight
in a visually attractive format
and again you can see the squares over
here this is our tree map over here with
the each block also has this information
inside of its different blocks
a heat map helps to visualize measures
against Dimensions with the help of
colors and size to compare one or more
dimensions and up to two measures the
layout is similar to a text table with
variations in values encoded as colors
in heat map you can quickly see a wide
array of information
and in this one you can see they use the
colors to denote one thing and the size
of the little square to denote something
else
a lot of times you can even graph this
into a three-dimensional graph with
other data so it pops out but again a
heat map is the color and the size
using the sample Superstore data set
display the top five and bottom five
customers based on their profit so you
start by dragging the customer name
field onto rows and profit on columns
right click on the customer name column
to create a set
give a name to the set and select top
tab to choose top 5 customers by some
profit similarly create a set for the
bottom five customers by some profit
select both the sets right click to
create a combined set give a name to the
set and choose all members in both sets
and then you can drag top and bottom
customer sets onto the filters and
profit field onto color to get the
desired results
as we get down to the end of our list
we're going to try to keep you on your
toes we're going to skip back to numpy
how to to print four random integers
between 1 and 15 using numpy to Generate
random numbers using numpy we use a
random random integer function and you
see here we did the import numpy is in P
random Arrangement equals np.random dot
random integer 1 through 15 of 4.
from the below data frame I'm going to
jump again on you now we're into pandas
how will you find the unique values for
each column and subset the data for age
less than 35 and height greater than 6.
to find the unique values and the number
of unique elements use the unique and
the in unique function
you can see here we just did DF Heights
we're selecting just the height column
and we want to look for The Unique that
returns an array or in unique if we do
that on the height or the age we'll
return just the number of unique values
and then we can do a subset the data for
ages less than 35 and height greater
than 6. so if we look over here we have
a new DF remember this is going to be
taking slices of our original data frame
it doesn't actually change the data
frame so our new DF equals the data
frame or DF the data frame where age is
less than 35
and the height is greater than 6.
and in case you're not using uh tab blue
which has a lot of its own uh different
mapping programs in there make sure you
understand how to use the basics of
matplot Library plot a sine graph using
numpy and matplot library in Python and
the way we did this is we went ahead and
generated an X we know our y equals NP
dot sine of x if you print out X you'll
see a whole value here our matplot
library Pi plot as PLT if you are
working in Jupiter notebook make sure
you understand the matplot library
inline that little percentage sign
matplot Library Online that prints it on
the page in the jupyter notebook the
newer version of jupyter notebook or
Jupiter Labs automatically does that for
you but I usually put it in there just
in case I end up on an older version
if you print y you can see here we have
our different y values and our different
X values
you simply put in plt.plot x y and do a
plot show
and before we go let's get one more in
we're going to do a pandas using the
below pandas data frame find the company
with the highest average cells derive
the summary statistics for the cells
column and transpose these statistics
that's a mouthful and just like any of
these computer problems break it apart
so first of all we're looking for the
highest average cells so group the
company column and use the mean function
to find the average cells and you see
here by company equals df.group by
company
once we've done that using the describe
function we can now go ahead and look at
the summary of statistics on here use
the describe function to find the
summary so by company those are groups
we're just going to describe them and
you could actually bundle those together
if you wanted and just do a Mullen one
line so here we go by company dot
describe you can see we have a nice
breakout always good to remember whether
you're using any of the packages whether
it's tab blue or pandas in python or
even r or some other package being able
to quick look and describe your data is
very important and then we can go ahead
and just do a basic apply a transpose
function over the describe method to
transpose the statistics
all we've done here is flip the index
with the column names but if you're
following the numbers a lot of times
it's easier to follow across one line or
maybe you want to average out the count
or it's all kinds of different reasons
to do that we have reached the end of
the session on the full data analytics
course should you need any assistance
PPT project code and other resources
used in the session please let us know
in the comment section below and a team
of experts will be happy to help you as
soon as possible until next time thank
you and keep learning stay tuned for
more from Simply learn
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
foreign
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here