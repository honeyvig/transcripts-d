welcome to the fascinating world of
machine learning libraries in the realm
of artificial intelligence and data
science these powerful tools are the
Bedrock for developing intelligent
systems and making sense of complex data
python being one of the most popular
programming languages for data science
offers a plethora of exceptional machine
learning libraries that have
revolutionized the field at the
Forefront of Python's machine learning
ecosystem is numpy which is widely used
in scientific Computing data analysis
and machine learning due to its
efficient and convenient array
manipulation capabilities it offers an
extensive set of mathematical functions
for array operations including linear
algebra Fourier Transformers random
number generation and more another
influential library is tensorflow that
is developed by Google known for its
flexibility and scalability another one
is pi torch an open source Library
backed by Facebook's AI research lab
which has gained tremendous popularity
in recent years it offers dynamic
computational graphs making it easier to
experiment with and prototype models
these are just a few example of the vast
area of machine learning libraries
available in Python each library has its
own unique features strengths and areas
of specialization whether you are at
data scientist a researcher or an
Enthusiast these libraries Empower you
to unravel the mysteries of data and
unlock the potential of artificial
intelligence so before diving in join us
on a journey into the fascinating world
of AI and machine learning with our
Caltech postgraduate program in Ai and
machine learning in partnership with IBM
this a and ml course scores the latest
tools and Technologies from the AI
ecosystem and features master classes by
Celtic faculty IBM experts hackathons
and ask me anything sessions this
program showcases Caltech ctm is
excellence and IBM's industry progress
the artificial intelligence course calls
key Concepts like statistics data
science with python machine learning
deep learning NLP reinforcement learning
and through an Interactive Learning
model with live sessions enroll now
analog exciting Ai and ml opportunities
the course link is mentioned in the
description box with that having said
hey everyone welcome to simple learns
YouTube channel but before we dive into
that don't forget to like subscribe and
share so are you ready to explore and
let the tools guide you on an
extraordinary journey into the realm of
machine learning now over to our
training experts so let's talk about
this amazing Library tensorflow which is
all
favorites so tensorflow is a library for
high performance numerical competitions
with around 35
000 GitHub comments and a Vibrant
Community of around 1500 contributors
and it's used across various scientific
domains it's basically a framework where
we can Define and run computations which
involves tensors and tensors we can say
are partially defined computational
objects again where they will eventually
produce a value that was about
tensorflow let's talk about the features
of tensorflow so tensorflow is majorly
used in deep learning models and neural
networks where we have other libraries
like torch and thiano also but
tensorflow has hands down better
computational graphical visualizations
when compared to them also tensorflow
reduces the error largely by 50 to 60
percent in neural machine translations
it's highly parallel in a way where it
can train multiple neural networks and
multiple gpus for highly efficient and
scalable models this parallel Computing
feature of tensorflow is also called
pipelining also tensorflow has the
advantage of seamless performance as
it's backed by Google it has quicker
updates frequent new releases with the
latest of features now let's look at
some applications tensorflow is
extensively used in speech and image
recognition text-based applications time
series analysis and forecasting and
various other applications involving
video detection so favorite thing about
tensorflow that it's already popular
among the machine learning community and
most are open to trying it and some of
us are already using it now let's look
at an example of a tensorflow model in
this example we will not dive deep into
the explanation of the model as it is
beyond the scope of this video so here
we're using amnest dataset which
consists of images of handwritten digits
handwritten digits can be easily
recognized by building a simple
tensorflow model let's see how when we
visualize our data using matplotlab
Library the inputs will look something
like this then we create our tensorflow
model to create a basic tensorflow model
we need to initialize the variables and
start a session then after training the
model we can validate the data and then
predict the accuracy this model has
predicted 92 percent accuracy let's see
which is pretty well for this model so
that's all for tensorflow if you need to
understand this tutorial in detail then
you can go ahead and watch our deep
learning tutorial from Simply learn as
shown in the right corner interesting
right let's move on to the next library
now let's talk about a common yet a very
powerful python Library called numpy
numpy is a fundamental package for
numerical competition in Python it
stands for numerical python as the name
suggests it has around 18 000 comments
on GitHub with an active community of
700 contributors it's a general purpose
array processing package in a way that
it provides high performance
multi-dimensional objects called arrays
and tools for working with them also
numpy addresses the slowness problem
partly by providing these
multi-dimensional arrays that you talked
about and then functions and operators
that operate efficiently on these arrays
interesting right now let's talk about
features of number it's very easy to
work with large arrays and mattresses
using numpy numpy fully supports
object-oriented approach for example
coming back to ND array once again it's
a class possessing numerous methods and
attributes ndra provides for larger and
repeated computations numpy offers
vectorization it's more faster and
compact than traditional methods I
always wanted to get rid of loops and
vectorization of numpy clearly helps me
with that now let's talk about the
applications of numpy numpy along with
pandas is extensively used in data
analysis which forms the basis of data
science it helps in creating the
powerful n-dimensional array whenever we
talk about numpy the mention of the
array we cannot do it without the
mention of the powerful n dimensional
array also number is extensively used in
machine learning when we are creating
machine learning models as in where it
forms the base of other libraries like
sci-fi scikit-learn
Etc when you start creating the machine
learning models in data science you will
realize that all the models will have
their basis numpy or pandas also when
numpa is used with sci-fi and matplotlib
it can be used as a replacement of
Matlab now let's look at a simple
example of an array in numpad as you can
see here there are multiple array
manipulation routines like their basic
examples where you can copy the values
from one array to another we can give a
new shape to an array from maybe one
dimensional do we can make it as a two
dimensional array we can return a copy
of the array collapse into one dimension
now let's look at an example where this
is Jupiter notebook and we will just
create a basic array and for detailed
explanation you can watch our other
videos which Targets on these
explanations of each libraries so first
of all whenever we are using any library
in Python we have to import it so now
this NP is the Alias which we will be
using let's create a simple array
let's look what is the type of this
array
so this is an indiary type of array Also
let's look what's the shape of this
array
so this is a shape of the array now here
we saw that we can expand the shape of
the array
so this is where you can change the
shape of the array using all those
functions now let's create an array
using arrange functions if I give
arrange 12 it will give me a one day
array of 12 numbers like this now we can
reshape this array
to 3 comma 4 or we can write it here
itself
so this is how arrange function and the
reshape function works for numpy now
let's discuss the next Library which is
scipy so this is another free and open
source python Library extensively used
in data science for high level
computations so this Library as the name
suggests stands for Scientific Python
and it has around 19 000 commits on
GitHub with an active community of 600
contributors it is extensively used for
scientific and Technical computations
also as it extends numpy it provides
many user-friendly and efficient
routines for scientific calculations now
let's discuss about some features of
scipy so scipy has a collection of
algorithms and functions which is built
on the numpy extension of python
secondly it has various high-level
commands for data manipulation and
visualization also the ndmh function of
scipy is very useful in
multi-dimensional image processing and
it includes built-in functions for
solving differential equations linear
algebra and many more more so that was
about the features of scipy now let's
discuss its applications so cyber is
used in multi-dimensional image
operations it has functions to read
images from disk into numpy arrays to
write arrays to discuss images resize
images
Etc solving differential equations
Fourier transforms then optimization
algorithms linear algebra Etc let's look
at a simple example to learn what kind
of functions are there in sci-fi here
I'm importing the constants package of
scipy Library so in this package it has
all the constants
so here I'm just mentioning C or H or n
a and this Library already knows what it
has to fetch like speed of light
Planck's constant Etc so this can be
used in further calculations data
analysis is an integral part of data
science data scientists spend most of
the day in data munching and then
cleaning the data also hence mention of
pandas is a must in data science life
cycle yes pandas is the most popular and
widely used python library for data
science along with numpy and matplotlib
the name itself stands for python data
analysis with around 17 000 comets on
GitHub and an active community of 1200
contributors it is heavily used for data
analysis in cleaning as it provides fast
flexible data structures like data
frames CVS which are designed to work
with structured data very easily and
intuitively now let's talk about some
features of pandas so pandas offers this
eloquent syntax and Rich functionalities
like there are various methods in pandas
like Drop n a fill any gives you the
freedom to deal with missing data also
Partners provides a powerful apply
function which lets you create your own
function and run it across a series of
data now forget about writing those for
Loops while using pandas also this
library's high level abstraction over
low level numpy which is written in pure
C then it also contains these high level
data structures and manipulation tools
which makes it very easy to work with
pandas like their data structures and
series now let's discuss the
applications of pandas so pandas is
extensively used in general data
wrangling in data cleaning then pandas
also Finds Its usage in ETL jobs for
data transformation and data storage as
it has excellent support for loading CSV
files into its data frame format then
pandas is used in a variety of academic
and Commercial domains including
statistics Finance Neuroscience
economics web analytics Etc then pandas
is also very useful in Time series
specific functionality like date range
generation moving window linear
regression date 15 Etc now let's look at
a very simple example of how to create a
data frame so data frame is a very
useful data structure in pandas and it
has very powerful functionalities so
here I'm only enlisting important
libraries in data science you can
explore more of our videos to learn
about these libraries in detail so let's
just go ahead and create a data frame
I'm using Jupiter notebook again and in
this before using pandas here I am
importing the pandas Library
let me go and run this so in data frame
we can import a file a CSV file Excel
files there are many functions doing
these things and we can also create our
own data and put it into Data frame so
here I am taking random data and putting
in a data frame also I'm creating an
index and then also giving the column
names so PD is the Alias we've given for
pandas random data of 6x4 index which is
taking a range six numbers and column
name I'm giving as ABCD now let's go
ahead and look at it
so here it has created a data frame with
my column names ABCD my list has six
numbers 0 to 5 and a random data of six
by four so data frame is just another
table with rows and columns where you
can do various functions over it also I
can go ahead and describe this data
frame to see so it's giving me all these
functionalities where count and mean and
standard deviation
Etc okay so that was about pandas now
let's talk about next library and the
last one so matplotlib for me is the
most fun Library out of all of them why
because it has such powerful yet
beautiful visualizations we'll see in
the coming slides plot and mac.lib
suggest that it's a plotting library for
python it has around 26 000 comments on
GitHub and a very Vibrant Community of
700 contributors and because of such
graphs and plots that it produces it's
majorly used for data visualization and
also because it provides an
object-oriented API which can be used to
embed those plots into our applications
let's talk about the features of
matplotlib the pi plot module of
matplotlab provides Matlab like
interface so matplotlib is designed to
be as usable as Matlab with an advantage
of being free and open source also it
supports dozens of backends and output
types which means you can use it
regardless of which operating system you
are using or which output format you
wish pandas itself can be used as
wrappers around matplotlips API so as to
drive Mac broadly via cleaner and more
modern apis also when you start using
this Library you will realize that it
has a very little memory consumption and
a very good runtime Behavior now let's
talk about the applications of
matplotlib it's important to discover
the unknown relationship between the
variables in your data set so this
Library helps to visualize the
correlation analysis of variables also
in machine learning we can visualize 95
confidence interval of the model just to
communicate how well our model fits the
data then matpatliff Finds Its
application in outlier detection using
scatter plot Etc and to visualize the
distribution of data to gain instant
insights now let's make a very simple
plot to get a basic idea I've already
imported the libraries here so this
function matplotlib inline will help you
show the plots in the Jupiter notebook
this is also called a magic function I
won't be able to display my plots in the
jupyter notebook if I don't use this
function I am using this function in
numpy to fix random state for
reproducibility now I'll take my n as 30
and will assign random values to my
variables so this function is generating
30 random numbers here I am trying to
create a scatter plot so I want to
decide the area let's
put this so just multiplying 30 with
random numbers to the power 2 so that we
get the area of the plot which we will
see in just a minute so using the
scatter function and the Alias of
matplotlip as PLT I've created this if I
don't use this in a very small circle
since my scatter plot it's colorful it's
nice so that's one very easy plot I
suggest that you Explore More of
matplotlib and I'm sure you will enjoy
it let's create a histogram so I'm using
my the style is GG plot and assigning
some values to these variables any
random values
now we are assigning bars and colors and
Alignment to the plot and here we get
the graph so we can create different
type of visualizations and plots and
then work upon them using matplotlib and
it's just that simple so that was about
the leading python libraries in the
field of data science but along with
these libraries data scientists are also
leveraging the power of some other
useful libraries for example like
tensorflow Keras is another popular
Library which is extensively used for
deep learning and neural network modules
kelas drafts both tensorflow and theano
back-ends so it is a good option if you
don't want to dive into details of
tensorflow then scikit-learn is a
machine learning library it provides
almost all the machine learning
algorithms that you need and it is
designed to interpolate with numpy and
sci-fi then we have c bond which is
another library for data visualization
we can say that Seaborn is an
enhancement of matplotlib as it
introduces additional plot types so
let's start with what is number numpy is
the core library for scientific and
numerical Computing in Python it
provides high performance
multi-dimensional array object and tools
for working with arrays and I'll go a
step further and say there are so many
other modules in Python built on numpy
so the fundamentals of numpy are so
important to latch onto for the python
so you can understand the other modules
and what they're doing numpy's main
object is a multi-dimensional array it's
a table of elements usually numbers all
of the same type indexed by a tuple of
position integers in numpy dimensions
are called axes take a one-dimensional
array or we have remember dimensions are
also called axes you can say this is the
first axis 0 1 2 3 4 5. and you can see
down here it has a shape of six why
because there's six different elements
in it in the one dimension array and
they usually denote that as six comma
with an empty node on there and then we
have a two dimensional array where you
can see zero one two three four five six
seven and in here we have two axes or
two dimensions and the shape is two four
so if you were looking at this as a
matrix or another mathematical functions
you can see there's all kinds of
importance on shape we're not going to
cover shape today but we will cover that
in part two did you know that numpy's
array class is called ND array for numpy
data array now we're going to take a
detour here because we're working in
Python and two of my favorite Tools in
Python is the Jupiter notebook and then
I like to use that sitting on top of
anaconda and if you flip over to
jupiter.org that's j u p e y t e r dot
org you can go in here you can install
it off of here if you don't want to use
the Anaconda notebook but this is the
Jupiter setup the documentation on the
Jupiter Jupiter opens up in your web
browser that's what makes it so nice is
this portable the files are saved on
your computer they do run an IPython or
iron Python and you can create all kinds
of different environments in there which
I'll show you in just a minute I myself
like to use Anaconda that's
www.anaconda.com if you install Anaconda
it will install the jupyter notebook
with the Anaconda separate and you can
install Jupiter notebook and it'll run
completely separate from anaconda's
Jupiter notebook and you can see here
I've now opened up my anaconda Navigator
what I like about the Navigator and this
is a fresh install on a new computer
which is always nice I can launch my
Jupiter notebook from in here I can
bring other tools so the Anaconda does a
lot more and under environments I only
have the one environment and I can open
up the terminal specific to this
environment this one happens to have
python 37 in it the most current version
as of this tutorial and then you open a
terminal if you're going to do your pip
installs and stuff like that for
different modules you can also create
different environments in here so maybe
you need a python36 python35 you can see
we're having a nice framework like
Anaconda really helps so you don't have
to chat track that on your own in the
Jupiter notebook in your different
Jupiter notebook setups we'll go ahead
and launch this Jupiter notebook and
then I've set my browser window for a
deep fault of chrome so it's going to
open up in Chrome and you can see here
this opens up a folder on my computer we
have a couple different options on here
remember I set the environment up as
python 3.7 you would install any
additional modules that aren't already
installed in your python on this and it
keeps them separate so you do have to
for each environment install the
separate modules so they match the
environment on there and in here we have
a couple things we can look up what's
running do you have your different
clusters again this is I just installed
this on a new machine so I just have the
one a couple things in here that were
run on here recently and what we go on
here is we then have on the upper right
new and from the pull down menu you'll
see python3 and this will open up a new
window
and now we're in Jupiter python so this
is a python window and we'll just do a
print
and this of course is this little hello
world
and we'll run that and it prints out
hello world in the command line there's
a couple special things you have to know
we're not going to do today which is on
Graphics if you've never seen this
one of the things you can do is you can
also do a equals hello world and if you
just put the a in there now if you do a
bunch of these we have a equals hello
world b equals goodbye world and you put
a b a then return B it'll only run the
last one but you can see here if you put
the variable down here it will show you
what's in that variable
and that has to do with the Jupiter
notebook inline coding so that's not
basic python that's just jupyter
notebook shorthand which you'll see in a
little bit so back to our numpy numpy
array versus python list python list
being the basic list in your python why
should we use numpy array when we have
python list well first it's fast the
numpy array has been optimized over
years and years by multiple programmers
and it's usually very quick compared to
the basic python list setup it's
convenient so it has a lot of
functionality in there that's not in the
basic python list and it also uses less
memory so it's optimized both for Speed
and memory use and let's go ahead and
jump into our Jupiter notebook since
we're coding best way to learn coding is
to code just like the best way to learn
how to write is write and the best way
to learn how to cook is cook so let's do
some coding here today and just like any
modules we have to import numpy we
almost always import it as NP that is
such a standard so you'll see that very
commonly we can just run that and now we
have access to our numpy module inside
our Python and then the most common
thing of course is to go and create a
number array
and in here we can send it a regular
list
and so we'll go ahead and send this a
regular array let's go one two three to
make it simple and then I'm just going
to type in a and we'll run this
and so you can see down here the output
is an array of one two three and we
could also do
print just a reminder that this is an
inline command so that wouldn't work if
you're using a different editor you can
see that it's an array one two three but
we'll go and leave it as a
kind of a nice feature so you can see
what you're doing really quick in the
jupyter notebook and just like all your
other standard arrays I can go a of zero
which is going to be a value of one of
course we do a of one you go all the way
through this
if one has a value of 2 in it so whether
using the numpy array or the basic
python list that's going to be the same
that should all look pretty familiar and
be pretty straightforward remember the
first value is always zero
and when we set on there so let's take a
look why we're using numpy because we
went over the slide a little bit but
let's just take a look and see what that
actually looks like and what we want to
look at is the fact that it's fast
convenient and uses less memory so let's
take a glance at that in code and see
what that actually looks like when we're
writing it in Python and what the
differences are
and to do this I'm going to go ahead and
import a couple other modules we're
going to import the time module so we
can time it and we're going to import
the system module so that we can take a
look at how much memory it uses and
we'll go and just run those so those are
imported and so we'll do b equals a
range of one yeah 1000 is fine
and so that's going to create a list of
100 0 to 999 remember it starts at zero
and it stops right at the 1000 without
actually going to the 1000. and let's go
ahead and print
and we want system dot get size of
and we'll pick any integer because we
have you know zero to a thousand we'll
just throw one in there five it doesn't
matter because it's gonna whatever
integer we put in there is going to
generate the same value because looking
the size of how how much memory it
stores an integer in
and then we want to have the link of the
B that's how many integers are in there
and if we go ahead and execute this and
run this in a line we'll see Oops I did
that wrong comma if we multiply them
together
we'll see it generates 28 000. so that's
the size we're looking at is twenty
eight thousand I believe that's bytes
that sounds about right
so let's go ahead and create this in
numpy and we'll go with C equals NP and
this is a range
so that's the numpy command to do the
same thing that we were just doing in a
list
and we'll also use the same value on
there the 1000
. once we've created the C value of C
for NP dot a range let's go ahead and
print and we can do that by doing C dot
size
times C dot item size
well that's very similar we did before
we did get the size of so the C size is
the size of the array and each item size
just reversed so the size of an integer
five item size is going to be the
integers and C size now let's just take
a look and see what that generates
and wow okay we got 4 000 versus 28 000.
that's a significant difference in
memory how much memory we're using with
the array and then let's go ahead and
take a look at speed let's do oh let's
do size we trade this with lower values
and it would happen so fast that the
npra kept coming up with zero
because it just rounded it off so size
and let's create an L1
Bulls range of size
and we'll do an L2
I'll just set up to the same thing it's
also range
of size on there there we go
and then we can do on A1
equals NP Dot
a range size
and then let's do an A 2 equals NP dot a
range we'll keep it the same size
and what we're going to do is we're
going to take these two different arrays
and we're going to perform some basic
functions on them but let's go ahead
actually just load these up now we'll go
ahead and run this so those are all set
in memory
except for the typo here
quickly fix that
there we go so these are now all loaded
in here and let's do a start equals
time dot time
so it's just going to look at my clock
time and see what time it is and it will
do result equals and let's do oh let's
say we got an array and we're going to
say
let's do some addition here X Plus y
for X comma y
in and we'll zip it up here
two different arrays so here's our two
different arrays we're going to multiply
each of the individual things on here L1
L2
there we go so that should add up each
value so L1 plus L2 each value in each
array
and then we want to go ahead and print
and let's say python list took
and then we'll do
time
dot time
we'll just subtract the start out of
there so time whoops I messed up on some
of the quotation marks on there
okay there we go
time minus the start
and we'll convert that to seconds so
we'll go to this in milliseconds or
times 1000.
and let's hit the run on there it's kind
of fun because you also get a view while
we're doing this of some ways to
manipulate the script
and as you can see also my bad typing
there we go okay so we'll go ahead and
run this
and we can see here that the python list
took 34
actually I have to go back and look at
the conversion on there but you can see
it takes roughly 0.34 of a second and we
can go ahead and print the result in
here too
let's do that
and we'll run that just so you can see
what the what kind of data we're looking
at
and we have the zero two four six eight
so it's just adding them together it
looks pretty straightforward on there
and if we scroll down to the bottom of
the answer again we see python list took
46 a little different time on there
depending on what um core because I have
this is on an eight core computer so it
just depends on what course running on
what else is pulling on the computer at
the time and let's go back up here and
do our start time paste that into here
and this time we're going to do a result
equals and this is really cool notice
how elegant this is It's So
straightforward this is a lot of reason
people started using numpy is because it
can add the two arrays together by
simply going A1 Plus A2 makes a lot of
sense both looking at it and a system
very
convenient remember this slide we're
looking at fast convenient and less
memory so look how convenient that is
really easy to read real easy to see and
I don't know if we don't need to print
the result again so let's just go ahead
and print the time on here and we'll
borrow this from the top part
because I really am a lazy type
this isn't the python list this is the
numpy list or numpy array
and let's go ahead and see how that
comes out and we get 2.99 so let's take
a look at these two numbers 46 versus
2.99 so we'll just round this up to
three that's a huge difference that's
that's like more than 10 times faster
that's like 15 times roughly at a quick
glance I'd have to go do the math to
look at it and it's going to vary a
little bit depending on what's running
in the background the computer obviously
so we've looked at this and if we go
back here we found out it's much faster
yes there's different going to be
different speeds depending on what
you're doing with the array very
convenient easy to read and it uses less
memory so that's the core of the numpy
that's why a lot of people base so many
other modules on numpy and why it's so
widely used
so we did glance at a couple operations
when we were looking at speed and size
let's dive into a little bit more into
the basic operations
and these are always nice to see I mean
certainly you want to go get a cheat
sheet if you're using it for the first
time you know look things up Google is
your friend we did this where the most
basic numpy dot array or NP dot array
and we'll go ahead and create an array
let's do pairs one comma two
and then let's do a three comma four and
if we do that let's do five comma six
there you go and if we go ahead and take
this and run this I can go ahead and do
our a down here so it's in line it'll
print that out
you can see it makes a nice array for us
so we have a and if you look at that we
have three different objects each with
two values in them and hopefully you're
starting to think well how many
dimensions or indexes is that and you'll
see three by two so let's go ahead and
take a look and let's go how about a dot
in Dimensions speaking of which we'll
run that and we have two dimensions for
each object
and then we can do the item size so a
DOT we saw this earlier we looked up how
many items it was up here where we
wanted to multiply item size times the
actual size of the object so the memory
is being used versus the item size
and we should see four there
memory is compressed down that's always
a good thing
and then the shape the shape is so
important when you're working with data
science and you're moving it from one
format to another
so we have our shape we just talked
about that we have three by two three
rows by two objects in each one
generally I don't look too much at the
size but the dimensions I'm always
looking up this is nice you can automate
it so you might be converting something
you might need to know how many
dimensions are going into the next
machine learning package so that you can
automatically just have it send that
information over
so we looked at a shape let's go and
create a slightly different array NP dot
array let's go ahead and just do as our
original
set up here
and one of the features we can do which
is really important is we can do D type
equals in this case let's do NP
float 64. and so what we've done is
converting all of these into a float and
we type in a
and now instead of having one two three
four five six you see they're all float
values 1.0 there's no actually 0 in
there just there's a one dot or the one
period two three period four period five
period six period
and this again data science I don't know
how many times I've had to convert
something from an integer to a float so
it's going to work correctly in the
model I'm using so very common features
to be aware of and to be able to get
around and use
and we'll also do let's just curiosity
item size
we'll go and run that
and we see that it doubled in size so
it's not a huge increase well doubling
is always a big increase in computers
but it's not a huge increase compared to
what it would be if you're running this
in the python list format
and then we did the shape earlier
without having it set to the float 64.
let's go ahead and do a shape with it
set to 64. and it should be the same
three common two so it all matches so
we've gone through and remember if you
really if this is all brand new to you
according to the Cambridge study at the
Cambridge University if you're learning
a brand new word in a foreign language
the average person has to repeat it 163
times before it's memorized
so a lot of this you build off of it so
hopefully you don't have to repeat it
163 times but we did manage to repeat it
at least twice here if not a little bit
more and let's go ahead and take this
we're going to go look at one more setup
on here and let me just take this last
statement here on the converting our
properties of our data and instead of
float 64
let's do complex let's just see what
that looks like and let's go ahead and
print that out and run it
and so we now have a complex data set up
and you'll see it's denoted by the one
dot plus 0 dot j
and if we flip over here and do a basic
search for numpy data types better to go
to the original web page but pull up a
bunch of these you can see there's a
whole list of different numpy data types
shorthand complex we have complex
complex 64 complex 128 complex number
represented by 264-bit floats real and
imaginary components
one option on there float 16 float 32
float shorthand for float 64 most
commonly used and of course all the
different ones that you can possibly put
into your numpy array so we covered a
basic Edition up there we're comparing
how fast it runs but some very basic
components how to set up a numpy array
how many dimensions it has item size
data type item again we went to item
size and there's also the
shape probably one of the more used I
used a shape all the time very commonly
used
and then down here you can see where we
actually created a numpy complex data
type
so let's look at some other features in
numpy one of them is you could do numpy
Dot
zeros
and we're going to do three comma four
there we go and we'll go ahead and run
this and you can see if I do NP dot
zeros I create an empty array of zeros
this is really important I was building
my own neural network and I needed to
create an array where I initialized the
weights and I want them all to be the
same weight in this case I want them to
start off with zero for the particular
project I was working on and there's
other options that you can do mp1s
and we'll do the same thing three comma
four we'll run that and you can see I've
created a an array of numpy ones in this
case it comes out as a float array
and this is an interesting to note
because we have let's go back to our
Python and do L Range Five
and we'll print the L so there's our
list and if I run that
it doesn't create the range and tell
after the fact until you actually
execute it that's an upgrade in python
python 27 actually created the array
zero one two three four this one
actually creates the script and then
once it's used it then actually
generates the array and if we do that in
numpy a Range remember that from before
and if we do a numpy a range 5
and let's do a
l or we can just leave it as numpy
that's right there we go just run that
you can see there we actually get an
array 0 1 2 3 4 for the value of the
numpy arrange a range five generates the
actual array
and for part one we're going to do just
one more section on basic setup
and we're going to concatenation
concatenation out example
there we go we're gonna do strings let's
take a look at strings and what's going
on with there and let's do
oh let's see print
Let's do an NP character something new
here and we're going to add and then
here's our brackets for what we're going
to add
oh and let's say
um let's do
hello
comma High
and in the brackets on there let's
create another one
and this one's going to be
ABC and we'll do
x y z so we're just creating some
randomly making sum up on here and then
we'll go ahead and just print this
if we run that and come down here and of
course make sure all your brackets are
open and closed correctly and then you
can see in here when we concatenate the
example in numpy it takes the two
different arrays that we set up in there
and it combines the hello with the ABC
and the high with x y z
and if we can also do something like
print oh let's do NP character dot
multiply so there's a lot of different
functions in here again you can look
these up it's probably good to look them
all up and see what they are but it's
good to also just see them in action
let's do hello space comma three
and we'll run this one
and run that without the error and
you'll see it does hello hello hello so
we multiplied it by three and we can
also let's just take this whole thing
here instead of retyping it
and we can do character Center so
instead of multiply this to Center
and over here keep our hello going
to space out of there and let's do
Center it 20.
and fill character
equals we'll fill it with dashes
so if we run this
you can see it prints out the hello with
dashes on each side and we keep going
um with that we can also in addition to
doing the fill function we can play with
capitalize we can title we can do
lowercase we can do uppercase we can
split split line strip join these are
all the most common ones and let's go
ahead and just look at those and see
what those look like each one of them
so we're going to do the hello world
all-time favorite of mine I always like
to say Hello Universe and you can see
here we do Capital H with the world but
so we want to capitalize so capitalize
is the first one in the array so we get
Hello World on there and we can also
take this and instead of capitalizing
another feature in here is title and
let's just change this to how are we
doing
how are you doing instead are we able to
do you and let's run that
and you can see here because we created
it as a title it capitalizes the first
letter in each word
and in this one we're going to do
character lower
two different examples here we have an
array we have Hello World all
capitalized and we have just hello and
you can see that one is an array and one
is just a string if we run that you get
a an array with Hello World lowercase
and hello lowercase and if we're going
to do it that way we can also do it the
opposite way there's also upper
and let's paste those in there and you
can see here we have character Dot Upper
opposite there
python.data and we'll do python is easy
hopefully you're starting to get the
picture that most of the Python and the
scripting is very simple it's when you
put the bigger picture together and
starts building these puzzles and
somebody asks you hey I need the first
letter capitalized unless it's the title
and then we have you start realizing
that this can get really complicated so
numpy just makes it simple and we like
that and so in this case we did python
data it's all uppercase python is easy
like like shouting in your messenger
python is easy and then if you're ever
processing text and tokenizing it a lot
of times the first thing you do is we
just split the text and we're just going
to run this in P dot character.split are
you coming to the party if we do that it
returns an array of each of the
individual words are you coming to the
party splitting it by the spaces
and then if you're going to split it by
spaces we also need to know how to split
it by lines
and just like we have the basic split
command we also have split lines hello
and you'll see here the scoop in for our
new line
and when we run that if you're following
the split part with the words you should
see hello how are you doing the two
different lines are now split apart
and let's just review three more before
we wrap this up commonly used string
variable manipulations we have strip and
in this case we have Nina admin Anita
and we're going to strip a off of there
let's see what that looks like and then
you end up with nin dominate it
basically takes up all leading and
trailing letters in this case we're
looking for a more common would be a
space in there but it might also be
punctuation or anything like that that
you need to remove from your letters and
words and if we're going to strip and
clean data we also need to be able to
reformat it or join it together so you
see here we have a character joined
we'll go ahead and run this
and it has on the first one it splits
you so letters up by the colon and the
second one by the dash and you can see
how this is really useful if you're
processing in this case a date we have
day month year year month date very
common things to have to always switch
around and manipulate depending on what
they're going into what you're working
with
and finally let's look at one last
character string we're going to do
replace if you're doing misinformation
this is good pulling news articles
replacing is and what in this case we're
just doing his a good dancer and we're
going to replace is with was
and you can see here he was a good
dancer hopefully that's not because he
had a bad fall he just was from like you
know 1920s and it's gotten old
so there we go we've covered a lot of
the basics in numpy as far as creating
an array very important stuff here when
you're feeding it in how do we know the
shape of it the size of it what happens
when we convert it from a regular
integer into a float value as far as how
much space it takes we saw that that
doubled it item size you have your in
dimensions and probably the most used is
shape I will cover more on shape in part
two so make sure you join us on part two
because there's a lot of important
things on shaping in there and setting
them up we also saw that you can create
a zeros based array you can create one
with ones if we do a range you can see
how it is a lot easier to use to create
its own range or a range as it is in
numpy you saw how easy it was to add two
arrays we saw that earlier just plus
sign then we got into doing strings and
working with strings and how to
concatenate so if you have two different
arrays of strings you can bring them
together we also saw how you can fill so
you can add a nice headline dash dash
dash we saw about capitalize the first
letter we saw about turning it into a
title so all the first letters are
capitalized doing lowercase on all the
letters upper for all the letters just
lower and upper nice abbreviation we
also covered how to split the character
set how to strip it so if you want to
strip all the A's out from leading A's
and ending A's or spaces you can do that
very easily also how to join the data
sets so here's a character join option
for your strings and finally we did the
character replace now let's go ahead and
dive in there since we're going right
into part two which is getting some
coding going under our belt and here in
our Jupiter notebook we can go under new
and create a new folder python3 I think
I forgot to do this last time but we
could just do the control plus plus
which in any browser enlarges a page
makes it a lot easier to see always a
nice feature another beautiful benefit
of using jupyter notebook and let me go
ahead and show you a neat thing can do
in Jupiter this is nice if you're
working with people and you're doing
this as a demo on a large screen I'm
going to do the hashtag or pound symbol
array manipulation kind of a title that
we're working on and then I'm going to
call this cell cell type markdown as
opposed to code and you'll see it
highlights it here and then if I run it
it just turns it into array manipulation
and then we're specifically going to be
working on array manipulation changing
shape to start with and we'll go ahead
and Mark this cell also a markdown so
has a nice little look there and then it
comes up and you can see it just like I
said it just highlights it it makes it
very bold print just making it easier to
read not a python thing but a Jupiter
thing that's good to know about
especially if you're working with the
shareholders since they're investing
money in you of course the first thing
you want to do is import we're going to
import numpy
as in p and that should be standard by
now by now you you start a Python
program you're doing some data science
numpy is just something you bring in
there and let's go ahead and create our
array and we're going to do that as the
NP dot a range remember that's uh zero
well we're going to do 0 to 9.
and uh we'll print
a little title on the original array
we'll just print that array a remember
from the first lesson so we have our
array which is zero one two three four
five six seven eight and let's add a
print space in between
let's create a second array B but we
want this to reshape array a and what
does that mean
and the command is simply reshape and
then we have nine items in here and this
is so important right now so be very
aware if I did some weird numbers in
here it's not going to work
and we want multiples of nine we know
that three times three is nine so we're
going to reshape our a array by 3 by 3
and then we're going to print well let's
give it a title oops I had too many
brackets in there modified array and
then let's go ahead and print
RB and let's see what that looks like
and as we come down here you can see
we've taken this and it's gone from 0 1
2 3 4 5 6 7 8 to an array of arrays and
we have 0 1 2 3 4 5 6 7 8. and so we
split this into three by three and you
can guess that if I tried to reshape
this let's just do a five by three which
is 15. that's going to give me an error
so it's not going to work you're not
going to reshape something unless the
shape all the the data in there matches
correctly
so we can take this nine this flat 9 and
we call it a flex it's just a single
array and we can reshape it into a three
by three array and first you might think
matrixes which this is used for that
definitely I use it a lot in graphing
because it'll come in that I have an
array that's X Y comma X Y 1 y 1 comma
X2 Y2 and so the shape of it might be
two by the length of the number of
points and I need to separate that into
X flat array and a y flat array and you
can see this can be very easy to reshape
the array doing that and we can of
course go back we can do B and print
and we'll do B dot platin remember I
said it's called a flatten array and if
we run that you'll see this goes back to
the original one it takes this zero one
two three four five six seven eight and
flattens it back to a single array
and then one other feature to be aware
of is if we flatten it one of the
commands we can put in there is order
let me just go ahead and do that order
equals
F strangely enough F stands for Fortran
they hold fourtran days I remember
actually studying Fortran programming
language in this case you'll see that it
uses the first like zero three six is
the order so instead of flattening it
like we had before zero one two three
four five six seven eight it now does
zero three six one four seven two five
eight and if you go to the numpy array
page you can see here that they have the
flatten and you just open up the numpy
and D array flatten setup to look it up
and they have three different options
they have c f and a and it's whether to
flatten in C which was based on how the
C code works for flattening originally
worked which is row major
Fortran which is column major or
preserve the column Fortran ordering
from a so whatever it was in the default
is the C version so the default that you
saw you could put orders equals c and
it'd have the same effect as we saw
there before you could even do order
equals a that would also have the same
effect because that's the default so
really the only other thing you need to
change on here is to change it to C if
you need it and you can see right here
or F I mean not C the only thing you
really want to change it to is to your f
for the Fortran order which then does it
by column versus by row and let's look
at here we go reshape so let's create a
range of 12
and let's reshape it
I will do 4 comma 3 for this one and
remember this is numpy I forgot the NP
there
NP dot arrange
and we can type in just a for print or
you can do full print a and of course a
jupyter notebook you even have a little
extra print at the beginning we run this
we'll see we create a nice array of 0 1
2 it's reshaped it so we have four rows
and three columns or you could call that
three columns and four rows zero one two
three four five six seven eight nine ten
eleven
but this one is so important we'll do NP
transpose
a let's go ahead and run that
and it helps if I get all the s's in
there and don't leave an S out and
you'll see here we've taken our array if
you remember correctly we had 0 1 2 3 4
5 6 7 8 9 10 11 and we've swapped it so
we've gone from a three by four or a
four by three to a three by four
and this really helps if you're looking
at like a huge number of rows and the
data all comes in like let's say this is
your features in row one your features
in row two and this is x y z well when
you go to plot it you send it all of X
in one array all of one another one
array and all Z in another array
it's really important that we can
transpose this rather quickly
this is kind of a fun thing I can
highlight it and do brackets around it
if you remember correctly
because we're in Jupiter it doesn't
matter where we do the print or not
it'll automatically print it for us and
you see if I hit the Run button it comes
up at the same exact thing and let's
play with the reshape and you don't want
to zoom this up a little bit here
make that even bigger so you can really
see what's going on and let's play with
the reshape just a little bit more we'll
do b equals
in P dot a range let's do eight
reshape we'll do two comma four
let's go ahead and print B
and then run that and you'll see we have
now the two rows this is a bit more like
so we have four maybe two rows of four
things so this might be all of our X
components and our y components so we
can switch it back and forth real easy
important to note here whether we do two
comma four or in the case of four comma
three this has 12 elements and so
however you split it up it's got to
equal 12. so 4 times 3 equals 12 that's
pretty straightforward same thing down
here 2 times 4 equals eight if I change
this and let's say I do two comma three
let's just run that in and you'll find
we get an error because you can't split
8 up into two rows by three
you have to pick something that it can
split up and arrange it in so let's go
ahead and run that and just for fun
let's go um
reshape our B again if I can type
reshape our B again and what else goes
into eight well we could do two by two
by two
so we can take this out to three
different dimensions
and then if of course if we um because
this is going to come out you as a
variable we can just go ahead and run it
and it'll print it we can also do a
print statement on there just like we
did before and you'll see we have two
different groups of two variables of two
different dimensions so two by two by
two and let's go ahead and assign this
to a variable C equals B reshaped and
let's do something a little different
let's roll the axes roll
axes
and we'll take our C and do two comma
one
and if we go ahead and run this it's
going to print that out whoops
hit a wrong button there let's do that
one again and you roll the axis and you
can see that we now have a set of zero
one two three four five six seven we now
have the zero two one three four six
five seven
so what's going on here we're taking and
we're rolling the numbers around and
let's just simplify this we'll just do
it with C comma 1 and run that and so if
we roll a single axis you get 0 1
and then it rolled the four five up and
then we have two three six seven and if
we do two let me see what happens there
and this is one of those things you
really have to play with and start
feeling what it's doing
we've now taken 0 2 4 6 1 3 5 7. so you
can see we've now rolled by two digits
instead of rolling the one set up we now
rolled two digits up there and so if we
go back and we do the one
so we've rolled it up zero one four five
and then we're going to take the 2 in
there and we've rolled the 0 1 2 3 4 5
and 6 7. so we start rolling these
things around on here there's a lot of
different things you can do on this
what's another way to manipulate the
numbers on your numpy
and finally let's go ahead and
swap
axes we'll do c and let's just go ahead
and run that it's going to error on
there
that's because it requires a multiple
arguments left out the arguments so now
we can swap them we get the zero two one
three four six five seven so you can see
everything's been swapped around
so next thing we want to go over is we
want to go over numpy arithmetic
operations
how can we take these and use these let
me just go ahead and put this cell as a
markdown there we go we'll run that so
it has a nice thing all right nice title
on there that's always helpful
and let's start by creating two arrays
we'll do a as an EP NP
range a range nine and let's reshape
this
three by three so by now you should be
saying this reshape stuff and this
should all look pretty familiar we have
our zero one two three four five six
seven eight on there and let's create a
second one B okay this time instead of
doing a range let's do NP array we'll
just create a straight up array
and we'll do an array of three objects
so it's going to be three by one and if
we go ahead and print a b out let me run
that this is actually pretty common to
have something like this where you have
a three by whatever it is in a three by
by three array when you're doing your
math you kind of have that kind of setup
on there
and what we can do is we can go
P dot add a b don't forget we can always
put a print statement on there so if we
add it you'll see that it just comes in
there and it goes okay we're adding 10
to everything and we could actually do
something more oh make it more
interesting 11 10 11 12. so let's change
B's now 10 11 12 and let's run that
and you can see that we have 10 and then
you had 1 plus 11 is 12. 2 plus 12 is 14
13 so 10 plus 3 is 13 11 plus 4 is 15
and 12 plus 5 is 17 and so on
we'll put this back since that's how the
original setup was let's do 10 by 10 by
10 and run that and run that and get the
original answer and if you're going to
add them together we need to go ahead
and subtract
a b
and we run that
we get minus 10 minus 9 minus 8 just
like you would expect so we have our
subtraction 0 minus 10 is minus ten and
so on and if you're going to add and
subtract you can guess what the next one
is we're going to multiply
and we'll multiply
a b
and this should be pretty
straightforward you should expect this
if we multiply 10 times 0 we got 0 10
times 10 is 10 and so on
and finally if you're going to multiply
let's
all odd is divide
what happens when you do divide a by B
and we run this
we're going to get 0 and this is 0
divided by 10 is 0 1 divided by 10 is
0.1 2 divided by 10 is 0.2 and so on and
so on so that math is pretty
straightforward it just makes it very
easy to do the whole setup and again if
we went this and let's say well let's
change this up up here instead of 10 we
do a hundred
and make this a thousand there we go if
we run that and then we do the add you
can see we got 10 plus 100 plus a
thousand same thing with the subtract
same thing with the multiply
then you can also see the same thing
here with the Divide so a lot of control
there with your array and your math
again let's set this back to 10 oops
it's right up here wrong section
there we go 10 and we'll just go ahead
and run these and get back to where we
were
and this brings us to our next section
which is slicing and let's put in our
just make this a cell
of course it gives us a nice looking
slicing there and slicing means we're
just going to take sections of the array
so let's create an array NP a range
let's just do 20.
and if remember if we do a we have a 0
to 19.
and then we can do a and remember we can
always print these this can always be
put in a print but because I'm in
Jupiter if you're doing a demo in
Jupiter that is it's just so great that
you have all these controls on here so
we could slice four on and this should
look familiar because this is the same
as a python and a lot of other different
scripting languages if we do four we'll
go 0 1 2 3 as the first four in the
thing and the skip sum and starts with
this one the first four are skipped then
from there on you can also do the
opposite
and go till the fourth one if we run
that we get 0 1 2 3 quite the opposite
on there we can do a single item so we
can pick object number five on the list
run that and five happens to be five
because that's the order they're in and
then this one's interesting because I
can do s equals slice
and let's create a slice here and let's
do two comma 9 comma
yeah let's leave a 2 on there so we'll
create an S Slice on here and then if we
take our array and we do array of s
we're taking our slice in there and
let's go ahead and run that and let's
take a look and see what it generated
here first off we started with two so we
have two at the beginning we're going to
end at 9 which happens to be eight so it
stops before the nine remember when
we're doing arrays in Python and then we
step two so two four six eight we could
do this as three let me run that and you
can see how that changes two five eight
and we could do this as let's leave this
at three and if we change this to 10
oops
12. there we go
and we run that we have 2 5 8 11. so
that's pretty straightforward it's a
very nice feature to have on here where
you can slice it and take different
parts of the series right out of the
middle so now that we've accessed the
different pieces of our array and let's
get into iterating iteration and this is
interesting because my sister who runs a
college data science division is first
question she asks is how do you go
through data and she's asking can you do
you know how to iterate through data do
you know how to do a basic for Loop do
you know how to go through each piece of
the data and in numpy they have some
cool controls for that just sends a mark
down there we go and run it
and it's called the nditter
sure what the ND stands for but ND it or
for iterator
so before we do that though let's create
an array or something we can actually
iterate through we'll call it a equals
NP a range let's do something a little
funny here or funky
and we'll do 0 45 5. I'm not sure why
the guys in the back pick this
particular one was kind of a fun one and
if I run that we do this you can see um
we get 0 5 10 15 20 25 30 35 40. that's
what this array looks like
and this is from our slice you could
this is just a slice that's all that is
is we created a slice of 0 45 0 to 45
step five and so we can do with this we
can also do a equals V shape let's go
ahead and take and reshape this and
since there's nine variables in there
we'll do a reshape at three by three so
if we run that oops miss something there
that is the a that really helps so if we
do the a reshape and we'll go ahead and
print that out we get 0 5 10 15 20 25 30
35 40. and then we simply do 4X in
our numpy ND enter of a
colon and we'll just go ahead and print
X
and let's see what happens here when we
run through this and we print each one
of those
it goes all the way through the whole
array so it's the same thing we just saw
before we got 0 5 10 15 20 25 30 35 40.
so it prints out each object in the
array so you can go through and view
each one of these
and certainly if you remember you could
also flatten the array and just do for a
and that also and get the same result
there's a lot of ways to do this but
this is the proper way with the ND
iterator because it'll minimize the
amount of resources needed to go through
each of the different objects in the
numpy array
and hopefully you asked this question
when I just did that
and the question is how can I change
this instead of doing each object so
first of all let's go ahead and take my
cell type Mark that down run it and so
we're going to work on iteration order C
Style
style C because it came from the C
programming and that's because it came
from the old Fortran programming so
let's give us a reminder I will do a
print a and we'll do four x n n p
iterate a but we also want to do this in
a specific order and you know what I'm a
really lazy typer so let's go back up
here
this is the ND iterator I know I'm
missing the ND part of a let's do order
equals
C we'll print X on there and let's do
that again and this time order
equals
f
let me go order equals F let's go ahead
and run this
and see what happens here and the first
thing you're going to notice our
original array 0 5 10 15 20 25 30 35 40.
when we do order C that's the default 0
5 10 15 20 and so on and then when you
come down here
you'll see F order f is 0 15 30. so it
takes the first digit of each of the sub
arrays or the second dimension and then
it goes into the second one 520 35 10 25
40. so slightly different order for
iterating through it if you need to do
that so we've covered reshaping we
covered math we've covered iteration
we've covered a number of things the
next section we want to go ahead and go
over
is going to be joining arrays so we need
to bring them together let me go ahead
and take the cell and make it a markdown
cell type markdown there we go and run
that so let's work on joining arrays so
we can bring them together and what
different options we have
and let's do we'll do an NP array one
two comma three four
we'll go ahead and print
oops
first
these Rays aren't that big so let's just
go ahead and keep it all on one line a
if we run this first array one two three
four whoops I forgot that it
automatically wraps it when you do it
this way so we'll go ahead and keep it
separate
print a there we go and let's go ahead
and do a b
and we'll do five six seven eight and
notice I'm keeping the same shape on
these two arrays depending on what
you're doing those shapes have to match
let's go ahead and print
second array
print
B go and run that that's
missed something up there let me fix
that real quick
and I was reformatting it to go on
separate lines I messed that up there we
go run all right so we have first array
one two three four second array five six
seven eight
and we'll put a carried return on there
and the keyword we use is concatenate
and if you're familiar with Linux it
usually means you're adding it to the
end on there and we're going to do what
they call a long axis zero so we have
concatenate a b along axis zero let's go
ahead and run that and see what that
looks like
so we have one two three four five six
seven eight so now we have an array that
is four by two has a nice shape of four
by two one here
and if we're going to do it along the
axis 0
you should guess what the next one is
we're going to do it along the one axis
and let's see how those differ from each
other let's just go ahead and run that
and again all we're doing is adding in
the axes equals one so we have our
concatenate we have a b and then axis
one remember a couple things one these
are the same shape so we have a two by
two same dimensions going in there
you're going to get an error if you're
concatenating and they're not if you
have something that instead of one two
is one two three four five six with a
five six seven eight they'll give you an
error on there in fact let's take a look
and see what happens when we do that let
me just take this
one two three three four five and let's
run that and if we come down here up we
got there it says it says all the input
array Dimensions except for the
concatenation axes M must match exactly
so it will let you know if you mess up
that's always a good thing let's go
ahead and take this back here and let's
go ahead and run that
and so we have our zero axes which is
one two three four five six seven eight
and we bring them together and you'll
see a very different setup here where we
do it along the axis one we end up with
instead of four by two we end up with a
two by four one two five six three four
seven eight and this is changing which
axes we're going to go ahead and
concatenate on what I find is when
you're talking about the concatenate or
the joining arrays you really got to
play with these for a while to make sure
you understand what you mean by the axes
it looks very intuitive when you're
looking at it actually 0 1 2 3 4 5 6 7 8
axis one is then splitting in a
different way one two five six three
four seven eight
when you're actually using real data you
start to really get a feel for what this
means and what this does
so if we're going to do that let's go
ahead and look at splitting the array
and do that in a markdown and run it
there we go so you have a nice little
title there
and we'll go ahead and create an array
of nine let's do NP split
we'll do a and we're going to split it
by three
let's just see what that looks like so
if we split it we get an array 0 1 2 3 4
5 we get three separate arrays on here
now remember we're looking at let me
just print a up here
so we're looking at zero one two three
four five six seven eight and then we
can split it into three separate arrays
and let's take this we're going to do
this right down here let's move the a
split down here instead of the three
let's do four comma five and put that in
Brackets
and so we do it this way we have zero
one two three four five six seven eight
and that's kind of interesting I wasn't
sure what to expect on that but we get
when you split an a by four comma five
you get a totally different setup on
here as far as the way it split the
array
and to understand how this works I'm
going to change the 5 to a 7. and this
will visually make this a little bit
more clear
so we had four and five I went zero one
two three four five six seven eight and
you see the markers four and five when
we do four and seven I get zero one two
three four five six seven eight and so
what you're looking at here is the first
markers this is going to go to four
so there's our first split at the four
the marker of four and then the second
split is going to be at position seven
and this is the same thing here a four
position five that's why we're splitting
it in those two sections we could also
do it seven let's just see what that
looks like run and you can see I now
have zero one two three four five six
seven eight
so we could split in all kinds of
different ways and create a different
set of multiple arrays on here and split
it all kinds of different ways and
before we get into the graphs and other
miscellaneous stuff
let's go ahead and look at resizing the
array I'm going to take the cell and
sales a markdown and run it
give us a nice title there and we'll do
an array an in Peru of one two three and
four five six here I'm just going to
just print
let's go print a DOT shape then we'll go
ahead and run that whoops hit a wrong
button there
hit the comma instead of the dot so we
have a shape of two comma three here
and this is important to note because we
start resizing it it's going to mess
with different aspects of the shape
and so we'll go ahead and do a print
scoop in for a blank line there we go
let's do b equals
NP dot resize
we're going to resize a
and let's resize it with three by two
and then we'll just go ahead and
print
B
and print the period shape not a comma
I'll run that
oops forgot the quotation marks around
the end we'll go ahead and run that and
let's just see what that looks like so
we have one two three four five six
original array with the shape of two
three
and then we want to go ahead and resize
it by three two and we end up with one
two three four five six and we end up
with the shape of three two that
shouldn't be too much of a surprise you
know we got six elements in there we can
resize it by two three was the original
one and then we're actually just
reshaping is how that kind of comes out
as when you resize it like that
but what happens if we do something a
little different
and let's go ahead and just take this
whole thing and copy it down here so we
can see what that looks like
and instead of doing three two remember
last time I did the um to reshape it I
messed with the numbers and it gave me
an error but when you resize it you
don't have to match the numbers they
don't have to be the same dimensions so
we can instead of going from a 2 3 to a
3 2 we can resize it to a three three so
let's take a look and see how it handles
that and we come down here to 3 3 we end
up with one two three four five six and
it repeats one two three
so it actually takes the data and just
adds a whole other Block in there based
on the
and repeating it
all right now at this point you know
we've been looking at tons of numbers
and moving stuff around we want to go
ahead and do is get a little visual here
because that um
certainly you can picture all the
different numbers on there but let's
look at the histogram let's put this
into a histogram let me go ahead and run
that
and to do that we're going to use the
matplot library so from matplot library
we're going to import pipelot as PLT
that's usually the notation you see for
pi plot so if you ever see PLT in a code
it's probably Pi plot in the matplot
library
and then the guys in the back did a nice
job and gals too guys and gals back
there our team over at simply learn put
together a nice array for me 2087 4 40
53 with a bunch of numbers that way we
had something to play with and what we
want to do is we want to do plot the
histogram I remember a histogram says
how many times different numbers come in
and then we're going to put them in bins
and we have been 0 to 20 to 40 to 60 to
80 to 100.
you might in here with the matplot
library they call them bins you might
hear the term buckets or they put them
in buckets as a really common term then
we want to give it a title so the way it
works is you do your plt.hist for
histogram your PLT title and your PLT
show and we're doing just a single array
in here in the numpy array of a and
let's go ahead and run this piece of
code
taking a moment to come out there says
finger size so it's generating the graph
and you can see we have and let's just
take a look at this let me go down a
size there we go okay so now we can see
we're taking a look at here so between 0
and 20 we have three values so we have a
20 here we have a four and a 11 and a
15. zero one two three it's actually
Four values but they start at zeros
remember we always count from zero up
and from 20 to 40 we got 20 this is 142.
2 3 4
5 6.
and so you can see in the histogram it
shows that the most common numbers
coming up is going to be between the 40
and 60 range least common between the 80
and 100 this looks like an age
demographics that's what this looks like
to me and you can see where they would
have put it in the buckets of different
age groups which would be a nice way of
looking at this histograms are so
important and so powerful when you're
doing demos and explaining your data so
being able to quickly put a histogram up
that shows what's common and how it's
trending is really important and using
that with a numpy is really easy and you
know what let's take the same data and I
want to show you why we do bins or why
we have buckets of data I'm used to
calling it buckets why we have bins
let's do it instead of by 20 let's do it
by tens and see what happens
and what happens when you do it by tens
is you miss out on the you can see a
nice curve here on the first one and on
the second one it looks like a ladder
going up and a plummet a letter going up
and a plummet and a ladder going down
so the first would be more indicative of
an age group and the second one would be
what she would get if you divide it
incorrectly you wouldn't see the natural
trend
I don't know what this would be maybe
how much food they eat hopefully not
because I'm in 50 so I'm right in the
middle there that which means I get a
ton of food compared to everybody else
but it's some kind of democrat maybe
it's mental maybe it's knowledge because
we we hit a certain point and we start
losing our marbles start leaking out or
something so you start off knowing
something and then as you get older you
grow more but you can see here we lose
that you lose that continuity in the
thing if you split the histogram into
too many bins or too many buckets
and if you actually plotted this by the
individual numbers it would just be a
bunch of dots on
that wouldn't mean a whole lot
and we've looked at graphs there are
turns out there are a ton of useful
functions in numpy
I'm sure there's even new ones that are
aren't going to be in here but let's
just cover some important ones you
really need to know about if you're
using the numpy framework
one of them is Lane space function this
is generating data so you have a line
space we have one three ten and when we
do that we end up with 10 numbers so if
you count them there's 10 numbers
they're between 1 and 3 and they're
evenly spaced we get 1 1.222 but these
are all there's a total of 10 here and
it's right between the one and three
range
that could be there's a lot of uses for
that but they're probably more obscure
than a lot of the other common numpy
arrays set up
but a real common one is to do summation
so we'll do summation where you do in
this case we create a numpy array of one
of
two different arrays One Two Three or
two different dimensions one two three
three four five and we're going to sum
them up under axes zero which is your
columns and if you remember correctly
columns is the one plus three two plus
four three plus five so we have three
columns and if we change this we'll just
flip this to one
we get two numbers so we get one two
three all added together which equals
six and three plus four plus five which
equals twelve
back to zero
there we go since it said it was just
looking at zero
and these probably could have been some
of these could be able to start math
section square root and standard
deviation two very important tools we
use throughout the machine learning
process and data science and simply we
take the NP array we have again the one
two three four five six three four five
I don't know I need to keep recreating
it I probably could have just kept it
but we can take the square root of a so
it goes through and it takes a square
root of all the different terms in a and
we can also take the standard deviation
how much they deviate in value on there
and there's a ravel function we can run
that
and NP array is X we're going to do x
equals a we change it from a to x x
equals Ravel and this sets it up as
columns so we have one two three four
five this is all columns on here very
similar to the flattened function
so they kind of look almost identical
but we also have the option of doing a
ravel by column and then another one is
log so you can do mathematical log on
your array in this case we have one two
three and we'll find the log base 10 for
each of those three numbers there's a
couple of them they don't you can't just
do any number here after log but there
is also log base 2. log base 10 is
pretty commonly used on here I'm going
to run that there we go
before we go let's have a little fun
let's do a little practice session here
on some more challenging questions so
you start to think how this stuff fits
together right now we just looked at all
the basics and all the basic tools you
have so let's do some numpy practice
examples and let's start by figuring out
how do you plot say a sine wave in numpy
what would that look like and so in this
project we wouldn't have to do this
because I've already run these but we'd
want to go ahead and import our numpy as
NP and import our matplot library
pipelot as PLT so we get our tools going
here and then we'll break it into two
sections because we need our x Y
coordinates in here so first off let's
create our x coordinates and our x
coordinates we're going to set to an a
range
and we want this error a range since
we're doing sine and cosine it's going
to be between 0 and 0.1
and then we use our NP and we actually
can look up numpy stores Pi see how the
option of just pulling Pi in there
directly from numpy it has a few other
variables that it stores in there that
you can pull from there but we have
numpy pi and we generate a nice range
here and let's go ahead and run this and
just out of curiosity let's see what x
looks like I always like to do that so
we have
0.1.2.3.4 so we're going 0 to in this
case 9.4 3 times numpy pi remember Pi is
like three point something something
something that makes sense it should be
about nine and we're doing intervals of
0.1 so we create a nice range of data
and then we need to create our y
variable and so Y is going to Simply
equal NP or numpy dot sine of X and then
once we have our X and Y and if we print
let's go and just print y you can see
all that we'll do this let's do this so
it looks print
X print y
so we basically have two arrays of data
so we have like our X axes and our y
axes going on there
and this is simply a
plt.plot because we're going to plot the
points and we'll do X comma Y and then
we want to actually see the graph so
we'll do plot dot show and we'll go
ahead and run that and you see we get a
nice sine wave and here's our number 0
through 9 and here's our sine value
which oscillates between -1 and 1 like
we'd expect it to then for the next
challenge let's create a six by six two
dimensional array and let one and zero
be placed alternatively across the
diagonals
oh that's a little confusing so let's
think about that we're going to create a
six by six two dimensional so the shape
is six by six two dimensional array and
let one and zero be placed alternatively
across the diagonals
now if you remember from lesson one we
can fill a whole numpy array with zeros
or ones or whatever so we're going to do
NP create a note B zeros and we're going
to do a six by six and we'll go ahead
and make sure it knows it's an integer
even though it's usually the default and
just real quick let's take a look and
see what that looks like so if I run
this you can see I get six by six grid
so six by six zero zero zero zero zero
now if I understand this correctly when
they say ones and zero placed
alternatively across the diagonals they
want the center diagonal maybe that's
going to stay zero all the way down
and then the next diagonal will be ones
all the way across diagonally and then
the next one zeros the next one ones the
next one zeros and so on I hope you can
see my mouse lit up there and
highlighting it so let's take a little
piece of code here
and we'll do Z One colon colon two comma
colon colon 2 equals one and well that's
a mouthful right there so let's go ahead
and run this and see what that's doing
and so what we're doing is we're saying
hey let's look at in this case Row one
there's one and then we're going to go
every other row two so we're going to
skip a row so skip here skip here skip
here so we're going down
this way and we're going every other row
going this way it's hard to highlight
columns so you can see right here where
the that we're not touching each row is
like this row right here is not being
touched okay so we're going to start
with Row one and then we're going to
skip a row and another one and so we're
going every two rows and then in every
two rows we're looking at every two
starting with the beginning that's what
this thing blank means so we're going to
start with the beginning and we're going
to look at all of them but we're going
to skip every two so starting with Row
one
we look at all the rows but we do we do
it by two steps so we go one Skip One
you know one Skip One One skip one one
if you lift this out and do every one
this is just be ones in fact let's see
what this looks like if I go like this
and run it you can see that I guess get
ones
so this notation allows us to go down
each row row by row and we're going to
do every other row set up on there and
so if we're going to start with Row one
we also
control Z
try that there we go we'll start with
row zero again we're going to go each
row step two so we'll start with row
zero and we'll go every other row and
this time we'll start with one column
one and again we go every other one
going down
step that's what that step two is
skipping every other one we're going to
set that equal to one so let's see what
that looks like and you can see here we
get our answer we get zero one zero zero
but it has the ones going in diagonals
on every other diagonal and zero on
every other one
a little bit of a brain teaser that one
trying to get that one to work out so
you can see how you can arrange your
rows and here's your step in your
different axis on there
and then the next one is find the total
number and locations of missing values
in the array the first challenge is to
create some missing numbers so let's
create our array Z we're going to do a
numpy DOT random dot Rand 10 comma ten
and before we do the second part let me
just take the second part out
and let's just see what that looks like
so let's run that and there we go so we
have a 10 by 10 random array it randomly
is picking out numbers
and next we want to go ahead and take
our random integer size equals 5. and
then we're going to do a random random
10 size equals 5. so in the Z we're
going to select a number of random
spaces here and set them equal to null
value
now let's go ahead and run that so you
can see what that looks like and if we
look at the array
we've created one two
three four this should be a fifth one in
here my eyes may be filling me so we've
created a series of oh because zero zero
to five zero one two three four so we
got five there are different null values
in here and
this is kind of a neat notation to
notice that we can generate
random integers size equals five so this
generates five by five miniature grid
inside of this to tell it where to put
the nands at so that's kind of a cool
little thing you can do
and we want to look up and see how many
null values are in there
and this is simply just NP is net of Z
simple so if it is nand then we want to
sum it up so we're going to sum up all
of the different null values on there
now let's do one one more feature in
here which is really cool
let's go ahead and print the indexes so
NP argue where NP is
Nan of Z so we're going to create our
own another NP array and let's run this
and we'll see here that comes up with
the four indexes so we did Count four of
them up there
it tells you where they are one nine two
zero four six five four
and then let's go ahead and run this
again run run there we go this time I
got five let's go for random numbers
another fun one that I always like to do
it's very similar is we have NP is nand
z dot sum so we're summing the number of
nands and we can get the indexes and you
can reshape the indexes but you can also
just do we'll do an inds where NP is Nan
of Z
and let's just print let's print that
print inds let's see what that looks
like
and it's very similar we have we have
zero one three zero six three eight six
nine three if I have split it into two
different arrays
so we have our X and our y kind of
coordinates going there and what I can
now do is I can now do Z inds
equals and at this point you can also
instead of getting the sum you can get
the means or the well the numbers and
that kind of thing you have or the
average as it is so be one thing you
could do and you can pick up the average
that's very common in data science to
get the average and just use that for a
value
but we'll go ahead and just set it to
zero and then let's go ahead and print
our Z and run that and you can see we
come down here we have wherever there
was a null value it is now zero and you
could set this whatever you want this is
another way to replace data or help
clean data depending on what it is
you're doing
so wow we covered a lot of stuff so
quick rehash going over everything we
went into there we looked at array
manipulation changing the shape
how to switch that around we even had
the flatten down there which remember we
have another command lower that's
similar we could change the order by F
remember F stands for Fortran very
strange connotation but there's C and F
C is the standard and F switches it to a
different order
to be honest I usually have to look it
up because I almost never use f but when
you need it you're like oh my gosh it
was the other order just do a quick
Google so we talked about reshape making
sure that the dimensions are the same
you don't want to have like something
that has 12 objects in it and reshape it
to C 11 and 5 because it doesn't work it
doesn't divide into 12. we can transpose
so we can switch them so we can go from
a four by three to a three by four Oops
I did that the other way around
three by four to four by three
we covered reshaping the array we did
the roll the axes you can do some weird
things with swapping and rolling axes
and transposing the numbers
we dug a little bit into the arithmetic
so we talked about adding we talked
about subtracting multiplying dividing
and you know at this point it's so
important we just look up the numpy
mathematics and you can see here they
have just about everything your
trigonometry
uh your hyperbolic functions rounding
sums products differences there are so
many all you different miscellaneous
mathematical connotations so you know
Google it go to the main numpy page and
look at the different setups you can do
on there so we covered that and we did
slicing how to break it apart we did
iterating over the array we covered
joining arrays and how to concatenate
remember concatenate just means add-on
to it so in this case how are you adding
B onto a is how you'd read that from
Linux you should catch the concatenate
because that's used regularly there
splitting the array we talked about how
to split the array in different ways so
you can split it in Array of arrays all
kinds of different ways to split the
array up how to resize it and remember
resize does not have to have the same
shape but if you resize it it will take
the data and begin at the beginning and
add new rows on if the size is bigger if
it's smaller it truncates it it just
cuts the end off
we looked at how to do a histogram and
how to plot that
we mentioned buying buckets or bins as
they call them in pi plot and then we
covered a lot of other useful functions
in numpy talked about the line space
setup for doing
um numbers in a series how to sum the
axes up again that's part of the
mathematical formulas there that we
looked at there's a sum there's also
means and median all of those you can
compute in numpy and you can also do the
square root and standard deviation
the Ravel function very similar to the
flat
to be honest I almost always just use
the flat but you know the Revel has its
own kind of functionality that it does
and then we went into some numpy
practice examples we challenged you to
create a sine wave in numpy and how to
do that we're kind of looking for that a
range remember how we do the a range and
you can
have your beginning value your in value
which they did is three times pi number
pi and we're going to do intervals of
0.1
and then y just equals the numpy sine of
x there's our math from the math page we
were just looking at remember that's
right at the top
and finally we went down here we had
this kind of a little brain teaser how
to do diagonal zeros and Ones playing
with the different connotations of Z of
the numpy array
and then we did a random size and we
played a little bit with how to with the
null values playing with null values
if you're doing any data science you
know null values are like a headache
what do you do with them big sets of
data you get rid of them small sets of
data you have to factor something in
there like figure out the average or the
median there and then replace it with
that join us on a journey into the
fascinating world of AI and machine
Learning Without Caltech postgraduate
program in Ai and machine learning in
partnership with IBM this a and ml
course because the latest tools and
Technologies from the AI ecosystem and
features master classes by Caltech
faculty IBM experts hackathons and ask
me anything sessions this program
showcases Caltech ctm is excellence and
IBM's industry progress the artificial
intelligence course because key Concepts
like statistics data science with python
machine learning deep learning NLP
reinforcement learning and through an
Interactive Learning model with live
sessions enroll now analog exciting Ai
and ml opportunities the course link is
mentioned in the description box
start with just some real General what
is pandas pandas is a tool for data
processing which helps in data analysis
and provides functions and methods to
efficiently manipulate large data sets
now this is a step down from say using
spark or Hadoop in Big Data so we're not
talking about Big Data here but we are
talking about pandas when there is some
connections there's like an interface
going on with that so there is
availability but you really should know
your pandas because if you're working in
Big Data you'll know there's data frames
well pandas is a data frame primarily it
has a couple different pieces we'll look
at here and if you've never worked with
data frames before a data frame is
basically like an Excel spreadsheet you
have rows and columns you can access
your data either by the row or the
column when you have an index and
different that kind of setup and we'll
dig more into that as we get deeper into
pandas but think of it as like a giant
Excel spreadsheet that's optimized to
run on larger data on your computer and
then I said it that it's a data frame so
the data structures in pandas are series
one-dimensional arrays and then we have
data frame two-dimensional array and it
really centers around the data frame the
series just happens to be part of that
data frame and here's a closer look at a
pandas series series is a
one-dimensional array with labels it can
contain any data type including integers
strings floats python objects and more
so it's very diverse if you remember
from numpy we studied they had to be all
uniform not in pandas and pandas we can
do a lot more and pen is actually kind
of sits on numpy so you really need to
know both of those if you haven't done
the numpy tutorials and you can see here
we have our index one two three four
five and then our data a b c d and e
very straightforward it's just two
columns and we have a nice index label
and a column label for the data and then
a data frame is a two-dimensional data
structure with labels we can use labels
to locate data and you can see here we
had if we go back one we had our index
one two three four 5 so in each one of
these series they would share the same
index over there the row index so you
have your row index DF dot index and
then you have a column index DF dot
columns and this is like I said this
would be really familiar if you've done
any work with spreadsheets Excel so it
kind of resembles that this does make it
a lot easier to manipulate data and add
columns delete columns move them around
same thing with the rows so you have a
lot of control over all of this now
we're of course going to do this in our
Jupiter notebook you can use any of your
python editors but I highly suggest if
you haven't installed Jupiter and
haven't worked with it it is probably
one of the best ways for easily
displaying a project you're working on I
skip between a lot of different user
interfaces or Ides for editing my Python
and it's just simply jupiter.org
j-u-p-y-t-e-r dot org and then I always
let mine sit on anaconda anaconda.com
and just real quick we'll open that up
for you oops offline mode don't show me
that again
but you can see here that I have
different tools that I can actually
install in my anaconda including the
Jupiter notebook which comes by default
and then I have access to the
environments and again that's
anaconda.com named after the very large
one of the largest world's largest
snakes and then Jupiter notebook in this
case jupiter.org and when we're in our
I'm going to go in here to our Jupiter
notebook and we're going to go ahead and
just do new and a Python 3 and this will
open up a Python 3 Untitled folder
so diving right in let's go ahead and
give this a title pandas tutorial and
we'll go up to cell and we'll change the
cell type to markdown so it doesn't
execute it as actual code one of those
wonderful tools when you have jupyter
notebooks you can do demos with this and
let's go ahead and import pandas
and usually people just call it PD that
has become such a standard in the
industry so we'll go ahead and run that
now we have our pandas has been imported
into our jupyter notebook
and then oh we can go ahead and let me
do the control plus it's just Internet
Explorer I can enlarge it very easily so
you have a nice pretty view oops too big
there we go and whenever you're working
with the new modules good to check your
version of the module in pandas you just
use the in this case PD dot underscore
underscore version underscore underscore
that's actually pretty common in most of
our python modules there's different
ways to look up the version but that's
one of the more common ones and we'll go
ahead and run that we get
0.23.4 and if we go to the pandas site
we see
0.23.4 is the latest release and of
course a reminder that if you're going
to an environment you need to install it
so you'll need a new pip install pandas
if you're using the PIP installer we'll
go and close out of that
and the first thing we want to do is
we're going to work with series a lot of
the stuff you do in series you can then
do on the whole data set we need to do
what create one we need to manipulate it
take pieces of it so query it query it
delete so you can delete different parts
of it so you want to do all those things
with the series and we'll start with the
series and then almost all the code in
fact all the code does transfer right
into
the actual data table so we go from a
series of a single list of one column
and then we'll take that and we'll
transfer that over to the whole table
and we'll start by creating let's put up
there we go creating a series from
list
and let's just call this ARR equals and
we'll do 0 1 2 3 4. if you remember from
our last one we could easily do R equals
range of 5 which would be zero to four
what we'll do R equals zero to four and
we'll call this S1 and we'll go PD and
series is capitalized this one always
throws me is which letters do you
capitalize on these modules they're
getting more and more uniform but you
got to watch that with python and we're
just going to go ahead and do ARR so
we're just going to take this python
list and we're going to turn it into a
series and then because we're in Jupiter
we don't have to put the print statement
we can just put S1 and it'll print out
this series for us and let's go ahead
and run that and take a look
and you'll see we have two rows of
numbers so the first one is the index
now it automatically creates the index
starting with zero unless you tell it to
do differently so we get 0 index row 0
is 0 1 1 2 2 3 3 4 4. and because it's a
series it doesn't need a title for the
column there's only one column so why
title it
and this also lets you know that it's a
data type of integer 64. so we print
this out this is our series our basic
series we've just created now let's do a
second series
PD and we'll use the same
data list and let's go ahead and do
order we'll give it an order equals oh
let's do it this way let's go index
equals order
it helps if we actually give it an order
so we'll do order equals and let's do
one two three four five so instead of
starting with zero we're going to give
it an order starting with one we're
going to run that and we'll go ahead and
print it out down here S2
and we'll see that we now have an index
of one two three four five and that
represents 0 1 2 3 4 in the series and
we're still data type integer 64. and
very common is you're missing with numpy
arrays is we can import or numpy as NP
remember that from our numpy tutorials
we can go ahead and create a numpy at a
random with the random numbers of five
and let's just see what that end looks
like so we can see what our number looks
like so we have some nice random float
values here 2.33 so on and that's from
our last tutorial the numpy tutorial one
and two and instead of calling it order
let's call it index and we're going to
set our index equal to a b c d and e I
want to show you that the index doesn't
have to be an integer so it can be
something very different here and then
let's go ahead and create our we'll just
use S2 again and here's our NP for numpy
series capital s and n is our and P for
numpy
PD for pandas there we go switching my
anachronisms so we have pd.series of N
and we want to do our index equals our
index we just created
and then let's go ahead and see what
that looks like S2 is a primit and let's
run that and we can see here we have a
nice Series going on a b c d and e for
our indexes so instead of it being 0 1 2
3 or 4 we can make this index whatever
we want and you can see the numbers here
going down that we randomly generated
from the numpy array so we use numpy to
create our Panda Series right here
and so continuing on with creating our
Series this one I use so often we create
a series from a dictionary so we have
our dictionary in this case we went
ahead and did a of one B is 2 C of three
D4 EF5 so each one of those is a key and
then a value and then we're going to use
oh let's use S3 equals PD for pandas
series and then we want to go ahead and
just do D in here
print out S3 here and let's go ahead and
run this and you can see we got a is one
B is 2 C is three D is four e is 5. and
it's still of integer 64 Because the
actual data is one two three four five
and it's all integers 64 type 64. and
the last thing we want to do in the
creating section of our series is to go
ahead and modify the index because we're
going to start modifying all this data
so let's start with modifying the index
of the series and if you remember let's
do a print this time S1
I'll go ahead and run this and the
reason I did print is because it only
prints out the last variable so if I put
S1 up here and we're going to do another
variable back down lower it won't print
the first one just the last one and
we're going to go ahead and take S1
the index and we're just going to set it
equal to a new index and obviously the
number of objects in our index has to
equal the number of objects in our data
and then because it's the last variable
we can go ahead and just do an S1 and
let's run that and you can see how we
went from 0 to 0 0 1 2 3 4 as our index
we've now altered it to a b c d and e so
this would be much more readable or
might be representational of a larger
database you're working with
So Cool Tools we've covered creating
database based on our basic array python
array we've showed you how to reset the
index
that we showed you how to use a numpy
array so you can put a numpy array in
there it's all the same you know
pd.series numpy array and then we can
set the index on there and the same
thing with the dictionary so it's very
versatile how it pulls in data and you
can pull in data from different sources
and different setups and create a new
series very easily in the pandas and
then we looked on changing your index so
now we have a new index on here
and then we want to go ahead and do some
selection let's do some basic
slicing most common thing you'll
probably do on here and we'll just do S1
this notation should start to look
really familiar again this is going to
put an output so I'd usually it doesn't
change S1 this just selects it so we
might do a equals S1 and then print a
and you'll see that it just looks at the
first three zero one two and we can do
the same thing by not having the a in
there I'll go ahead and take that out
but it's just a reminder that it's not
actually changing S1 it's just viewing
S1 so simple slicing on here and we can
likewise do an append oops before we do
a pin let's just do a quick kind of fun
one we'll do two minus one and you'll
see it covers everything but the E of
course you can do minus two on this side
so one another way to select it is to go
how far from the end and likewise we can
do a 2 here c d e to the end so it
starts at the second one and another way
we can do this is we can do a minus 2
over here and that looks at just the
last two in the slice so you can see how
easy it is to slice the data and of
course there's no reason to do this but
you could select all of them if you
wanted to view all of them on there oops
32 there's not 32 so it's just going to
show the first three there we go and
then we can also append so I can take
and oh let's create another series and
append one to it and if you remember we
had S3 there's our S3 and we have our S1
I'm going to do S1
and let's go ahead and do let's call it
S4
equals S1
a pin S3
so we're just going to combine those two
into S4
and if we go ahead and print S4 on here
you'll now see that we have a b c d e a
b c d e zero one two three four one two
three four five because we started the
data at one
so very easy to compend one series to
the next and if we're going to append
one series to the next we need to go
ahead and drop or delete one and drop is
a keyword for that and let's just do e
or index e and so if I run this
you'll see that it'll print it out and
ABCD there's no e and remember all these
changes if I type in S4 again you'll see
that S4 still has e in it so this change
does not affect the series unless you
tell it to so I'd have to do like x S4
equals S4 dot drop e and there's another
way to do that which we'll show you
later on let me just cut this one out
there we go all right so we've covered
all kinds of Cool Tools here we have
appending we have slicing we did all the
creating stuff earlier as you can see
here on the setup how easy it is to
manipulate the series
so next what we want to get into is we
want to get into
operations that happen on the series let
me go ahead and change this cell to
mark down there we go and
run so series operations what can we do
with the series and let's start by
creating a couple arrays we'll call it
array one and we'll do 0 through 7 and
array two six through six seven eight
nine five I don't know let me through
the five on the end but let's go ahead
and run those so those load up into
Jupiter and we'll do this a little
backwards we're going to do S5 equals a
panda series of array two so I'm doing
this in reverse and then when we do S5
you'll see that we have zero to four it
automatically assign the index
67895 for our series and let's go ahead
and do the same and we'll call this S6
and we'll set this equal to PD series
for our first array and if we do an S6
down here to print it out
we'll see something similar I got zero
through six zero one two three four five
seven for the data so those are two
series we just created series six five
and six
and one of the first things we can do is
we can add one series to the next so I
can do S5 dot add S6 and let's see what
that generates and just a quick thing if
you've never used pandas what do you
think is going to happen with the fact
that this only has five different values
in it and this one has seven values
so let's see what that does and we end
up with 6 8 10 12 9 and it goes oh I
can't add this there's nothing there so
it gives us a null return very different
than the numpy that would have given you
an error this instead tells you there's
no value here because we couldn't
generate one so we can easily add S5 dot
add S6 and likewise we can do S5 Dot
sub for subtract S6 and we'll run that
and on the add the subtract and you
guessed it we're going to do multiply
and divide next again you can see
there's the null values where it can't
subtract the two because there's no
values there to subtract we can also do
S5 multiply mul they're all three
letters on these that's one of the ways
to remember how they figured out the
code for this so remember these are all
three letters mole we'll go ahead and
run this and you can again you can see
how they're multiplied together and then
we can also do the S5 div three letters
again S6 and run that
and you'll see here this goes to
Infinity because we have 0 in the wrong
position so it actually gives you a
whole different answer here that's
important to notice and then in the null
values because there's no data and it
can't actually produce an answer off of
null off of missing data and since we're
in data science let's do S6
median so let's look at the median data
which is simply median sorry for those
who are following the three letters
because median is not three letters and
you can see an S6 is 3.0 and let's do a
print here and we'll do median
or average S6
and let's print Max
S6 and just like median there's max
value and if we're going to have a max
value we should also have a minimum
value so let's pop in minimum
we'll go ahead and run this and you're
starting to see something that would be
generated like say an R where you're
starting to get your different
statistics we have a median value of
three max value of 7 and a minimum value
of zero and what it does when it hits
these null values if there is no values
in there because we could still do that
we could actually you know what let's go
up here and do
let's pick this one where we multiplied
let's go s seven equals
I'm going to print the S7 just so I keep
it nice and uniform so I still have my
S7 down there and run it and then I want
to take the S7
because S7 now has null values and an
Infinity value and let's see what
happens
this is going to be interesting because
I want to see what it is with infinity
and we end up with a median of six
maximum of 27 and minimum of zero which
is correct it drops those values so when
it gets to there and it doesn't know
what to do with them it just drops those
values and then it computes it on the
remaining data on there so that's
important to know when you're making
these computations you're looking at Min
and Max and median you're not going to
know that there's no values unless you
double check your data for the null
values that's a very important thing to
note on there so just a real quick
review on there we've done our created
our PD series and we've gone ahead and
done addition subtraction multiplication
division all those are three letters so
sub Min div add and then we looked at
median maximum and minimum so we're
going to go ahead and jump into the next
big topic which is to create a data
frame so now we're going to go from
series and we're going to create a
number of series and bundle them
together to make a data frame
there we go cell type markdown and run
that so we have a nice title on there
it's always good to have a good title
all right so our first data frame we'll
jump in with some stuff that looks a
little complicated we'll break it down
first I'm going to create some dates and
you know what let's just go ahead and do
this I want you to see what that looks
like what I'm creating here I've created
a series of dates PD date range and
we're going to use these for the index
okay so when you look at this you'll see
that it's just basically it comes out
kind of like a basic python list or
numpy array however you want to look at
it with our different dates going down
and we've generated six of them and it's
going to have whatever time it is right
now on your on the thing for the date
for the time that's that time stamp
right there and then you'll see we have
11 19 2008 11 20 11 19 and looking into
the future there so that's all this is
is generating a series of dates that
we're going to use as our index and this
is a pandas command so we have a date
range which is nice it's one of the
tools hidden in pandas that you can use
and next we're going to use numpy to go
ahead and generate some random numbers
in this case we'll do the
np.random.random and six comma four you
can look at this as rows and columns as
we move it into the pandas and of course
you could reshape this if you had those
backwards on your data but we want the
six to match the rows and we have six
periods so our indexes should match
along with the rows on there and then
you know before we do the next one let's
go ahead and just print out our numpy
arrays and see what that looks like here
we have it one two three four by one two
three four five six four by six so it's
a nice little setup on there and since
working with data frames can be very
visual let's give our columns we have
four columns and we're going to give
them names A B C and D so now we have
columns on there also and then let's put
this all together in a data frame and we
can actually you know what let's do this
since I did it with everything else
let's go ahead and do columns and you
can see there's our columns on there
and we'll go ahead and do df1 equals
pandas dot data frame and note that the
D and the f are capitalized series it
was just the S and I always highlight
this because you don't know how many
times these things get retyped when you
forget what's capitalized on there it's
a minor thing you'll pick it up right
away if you do a lot of it and the first
thing we want to do is we want to go
ahead and take our numpy array because
we're going to create our data frame off
of is the numpy array and then we want
our index equal to our dates so there's
our index in there and then we also have
columns equals columns and then finally
let's see what that looks like now
remember we had all the different data
that just looked like a jumble of data
we have our column names and everything
else our numpy array kind of just a
jumble array over there four by six you
could sort of read it but look how nice
this looks I mean this is you come into
a board meeting you're working with your
shareholders
this is pretty readable this is you know
this is our date this is our a b c d
whatever it is maybe it's one of these
dates as your leads
closures lost leads total dollar made
you know whatever it is if it's in a
business maybe it's measurements on some
scientific equipment whether searching
material you know where this is like
high of the temperature low of the day
humidity of the day whatever it is so
you can see that we can really create a
nice clear chart and it looks just like
a spreadsheet you know we have our rows
and we have our columns and we have our
data in there now this one I use all the
time if we're going to create we can
create it like you saw here with our
numpy array very easy to do that and
reshape it you can also create it with a
dictionary array so here we have some
data and let me just go down a notch so
you can see all the data on there we
have an animal in this case cat cat
snake dog dog cat snake cat dog we have
the age so we have an array of Ages we
have the number of visits and the
priority was it a high priority yes no
and then we're going to take that we're
going to create some labels we have a b
c d e f g h i and what I want you to
notice on this is we have a title animal
and then we have basically a python list
and these lists they don't necessarily
have to be equal because we can have
non-data you know np.net numpy array
null value but we want to go ahead and
create labels that are equal to the
number in the list so a the first cat B
the second cat C the snake D the dog and
so on so we'll go ahead and create our
labels which we're going to use as an
index and we'll call this DF let's do it
this way we'll call this df2
equals PD for pandas data frame
and then we have our data just like we
did before and then we have our index
equals
labels
and if we're going to go from there
let's go ahead and print it out so we
can see what that looks like df2 so
let's go ahead and run that another
again you have a nice very clean chart
to look at we've gone from this mess of
data here to what looks like a very
organized spreadsheet very Visual and
easy to read animal age visits priority
and then a through J cats and all the
different animals so on and so on and
then when you do programming a lot of
times it's important to know what the
data types are so we can simply do df2
D types
and if we run that we can see that our
animal
is an object because it's just a string
but it comes in as an object age is a
float 64 integer 64 and then priority
again is just an object
and exploring this this one's very
popular let's go df2 head and if we
print that out the df2 head Returns the
first five and we can change this you
don't have to do five you might want to
just look at the top two maybe you want
to look at let's see now let's do six so
maybe we'll look at just the top six in
the database in your data frame and you
can actually this creates another data
frame so I could have a df3 equal to df2
and this now takes the df2 and just the
first six values so if we do df3
run get the same answer
and if we do it the head of the data we
can also do the tell it's the same thing
DFT you can look at the last we'll just
do the tell which by default does five
the last five and of course you can just
look at the last three of those real
quick just to see what's at the end of
the data and this is the tell I love
doing the tell of one because I'll have
like the index or something like that
and it will just show me the last
whatever the last entry was looking at
stock values and I might want to look at
just the last five days of the stock
values I can do that with the data frame
tail
and some other key things to look up are
the index so we can do df2 dot index
and I want you to notice that this isn't
a call function so if I put the brackets
on the end it'll give me an error
because index is not callable it's just
an object in there so we do df2 dot
index there's also columns
so we can go ahead and let's do uh let's
print this remember the first one is not
going to show unless I print it and then
df2 columns so now we can see we have
our indexes and we have our columns
listed here df2 dot columns animal age
visits priority that tells you what kind
of object it is or what kind of data
type it is and they're both object and
then finally df2 dot values and again
there's no brackets on the end of
df2.values because this is an actual
object it's not a callable function so
we'll go ahead and run that and it
creates this displays a nice array a
very easy way to convert this back to a
numpy array basically so before I go
into the next section let's just take a
quick look at what we covered so far
with the data frame we came up here we
created our data frame we did it from a
numpy array first setting the columns
and the index the index is setting it up
is the same as when we set up the series
so that should look very familiar so is
the whole format the numpy array the
index dates and the columns columns and
remember in our numpy array we're
looking at row comma column so six rows
four columns is how that reads in the
data frame
and we went ahead and also did that from
a dictionary in this case animal was the
column name with all the date data
underneath that column and then age with
that data visits that data priority that
data and then of course we added our
labels in there for our index so there's
no difference in there but it
automatically pulled the column names
important to know when you're dealing
with the data frame and importing a data
frame this way
and then we did looking up D type we
looked at head and tail looking at your
data really quick we also did index and
columns and values and note these don't
have the brackets on the end
so the next thing we want to do is go
ahead since we're dealing with data
science is we want to go ahead and
describe the data so we have
df2.describe to do that and we're going
to manipulate it in just a minute but
let's just see what this generates
and you can see right here we have age
and visits so looking at our data from
up above let me just go all the way up
here animal age business priority
and it does a nice job generating your
age versus visits which has all the data
you have your account your means your
standard deviation your minimum value 25
or in this group 50 75 and your maximum
value so this should look familiar as a
data science setup with your describe
for a quick look at your data Frame data
so let's start manipulating this data
frame and moving stuff around and we'll
start with transposing and it is simply
capital T for transpose and when we run
that it flips The Columns and the
indexes so now the indexes are all
column names and the columns are all
indexes animal age visits priority so if
we had come in here with our data shaped
wrong up above where we had a four by
six we can quickly just swap it if we
had it backwards not a big deal and we
can also sort our data so something that
you can't deal which is more difficult
to do with a lot of other packages and
the data frame is really easy to do take
our data frame df2 and we're going to
sort underscore values buy equals age
and so when we run this you'll see the
default is ascending so we have 0.5 to
2.53 and everything else is organized so
if you look at your indexes they've been
moved around because each index it moves
a whole row not just the one piece of
data is not being sorted so a very quick
way to sort by age are different data in
the data frame and in addition to
sorting it we can also slice the data
frame so I could do df2 and this should
look familiar from earlier we'll just do
one to three so we're going to pull out
oops it does help if I use a DF instead
of just D and we're going to pull up
just between one and three so we have
not zero which is a we have B which is 2
or B which is one and C which is two so
one two and then it does not include 3
which is the standard in Python and we
can even do something like this we can
combine them which is always fun because
remember this returns a data frame so if
I take df2 dot sort foreign values and
we'll do by equals age this is just kind
of fun and then I'm going to slice it
there we go double check my typing and
run it and now you should see fa because
F A are now one and two on there
I'm seeing very quickly create a whole
string on here which narrows it you know
that you can sort it then slice it and
do all kinds of fun things with your
data frame we'll just go back to the
original one run there we go and if we
can slice it by row we can also query
the data frame so we can do df2 and this
is a little different because I'm going
to create an array within an array and
in this case we're going to look at oh
let's do age comma
visits so look at the different format
in here we have one to three so we've
done this by slicing by an integer value
and then on here I've done df2 H comma
visits in an array and when I run this
you can see that we get just these two
columns on here we get age and visits so
it's a quick way to select just two
columns or select number of columns
you're working with
and if you stop there we did the slicing
almost identical to slice is I location
which uses the integer location one
comma three there's a push in pandas to
move to this particular setup instead of
doing just a regular slice and that's
because this can be confusing when we
slice one to three and then we select
agent visits so there is a push to go
ahead and move to an eye location which
does the same thing you can see here BC
it's the same as up above there's also a
copy command so we can do df3 equals df2
copy we're just going to create a
straight copy of it
and if we do df3
that'll be the same as the df2 on there
so df3 equals df2. copy and then let's
do df3 dot is null so we're looking for
null values and this will return a nice
map and you'll see that everything is
false except when you go up here under
the cat or H they had a null there and
so if we go to have a couple up here
also underneath of let's see the dog
okay there's a bunch of nulls in here
there's D up here so let's look at D
down here and you'll see false true
there it is there's our null value so we
can create a quick chart of null values
you can use this to do other things we
can leverage that null value to maybe
take an average or something and fill
those null spaces with data and we can
also modify the location so here's our
df3 location
and notice this is location not I
location I location has I for integer
location uses the in this case the
variables on the left and what we can do
on here and we'll go and just set this
equal to 1 5 and
some I'll pick a spot let's go back up
here where we had let's do F A just
let's see what were they looking at oh
here we go let's do F and age and up
here f is set to age of 2.0 and we find
out that that's incorrect data so we go
ahead and switch to df3 equal and then
we're going to print out our df3
and if we go to F and H it is now 1.5 so
we're just changing the value in the D
of three and this is changing the actual
data frame remember a lot of our stuff
we do a slice and like it returns
another data frame this changes the
actual data frame and that value in the
data frame
so we've covered uh location and I
location is null making a copy here's
our I location which is equivalent of a
slice and also selecting columns so now
we want to dive just take a little
detour here and let's look at df3 means
and this is kind of nice because you can
do this you can either do this by as you
can select a single column here by the
way you can just add the column
selection right here like we did before
so we could have age
look up the mean that just creates a
series if I run that there's our age
but if I take that out instead of
selecting it we can do the whole setup
and it has age and visits so why doesn't
it have priority or animal well those
are not integers so it's really hard
they're non-numerical values so what is
the average I guess you could do a
histogram which probably will look at
that later on but the only two things we
can really look at is age and visits and
we have the average or the mean on the
age is 3.375 and the mean on visits is
1.9 and let's do df3
visits we'll go ahead and steal the
visits again
and remember all those different
functions we looked at for a series well
we can do those here we can do the sum
so if we run that we'll see that these
sum up to 19. you can also look up
minimum if you remember that from before
the minimum is 1 Max so all that
functionality is here I'll just go back
to summing it up and adding it all
together so real quick we've shown you
how to take the series operations and
put them into the data frame and then we
can actually this is interesting one we
can just do df3 sum run and you'll see
the different summations on there it
just combines them I like the way it
just combines the strings on there for
priority and animal we've looked at is
null we've also looked at copying along
with the different slices which we
talked about earlier so let's talk about
strings let's dive into the string setup
on there and let's go ahead and create a
string series string equals PD series
and we just put it right in there we
have a c d a a b a c a popped in a null
value cow and Al I don't know why they
picked Cal and Al in the background
someone must like those animals and of
course we can just do string if we run
that you'll see leave the r out we'll
get an error but if we put it in there
you'll see that we have a simple series
0 a 1C 2D and it automatically indexes
it zero to eight and then we can go
string dot lower so when we're talking
about our data frame in this case or our
data series string in this case we use
the string function Str and we're going
to make it lower and we go ahead and put
the brackets on there and you'll see
that we've gone from capital a Capital C
so on to ABC and Baka CBA Cal Al they
were all lowercase already and of course
if you want to go lower you can also do
upper and we'll go ahead and run that
and you can see we now have ACD AAA
everything's capitalized except for the
null value which is still null all right
so we looked at a few basic string you
can see that string functions upper and
lower we're going to jump into a very
important topic I'm even going to give
it its own header on here because it's
such an important topic what do you do
with missing values Panda has some great
tools for that so we'll dive into those
and we'll call we'll work with df4 and
if you remember the DF copy from above
we're just going to make a copy of df3
and let's just take a quick look at the
data we're working with oops df3 forgot
the three on there there we go so here
we have our cats snakes and dogs
hopefully not all in the same container
because that would be
probably mean to all of them so we made
a copy we're going to be working with
df4 and the reason we made a copy is we
want to go ahead and fill the data and
we just simply do fill in a and then
we're going to give it the value we want
to put in there we'll give it the value
4. so I can run in here and you'll see
now that df4 now has where the N A was
is filled with the value of four same
thing down here a lot of times we'll
compute the mean first so I might do a
mean page equals df4 and then we want to
go ahead and do age
and dot mean
and then I'll do something like this df4
I only want to select the age and I want
to fill that
with the mean h and I run in there and
you'll see that our df4h now has the
means in there just a quick way of
showing you how you can combine these
and let me go back to our original one
there we go and run that and keeping
with good practices df5 equals df3 dot
copy
a little printer df5 which should be the
original one
and then on the df5 we can now drop our
missing data
I'm going to Simply drop in a and we're
going to use how equals any so I'm going
to drop any row that has missing data in
it and you'll see we had D here with
missing data and H and then let's go
ahead and see what df5 looks like when
we do that
there we go and there it is D is gone
and so is H so we create a new data
frame off of this missing those values
now if you have a lot of data dropping
values is a good way to take care of it
because you don't miss some data if you
have not a whole lot of data you're
working with like the iris data set or
something like that or something small
you want to start trying to find a way
to fill that data in so you don't lose
your computational power of the data you
got so just a quick look at processing
null values
or missing values you can fill them
usually with the means some people use
medium or the mode there's different
ways you can fill it one way is means
and we can also just drop those rows
those are the two main things we do with
missing data
here we go uh we're going to cover next
this is I so love data frames for this
file operations it saved me so much time
because they have so many different
tools for bringing data in and saving
data so we're looking at the data frame
file operations it's really streamlined
I don't know how many times I'll go on
to different data downloads and they'll
have Panda download standard on there
just because it's so widely used so
let's start with the most common file is
a CSV so we have df3 to CSV or animal
and let me just show you the folders
going into right now I have some
Untitled a few things in here but
nothing labeled animal so we go ahead
and run this and this is now save the
animal to my hard drive and you can now
see the animal folder up here and if I
let's do edit with a notepad oh let's
open up with just a regular notepad
there we go or wordpad if I open that up
you can see it's comma separate did our
titles they don't have an index on the
categories on the top in the index comma
then all the different data is separated
by commas
standard CSV file on there and if we're
going to send it to CSV and notice the
format is dot 2 underscore CSV and it's
just the name of the file we're sending
it to you can also put the complete path
by default it's going to go whatever the
active directory this program is running
on that's why those other folders are in
there so we have our df3 to CSV and then
if we're going to put it in there we
want to also get it back out and we'll
call this one DF underscore animal
equals PD read underscore CSV I always
have to remember is two underscore CSV
and read underscore CSV I always want to
do like a capital in there and not the
underscore we're going in here again
it's the active directory so if I now do
print out my DF animal
and let's just do the head we only want
to look at the first three lines so if I
go ahead and run this we'll see the
first three lines and they should match
up here what we saved to our CSV so very
easy to save and import from our CSV
files on here
and it turns out DF 3 also has a two
Excel they actually have a lot of
different formats but you know old
school Excel is real popular for so long
still is we can go ahead and save it as
animal dot xlsx we're going to call the
sheet named sheet1 and then I can also
do DF we'll call it animal 2.
two and this one's going to come from
and the same format on here there we go
so we still have our animal xlsx
the sheet 1 that's where it's coming
from index columns equals none so we're
not going to we're going to suppress the
indexing on the columns n a values and
it'll just assign that zero one up on
your indexes so if it says index columns
equals none that's what it does and then
we've added null values because null
values in here and we want to just make
sure that they're marked as n a and
we'll go ahead and just print out the
animal animal to there we go and let's
run that let's make this let's just do
the whole thing so we'll go ahead and
run that and it probably doesn't help
that I completely forgot the read so
animal 2 equals PD dot read
Excel there we go Excel so now we go
ahead and run it and what we expect is
happening here we have the same data
frame on here and if I flick back to my
folder you can now see that we have the
animal one of these is in Excel and one
of these is a CSV on here and so there's
our two file Types on there and they
have other formats these are just the
two most common ones used and I don't
know how many times I've had stuff from
Excel I need to pull out if you've ever
played with Excel it's a nightmare on
the back end because of the way they do
the indexing
so this just makes it quick and easy to
pull in an Excel spreadsheet so we
looked at two different ways to bring
data in and save it to files we've
looked at all kinds of different ways of
manipulating our data set and slicing it
and creating it for our data frame let's
get in there put your visualization
always a big thing at the end because
one it lets you check to see what you
did make sure it looks right and then
also if you're going to show somebody
else it makes it very clear what's going
on if they see something visual so this
is where a really important part of data
science is so let's go ahead and bring
in our tools we're going to do import
numpy as NP we want to make sure we have
our Amber sign matplot library in line
this just lets Jupiter know that we're
going to print it on this page if you're
using a different IDE you don't really
necessarily need that but this does help
it displays quickly in Jupiter notebook
and if you remember for earlier we could
create a we're going to call it TS we're
going to create a pandas which are cute
cuddly creatures versus a pandem short
for pandemonium no so we have TS equals
PD series and we're just going to create
a random setup of 50. we'll do an index
we'll set it equal to the pandas date
range today periods equals 50. so the
50s should match and I want you to
notice something here I did not import
the matplot library why because it's
already in there pandas already has its
built-in connection and interface with
matplot Library so you don't have to
import it and we'll go ahead and do TS
equals TS dot cumulative sum we're going
to do the cumulative sum
it's a little reformatting there and
we'll go ahead and plot it and let's
take a look at what that looks like so
we have a nice graph here we have the
dates on the bottom we've set this up so
we have a nice range between in this
case minus four to looks like about two
maybe or one minus four and one so what
we've done here we've plotted a basic
series just a single row of data and
we've set indexes on there but we can
also do the whole data frame on there
and let's see what that looks like so
first let's go ahead and create the data
frame we have here random number so
we're going to do 50 by 4 and then we'll
go ahead and create columns a b X and Y
just because we can index is a ts.index
on there so we're going to use the same
index as before just to keep it nice and
uniform we've already generated the
dates to go with it and then we can do
just like we did with the series we can
also do with the data frame
DF equals DF cumulative sum so we're
going to sum the whole data frame and
then we'll do simply DF plot and this
puts that in and let's go ahead and run
this and look how easy and quick that
was to generate a nice graph with all
the different data on there so we have
our shared index we have the shared
columns and then we have the different
data from each one that we can easily
look at and compare so very quick way of
displaying data you can imagine if you
were working in oh I think I mentioned
stock earlier because they've been doing
some analysis of stock lately so you'd
have your date down here and then you
would have stock a Stock B stock X Y
whatever it is and you can put them all
on one chart and see how they what they
look like next to each other and this
isn't too far off from what some of
those graphs looks like and this is just
randomly generated so stock has a lot of
Randomness in it which is one of the
reasons I actually play with it for
doing some of my models on for testing
them out now there are a lot of features
in pandas so we're going to show you one
more thing on here there's some of the
things like I didn't go too deep we
looked at the top two for importing data
from a CSV and from an Excel spreadsheet
showed you how to quickly plot the data
there's more settings in there you can
do we're going to do one more thing down
here and this is kind of a fun one
changes to a markdown and run that so
how would you remove repeated data using
pandas
and this is where you have a data set
that comes in and maybe it's feeding
from one location and instead of noting
that it's repeated the date like oh
let's go back to stocks that's a good
visual we have the stocks from the 23rd
and it adds another row and it's the
same row it's importing the 23rd again
and again so now you have that data
repeated three times and you need to go
back and figure out how to get rid of it
how do you track that down so let's
start by creating a quick database our
data frame not a database I keep saying
databases the data frame and we'll just
make this data frame has using our
dictionary going in this data frame only
has one data Series in it which is fine
so if we do DF to print it out you'll
see a one two two two two four five six
seven and so on and so how would you
remove that well there is a neat feature
in data frames called shift
along with another feature that lets us
select just certain date information and
we'll go with the location function put
that in Brackets remember that from
above location and then in the location
let me just spread this out a little bit
so it's really easy to read in fact I'm
going to go upscale on that since we're
doing some a little bit more complicated
here
what you can see on this on the location
is I have DFA dot shift so this is going
to shift up one by default you can
actually change this to two or three you
can even do a minus one and it shifts
the other way but it's going to shift up
by one by default that's going to say if
that does not equal DF of a then we want
that if you look down here we had one
two two two two when we run this Logic
on here and we do the shift it now gets
rid of all the duplicates so we went
from one two two two two four four five
whatever it was here it is one two two
two four four four five five five six
six six two one two four five six seven
eight and you'll see on the index it
just deletes them out of there so the
index stays the same obviously you don't
want the dates to change if you're
working with an index dated setup so it
just deletes those duplicates out of
there this is just a quick way to
introduce you to one the fact that you
can add logic gates into here and two
the I location allows you to use shift
so there's the shift function and then
the eye location selects that based on
true or false
wow so we've actually covered a lot
today in pandas we've really covered
into the basics of selecting your
different series out of your column out
of your data frame how to index rows how
to slice how to plot hopefully you'll
take this beyond that and start
combining these different things and you
can create long strings and really
explore your data generate some nice
graphs if you're in jupyter Notebook
it's a great demo to show others
and I didn't know this about jupyter
notebook you can do this in jupyter
Notebook and then you can download and I
always I never really look too closely
at all the downloads but you download as
an HTML and post it to your blog so it's
got a neat feature in there but any of
this is really powerful tool all of this
is really powerful tools for doing your
data science in this case let me go
ahead and run this and you'll see it has
a printed data out for us and here's our
whoops must have missed oh there we go
it doesn't help that I put it over the
old one there we go okay
so now you have your default histogram
and now we have a cumulative histogram
and we should have 50 steps in there and
let's just find out if that's true not
So Much by counting them I'm not going
to count them if you want to you can
count them let's just change it to 10
and see what happens and we see here we
have now 10 counts of that
and we could set that for five
and run that
and then we have our five on there and
we go ahead and take the cumulative
equals true out just so you can see what
that looks like and let me run that on
here too
that looks just like I did before I
think there's what one two three four
five six seven eight they have eight
different bins on here is what the
default came out of
back in there run
and so now it should look almost
identical and it does and then we and
then we can put the cumulative back in
see what that looks like with the
cumulative
and run that
and we can see how that shifts
everything over and has a slightly
different look wait it shifts it all to
the right no it doesn't actually shift
it to the right it's cumulative so it's
the total of the different occurrences
and so what that means is like if you
consider this like for the year of
rainfall we have like day one you had a
little bit of rain day two we have more
rain and so if you look at the number
this is a hundred thousand thirty five
thousand so it's a cumulative detail the
histogram the currents as it grows and
rainfall is a good one because that
would be a cumulative histogram of how
much rain occurred throughout the year
and we're going to look at two more
graphs we've already looked at a bunch
of them we looked at our radar graph
we've looked at scatter step bar fill in
basic plots we've looked at different
ways of showing the data and we can
increase the size of the line the look
the color the alpha setting
so let's look at contour maps just put
that in there there we go draw a contour
map and before we draw a contour map we
need to go ahead and create data for it
and if you have Contours your data is
all going to have three different values
so let's go ahead and create the data
here we have our you'd import your
matplot library your numpy so we have
our numbers array
and we'll import
matplot.cm and that's your color Maps so
you have all these different color maps
you can look at there's like hundreds of
color Maps so if you don't want to do
your own color you need to do your own
color map they're pretty diverse and of
course our PLT we're going to our PI
plot and to generate our different data
we're going to create a Delta 0.025 and
we'll start with X and we're going to
create an array between -3 and 3 and
Delta increments of 0.025
and we'll have our y we'll do something
similar and then we'll create our X Y
into a mesh grid again these are all
numpy commands so if you're not familiar
with these you'll want to go back and
review our numpy tutorial and we'll do
an exponential line here minus x squared
minus y squared for our Z1 we'll do a Z2
so we have two different areas and Z
equals z of one minus Z2 times 2. so
we've created a number of values here
and let me go ahead and run this and
let's plug that in so you can see where
those values are going so once we've set
these we're going to create our figure
and our X from our PLT subplots we're
going to create the variable Cs and this
is going to be our Contour so right here
CS is our Contour service and we're
feeding it X Y and Z if you remember XY
we created as our X and Y components
using our mesh grid and you know what
let's do this just because it's kind of
good to see this let's go ahead and
print X and let's print Y and I always
like to do this when I'm working with
something this either is really
complicated in this case is what we're
looking at or you don't understand yet
so we've created a mesh grid we have x y
and when we're done with this we end up
with here's our X
and this set of values in our y so those
are X and Y coordinates and then we've
also created Z based on our X and Y so
we have x capital x capital Y and
capital Z is our three components X and
Y being the coordinates well Z is going
to be our actual height since we're
doing a contour map so we created our
contour map from our X Y and Z
coordinates we want to go ahead and put
in a c label maybe we want to go ahead
and do an title on here
we'll put that in our set title and this
is a contour there we go contour map and
let's go ahead and run this and see what
that looks like and you'll see we
generated a nice little contour map
there's different settings you can play
with on this but you can picture this
being you're on a mountain climb and
here we have a line that's represents
zero maybe at sea level and then moving
on up you have your Contours of 0.5 and
-1 and different setups little Hills I
guess if it's minus that's like a pit so
I guess you're going down into a pit at
minus five and minus one but on the
other side you can see you're going up
in levels so here's a Mountaintop and
here's like a basin of some kind and in
data science this could represent a lot
of things this could also be
representing two different values and
maybe profits and loss I don't know if
I'd ever really do that as a contour map
but I'm sure you could be creative and
find something fun to do with a contour
map and then we're going to look at one
last map which is the 3D map and those
are can be really important as a final
product because they can show so much
additional information that you can't
fit on two-dimensional graphs
there we go draw a 3D image and so we're
going to import from our MPL toolkits
the implant 3D and the axis 3D we're
going to import axis 3D this is what's
going to let us work with the 3D image
and this should look familiar we're
going to create another figure just like
we did before figure size 14 by 6 that's
a good fit on the screen we'll go ahead
and run that so we have our figure and
let's go ahead and take our X and we're
going to set that equal to Fig dot add
subplot that should also be familiar
from earlier and we're going to work
with this sets the settings for the
projection and we're going to use one
two one projection 3D and we'll see what
that looks like in just a minute and we
just created some three-dimensional data
here before where we had X Y and Z
capital X Y and Z so we're going to
reuse that data we're just going to use
that since this also this is also a
three-dimensional image so let's use
that for a three-dimensional graph and
we simply do ax plot underscore
surface and a capital x capital y
capital Z so there's our data coming in
and we're going to add some settings in
here we're going to do R stride 4 C
stride four and line width 0. I'll show
you what that is here in just a minute
let's go ahead and run that so we can
see our graph and of course it helps if
I don't add an extra comma in there and
you can see it generates this really
beautiful three-dimensional graph
so let's take a little bit time to
explore some of these numbers we have
going in here
we have the r stride 4 the C stride four
and the projection 3D projection 3D is
the important one because that's telling
us that this is a 3D graph here
so what are these first numbers one two
one let's just change one of these I'm
going to change this to five
and it's going to give me an error let's
change it to 1.
and oop that didn't work let's change
this middle one to three instead and
you're going to see how it starts
reshaping the size and how it fits on
the screen and we'll change the first
one to two and we'll run that one and
again it's changed the dimensions and
the size and how it fits on here play
with these numbers to get a nice look
and feel for it part of it is the Tilt
and the angle
we'll do seven on this one
there we go you can see it really
shifted it there
but again that changes the size now it
fits on the canvas but we'll leave it at
the one
two and just so you get a good look at
what we're talking about here this is
column width and index from before
if we do one one one you can see that it
now spreads it out all the way across
uses the whole setup on there so this
has to do with the size and how big you
want it to be now there's one term that
we didn't cover in this yet but we've
used it throughout the whole setup
and I'm just going to type that down
here even though we're not going to go
into detail and that's the term heat map
you might see that is kind of starting
to lose ground as far as a common
reference but there sure are a lot of
people still talking about heat Maps
what is a heat map well it is simply a
color map that's all it is so if you
ever see the term heat map
that refers to the fact this is in
different colors representing different
heights
that one is in the heat map but you can
see up here we switched into let me go
back up here here we go this one has
different colors for the different
values a lot of times you'll use like
instead of X and Y you might do a heat
map
where you have a fourth value and the
fourth value represents the color and so
you'll see this 3D image into nice
colors represented by heat map that's
all it is so if you see the term heat
map that only means we're plotting some
of the data in color to make it stand
out or to give it a fourth dimension in
this case
so we've covered a lot of things on
matplot and that brings us covered all
the basics so that brings us to practice
example and this is going to be the
challenge for you and let me go ahead
and change our cell cell type mark down
and run that so it looks pretty
practice example write a Python program
to create a pie chart of the popularity
of programming languages
okay excellent
and if you're going to have a challenge
we need some data and I'll just throw in
our import our matplot library at the
beginning you should do that
automatically and so for our data to
plot we're going to have our languages
we're going to have python we're going
to have Java PHP JavaScript c-sharp C
plus plus so those are six categories
and then we have our popularity oops
misspelling there popularity we'll give
the first one 22.2 percent Java 17.6 and
I don't know if these are real numbers
they pulled my guess is that they might
have just been made up because I don't
know if Python's really that much more
popular than the other ones maybe
specific to data science because
Python's very popular in data science
right now because it has so many options
the only other program that's highly
used and exclusively for data science is
R so Python's big and python also does a
lot more it's a full programming
language where R is primarily for data
science they didn't put R in here so we
have python we have Java we have our PHP
you see the different values they've
given it or different percentages
and I did add these up does not add up
to 100 it adds up to 71 percent or
something like that
and then we're going to give colors and
we've chosen these guys in the back
brought in these colors I'm not sure
what these colors are we'll find out in
a minute so I'll be exciting but you can
see they're using the actual color
values you can pull off of a color wheel
or something like that it could have
just as easily done blue red green if
you're too lazy to pick the exact colors
and then let's go ahead and solve this
and see we got here we're going to do
something a little fancy just because we
can the first thing we're going to do is
we're going to use a variable called
explode and you'll notice that there's
six variables in here so that matches
our six different categories and the
first one we've done is 0.1 and then
zero zero zero zero zero point one when
we put this in here under the explode in
the plot it will actually push that
square out so it's a really cool feature
to highlight certain information on a
pie chart
and this is simply PLT dot pi and we're
plotting
popularity there we go and before we add
in all the really cool settings for this
let's go ahead and run it and you'll see
we generate a nice flat pie not too
exciting there and then we'll go ahead
and put in all the extras I talked about
explode or we can explode one of the
values out so here's our explode equals
explode labels as languages because we
want to know what the different colors
mean here's our colors equals colors our
Auto picture and this is standard print
format so that's a python setup on there
and that's just going to put the value
on the pi slice and then we're going to
add Shadow because it just looks cooler
with the shadow gives a little 3D look
and we'll do a start angle of 140. let's
go ahead and run this and take a look
and see what comes out of that
and look how that changes the whole
setup so here's our labels there's our
value we put on there there's our slice
it's pushed out there's our shadow a 3d
effect and then we started at 140. we
could also rotate this let's just do
this angle 90.
and if we run it
you'll see the blue pie slices moved up
a little bit we could actually do
actually let's just take the whole
starting triangle out and run he'll
default to zero
this is what it looks like if it
defaulted to zero so depending on where
you want the highlighted slice to appear
usually you want that to appear on the
left because people read left to right
and so it draws a focus onto in this
case Python and how great python is I'm
a little biased we're teaching a Python
tutorial so it should be understandable
that we're looking at Python and one
last reference before we close you can
go over to the matplot library.pi plot
setup and if you go underneath there the
different functions on there you can
look this up on their website you'll see
a full list and this is why it's so
important to go through a tutorial like
this because this list is just so
massive I'm trying to figure out like
here's our bar plot there's a bar H you
can add barbs there's a box plot we
didn't cover C labels a totally
different kind of for your Contour plot
you can set up in there if you go down
here we have our figures we used on
there we showed you the basics how to do
the figure you'll see some closer
references on those there's a histogram
down here h-i-s-t there's also the hist
2D makes a 2d histogram plot H lines all
of this these are all the different
commands that are underneath of here and
you can see it's pretty extensive we've
covered all the basic ones so that you
know have a solid ground to look at
these different options so when you come
to these functions some of them are
going to look a little off or not off
will look unfamiliar but you'll still
have the availability to probably
understand most of this and have a basic
understanding of your matplot library
join us on a journey into the
fascinating world of AI and machine
learning with our Caltech postgraduate
program in Ai and machine learning in
partnership with IBM this a and ml
course scores the latest tools and
Technologies from the AI ecosystem and
features master classes by Caltech
faculty IBM experts hackathons and ask
me anything sessions this program
showcases Caltech ctm is excellence and
IBM's industry prowess the artificial
intelligence course key Concepts like
statistics data science with python
machine learning deep learning NLP
reinforcement learning and through an
Interactive Learning model with live
sessions enroll now analog exciting Ai
and ml opportunities the course link is
mentioned in the description box what is
tensorflow what is tensorflow tensorflow
is a popular open source Library
released in 2015 by Google brain team
for building machine learning and deep
learning models it is based on Python
programming language and performs
numerical computations using data flow
graphs to build models so let's take a
look at some of the features of
tensorflow it works efficiently with
multi-dimensional arrays if you've ever
played with any of the simpler packages
of neural networks you're going to find
that you have to pretty much flatten
them and make sure your your stuff is
set in a flat model tensorflow works
really good so we're talking pictures
here where you have X and Y coordinates
with where the picture is and then each
pixel has three or four different
channels that's a very complicated array
very multimensional array it provides
scalability of computation across
machines and large data sets this is so
new right now and you might think that's
a minor thing but when python is
operating on one computer and it has a
float value and it truncates it
differently on each computer you don't
get the same results and so your
training model might work on one machine
and then on another it doesn't this is
one of the things that tensorflow
addresses and does a very good job on it
supports fast debugging and model
building this is why I love tensorflow I
can go in there and I can build a model
with different layers each layer might
have different properties they have like
the convolutional neural network which
you can then sit on top of a regular
neural network with reverse propagation
there's a lot of tools in here and a lot
of options and each layer
that it goes through can utilize those
different options and stack differently
and it has a large community and
provides tensorboard to visualize the
model tensorboard is pretty uh recent
but it's a really nice tool to have so
you when you're working with other
people or showing your clients or the
shareholders in the company you can give
them a nice visual model so they know
what's going on what are they paying for
and let's take a glance at some of the
different uses or applications for
tensorflow when we talk about tensorflow
applications
clearly this is data analytics we're
getting into the data science I like to
use data science as probably a better
term because this is the programming
side
uh and it's really the sky is a limit
um we can look at face detection
language translation fraud detection
video detection there are so many
different things out there that
tensorflow can be used for when you
think of neural networks because
tensorflow is a neural network
think of complicated chaotic data this
is very different than if you have a set
numbers like you're looking at the stock
market you can use this on the stock
market but if you're doing something
where the numbers are very clear and not
so chaotic as you have in a picture then
you're talking more about linear
regression models and different
regression models when you're looking at
that when you're talking about these
really complicated data patterns then
you're talking neural networks and
tensorflow and if we're going to talk
about tensorflow we should talk about
what tensors are after all that is what
tensor that's what this is named after
so we talk about tensors in tensorflow
tensorflow is derived from its core
component known as a tensor a 10 answer
is a vector or a matrix of n Dimensions
that represent all types of data and you
can see here we have the scalar which is
just a single number you have your
vector which is two numbers might be a
number in a direction you have a simple
Matrix and then we get into the tensor I
mentioned how a picture is a very
complicated tensor because it has your X
Y coordinates and then each one of those
pixels has three to four channels for
your different colors and so each image
coming in would be its own tensor and in
tensorflow tensors are defined by a unit
of dimensionality called as rank
and you can see here we have our scalar
which is a single number that has a rank
of zero because it has no real
Dimensions to it other than it's just a
single point and then you have your
vector which would be a single list of
numbers
so it's a rank one Matrix would have
rank two and then as you can see right
here as we get into the full tensor it
has a rank three
and so the next step is to understand
how a tensorflow works and if you
haven't looked at the basics of a neural
network in Reverse propagation that is
the basics of tensorflow and then it
goes through a lot of different options
and properties that you can build into
your different tensors so a tensorflow
performs computations with the help of
data flow graphs it has nodes that
represent the operations in your model
and if you look at this you should see a
neural network going on here we have our
inputs b c and d and you might have x
equals B plus c y equals D minus 4 a
equals x times Y and then you have an
output and so even though this isn't a
neural network here it's just a simple
set of computations going across you can
see how the more complicated it gets the
more it can actually one of the tensors
is a neural network with reverse
propagation but it's not limited to that
there's so much more you can do with it
and this here is just a basic uh flow of
computations of the data going across
and you can see we can plug in the
numbers uh b equals force equals three D
equals six and you get x equals four
plus three so x equals seven y equals
six minus four so y equals two and
finally a equals 7 times 2 or a equals
fourteen like I said this is a very
simplified version of how tensorflow
Works each one of these layers can get
very complicated
um but tensorflow does such a nice job
that you can spend different setups up
very easily and test them out so you can
test out these different models to see
how they work now tensorflow has gone
through two major stages we had the
original tensorflow release of 1.0 and
then they came out with the 2.0 version
and the 2.0 addressed so many things out
there that the 1.0 really needed so when
we start talking about tensorflow 1.0
versus 2.0 I guess you would need to
know this for a legacy programming job
if you're pulling apart somebody else's
code the first thing is that tensorflow
2.0 supports eager execution by default
it allows you to build your models and
run them instantly and you can see here
from tensorflow 1 to tensorflow 2 we
have almost double the code to do the
same thing so if I want to do with
tf.session or tensorflow session as a
session the session run you have your
variables your session run you have your
tables initialized and then you do your
model fit X train y train and then your
validation data your x value y value and
your epics and your batch size all that
goes into the fit and you can see here
where that was all just compressed to
make it run easier you can just create a
model and do a fit on it and you only
have like that last set of code on there
so it's automatic that's what they mean
by the eager so if you see the first
part and you're like what the heck is
all this session thing going on that's
tensorflow 1.0 and then when you get
into 2.0 it's just nice and clean if you
remember from the beginning I said cross
on our list up there
and crosses the high level API and
tensorflow 2.0 cross is the official
high level API of tensorflow 2.0 it has
incorporated across as tf.cross cross
provides a number of model building apis
such as sequential functional and
subclassing so you can choose a right
level of abstraction for your project
and uh we'll hopefully touch base a
little bit more on this sequential being
the most common form that is your your
layers are going from one side to the
other so everything's going in a
sequential order
functional
is where you can split the layer so you
might have your input coming on one side
it splits into two completely mod
different models and then they come back
together and one of them might be doing
classification the other one might be
doing just linear regression kind of
stuff or a neural basic reverse
propagation neural network and then
those all come together into another
layer which is your neural network
reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models into Karas
so very powerful tools here this is all
the stuff that's been coming out
currently in the tensorflow cross setup
a third big change we're going to look
at is it in tensorflow 1.0
in order to use TF layers as variables
you would have to write TF variable
block so you'd have to pre-define that
in tensorflow 2 you just add your layers
in under the sequential and it
automatically defines them as long as
they're flat layers of course this
changes a little bit as a more
complicated tensor you have coming in
but all of it's very easy to do and
that's what 2.0 does a really good job
of and here we have a little bit more on
the scope of this and you can see how
tensorflow 1 asks you to do these
different layers and values if you look
at the scope and the default name you
start looking at all the different code
in there to create the variable scope
that's not even necessary intense 2.0 so
you'd have to do one before you do do
what you see the code in 2.0 in 2.0 you
just create your model it's a sequential
model and then you can add all your
layers in you don't have to pre-create
the um
variable scope so if you ever see the
variable scope you know that came from
an older version and then we have the
last two which is our API cleanup and
the autograph in the API cleanup
tensorflow 1 you could build models
using TF Gans TF app TF contrib TF Flags
Etc and tensorflow 2 a lot of apis have
been removed and this is just they just
clean them up because people weren't
using them and they've simplified them
and that's your TF app your TF Flags
your TF logging are all gone
so there's those are three Legacy
features that are not in 2.0 and then we
have our TF function and autograph
feature in the old version tensorflow
1-0 the python functions were limited
and could not be compiled or exported
re-imported so you were continually
having to redo your code you couldn't
very easily just put a pointer to it and
say hey let's reuse this
in tensorflow 2 you can write a python
function using the TF function to mark
it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of TF
function helps to write graph code using
natural python syntax
uh now we just threw in a new word a new
graph graph is not a picture of a person
you'll hear graph x and some other
things graph is what are all those lines
that are connecting different objects so
if you remember from before where we had
the different layers going through
sequentially each one of those white
lined arrows would be a graph x that's
where that computation is taken care of
and that's what they're talking about
and so if you had your own special code
or python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using in
neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic Tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look at tensorflow toolkits
in their hierarchy at the high level we
have our object oriented API so this is
what you're working with you have your
TF cross you have your estimators this
sits on top of your TF layers TF losses
TF metrics so you have your reusable
libraries for model building this is
really where tensorflow shines is
between the cross running your
estimators and then being able to swap
in different layers you can your losses
your metrics all of that is so built
into tensorflow makes it really easy to
use and then you can get down to your
low level TF API you have extensive
control over this you can put your own
formulas in there your own procedures or
models in there you could have it split
we talked about that earlier so with the
2.0 you can now have it split One
Direction where you do a linear
regression model and then go to the
other where it does a neural network and
maybe each neural network has a
different activation set on it and then
it comes together into another layer
which is another neural network so you
can build these really complicated
models and at the low level you can put
in your own apis you can move that stuff
around and most recently we have the TF
code can run on multiple platforms and
so you have your CPU which is basically
like on the computer I'm running on I
have eight cores and 16 dedicated
threads I hear they now have one out
there that has over a hundred cores uh
so you have your CPU running and then
you have your GPU which is your graphics
card
and most recently they also include the
TPU setup which is specifically for
tensorflow models uh neural network kind
of setup so now you can export the TF
code and it can run on all kinds of
different platforms for the most
diverse setup out there and moving on
from the hierarchy to the architecture
in the tensorflow 2.0 architecture we
have you can see on the left this is
usually where you start out with and
eighty percent of your time in data
science is spent pre-processing data
making sure it's loaded correctly and
everything looks right so the first
level in tensorflow is going to be your
read and pre-processed data your TF data
feature columns
this is going to feed into your TF Cross
or your pre-made estimators and kind of
you have your tensorflow Hub that sits
on top of there so you can see what's
going on uh once you have all that set
up you have your distribution strategy
where are you going to run it are you
going to be running it on just your
regular CPU are you going to be running
it with the GPU added in like I have a
pretty high-end graphics card so it
actually grabs that GPU processor and
uses it or do you have a specialized TPU
set up in there that you paid extra
money for it could be if you're in later
on when you're Distributing the package
you might need to run this on some
really high processors because you're
processing at a server level for let's
say net you might be processing this at
a distribute you're Distributing it not
the distribution strategy but you're
Distributing it into a server where that
server might be analyzing thousands and
thousands of purchases done every minute
and so you need that higher speed to
give them a to give them a
recommendation or a suggestion so they
can buy more stuff off your website or
maybe you're looking for a data fraud
analysis working with the banks you want
to be able to run this at a high speed
so that when you have hundreds of people
sending their transactions in it says
hey this doesn't look right someone's
scamming this person and probably has a
credit card so when we're talking about
all those fun things we're talking about
saved model this is we were talking
about that earlier where it used to be
when you did one of these models it
wouldn't truncate the float numbers the
same and so a model going from one you
build the model on your machine in the
office and then you need to distribute
it and so we have our tensorflow serving
Cloud on premium that's what I was
talking about if you're like a banking
or something like that now they have
tensorflow Lite so you can actually run
it tensorflow on an Android or an iOS or
Raspberry Pi a little breakout board
there in fact they just came out with a
new one that has a built-in is this many
TPU with the camera on it so it can
pre-process a video so you can load your
tensorflow model onto that talking about
an affordable way to beta test a new
product you have the tensorflow JS which
is for browser and node server so you
can get that out in the browser for some
simple computations that don't require a
lot of heavy lifting but you want to
distribute to a lot of endpoints and now
they also have other language bindings
so you can now create your tensorflow
back in save it and have it accessed
from C Java go C sharp rust r or from
whatever package you're working on so we
kind of have an overview of the
architecture and what's going on behind
the scenes and in this case what's going
on as far as Distributing it let's go
ahead and take a look at three specific
pieces of tensorflow
and those are going to be constants
variables and sessions so very basic
things you need to know and understand
when you're working with the tensorflow
setup so constants in tensorflow in
tensorflow constants are created using
the function constant in other words
they're going to stay static the whole
time whatever you're working with the
Syntax for constant value d-type 9 shape
equals none name constant verify shape
equals false that's kind of the syntax
you're looking at and we'll explore this
with our hands on a little more in depth
and you can see here we do Z equals
tf.constant 5.2 name equals x d type is
a float that means that we're never
going to change that 5.2 is going to be
a constant value and then we have our
variables in tensorflow variables in
tensorflow are in memory buffers that
store tensors and so we can declare a
two by three tensor populated by ones
you could also do constants this way by
the ways you can create a an array of
ones for your constants I'm not sure why
you do that but you know you might need
that for some reason
and here we have V equals TF dot
variables
and then in tensorflow you have tf.1s
and you have the shape which is 2 3
which isn't going to create a nice two
by three array that's filled with ones
and then of course you can go in there
in their variables so you can change
them it's a tensor so you have full
control over that and then you of course
have uh sessions in tensorflow a session
in tensorflow is used to run a
computational graph to evaluate to the
nodes and remember when we're talking a
graph or graph x we're talking about all
that information then goes through all
those arrows and whatever computations
they have they take it to the next node
and you can see down here where we have
import tensorflow as TF if we do x
equals a TF dot constant of 10 we do y
equals a TF constant of 2.0 or 20.0 and
then you can do Z equals TF dot variable
and it's a TF dot add X comma y
and then once you have that set up in
there you go ahead and knit your TF
Global variables initializer with TF
session as session you can do a session
run knit and then you print the session
run y uh and so when you run this you're
going to end up with of course the 10
plus 20 is 30. and we'll be looking at
this a lot more closely as we actually
roll up our sleeves and put some code
together
so let's go ahead and take a look at
that and for my coding today I'm going
to go ahead and go through anaconda and
then I'll use specifically the Jupiter
Notebook on there and of course this
code is going to work uh whatever
platform you choose whether you're in a
notebook the Jupiter lab which is just a
Jupiter notebook but with tabs for
larger projects we're going to stick
with Jupiter notebook pie charm whatever
it is you're going to use in here you
have your spider and your QT console for
different programming environments the
thing to note is kind of hard to see but
I have my main Pi 36 right now when I
was writing this tensorflow works in
Python version three six if you have
python version 3 7 or 3 8 you're
probably going to get some errors in
there might be that they've already
updated it and I don't know it now you
have an older version
but you want to make sure you're in a
python version 3 6 in your environment
and of course in Anaconda I can easily
set that environment up make sure you go
ahead and and pip in your tensorflow or
if you're in Anaconda you can do a conda
install tensorflow to make sure it's in
your package so let's just go ahead and
dive in and bring that up
this will open up a nice browser window
I just love the fact I can zoom in and
zoom out depending on what I'm working
on making it really easy to adjust a
demo for the right size go under new and
let's go ahead and create a new Python
and once we're in our new python window
this is going to leave it Untitled let's
go ahead and import import tensorflow as
TF at this point we'll go ahead and just
run it real quick no errors yay no
errors I
I do that whenever I do my imports
because I unbearably will have opened up
a new environment and forgotten to
install tensorflow into that environment
or something along those lines so it's
always good to double check
and if we're going to double check that
we also it's also good to know what
version we're working with and we can do
that simply by using the version command
and tensorflow
which you should know is is probably
intuitively the TF
dot underscore underscore version
underscore underscore
and you know it always confuses me
because sometimes you do tf.version for
one thing you do TF dot underscore
version underscore for another thing
this is a double underscore in
tensorflow for pulling your version out
and it's good to know what you're
working with we're going to be working
in tensorflow version 2.1.0 and I did
tell you the the um we were going to dig
a little deeper into our constants and
you can do an array of constants and
we'll just create this nice array a
equals tf.constant and we're just going
to put the array right in there four
three six one we can run this and now
that is what a is equal to and if we
want to just double check that remember
we're in Jupiter notebook where you can
just put the letter a and it knows that
that's going to be print otherwise you'd
run you'd surround it in print and you
can see it's a TF tensor it has the
shape the type and the and the array on
here it's a two by two array and just
like we can create a constant we can go
ahead and create a variable and this is
also going to be a two by two array and
if we go ahead and print the V out we'll
run that and sure enough there's our TF
variable in here
then we can also let's just go back up
here and add this in here I could create
another tensor and we'll make it a
constant this time
and we'll go and put that in over here
we'll have b t f constant and if we go
and print out V and B I'm going to run
that and this is an interesting thing
that always that happens in here you'll
see right here when I print them both
out what happens it only prints the last
one unless you use print commands so
important to remember that in jupyter
notebooks we can easily fix that by go
ahead and print and Surround V with
brackets and now we can see with the two
different variables we have we have the
three one five two which is a variable
and this is just a flat constant so it
comes up as a TF tensor shape two kind
of two and that's interesting to note
that this label is a tf.tensor and this
is a TF variable
so that's how it's looking in the back
end when you're talking about the
difference between a variable and a
constant
the other thing I want you to notice is
that in variable we capitalize the V and
with the constant we have a lowercase C
little things like that can lose you
when you're programming and you're
trying to find out hey why doesn't this
work uh so those are a couple little
things to note in here and just like any
other array in math we can do like a
concatenate or concatenate the different
values here and you can see we can take
a b concatenated you just do a tf.concat
values and there's our a b axes on one
hopefully you're familiar with axes and
how that works when you're dealing with
matrixes and if we go ahead and print
this out you'll see right here we end up
with a tensor so let's put it in as a
constant nut as a variable and you have
your array 4 3 7 8 and 6145 is
concatenated the two together and
a couple things on this our axes equals
one this means we're doing the columns
so if you had a longer array like right
now we have an array that is like you
know has a shape one whatever it is two
comma two
axes zero
is going to be your first one and axis
one is going to be your second one and
it translates as columns and rows if we
had a shape let me just put the word
shape here
um
so you know what I'm talking about it's
very clear and this is I'll tell you
what I spend a lot of time
looking at these shapes and trying to
figure out which direction I'm going in
and whether to flip it or whatever
so you can get lost in which way your
Matrix is going which is column which is
rows are you dealing with the third axes
or the second axis axis one you know
zero one two that's going to be our
columns and if you can do columns then
we also can do rows and that is simply
just changing the concatenate we'll just
grab this one here and copy it we'll do
the whole thing over
control copy
Ctrl V and changes from axis one
to axis zero and if we run that you'll
see that now we concatenate by row as
opposed to column
and you have four three six one seven
eight four seven so it just brings it
right down and turns it into rows versus
columns you can see the difference there
your output in this really you want to
look at the output sometimes just to
make sure your eyes are looking at it
correctly and it's in the format I find
visually looking at it is almost more
important than understanding what's
going on because conceptually your mind
just just too many dimensions sometimes
the second thing I want you to notice is
it says a numpy array so tensorflow is
utilizing numpy as part of their format
as far as Python's concerned and so you
can treat you can treat this output like
a numpy array because it is just that
it's going to be a numpy array another
thing that comes up uh more than you
would think is filling uh one of these
with zeros or ones and so you can see
here we just create a tensor tf.zeros
and we give it a shape we tell it what
kind of data type it is in this case
we're doing an integer and then if we
print out our tensor again we're in
Jupiter so I can just type out tensor
and I run this you can see I have a nice
array of the shape three comma four of
zeros one of the things I want to
highlight here is integer 32. if I go to
the tensorflow data types I want you to
notice how we have float 16 float 32
float 64 complex if we scroll down
you'll see the integer down here 32. the
reason for this is that we want to
control how many bits are used in the
Precision
this is for exporting it to another
platform so what would happen is I might
run it on this computer where python
goes does a float to indefinite however
long it wants to and then we can take it
but we want to actually say hey we don't
want that high Precision we want to be
able to run this on any computer and so
we need to control whether it's a TF
float16 in this case we did an integer
32.
we could also do this as a float so if I
run this as a float32 that means this
has a 32-bit Precision you'll see Zero
Point whatever and then to go with zeros
we have ones if we're going from the
opposite side and so we can easily just
create a tensorflow with ones
and you might ask yourself why would I
want zeros and ones and your first step
might be to initiate a new tensor
usually we initiate a lot of this stuff
with random numbers because it does a
better job solving it if you start with
the uniform set of ones or zeros you're
dealing with a lot of bias so be very
careful about starting a neural network
for one of your rows or something like
that with ones and zeros on the other
hand uh I use this for masking you can
do a lot of work with masking you can
also have it might be that one tenth a
row is masked you know zero is is false
one is true or whatever you want to do
it
um and so in that case you do want to
use the zeros and ones and there are
cases where you do want to initialize it
with all zeros or all ones and then swap
in different numbers as a as the tensor
learns so it's another form of control
but in general you see zeros and ones
you usually are talking about a mask
over another array and just like in
numpy you can also do reshapes so if we
take our remember this is shape three
comma four maybe we want to swap that to
four comma three
and if we print this out
you will see let me just go and do that
control V let me run that and you'll see
that the the order of these is now
switched instead of four across now we
have three across and four down
and just for fun let's go back up here
where we did the ones
and I'm going to change the ones to TF
dot random uniform uh and we'll go ahead
and just take off well we'll go and
leave that I'm going to run this
and you'll see now we have 0.0441 and
this way you can actually see how the
reshape looks a lot different uh
0.041.15.71 and then instead of having
this one it rolls down here to the 0.14
and this is what I was talking about
sometimes you feel a lot of times you
feel these with random numbers and so
this is the random.uniform is one of the
ways to do that now I just talked a
little bit about this float 32 and all
these data types one of the things that
comes up of course is recasting your
data
so if we have a d-type float32 we might
want to convert these two integers
because of the project we're working on
I know one of the projects I've worked
on ended up wanting to do a lot of round
off so that it would take a dollar
amount or a float value and then have to
run it off to a dollar amount so we only
wanted two decimal points and in which
case you have a lot of different options
you can multiply by 100 and then round
it off or whatever you want to do
there's a lot of or then convert it to
an integer was one way to round it off
kind of
the cheap and dirty trick
so we can take this we can take the same
tensor and we'll go ahead and create a
as an integer and so we're going to take
this tensor we're going to tf.cast it
and if we print
tensor and then we're going to go ahead
and print
our
tensor
let me just do a quick copy and paste
and when I'm actually programming I
usually type out a lot of my stuff just
to double check it in doing a demo copy
and paste works fine but sometimes be
aware that copy and paste can copy the
wrong code over
personal choice depends on what I'm
working on
and you can see here we took a float 32
4.6 4.2 and so on and it just converts
it right down to a integer value sorry
integer 32 setup and remember we talked
about a little bit about reshape
as far as flipping it and I just did
four comma 3 on the reshape up here and
we talked about axis 0 axis one one of
the things that is important to be able
to do is to take one of these variables
we'll just take this last one tensor as
integer
and I want to go ahead and transpose it
and so I can do we'll do a
equals TF dot transpose
and we'll do our tensor integer in there
and then if I print the A out and we run
this
you'll see it's the same array but we've
flipped it so that our columns and rows
are flipped this is the same as
reshaping so when you transpose you're
just doing a reshape what's nice about
this is that if you look at the numbers
The Columns when we went up here and we
did the reshape they kind of rolled down
to the next row so you're not
maintaining the structure of your Matrix
so when we do a reshape up here they're
similar but they're not quite the same
and you can actually go in here and
there's settings in the reshape that
would allow you to turn it into a
transform
uh so when we come down here it's all
done for you and so there are so many
times you have to transpose your digits
that this is important to know that you
can just do that you can flip your rows
and columns rather quickly here and just
like numpy you can also do multiple your
different math functions we'll look at
multiplication and so we're going to
take matrix multiplication of tensors
we'll go ahead and create a as a
constant 5839 and we'll put in a vector
v 4 comma two and we could have done
this where they matched where this was a
two by two array but instead we're going
to do just a two by one array and the
code for that is your TF dot mat mole so
Matrix multiplier and we have a times V
and if we go ahead and run this up let's
make sure we print out our av on there
and if we go ahead and run this
you'll see that we end up with 36 by 30.
and if it's been a while since you've
seen The Matrix math this is five times
four plus eight times two
three times four plus nine times two
and that's where we get the 36 and 30.
now I know we're covering a lot really
quickly as far as the basic
functionality
so the Matrix or your Matrix multiplier
is a very commonly used back-end tool as
far as Computing
um different models or linear regression
stuff like that one of the things
is to note is that just like in numpy
you have all of your different math so
we have our TF math
and if we go in here we have functions
we have our cosines absolute angle all
of that's in here so all of these
are available for you to use in the
tensorflow model
and if we go back to our example and
let's go ahead and pull oh let's do some
multiplication that's always good we'll
stick with our av our
constant a and our vector v
and we'll go ahead and do some bitwise
multiplication and we'll create an AV
which is a times B let's go and print
that out
and you can see coming across here we
have the 4 2 and the 5839 and it
produces 20 32 618
and that's pretty straightforward if you
look at it you have four times five is
twenty four times eight is uh 32 that's
where those numbers come from
we can also quickly create an identity
Matrix
which is basically your main values on
the diagonal being ones and zeros across
the other side let's go ahead and take a
look and see what that uh
looks like we can do let's do this uh
so we're going to get the shape this is
a simple way very similar to your numpy
you can do a DOT shape and it's going to
return a tuple in this case our rows and
columns and so we can do a quick print
we'll do rows oops
and we'll do columns
and if we run this you can see we have
three rows two columns
and then if we go ahead and create an
identity Matrix
the scripts
the script for that hit a wrong button
there the script for that looks like
this
where we have the number of rows equals
rows the number of columns equals
columns and D type is a 32 and then if
we go ahead and just print out our
identity
you can see we have a nice identity
column with our ones going across here
now clearly we're not going to go
through every math module available but
we do want to start looking at this as a
prediction model and seeing how it
functions so we're going to move on to a
more of a
direct setup where you can actually see
the full tensorflow in use for that
let's go back and create a new setup
and we'll go in here new Python 3 module
there we go
bring this out so it takes up the whole
window because I like to do that
hopefully made it through that first
part and you have a basic understanding
of tensorflow as far as being a series
of numpy arrays you got your math
equations and different things that go
into them we're going to start building
a full
um
set up as far as the numpy so you can
see how Karo sits on top of it and the
different aspects of how it works the
first thing you want to do is we're
going to go ahead and do a lot of
imports date times warning SCI Pi SCI Pi
is your
maths or the back end scientific math
warnings because whenever we do a lot of
this you have older versions newer
versions and so sometimes when you get
warnings you want to go ahead and just
suppress them we'll talk about that if
it comes up on this particular setup and
of course date time
pandas again is your data frame think
rows and columns we import it as PD
numpy is your numbers array which of
course tensorflow is integrated heavily
with Seaborn for our graphics and the
Seaborn as SNS is going to be set on top
of our matplot library which we import
as MPL and then of course we're going to
import our matplot library pipelot as
PLT and right off the bat we're going to
set some graphic colors patch Force Edge
color equals true the style we're going
to use the 538 style you can look this
all up there's when you get into matplot
Library into Seabourn there are so many
options in here it's just kind of nice
to make it look pretty when we start the
when we start up that way we don't have
to think about it later on
uh and then we're going to take and we
have our mplrc we're going to put a
patch Ed color dim Gray Line with again
this is all part of our Graphics here in
our setup we'll go ahead and do an
interactive shell node interactivity
equals last expression uh here we are PD
for pandas options display Max columns
so we don't want to display more than 50
and then our matplot library is going to
be inline this is a Jupiter notebook
thing the matplot library inline then
warnings we're going to filter our
warnings and we're just going to ignore
warnings that way when they come up we
don't have to worry about them not
really what you want to do when you're
working on a major project you want to
make sure you know those warnings and
then
filter them out and ignore them later on
and if we run this it's just going to be
loading all that into the background
so that's a little back-end kind of
stuff then we want to go ahead and do is
we want to go ahead and import our
specific packages that we're going to be
working with which is under Karas now
remember cross kind of sits on
tensorflow so when we're importing cross
and the sequential model we are in
effect importing
tensorflow underneath of it we just
brought in the math probably should have
put that up above
and then we have our cross models we're
going to import sequential now if you
remember from our slide there was three
different options let me just flip back
over there so we can have a quick uh
recall on that and so in Cross we have
sequential functional and subclassing so
remember those three different setups in
here we talked about earlier and if you
remember from here we have uh sequential
where it's going one tensorflow layer at
a time you go kind of look at think of
it as going from left to right or top to
bottom or whatever Direction it's going
in but it goes in One Direction all the
time where functional can have a very
complicated graph of directions you can
have the data split into two separate
tensors and then it comes back together
into another tensor all those kinds of
things and then subclassing is really
the really complicated one where now
you're adding your own subclasses into
the tensor to do external computations
right in the middle of like a huge flow
of data but we're going to stick with
sequential it's not a big jump to go
from sequential to functional but we're
running a sequential tensorflow and
that's what this first import is here we
want to bring in our sequential and then
we have our layers and let's talk a
little bit about these layers this is
where cross and tensorflow really are
happening this is what makes them so
nice to work with is all these layers
are pre-built so from Cross we have
layers import dents from Cross layers
import LST M when we talk about these
layers
cross has so many built-in layers you
can do your own layers the dense layer
is your standard neural network by
default it uses relu for its activation
and then the lstm is a long short term
memory layer since we're going to be
looking probably at sequential data uh
we want to go ahead and do the lstm and
if we go into
cross and we look at their layers this
is a cross website you can see as we
scroll down for the cross layers that
are built in we can get down here and we
can look at let's see here we have our
layer activation our base layers
um activation layer weight layer weights
just a lot of stuff in here we have the
rayleu which is the basic activation
that was listed up here for layer
activations you can change those and
here we have our core layers and our
dense layers we have an input layer a
dense layer and then we've added a more
customized one with the long-term
short-term memory layer and of course
you can even do your own custom layers
in Karas there's a whole functionality
in there if you're doing your own thing
what's really nice about this is it's
all built in even the convolutional
layers this is for processing Graphics
there's a lot of cool things in here you
can do this is why cross is so popular
it's open source and you have all these
tools right at your fingertips so from
Cross we're just going to import a
couple layers the dense layer
and the long short term memory layer and
then of course from sklearn our site kit
we want to go ahead and do our min max
scalar standard scaler for pre-editing
our data and then metrics just so we can
take a look at the errors and compute
those let me go ahead and run this and
that just loads it up we're not
expecting anything from the output and
our file coming in is going to be air
quality.csv let's go ahead and take a
quick look at that this is in Open
Office it's just a standard you know we
can do excel whatever you're using for
your spreadsheet and you can see here we
have a number of columns a number of
rows that actually goes down to like 8
000
the first thing we want to notice is
that the first row is kind of just a
random number put in going down probably
not something we're going to work with
the second row is Bandung I'm guessing
that's a reference for the profile if we
scroll to the bottom which I'm not going
to do because it takes forever to get
back up they're all the same the same
thing with the status the status is the
same we have a date so we have a
sequential order here
here is the jam which I'm going to guess
is the time stamp on there so we have a
date and time
we have our O3 CEO NO2 reading SO2
noco2 VOC and then some other numbers
here pm1 PM 2.5 pm4 pm10
without actually looking through the
data I mean some of this I can guess is
like temperature humidity I'm not sure
what the PMS are but we have a whole
slew of data here so we're looking at
air quality as far as an area and a
region and what's going on with our date
time stamps on there and so code wise
we're going to read this into a pandas
data frame so our data frame DF is a
nice abbreviation commonly used for data
frames equals pd.read CSV and then our
the path to it just happens to be on my
D drive separated by spaces and so if we
go ahead and run this
we'll print out the head of our data and
again this looks very similar to what we
were just looking at
being in Jupiter I can take this and go
the other way make it real small so you
can see all the columns going across and
we get a full view of it
or we can bring it back up in size
that's pretty small on there overshot
but you can see it's the same data we
were just looking at we're looking at
the number we're looking at the profile
which is the bend done the date we have
a time stamp our O3 count Co and so
forth on here
and this is just your basic pandas
printing out the top five rows we could
easily have done three rows
five rows ten whatever you want to put
in there by default that's five for
pandas now I've talked about this all
the time so I know I've already set it
at least once or twice during this video
most of our work is in pre-formatting
data what are we looking at how do we
bring it together so we want to go ahead
and start with our date time let's come
in in two columns we have our date here
and we have our time
and we want to go ahead and combine that
and then we have this is just a simple
script in there that says combine date
time that's our formula we're building
our we're going to submit our pandas
data frame
and the tab name when we go ahead and do
this that's all of our information that
we want to go ahead and create and then
goes for Iron Range DF shape zero so
we're going to go through the whole
setup and we're going to list tab a pin
DF location I and here is our date going
in there and then return the numpy array
list tab d-types date time 64. that's
all we're doing we're just switching
this to a date time stamp and if we go
ahead and do DF date time equals combine
date time and then I always like to
print we'll do DF head
just so we can see what that looks like
and so when we come out of this we now
have our setup button here and of course
it's edited on to the far right here's
our date time you can see the formats
changed so there's our we've added in
the date time column and we've brought
the date over and we've taken this
format here and it's an actual variable
with a zero zero zero one here well that
doesn't look good so we need to also
include the time part of this we want to
convert it into hourly data so let's go
ahead and do that to do that to finish
combining our date time let's go ahead
and create a a little script here to
combine the time in there same thing we
just did we're just creating a numpy
array returning a numpy array and crit
forcing this into a date time format and
we can actually spend hours just going
through these conversions how do you
pull it from the pandas data frame how
do you set it up so I'm kind of skipping
through it a little fast because I want
to stay focused on tensorflow and cross
keep in mind this is like 80 percent of
your coding when you're doing a lot of
this stuff is going to be reformatting
these things resetting them back up so
that it looks right on here and you know
it just takes time to to get through all
that but that is usually what the
companies are paying you for that's what
the big bucks are for
and we want to go ahead and a couple
things going on here is we're going to
go ahead and do our date time we're
going to reorganize some of our setup in
here convert into hourly data we just
put a pause in there
um now remember we can select from DF
our different columns we're going to be
working with and you're going to see
that we actually dropped a couple of the
columns those ones I showed you earlier
they're just repetitive data so there's
nothing in there that exciting and then
we want to go ahead and we'll create a
second
data frame here let me just get rid of
the DF head
and df2 is we're going to group by date
time and we're looking at the mean value
and then we'll print that out so you can
see we're talking about
we have now reorganized this so we put
in date time 03 Co so now this is in the
same order as it was before and you'll
see the date time now has our zero zero
same date one two three and so on so
let's group the data together so there's
a lot more manageable and in the format
we want and in the right sequential
order
and if we go back to there we go our air
quality you can see right here we're
looking at
um
these columns going across we really
don't need since we're going to create
our own date time column we can get rid
of those these are the different Columns
of information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there that's all we've
done is reformatted our data
grouped it together by date and then you
can see the different data coming out
set up on there and then as a data
scientist first thing I want to do is
get a description what am I looking at
and so we can go ahead and do the df2
describe and this gives us our you know
describe gives us our basic data
analytics information we might be
looking for like what is the mean
standard deviation
minimum amount maximum amount we have
our first quarter second quarter and
third quarter
numbers also in there so you can get a
quick look at a glance describing the
data or descriptive analysis and even
though we have our quantile information
in here we're going to dig a little
deeper into that we're going to
calculate the quantile for each variable
we're going to look at a number of
things for each variable we'll see right
here q1 we can simply do the quantile
0.25 percent
which should match our 25 up here and
we'll be looking at the Min the Max and
we're just going to do this is basically
we're breaking this down for each uh
different
variable in there one of the things
that's kind of fun to do we're going to
look at that in just a second let me get
put the next piece of code in here I
clean out some of our
um we're going to drop a couple thing
our last rows and first row because
those have usually have a lot of null
values and the first row is just our
titles so that's important it's
important to drop those rows in here and
so this right here as we look at our
different quantiles again it's it's the
same and we're still looking at the 25
quantile here we're going to do a little
bit more with this
so now that we've cleared off our first
and last rows we're going to go ahead
and go through all of our columns and
this way we can look at each column
individually and so we'll just create a
q1 Q3 min max Min IQR Max IQR and
calculate the quantile of I of df2
we're basically doing the math that they
did up here but we're splitting it apart
that's all this is
and this happens a lot because you might
want to look at individual if this was
my own project I would probably spend
days and days going through what these
different values mean
one of the biggest data science uh
things we can look at that's important
is use your use your common sense you
know if you're looking at this data and
it doesn't make sense and you go back in
there and you're like wait a minute what
the heck did I just do at that point you
probably should go back and double check
what you have going on
now we're looking at this and you can
see right here here's our attribute for
our O3 so we've broken it down we have
our q1 5.88 Q3 10.37 if we go back up
here here's our 5 8 we've rounded it off
10.37s in there
so we've basically done the same math to
split it up we have our minimum and our
Max IQR and that's computed let's see
where is it here we go uh q1 minus 1.5
times IQR and the IQR is your Q3 minus
q1 so that's the difference between our
two different quarters this is all data
science as far as the hard math I'm
really not that we're actually trying to
focus on cross and tensorflow you still
got to go through all this stuff I told
you eighty percent of your programming
is going through and understanding what
the heck happened here
what's going on what does this data mean
and so when we're looking in that we're
going to go ahead and say hey
um
we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your minimum IQR
that means something's going wrong there
and usually in this case is going to
show us an outlier so we want to go
ahead and find the minimum value if it's
less than the minimum minimum IQR it's
an outlier and if the max value is
greater than the max IQR we have an
outlier and that's all this is doing low
outliers found minimum value High
outlier is found really important
actually outliers are almost everything
in data sometimes sometimes you do this
project just to find the outliers
because you want to know crime detection
what are we looking for we're looking
for the outliers what doesn't fit a
normal business deal and then we'll go
ahead and throw in just threw in a lot
of code oh my goodness uh so we have if
your max is greater than IQR print
outlier is found what we want to do is
we want to start cleaning up these
outliers and so we want to convert we'll
do create a convert Nan x max IQR equals
Max underscore IQR men IQR equals Min
IQR so this is just saying this is the
data we're going to send that's all that
is in Python and if x is greater than
the max IQR and X is less than the Min
IQR x equals null we're going to set it
to null why because we want to clear
these outliers out of the data now again
if you're doing fraud detection you
would do the opposite you would be
cleaning everything else this not in
that Series so that you can look at just
the outlier and then we're going to
convert the Nan hum again we have x max
iqrs 100 percent Min iqrs Min IQR if x
is greater than Max IQR and X is less
than Min IQR again we're going to return
null value otherwise it's going to
remain the same value x x equals X
and you can see as we go through the
code if I equals our hum then we go
ahead and do that's the that's a column
specific to humidity that's your hem
column then we're going to go ahead and
convert do the run a map on there and
convert the none hum you can see here
it's this is just clean up we run we've
found out that humidity probably has
some weird values in it we have our
outliers
that's all this is and so when we go
ahead and finish this and we take a look
at our outliers and we run this code
here
we have a low outlier 2.04 we have a
high outlier 99.06 outliers have been
interpolated
that means we've given them a new value
chances are these days when you're
looking at something like these sensors
coming in they probably have a failed
sensor in there something went wrong
that's the kind of thing that you really
don't want to do your data analysis on
so that's what we're doing is we're
pulling that out and then converting it
over and setting it up method linear
so we interpolate method linears it's
going to fill that data in based on a
linear regression model of similar data
same thing with this up here with the
df2i interpolate that's what we're doing
again this is all data prep we're not
actually talking about tensorflow we're
just trying to get all our data
set up correctly so that when we run it
it's not going to cause problems or have
a huge bias
so we've dealt with outliers
specifically in humidity
and again this is one of these things
where when we start running
um
we ran through this you can see down
here that we have our outliers found
high low outliers
migrated them in we also know there's
other issues going on with this data how
do we know that some of it's just
looking at the data playing with it
until you start understanding what's
going on let's take the temp value and
we're going to go ahead and and use a
logarithmic function on the temp value
and it's interesting because it's like
how do you how do you have to even know
to use logarithmic on the temp value
that's domain specific we're talking
about being an expert in air care I'm
not an expert in Air Care
um you know it's not what I go look at I
don't look at Air Care data in fact this
is probably the first Air Care data
setup I've looked at but the experts
come in there and they come to even say
hey in data science this is a
exponentially very variable on here so
we need to go ahead and do
transform it
and use a logarithmic scale on that
so at that point that would be coming
from your
data here we go data science programmer
overview does a lot of stuff connecting
the database and connecting in with the
experts data analytics a lot of times
you're talking about somebody who is a
data analysis might be all the way using
a PhD level data science programming
level interfaces database manager that's
going to be the person who's your admin
working on it
so when we're looking at this we're
looking at something they've sent to me
and they said hey domain Air Care this
needs to be this is a skew because the
data just goes up exponentially and
affects everything else and we'll go
ahead and take that data let me just go
ahead and run this
just for another quick look at it we
have our uh we'll do a distribution DF
we'll create another data frame from the
temp values and then from a data set
from the log temp so we put them side by
side and we'll just go ahead and do a
quick histogram this is kind of nice
plot a figure figure size here's our PLT
from matplot library and then we'll just
do a distribution underscore DF there's
our data frames this is nice because it
just integrates the histogram right into
pandas love pandas and this is a chart
you would send back to your data
analysis and say hey is this what you
wanted this is how the data is
converting on here as a data science
scientist the first thing I note is
we've gone from a 10 20 30 scale to 2.5
3.0 3.5 scale
and the data itself has kind of been
adjusted a little bit based on some kind
of a skew on there so let's jump into
we're getting a little closer to
actually doing our
cross on here we'll go ahead and split
our data up and this of course is any
good data scientists you want to have a
training set and a test set and we'll go
ahead and do the train size
we're going to use 0.75 percent of the
data make sure it's an integer we don't
want to take a slice as a float value
give you a nice error and we'll have our
train size is 75 percent and the test
size is going to be of course the train
size minus the length of the data set
and then we can simply do train comma
test here's our data set
which is going to be the train size the
test size and then if we go and print
this let me just go ahead and run this
we can see how these values split it's a
nice split of 1298 and then 433 points
of value that are going to be for our
setup on here and if you remember we're
specifically looking at the data set
where did we create that data set from
that was from up here that's what we
called the logarithmic value of the temp
that's where the data set came from so
we're looking at just that column with
this train size and the test with the
train and test data set here and let's
go ahead and do uh converted an array of
values into a data set Matrix we're
going to create a little
um
setup in here we create our data set our
data set is going to come in we're going
to do a look back of one so we're going
to look back one piece of data going
backward
and we have our data X and our data y
for Iron Range length of data set look
back minus one
this is creating let me just go ahead
and run this actually the best way to do
this
is to go ahead and create this data and
take a look at the shape of it let me go
ahead and just put that code in here
so we're going to do a look back one
here's our trainex our train Y and it's
going to be adding the data on there and
then when we come up here
and we take a look at the shape
there we go and we run this piece of
code here
we look at the shape on this and we have
a new slightly different change on here
but we have a shape of X 1296 comma 1
shape of Y train y Test X text y
and so what we're looking at is that
the X comes in
and we're only having a single value out
we want to predict what the next one is
that's what this little piece of code is
here for what are we looking for well we
want to look back one that's the um then
what we're going to train the data with
is yesterday's data yesterday says Hey
the humidity was at 97 what should
today's humidity be at if it's 97
yesterday is it going to go up or is it
going to go down today if 97 does it go
up to 100 what's going on there and so
our we're looking forward to the next
piece of data which says Hey tomorrow's
is going to you know today's humidity is
this this is what tomorrow's humidity is
going to be that's all that is all that
is is stacking our data so that our Y is
basically X Plus 1 or X could be y minus
1.
and then a couple things to note is our
X data we're only dealing with the one
column but you need to have it in a
shape that has it by The Columns so you
have the two different numbers and since
we're doing just a single point of data
we have and you'll see with the train y
we don't need to have the extra shape on
here now this is going to run into a
problem and the reason is is that we
have what they call a Time step
and the time step is that long-term
short-term memory layer so we're going
to add another reshape on here let me
just go down here and put it into the
next cell and so we want to reshape the
input array in the form of sample time
step features
we're only looking at one feature
and I mean this is one of those things
when you're playing with this you're
like why am I getting an error in the
numpy array why is this giving me
something weird going on so we're going
to do is we're going to add one more
level on here instead of being 12.99 one
we want to go one more
and when they put the code together in
the back you can see we kept the same
shape the 12.99 we added the one
dimension and then we have our train X
shape one and this could have depends
again on how far back in the long
short-term memory you want to go that is
what that piece of code is for and that
reshape is and you can see the new shape
is now one uh 12.99 one one versus the
1299 one and then the other part of the
shape 432 one one again this is our TR
our X in and of course our test X and
then our Y is just a single column
because we're just doing one output that
we're looking for so now we've done our
80 percent
um you know that's all the the breading
all the code reformatting our data
bringing it in now we want to go ahead
and do the fun part which is we're going
to go ahead and create and fit the lstm
neural network and if we're going to do
that the first thing we need is we're
going to need to go ahead and create a
model and we'll do this sequential model
and if you remember sequential means it
just goes in order that means we have if
we have two layers the layers go from
layer one to Layer Two or layer 0 to
layer one this is different than
functional functional allows you to
split the data and run two completely
separate models and then bring them back
together we're doing just sequential on
here and then we decided to do the long
short term memory and we have our input
shape which it comes in again this is
what all this switching was we could
have easily made this one two three or
four going back as far as the end number
on there we just stuck to going back one
and it's always a good idea when you get
to this point where the heck is this
model coming from what kind of models do
we have available
and there's let me go and put the next
model in there uh because we're going to
do two models and the next model is
going to go ahead and we're going to do
dent so we have model equal sequential
and then we're going to add the lstm
model and then we're going to add a
dense model and if you remember from the
very top of our code
where we did the Imports oops there we
go our cross this is it right here
here's our importing a dense model and
here's our importing an lstm now just
about every tensorflow model uses dense
your dense model is your basic
forward propagation reverse propagation
error where it does reverse propagation
to program the model so any of your
neural networks you've already looked at
that lexan says here's the error and
sends the error backwards that's what
this is the long short term memory is a
little different the real question that
we want to look at right now is where do
you find these models what kind of
models do you have available and so for
that let's go to the Cross website which
is across.io if you go under API slash
layers and I always have to do a search
just search for cross API layers it'll
open up and you can see we have
your base layers right here class
trainable weights all kinds of stuff
like there your Activation so a lot of
your layers you can switch how it
activates relu which is like your
smaller arrays or if you're doing
convolutional neural networks the
convolution usually uses a relu your
sigmoid all the way up to softmax soft
plus all these different choices as far
as how those are set up and what we want
to do is we want to go ahead and if you
scroll down here you'll see your core
layers and here is your dense layer so
you have an input object your dense
layer your activation layer embedding
layer this is your your kind of your one
setup on there that's most common uh
convolutional neural networks or
convolutional layers these are like for
doing uh image categorizing uh so trying
to find objects in a picture that kind
of thing uh we have pooling layers so as
you have the layers come together
um usually you bring them down into a
single layer although we can still do
like Global Max pulling 3D and there's
just I mean this list just goes on and
on there's all kinds of different things
hidden in here as far as what you can do
and it changes you know you go in here
and you just have to do a search for
what you're looking for and figure out
what's going to work best for you
as far as which project you're working
on long short term memory is a big one
because this is when we start talking
about text what if someone says the what
comes after the The Cat in the Hat
little kids book there
starts programming it and so you really
want to know not only
um what's going on but it's going to be
something that has a history the history
behind it tells you what the next one
coming up is now once we've built all
our different you know we built our
model we've added our different layers
we went in there play with it remember
if you're in functional you can actually
link these layers together and they
Branch out and come back together if you
do a uh the sub
set up then you can create your own
different model you can embed a model in
there that might be coming linear
regression you can embed a linear
regression model as part of your
functional split and then have that come
back together with other things so we're
going to go ahead and compile your model
this brings everything together we're
going to put in what the loss is which
we'll use the mean squared error and
we'll go ahead and use the atom
Optimizer clearly there's a lot of
choices on here depending on what you're
doing and just like any of these
different prediction models if you've
been doing any PSI kit from python
you'll recognize that we have to then
fit the model
so what are we doing in here we're going
to send in our train X our train y we're
going to decide how many epics we're
going to run it through 500 is probably
a lot for this I'm guessing it'd
probably be about two or three hundred
probably do just fine our batch size
so how many different when you process
it this is the math behind it if you're
in data analytics you might try to know
what this number is as a data scientist
where I haven't had the PHD level math
that says this is why you want to use
this particular batch size you kind of
play with this number a little bit you
can dig deeper into the math
see how it affects the results depending
on what you're doing and there's a
number of other settings on here we did
verbose two I'd have to actually look
that up to tell you what verbose means I
think that's actually the default on
there if I remember correctly there's a
lot of different settings when you go to
fit it the big ones are your epic and
your batch size those are what we're
looking for
and so we're going to go ahead and run
this
and this is going to take a few minutes
to run because it's going through
500 times through all the data so if you
have a huge data set this is the point
where you're kind of wondering oh my
gosh is this going to finish tomorrow
if I'm running this on a single machine
and I have a terabyte terabyte of data
going into it
if this is my personal computer and I'm
running a terabyte of data into this you
know this is running rather quickly
through all 500 iterations but you got a
terabyte of data we're talking something
closer to days week
you know even with a
3.5 gigahertz machine in in eight cores
it's still going to take a long time to
go through a full terabyte of data
and then we want to start looking at
putting it into some other framework
like spark or something that will
prevail the process on there more across
multiple processors and multiple
computers
and if we scroll all the way down to the
bottom you're going to see here's our
square mean error 0.0088
if we scroll way up you'll see it kind
of oscillates between 0.088 and 08089
it's right around 2 to 250 where you
start seeing that oscillation where it's
really not going anywhere so we really
didn't need to go through a full 500
epics
uh you know if you're retraining this
stuff over and over again it's kind of
good to note where that error zone is so
you don't have to do all the extra
processing of course if you're going to
build a model
we want to go ahead and run a prediction
on it
so let's go ahead and make our
prediction remember we have our training
test set and our test set or the we have
the
trainx and the train y for training it
or train predict and then we have our
test X and our test y going in there so
we can test to see how good it did
and we come in here we have you'll see
right here we go ahead and do our train
predict equals model predict train X
and test predict model predict Test X
why would we want to run the prediction
on trainx well it's not a hundred
percent on its prediction we know it has
a certain amount of error and we want to
compare the error we have on what we
programmed it with with the error we get
when we run it on new data that's never
C the model's never seen before and one
of the things we can do we go ahead and
invert the predictions this helps us
level it off a little bit more
get rid of some of our bias we have
train predict equals and NP
exponential M1 the train predict and
then train y equals the exponential M1
for train Y and then we do it again that
with train test predict and test y
um again reformatting the data so that
we can it all matches and then we want
to go ahead and calculate the root mean
square error so we have our train score
which is your math square root times the
mean square root error train Y and train
predict and again we're just um this is
just feeding the data through so we can
compare it and the same thing with the
test
and let's take a look at that because
really the code makes sense if you're
going through it line by line you can
see we're doing but the answer really
helps to zoom in so we have a train
score which is
2.40 of our root mean square error
and we have a test score of 3.16 of the
root mean square error
if these were reversed if our test score
is better than our training score then
we've over trained something's really
wrong at that point you got to go back
and figure out what you did wrong
because you should never have a better
result on your test data than you do on
your training data and that's how we put
them both through that's why we look at
the error for both the training and the
testing when you're going out and
quoting your publishing this you're
saying hey how good is my model it's the
test score that you're showing people
this is what it did on my test data that
the model had never seen before this is
how good my model is and a lot of times
you actually want to put together like a
little formal code
where we actually want to print that out
and if we print that out you can see
down here
test prediction and standard deviation
of data set 3.16 is less than
4.40 I'd have to go back and we're up
here if you remember we did the square
means error this is standard deviation
that's why these numbers are different
it's saying the same thing that we just
talked about
3.16 is less than 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
and along with putting a formal print
out of there we want to go ahead and
plot what's going on
uh and this we just want a pretty graft
here so that people can see what's going
on when I walk into a meeting and I'm
dealing with a number of people they
really don't want these numbers they
don't want to say hey what's I mean
standard deviation unless you know what
statistics are you might be dealing with
a number of different departments head
of sales might not works with standard
deviation or have any idea what that
really means number wise and so at this
point we really want to put it in a
graph so we have something to display
and with displaying you gotta remember
that we're looking at the data today
going into it and what's going to happen
tomorrow
so let's take a quick look at this we're
going to go ahead and shift the train
predictions for plotting we have our
train predict plot
NP MP like data set train predict plot
set it up with null values
you know it's just kind of it's kind of
a weird thing where we're creating the
um the data groups as we like them and
then putting the data in there is what's
going on here so we have our train
predict plot which is going to be our
look back our length plus look back
we're just it's going to equal train uh
train predict so we're creating this
basically we're taking this and we're
dumping the train predict into it so now
we have our nice train predict plot
and then we have the shift test
predictions for the plotting we're going
to continue more of that oops looks like
I put it in here double no it's just uh
yeah they put it in here double
um didn't mean to do that
we really only need to do it once oh
here we go
um this is where the problem was is
because this is the test predict
so we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
data sets uh test predict plot length
prediction set up on there
and then we're going to go through the
plotting the original data set and the
predictions so we have a Time axes
always nice to have your time set on
there set that to the time array
time axes lap all this is setting up the
time variable for the bottom and then we
have a lot of stuff going on here as far
as setting up our figure
let's go ahead and run that and then
we'll break it down
we have on here our main plot we have
two different plots going on here the
ispu going up in the data and the ispu
here with all these different settings
on it
and so we look at this we have our ax1
that's the main plot I mean our ax
that's the main plot and we have our ax1
which is the secondary plot over here so
we're doing a figure PLT or plt.figure
and we're going to dump those two graphs
on there and so we take and if you go
through the code piece by piece which
we're not going to do we're going to do
the the
data set here
exponential reverse exponential so it
looks correctly we're going to label it
the original data set we're going to
plot the train predict plot that's what
we just created we're going to make that
orange and we'll label it train
prediction test predicts plot we're
going to make that red and label it test
prediction and so forth set our ticks up
let's actually just put ticks time axes
gets its ticks the little little marks
that are going along the axes that kind
of thing and let's take a look and see
what these graphs look like
and these are just kind of fun you know
when you show up into a meeting and this
is the final output and you say hey this
is what we're looking at here's our
original data in blue
here's our training prediction you can
see that it trains pretty close to what
the data is up there
I would also probably put a like a
little little time stamp and do just
right before and right after where we go
from train to test prediction and you
can see with the test prediction the
data comes in in red
and then you can also see what the
original data set looked like behind it
and how it differs and then we can just
isolate that here on the right that's
all this is is just the test prediction
on the right uh and it's you know
there's you'll see what the original
data set there's a lot of Peaks we're
missing and a lot of lows were missing
but as far as the actual test prediction
it's pretty does pretty good it's pretty
right on you can get a good idea what to
expect for your ispu and so from this we
would probably publish it and say hey
this is what you expect and this is our
area of this is a range of error that's
the kind of thing I put out
on a daily basis maybe we predict the
sales are going to be this or maybe
weekly so you kind of get a nice you
kind of flatten the um data coming out
and you say hey this is what we're
looking at the big takeaway from this is
that we're working with
let me go back up here oops oh too far
there we go
um is this model here this is what this
is all about we work through all of
those pieces all the tensorflows and
that is to build this sequential model
and we're only putting in the two layers
this can get pretty complicated if you
get too complicated it never it never
verges into a usable model so if you
have like 30 different layers in here
there's a good chance you might crash it
kind of thing so don't go too haywire on
that and that you kind of learn as you
go again it's domain knowledge and also
starting to understand all these
different layers and what they mean
the data
analytics behind those layers is
something that your data analysis
professional would come in and say this
is what we want to try
but I tell you as a data scientist a lot
of these basic setups are common and I
don't know how many times
working with somebody and they're like
oh my gosh if I only did a tangent H
instead of a relu activation I worked
for two weeks to figure that out well as
a data science I can run it through the
model in you know five minutes instead
of spending two weeks doing the the math
behind it so that's one of the
advantages of data scientists is we do
it from programming side in a data
analytics is going to look for it how
does it work in math and this is really
the core right here of tensorflow and
cross is being able to build your data
model quickly and efficiently and of
course with any data science putting out
a pretty graph so that your shareholders
again we want to take and reduce the
information down to something people can
look at and say oh that's what's going
on they can see stuff what's going on is
as far as the dates and the change in
the ispu join us on a journey into the
fascinating world of AI and machine
learning with that Caltech postgraduate
program in Ai and machine learning in
partnership with IBM this a and ml
course scores the latest tools and
Technologies from the AI ecosystem and
features master classes by Caltech
faculty IBM experts hackathons and ask
me anything sessions this program
showcases Caltech ctm is excellence and
IBM's industry progress the artificial
intelligence course course key Concepts
like statistics data science with python
machine learning deep learning NLP
reinforcement Lending and through an
Interactive Learning model with live
sessions enroll now analog exciting Ai
and ml opportunities the course link is
mentioned in the description box so what
is Apache spark let's learn about this
Apache spark is a open source in memory
Computing framework or you could say
data processing engine which is used to
process data in batch and also in real
time across various cluster computers
and it has a very simple programming
language behind the scenes that is Scala
which is used although if users would
want to work on spark they can work with
python they can work with Scala they can
work with Java and so on even R for that
matter so it supports all these
programming languages and that's one of
the reasons that it is called polyglot
wherein you have good set of libraries
and support from all the programming
languages and developers and data
scientists incorporate spark into their
application Asians or build spark based
applications to process analyze query
and transform data at a very large scale
so these are key features of Apache
spark now if you compare Hadoop West
spark we know that Hadoop is a framework
and it basically has mapreduce which
comes with Hadoop for processing data
however processing data using mapreduce
in Hadoop is quite slow because it is a
batch oriented operation and it is time
consuming if you if you talk about spark
spark can process the same data 100
times faster than mapreduce as it is a
in-memory Computing framework well there
can always be conflicting ideas saying
what if my spark application is not
really efficiently coded and my
mapreduce application has been very
efficiently coded well then it's a
different case however normally if you
talk about Co code which is efficiently
written for map reduce or for spark
based processing spark will win the
battle by doing almost 100 times faster
than mapreduce so as I mentioned Hadoop
performs batch processing and that is
one of the paradigms of mapreduce
programming model which involves mapping
and reducing and that's quite rigid so
it performs batch processing the
intermittent data is written to sdfs and
written right back from sdfs and that
makes hadoop's mapreduce processing
slower in case of spark it can perform
both batch and real-time processing
however a lot of use cases are based on
real time processing take an example of
Macy's take an example of retail giant
such as Walmart and there are many use
cases who would prefer to do real time
processing or I would say near real-time
processing so when we say real time or
near real time it is about processing
the data as it comes in or your are
talking about streaming kind of data now
Hadoop or hadoop's map reduce obviously
was started to be written in Java now
you could also write it in Scala or in
Python however if you talk about
mapreduce it will have more lines of
code since it is written in Java and it
will take more times to execute you have
to manage the dependencies you have to
do the right declarations you have to
create your mapper and reducer and
Driver classes however if you compare
spark it has few lines of code as it is
implemented in Scala and Scala is a
statically typed dynamically inferred
language it's very very concise and the
benefit is it has features from both
functional programming and object
oriented language and in case of Scala
whatever code is written that is
converted into byte codes and then it
runs in the jvm now Hadoop supports
Kerberos authentication there are
different kind of authentication
mechanisms kerberosis one of the
well-known ones and it can really get
difficult to manage now spark supports
authentication via a shared secret it
can also run on yarn leveraging the
capability of capitals so what are spark
features which really makes it unique or
in demand processing framework when we
talk about spark features one of the key
features is fast processing so spark
contains resilient distributed data sets
so rdds are the building blocks for
spark and we learn more about rdds later
so spark contains rdds which saves huge
time taken in reading and writing
operations so it can be 100 times or you
can say 10 to 100 times faster than
Hadoop when we say in memory Computing
here I would like to make a note that
there is a difference between caching
and in-memory Computing think about it
caching is mainly to support read ahead
mechanism where you have your data
pre-loaded so that it can benefit
further queries however when we say in
memory Computing we are talking about
lazy evaluation we are talking about
data being loaded into memory only and
only when a specific kind of action is
invoked so data is stored in Ram so here
we can say Ram is not only used for
processing but it can also be used for
storage and we can again decide whether
we would want that Ram to be used for
persistence or just for computing so it
can access the data quickly and
accelerate the speed of analytics now
spark is quite flexible it supports
multiple languages as I already
mentioned and it allows the developers
to write applications in Java Scala r or
python it's quite fault tolerance so
spark contains these rdds or you could
say execution logic or you could say
temporary data sets which initially do
not have any data loaded and the data
will be loaded into rdds only when
execution is happening so these can be
fault tolerant as these rdds are
distributed across multiple nodes so
failure of one worker node in the
cluster will really not affect the rdds
because that portion can be recomputed
so it ensures loss of data it ensures
that there is no data loss and it is
absolutely fault tolerant it is for
better than analytics so sparked has
Rich set of SQL queries machine learning
algorithms complex analytics all of this
supported by various spark components
which we will learn in coming slides
with all these functionalities analytics
can be performed better in terms of
spark so these are some of the key
features of spark however there are many
more features which are related to
different components of spark and we
will learn about them so what are these
components of spark which I'm talking
about spark core so this is the core
component which basically has rdds which
has a core engine which takes care of
your processing now you also have spark
SQL so people who would be interested in
working on structured data or data which
can be structurized would want to prefer
using spark SQL and Spark SQL internally
has components or features like data
frames and data sets which can be used
to process your structured data in a
much much faster way you have spark
streaming now that's again an important
component of spark which allows you to
create your spark streaming applications
which not only works on data which is
being streamed in or data which is
constantly getting generated but you
would also or you could also transform
the data you could analyze or process
the data as it comes in in smaller
chunks you have Sparks mlib now this is
basically a set of libraries which
allows developers or data scientists to
build their machine learning algorithms
so that they can do Predictive Analytics
or prescriptive descriptive preemptive
analytics or they could build their
recommendation systems or bigger smarter
machine learning algorithms using these
libraries and then you have Graphics so
think about organizations like LinkedIn
or say Twitter where you have data which
naturally has a network kind of flow so
data which could be represented in the
form of graphs now here when I talk
about graphs I'm not talking about pie
charts or bar charts but I'm talking
about Network related data that is data
which can be networked together which
can have some kind of relationship think
about Facebook think about LinkedIn
where you have one person connected to
other person or one company connected to
other companies so if we have our data
which can be represented in the form of
network graphs then spark has a
component called Graphics which allows
you to do graph based processing so
these are some of the components of
Apache spark spark core spark SQL spark
streaming spark ml lib and Graphics so
to learn more about components of spark
let's learn here about spark core now
this is the base engine and this is used
for large scale parallel and distributed
data processing so when you work with
spark at least and the minimum you would
work with is spark core which has rdds
as the building blocks of your spark so
it is responsible for your memory
management your fault recovery
scheduling Distributing and monitoring
jobs on a cluster and interacting with
storage systems so here I would like to
make a key point that Spar by itself
does not have its own storage it relies
on storage now that storage could be
your sdfs that is hadoop's distributed
file system it could be a database like
nosql database such as hbase or it could
be any other database say rdbms from
where you could connect your spark and
then fetch the data extract the data
process it analyze it so let's learn a
little bit about your rdds resilient
distributed data sets now spark core
which is the base engine or the core
engine is embedded with the building
blocks of spark which is nothing but
your resilient distributed data set so
as the name says it is resilient so it
is existing for a shorter period of time
distributed so it is distributed across
nodes and it is a data set where the
data will be loaded or where the data
will be existing for processing so it is
immutable fault tolerant distributed
collection of objects so that's what
your rdd is and there are mainly two
operations which can be performed on an
RDP now to take an example of this say I
want to process a particular file now
here I could write a simple code in
Scala and that would basically mean
something like this so if I say well
which is to declare the variable I would
say well X and then I could use what we
call as spark context which is basically
the most important entry point of your
application so then I could use a method
of spark context for example that is
text file and then I could point it to a
particular file so this is just a method
of your spark context and Spark context
is the entry point of your application
now here I could just give a path in
this method so what does this step do it
does not do any evaluation so when I say
Val X I'm creating a immutable variable
and to that variable I'm assigning a
file now what this step does is it
actually creates a rdd resilient
distributed data set so we can imagine
this as a simple execution logic a EMT
data set which is created in memory of
your node so if I would say I have
multiple nodes in which my data is split
and stored I am imagining that your yarn
your spark is working with Hadoop so I
have Hadoop which is using say two nodes
and this is my distributed file system
sdfs which basically means my file is
written to hdfs and it also means that
the file related blocks are stored in
the underlying disk of these machines so
when I say Val x equals SC dot text file
that is using a method of spark context
now there are various other methods like
whole text files parallelize and so on
this step will create an rdd so you can
imagine this as a logical data set which
is created in memory memory across these
nodes because these nodes have the data
however no data is loaded here so this
is the first rdd and I can say first
step in what we call as a dag a dag
which will have series of steps which
will get executed at later stage now
later I could do further processing on
this I could say well Y and then I could
do something on X so I could say x dot
map and I would want to apply a function
to every record or every element in this
file and I could give a logic here x dot
map now this second step is again
creating an rdd a resilient distributed
data set you can say second step in my
dag okay and here you have a external
rdd one more rdd created which depends
on the first RTD so my first RTD becomes
the base rdd or parent rdd and the
resultant and RTD becomes the child rdd
then we can go further and we could say
well Z and I would say okay now I would
want to do some filter on y so this
filter which I am doing here and then I
could give a logic might be I'm
searching for a word I am searching for
some pattern so I could say well Z
equals y dot filter which again creates
one more rdd a resilient distributed
data set in memory and a you can say
this is nothing but one more step in the
dag so this is my tag which is a series
of steps which will be executed now here
when does the execution happen when the
data get when will the data get loaded
into these rdds so all of this that is
using a method using a transformation
like map using a transformation like
filter or flat map or anything else
these are your Transformations so the
operations such as map filter join Union
and many others will only create rdds
which basically means it is only
creating execution Logic No data is
evaluated no operation is happening
right now only and only when you invoke
an action that is might be you want to
print some result might be you want to
take some elements and see that might be
you want to do a count so those are
actions which will actually trigger the
execution of this dag right from the
beginning so if I hear say Z dot count
where I would want to just count the
number of words which I am filtering
this is an action which is invoked and
this will trigger the execution of dag
right from the beginning so this is what
happens in a spark now if I do a z dot
count again it will start but the whole
execution of dag again right from the
beginning so my Z dot count second time
in action is invoked again the data will
be loaded in the first RTD then you will
have map then you will have filter and
finally you will have result so this is
the core concept of your rdds and this
is how RTD works so mainly in spark
there are two kind of operations one is
your Transformations and one is your
actions Transformations or using a
method of spark context will always and
always create an RTD or you could say a
step in the tag actions are something
which will invoke the execution which
will invoke the execution from the first
rdd till the last rdd where you can get
your result so this is how your rdds
work now when we talk about components
of spark let's learn a little bit about
spark SQL so spark SQL is a component
type processing framework which is used
for structured and semi-structured data
processing so usually people might have
their structured data stored in rdbms or
in files where data is structured with
particular delimiters and has a pattern
and if one wants to process the
structured data if one wants to use
spark to do in memory processing and
work on the structured data they would
prefer to use spark SQL so you can work
on different data formats say CSV Json
you can even work on smarter formats
like Avro parquet even your binary files
or sequence files you could have your
data coming in from an rdbms which can
then be extracted using a jdbc
connection so at the bottom level when
you talk about spark SQL it has a data
source API which basically allows you to
get the data in whichever format it is
now spark SQL has something called as
data frame API so what are data frames
data frames in short you can visualize
or imagine as rows and columns or if
your data can be represented in the form
of rows and columns with some column
headings so data frame API allows you to
create data frames so like my previous
example when you work on a file when you
want to process it you would convert
that into an rdd using a method of smart
context or by doing some Transformations
so in the similar way when you use data
frame so when you want to use spark SQL
you would use
Sparks context which is SQL context or
Hive context or spark which allows you
to work with data frames so like in my
earlier example we were saying Val x
equals SC dot text file now in case of
data frames instead of SC you would be
using say spark dot something so spark
context is available for your data
frames API to be used in older versions
like spark 1.6 and so on we were using
Hive context or SQL context so if you
were working with spark 1.6 you would be
saying well x equals SQL context dot
here we would be using spark dot so data
frame API basically allows you to create
data frames out of your structured data
which also lets spark know that data is
already in a particular structure it
follows a format and based on that your
Sparks backend dag scheduler right so
when I say about dag I talk about your
sequence of steps so spark is already
aware of what are the different steps
involved in your application so your
data frame API basically allows you to
create data frames out of your data and
data frames when I say I'm talking about
rows and columns with some headings and
then you have your data frame DSL
language or you can use spark SQL or
Hive query language any of these options
can be used to work with your data
frames so to learn more about data
frames follow in the next sessions when
you talk about spark streaming now this
is very interesting for organizations
who would want to work on streaming data
imagine a store like Macy's where they
would want to have machine learning
algorithms now what would these machine
learning algorithms do suppose you have
a lot of customers walking in the store
and they are searching for particular
product or particular item so there
could be cameras placed in the store and
this is being already done there are
cameras placed in the store which will
keep monitoring in which corner of the
store there are more customers now once
camera captures this information this
information can be streamed in to be
processed by algorithms and those
algorithms will see which product or
which series of product customers might
be interested in and if this algorithm
in real time can process based on the
number of customers based on the
available product in the store it can
come up with a attractive alternative
price so that which the price can be
displayed on the screen and probably
customers would buy the product now this
is a real-time processing where the data
comes in algorithms work on it do some
computation and give out some result and
which can then result in customers
buying a particular product so the whole
essence of this machine learning and
real-time processing will really hold
good if and when customers are in the
store or this could relate to even a
online shopping portal where there might
be machine learning algorithms which
might be doing real-time processing
based on the clicks which customer is
doing based on the clicks based on
customer history based on customer
Behavior algorithms can come up with
recommendation of products or better
altered price so that the sale happens
now in this case we would be seeing the
essence of real-time processing only in
a fixed or in a particular duration of
time and this also means that you should
have something which can process the
data as it comes in so spark streaming
is a lightweight API that allows
developers to perform batch processing
and also real-time streaming and
processing of data so it provides secure
reliable fast processing of live data
streams so what happens here in spark
streaming in brief so you have a input
data stream now that data stream could
be a file which is constantly getting
appended it could be some kind of
metrics it could be some kind of events
based on the clicks which customers are
doing or based on the products which
they are choosing in a store this input
data stream is then pushed in through a
spark streaming application now spark
streaming application will broke break
this content into smaller streams what
we call as discreticized streams or
batches of smaller data on which
processing can happen in frames so you
could say process my file every five
seconds for the latest data which has
come in now there are also some windows
based uh options like when I say windows
I mean a window of past three events
window of past three events each event
being of five seconds so your batches of
smaller data is processed by Spark
engine and this process data can then be
stored or can be used for further
processing so that's what spark
streaming does when you talk about mlib
it's a low level machine learning
library that is simple to use scalable
and compatible with various programming
languages now Hadoop also has some
libraries like you have Apache mahaut
which can be used for machine learning
algorithms however in terms of spark we
are talking about machine learning
algorithms which can be built using
mlibs libraries and then spark can be
used for processing so mlib eases the
deployment and development of scalable
machine learning algorithms I mean think
about your clustering techniques so
think about your classification where
you would want to classify the data
where you would want to do supervised or
unsupervised learning think about
collaborative filtering and many other
data science related techniques or
techniques which are required to build
your recommendation engines or machine
learning algorithms can be built using
Sparks ml lip Graphics is Spark's own
graph computation engine so this is
mainly if you are interested in doing a
graph based processing think about
Facebook think about LinkedIn where you
can have your data which can be stored
and that data has some kind of network
connections or you could say it is well
networked I could say x is connected to
y y is connected to z z is connected to
a so x y z a all of these are in terms
of graph terminologies we call as
vertices or vertex which are basically
being connected and the connection
between the
are called edges so I could say a is
friend to B so A and B are vertices and
friend a relation between them is The
Edge now if I have my data which can be
represented in the form of graphs if I
would want to do a processing in such
way this could be not only for social
media it could be for your network
devices it could be a cloud platform it
could be about different applications
which are connected in a particular
environment so if you have data which
can be represented in the form of graph
then Graphics can be used to do ETL that
is extraction transformation load to do
your data analysis and also do
interactive graph computation so graph x
is quite powerful now when you talk
about spark your spark can work with
your different clustering uh
Technologies so it can work with Apache
mesos that's how Spar came in where it
was initially to prove The credibility
of Apache mesos spark can work with yarn
which is usually you will see in
different working environments spark can
also work as Standalone that means
without Hadoop spark can have its own
setup with master and worker processes
so usually or you can say technically
spark uses a Master Slave architecture
now that consists of a driver program
that can run on a master node it can
also run on a client node it depends on
how you have configured or what your
application is and then you have
multiple executors which can run on
worker nodes so your master node has a
driver program and this driver program
internally has the spark context so your
spark Every Spark application will have
a driver program and that's driver
program has a inbuilt or internally used
spark context which is basically your
entry point of application for any spark
functionality so your driver or your
driver program interacts with your
cluster manager now when I say interacts
with clustered manager so you have your
spark context which is the entry point
that takes your application request to
the cluster manager now as I said your
cluster manager could be say Apache
mesos it could be yarn it could be spark
Standalone Master itself so your cluster
manager in terms of yarn is your
resource manager so your spark
application internally runs as series or
set of tasks and processes your driver
program wherever that is run will have a
spark context and Spark context will
take care of your application execution
how does that do it spark context will
talk to Cluster manager so your cluster
manager could be on and in terms of when
I say cluster manager for yarn would be
resource manager so at high level we can
say a job is split into multiple tasks
and those tasks will be distributed over
the slave nodes or worker nodes so
whenever you do some kind of
transformation or you use a method of
spark context and rdd is created and
this rdd is distributed across multiple
nodes as I explained earlier worker
nodes are the slaves that run different
tasks so this is how a spark
architecture looks like now we can learn
more about spark architecture and its
interaction with yarn so usually what
happens when your spark context
interacts with the cluster manager so in
terms of yarn I could say resource
manager now we already know about yarn
so you would have say node managers
running on multiple machines and each
machine has some RAM and CPU cores
allocated for your node manager on the
same machine you have the data nodes
running which obviously are there to
have the Hadoop related data so whenever
a application wants to process the data
your application via spark context
contacts the cluster managers that is
resource manager now what does resource
manager do resource manager makes a
request so resource manager makes
requests to the node manager of the
machines wherever the relevant data
resides asking for containers so your
resource manager is negotiating or
asking for containers from node manager
saying hey can I have a container of 1GB
RAM and one CPU core can I have a
container of 1GB RAM and one CPU core
and your node manager based on the kind
of processing it is doing will approve
or deny it so node manager would say
fine I can give you the container and
once this container is allocated or
approved by node manager resource
manager will basically start an extra
piece of code called App Master so App
Master is responsible for execution of
your application locations whether those
are spark applications or mapreduce so
your application Master which is a piece
of code will run in one of the
containers that is it will use the RAM
and CPU core and then it will use the
other containers which were allocated by
node manager to run the tasks so it is
within this container which can take
care of execution so what is a container
a combination of RAM and CPU core so it
is within this container we will have a
executed process which would run and
this executed process is taking care of
your application related tasks so that's
how overall spark Works in integration
with yarn now let's learn about the
spark cluster managers as I said spark
can work in a standalone mode so that is
without Hadoop so by default application
submitted to spark Standalone mode
cluster will run in fifo order and each
application will try to use all the
available nodes so you could have a
spark stand in loan class cluster which
basically means you could have multiple
nodes on one of the nodes you would have
the master process running and on the
other nodes you would have the spark
worker processes running so here we
would not have any distributed file
system because spark is Standalone and
it will rely on an external storage to
get the data or probably the file system
of the nodes where the data is stored
and processing will happen across the
nodes where your worker processes are
running you could have spark working
with Apache mesos now as I said Apache
mesos is a open source project to manage
your computer clusters and can also run
Hadoop applications Apache mesos was
introduced earlier and Spark came in and
as existence to prove The credibility of
Apache mesos you can have spark working
with hadoop's yarn this is something
which widely you will see in different
working environments so yarn which takes
care of your processing and can take
care of different processing Frameworks
also supports spark you could have
kubernetes now that is something which
is making a lot of news in today's world
it is a open source system for
automating deployment scaling and
management of containerized applications
so where you could have multiple Docker
based images which can be connecting to
each other so spark also works with
kubernetes now let's look at some
applications of spark so JPMorgan Chase
and company uses Spark to detect
fraudulent transactions analyze the
business pens of an individual to
suggest offers and identify patterns to
decide how much to invest and where to
invest so this this is one of the
examples of banking lot of banking
environments are using spark due to its
real-time processing capabilities and in
memory faster processing where they
could be working on fraud detection or
credit analysis or pattern
identification and many other use cases
Alibaba group that uses also spark to
analyze large data sets of data such as
real-time transaction details now that
might be based online or in the stores
looking at the browsing history in the
form of spark jobs and then provides
recommendations to its users so Alibaba
group is using spark in its e-commerce
domain you have IQ via now this is a
leading Healthcare company that uses
Spark to analyze patients data identify
possible health issues and diagnose it
based on their medical history so there
is a lot of work happening in healthcare
industry where real-time processing is
finding a lot of importance and real
time and faster processing is what is
required so healthcare industry and iqvi
is also using spark you have Netflix
which is known and you have Riot games
so entertainment and gaming companies
like Netflix and write games use Apache
spark to Showcase relevant
advertisements to their users based on
the videos that they have watched shared
or liked so these are few domains which
find use cases of spark that is banking
e-commerce Healthcare entertainment and
then there are many more which are using
spark in their day-to-day activities for
real time in memory faster processing
now let's discuss about the Spark's use
case and let's talk about conviva which
is world's leading video streaming
companies so video streaming is a
challenge now if you talk about YouTube
which has data you could always read
about it so YouTube has data which is
worth watching 10 years so that is huge
amount of data where people are
uploading their videos or companies are
doing advertisements and this videos are
streamed in or can be watched by users
so video streaming is a challenge and
especially with increasing demand for
high quality streaming experiences
conviva collects data about video
streaming quality to give their
customers visibility into the end user
experience they are delivering now how
do they do it Apache spark again using
Apache spark convert delivers a better
quality of service to its customers by
removing the screen buffering and
learning in detail about Network
conditions in real time this information
is then stored in the video player to
manage live video traffic coming in from
4 billion video feeds every month to
ensure maximum retention Now using
Apache spark conveyor has created an
auto diagnostics alert it automatically
detects anomalies along the video
streaming Pipeline and diagnosis the
root cause of the issue now this really
makes it one of the leading video your
streaming companies based on auto
diagnostic alerts it reduces waiting
time before the video starts it avoids
buffering and recovers the video from a
technical error and the whole goal is to
maximize the viewer engagement so this
is Spark's use case where conviva is
using spark in different ways to stay
ahead saying from pulling out different
links to pulling data off a website it's
a data scientist you might need to get
some information off a website that
doesn't have a direct API to pull that
information and in Python we have a
wonderful tool when you talk Python and
you talk web scraping we're talking
beautiful suit which is a package you
add into your python that you're running
and we can come over here to the website
www.crumi.com software beautiful soup
you can actually read a little bit about
it currently beautiful soup 4 is the
current version if you don't remember
the full website for you can always do
what I do which is go over and do a
search for beautiful soup official site
it almost always comes up right at the
top and you click on there and it'll
take you to the crummy.com software site
for beautiful soup now we're going to
use our whatever python interface you
want IDE I'm going to use Jupiter lab
which is built on Jupiter notebook
through Anaconda so when I open up my
anaconda Navigator you'll see that I
have my different tools available again
you might be using a different editor
and that's okay you might be in pie
charm or something like that we don't
need to do this and Jupiter lab is
Jupiter notebook with added tabs and
some added features it's basically in
beta testing so it's got a few little
glitches when you're saving things and
moving between projects but for the most
part it's a great upgrade to the jupyter
notebook and you can use them together
so you don't have to I mean it's built
on jupyter notebooks anything you do in
Jupiter notebook you can open up in
Jupiter lab and the first thing we need
to do is we need to go ahead in this
case I'm going under my environments
since it partitions the environments out
and I'm going to open up a terminal
window we have to install some package
pages in here to work with now there's a
lot of choices on this I because of the
Simplicity we'll be using conduct
install now you can use pip install for
the same thing and we're going to
install our beautiful soup 4. you have
to type out the whole thing beautiful
soup four and you can use a pip install
if you're using a different environment
and I am using python version 3.6
although according to beautiful soup
they also work on three seven all the
way from two seven through all of 3x now
according to the beautiful suit website
the beautiful soup 4 works on anything
you can install anything from python 2 7
all the way through any of the Python 3
versions this just happens to be
python36 because they do there's a lot
of other packages that don't work on
three seven yet and we'll go ahead and
run this install on here and let it go
through its environmental setup and of
course with conduct it goes in there and
finds all the dependencies PIP doesn't
do as much as far as finding
dependencies but you know exactly what's
on there with Pip so if you're doing a
huge distribution you probably want to
use your pair ship install so you can
track what's going on there with the
conda I like to just let it take over
since this isn't a major distributed
package going out another quick note
between pip and conda is that if you
start on a project in one of these
environments and you're using pip in
there stick with Pip if you're using
conda stick with conda they track the
packages and you can run into some
issues where they're not tracking the
same packages and something gets
overwritten so it's important to stay
very consistent with your install on
your environments and we'll also need to
go ahead and install our numpy
environment and our pandas on here so go
ahead and do that if you haven't added
those packages in go ahead and install
those into your environment that you're
working in and of course pandas is just
simply install pandas and let's just
install a couple more packages in this
case let's get our install our map plot
library because we're going to plot at
the end since we're going to be
collecting data and for this project
that will be all the packages we'll need
so we can go ahead and close out of our
installer or whatever setup you have and
we'll go back to home and we'll just
launch our Jupiter lab and that will
open up in our browser window now if
you're coming from Jupiter notebooks
from first time in lab we can go ahead
and just create our first notebook
python3 you can also do it under a file
launcher and you'll see a new notebook
it automatically opens up and we just
click right on there it'll pop open on
the left and I'll right click this and
we'll rename this we'll rename it just
beautiful and it is a i Pi NB file on
there so that should look familiar
because that's a Jupiter notebook file
this is a new one now I have little tabs
and in the past I usually hid this on
the other computer all my notes for the
lesson today but this is my notes going
down and we'll go ahead and just start
going through this and see what it looks
like to do a data pull from front to end
and see how that works as a data
scientist pulling that information in
from the website and the first thing I
want to do is I want to go ahead and
close the side window that way it looks
get the nice full screen and we can also
up the size a little bit one of the one
things about working in a browser window
just do that control plus thing the
packages we talked about is pandas so we
imported our pandas if you haven't
already that's our data frame if you
haven't done our pandas tutorial
definitely worthy of the time to go
through there and understand pandas
because it's such a powerful tool this
basically turns your data into a
spreadsheet data frame our numpy is our
number array so it kind of works with
pandas very closely as far as
manipulating data in arrays matplot we
want to go ahead and bring that in our
PLT so that we can plot the data at the
end and this line right here that says
matplot Library inline is for the
Jupiter notebook specifically it tells
it to print that on this page a lot of
the newer versions don't actually
require to have that line they'll still
printed on the page we should still
include that if you're in the Jupiter
lab setup and then we have our URL
library.request we're going to import
URL open for opening up the website and
then we have our our bs4 that's your
beautiful soup four we're going to
import beautiful soup and then our last
one is our re that is for manipulating
our regular expression so when you get
to that part of importing our data we
have to do a lot of reformatting so it's
something we can use and the re is one
of those tools we'll go ahead and run
this and just bring all that in so this
is all imported all these packages are
now into our web scraping program we're
going to run now if we're going to dive
in and pull data we should have a nice
website to pull from and let's go ahead
and we'll use the hubbertiming.com
results for the 2018 Martin Luther King
race and if we take this you can
actually just take this where did we get
this from well you can go in here and
find the website you're going to scrape
from and you'll see right here it says
you just copy that link right in there
that HTTP and this is a website that
we're looking at you can see right here
all the information that we're looking
at let's say we wonder to run some
statistics on this it sure would be nice
to be able to pull it off of here and if
they don't have a direct API that means
we need to pull it from their website
some of these will have a download
although if you've ever done a we have a
download click and maybe you're paging
through 100 websites in one case I was
pulling all the different United States
bills that are passed to track who voted
on them for a project and you can
imagine that there's you know hundreds
and hundreds of those thousands of these
documents that they voted on who voted
on and it goes to the Senate goes back
to the Congress so I opened up a website
pull all the links off of there that
match a certain criteria and we'll look
at that in just a minute how we go
through the HTML and then I had to
reformat them or I could hand download
each one one at a time which would just
be a nightmare so it's nice to automate
it in this case we're going to be
pulling up this chart we want to figure
out how to pull this chart off of this
website and so we go back into our
Jupiter notebook I've got my URL just um
our name for it and it's just a string
that's all this is nothing fancy there
you'll notice that on the slashes we now
have forward forward slash you can do a
single forward and HTTP is a double
forward this is just hey you have to
switch it to match setup in there and
then it's going to go ahead and use the
HTML equals URL open URL and that's from
our URL Library request so it's opening
a link to that website or at least
pointing to it and if we run this this
just sets it up so this is all set up
and then once we've done our setup let's
go ahead and create an object called
soup this will be and if you remember up
here here's our beautiful soup that we
imported from the bs4 and this is the
package that we're working with and so
we're going to do our beautiful soup on
here and on this we need to go ahead and
send it our HTML so it knows what it's
opening and then the second part is we
have to tell it how the format is coming
in and the most common one for your HTML
polls is an L XML setup and so almost
all of them you'll end up using the lxml
there's a few other options and because
this is so common in the newer versions
a lot of times they just leave it out
just because it's already on the default
we'll go ahead and leave it in here just
to remind us that it's there we'll go
ahead and run that and on the newer
versions uh they actually default it to
the XML setup and the HTML we'll just
leave it out and call it HTML so it's
just going to pull from this URL and
when we run that on here we've now
created an object soup that has pulled
the website into it so soup contains the
information along with information on
that website and what's going on so
let's just go over what we did real
quick before we start digging into the
actual suit before we start scooping out
stuff we imported our different modules
that we're going to use with our package
specifically the beautiful soup we did
install the beautiful soup remember
correctly you have to call it beautiful
soup four specifically so it knows what
you're bringing in and this line right
here is very key from bs4 because that's
how it installs the module we're
importing our beautiful soup and then we
found our URL in this case we're going
to go pull information from The Martin
Luther King dream run and then we set
our HTML to our URL open URL and you can
see right here we imported that so
here's our url.request import URL open
so we're requesting a connection and
once we send that connection into the
beautiful soup it creates an object
called suit and then this one of course
we chose soup just because it goes with
beautiful soup I guess we could have
chosen beautiful and now we can start
extracting information from our website
because we've pulled it down onto our
computer under soup then we can start by
looking at the title of the website soup
dot title and if we print title dot text
you'll see this a lot in beautiful soup
because title contains all kinds of
information and if we want just the text
from that title you add the dot text on
the end and you can see right here we
have our 2018 MLK dream Run 5K race
results if you look at the tab that's
the actual title up here 2018 MLK dream
Run 5K race that's what the title is on
the website and then you might be
curious what's in title what's the whole
title that it's storing up there well
let's go ahead and print it out here's
print title and print title.txt and when
we run that you can see it has the HTML
tags title on it and then the forward
slash title to end it and so we're
really just pulling off this piece of
the HTML code and then we look at the
text inside that particular part of the
HTML and earlier I mentioned links what
if you want to get all the links off
this page oh that was would be fun we
could do soup Dot and we'll do find
underscore all put this in bracket and
then quotation marks we're going to put
a a is the key find and you'll start
seeing a div and all the different
options you have for finding these
entities in a website and then let's go
ahead and just print our links and you
can see here that it now shows all the
different links in here that are marked
by eggs we did a find all a and then we
can also because this is a little bit
hard to pull off the H reference so we
can also add in our find all fine tune
that in this case the H reference equals
true will actually filter that out and
then finally we might do a four link in
links and we can simply do something
like this for each link we want to
actually find the H reference because we
know there's an H reference in it and if
I run this you can see it just comes
through and prints them out one at a
time some of these are really useful so
you might be looking for something that
has https in it and you know that's a
link running to something else or you
may be looking for the mail to tags you
know that's all the mail addresses but
either way you can easily find all the
links in your HTML document that you're
paging through and of course any
packages that have evolved over time you
can also do link dot get H reference
which should do the same thing as our
other format and you can see it
certainly does we get the same printout
up here in this particular case we
really want to get the data off the page
so let's go ahead and do that let's see
what that looks like and in data let's
call it all rows there we go equals and
then we have our soup dot find
underscore all there's our brackets and
then if we're looking for each row in a
database you'll remember your HTML code
we're looking for the tag TR so we want
to find all TR and we can take this
let's go ahead and just take all our
rows and do a print all rows and about
this time you're going to guess that
we're going to get a huge amount of
information just dumped onto our page
and sure enough we do if you look at
this it just kind of goes on forever but
this is an array each row is considered
an array so because of that we can do
something simply as putting brackets and
just print the first let's do the first
five rows so from beginning to five and
you can see here's our first five rows
on here I sometimes like to just do
let's just do row zero and we see that
row zero is finishers finisher is 191
and just out of curiosity which if
that's zero what's Row one male okay so
we're starting to see titles going
across here so if we come up here and we
do rows we did what up to ten let's just
take a look and see what 10 does again
and just take a look at that information
that comes across Place uh bib name
gender age city state chip chip paste
gender and so on so it comes all the way
down here we kind of have an ending
right here and then we have one and then
we actually it looks like we start to
have information so we have our one our
one 1191 Max Randolph that must be the
name male 29 and so on you start seeing
how patient starts getting displayed
going down so the next thing we want to
do with this I'll go back up here and
just edit the space we're in so it
starts to make a little bit more sense
keep it all together and so we want to
do for each row in all rows we're
looking at what information are we
looking at well we have our th up here
that's the header our TD down here which
looks like the individual information
and we really are looking for the actual
data so we're looking for our TD tags in
the rows and we can do that because when
remember when it stores the row it also
stores the tags underneath that so all
rows have all the different tags in it
and you can see right here as you print
each one of those out and so we look at
each row we can create another variable
and we'll call it row list and we'll set
this equal to in this case row because
we've already pulled all the rows out of
soup so now we want to find for each row
and in there we want to find our TD and
if we go ahead and just print I'm going
to do it if you notice I changed the
indent so I'm just going to print row
list what this does is the last value to
go into row list our last row is going
to print now and of course make sure you
have an underscore instead of a period
when you're typing so row dot find
underscore all TD and if we print the
last row you can see I have all the data
coming across here we have our 191 our
1216 Zuma Oak choa I hope I said that
right female I believe that's age 40 and
so on and then we can take our row list
and there's a lot of things we can do
with the row list what we'll do for
let's do object or let's just do cell in
row list and so we're going to look at
each cell because this is if you look at
this they have commas separated between
the different objects and then we're
going to go ahead and print cell dot
text let's just take a look and see what
that looks like and we can see here for
each row we get 191 there's our 191
there's our 12 16 12 16 our individual
who's in the race and so forth all the
way down for those different settings
let's go ahead and create a new variable
up here we'll call this all let's just
call it data we'll keep it simple so
here's our data and then we have our row
we take our row we break it up into
individual cells so we'll call this data
row we'll set this empty to an empty row
and we're going to take our cells tab
this and we know that each cell
generates a text and so what we want to
do is I want to take my data row let's
just replace that let's take our data
row and let's append our cell.txt so I'm
going to add the each row is going to be
a row of the different text on here and
then once I create each row I want my
data which is going to be everything to
append each row and here's our data row
and then if we go ahead and come down
here and let's print data now if we were
lurking with large data we'd be very
careful about just throwing all our data
on the page but you can see here we
throw the date on the page and we get
finishers 199 male 78 female 113 1 and
so on and if you look at this this is
the headers on the file we have
finishers uh male females just like some
general statistics on the first one and
then we have actually a
empty data set and then we have our data
that continues which actually the actual
information we're looking for so we have
1 1191 Max Randolph Mill 29 Washington
DC runtime one of 78 and so on on here
so we could really quickly get rid of
that a number of different ways to do
that one of them is just to do we're
going to set if we do data two on we
should get rid of everything but we want
to keep Randolph so make sure Randolph
is in there oh we lost Randolph let's
try one on there we go there's Max
Randolph on there so we can just simply
do redo our data on here we can do data
probably want to do it in all rows from
one on but I'm just going to do my data
equals data one on down here and there's
reasons to split it this way in data
science sometimes you don't want to
touch the original data in case you need
it in case we do need the first row so
we'll put it down here and maybe we'll
just call this titles titles equals data
of zero and so we could do something
here where we print we'll print up our
titles and we'll print our data in this
case instead of one on let's go minus
two let's look at the last two rows of
data so here we have our titles and for
some reason just put in finishers of 191
is expecting a little bit more up there
and we have our last couple people and
they look like the data on these looks
just fine on here turns out this is just
some generic statistics up here so we'll
get rid of titles completely it doesn't
really do us any good but we know that
data comes in here and we can look at
our data and look at the very end of the
data too the minus 2 to the end and we
can see it pulls the data in pretty good
we don't have anything too funky in here
we're looking at it looks pretty clean
now you got to be a little careful
because at some point we might have to
come back here and clean up the data if
we get an error if we're running a data
analysis we might find out there's some
unusual characters or something is
misled in the data itself and you also
notice that everything is a string so
when we're bringing it in we might have
to do some conversions to test it out
and convert them to whatever kind of
data format we're working with so at
this time we want to go ahead and bring
in our pandas and let's go ahead and
call this idea for data frame we'll set
it equal to and if you remember
correctly we imported pandas as PD and
that's standard you'll see that in most
code examples where they call they
import pandas is PD and it is capital D
capital F for data frame and we're just
going to bring in our data that's what
we called it on here and let's take this
and we'll print now when you're working
with data frames you're usually talking
large amounts of data and so you almost
never want to print the whole data frame
out we're going to go ahead and do that
anyway just so we can see what that
looks like and you can see in here
brings in our data frame coming in here
and we just have a mess of information
this is our data let's go ahead and
print DF and see what that looks like in
the data frame and this is nice because
it organizes it into a very easy to read
table and we have they set the label 0 1
2 3 4 5 6 and so on and then we have
each row we have Mel 78 none none going
across when we get all the way down here
we'll see Max Randall about number three
and the first thing this does is this
Flags me that I brought in a bunch of
information up here that we really
didn't want it's from three on that we
want and we can clean this up in one of
two ways we can try to clean it up under
the data or we can clean it up under the
data frame depending on what it is we're
trying to do and so to fix this
um I want to go ahead and just change it
up here in the actual data pull in we
don't need that information so I'll
rerun it reload our data from 4 on and
then when I run this we see we have Max
Randolph is right at the top of the list
like you should be and we have all the
data going down now with the data frame
remember I said we don't usually print
the whole data frame we'll go ahead and
do df.head and this prints the first
five rows and you can see that we have
13 columns here's Max Randall all the
way to Theo kinman and I use the also
print DF tell and the reason I like to
do these particular two setups I'm going
to change it just to two rows because
you can do that you can put as many rows
as you want is this good to look at the
first part and the end because those are
usually where you have extra data
brought in something's messed up and you
can also see that we have 190 rows in
here and it comes in with our Zuma
Alicia and they're both on here on the
list so now we have a nice data frame
columns and rows we can easily look at
it we can see the setup on here and we
can look at the names and everything now
at some point you might be looking at
these individual columns and find
different information that needs to be
re-edited if you can you try to do it
with the whole column under pandas you
can up in the upper part of the code
where you went from cell to cell or row
to row you could look at individual
cells maybe you find a marker in that
cell that's something specific like
remove all colons or semicolons or
something in there are brackets so
there's a lot of options in there but
you'll find that this one actually comes
in pretty clean on here all the way down
and the next thing we really want to do
is we want to look at the headers I
don't know about you but does it make
any sense to me when I have column one I
don't know what 1191 is or 1080 need
name Kaiser Runner I'm guessing that's
column two is names third one it looks
like male or female probably age but I
don't want to guess I want this to bring
in my column so I know exactly what I'm
looking at so how can we make beautiful
soup do that for us well let's take our
column headers we're going to set that
equal to our soup find underscore all
and then we're going to look for our
headers our th files and since we're in
Jupiter lab in this case Jupiter
notebook I can just type out column
headers if it's a last variable I have
listed it will automatically print it so
it's kind of a shorthand and we can see
right here we have place I'm guessing
that's bib name gender age city state
chip time chip pace and so on so we have
all our headers right there I shouldn't
have to type them all in and we'll go
ahead and do it before we'll go ahead
and do a header list equals our empty
array and then we can do for a column in
column headers and we can take our
header list and just a pin and what do
we want we want the text from the column
so we'll just do column dot text and
then if we come down here and we print
our header list let's see what that
looks like if we did it right we should
get a nice list of all the different
column headings we want so we have
placebib name and so on and then pandas
just because pandas is so cool we can
simply do DF columns equals our header
from our header list we can simply set
DF columns set to DF headers and then if
I print df.head we'll take a look at
that and we'll see right here it has
nicely placed our values on here Place
bib name gender age and so on so very
quickly we've created this nice data
frame we have the data displayed in nice
rows and columns and easy to read and
then as a data scientist the first thing
we want to know is the info what is in
these columns and rows and headers and
you see right here they all come up with
non-null object there's a big flag so if
I want to do anything with this these
are all coming through as strings or an
object I usually mean strings in this
case that they're a string variable and
we have you can quickly read through
this 191 entries date columns total 14
columns there's a total of 14 columns in
the data and it shows you all the
different names and what type of column
they are and it's probably good also to
look at the shape of the data df.shape
we'll go ahead and just run that you see
it's 191 by 14 14 columns 191 entries
this is more like a we look at a numpy
array 191 by 14 for the shape and
remember this is a variable so if I put
it on if it's a last variable or last
value in the set of cells Jupiter
automatically prints it out so if you're
in a different IDE you want to go ahead
and use the print statement on here and
then one of the things you'd want to
also go through we'll create a second
one D of 2 equals DF dot drop in a now
the axis is automatically equal to 0 so
a lot of times you'll see something like
X's equals zero comma How equals any
axis equals zero is default that means
we're looking at going down the rows you
could look at the column going across
let's remove the how index that's just
going to confuse you the axis is is
whether you're going down the columns or
if you're looking at a row by row by row
by row or you could be looking at it by
column by column by column this would
drop any column and it would drop off
the N A in any column and how equals and
we want any I always confuse all in any
because they both start with a all means
that all of them have to be non-value
where any means that any of them can be
there to drop it so this would drop any
column with a null value in it but we
want zero and it'll drop any value with
an null value and then because 0 is
always the default we'll just leave it
out and then as curious as to what the
shape is now did we lose anything was
there any null values in our df2 that we
draw from the DF and we'll go ahead and
run that and we see 191 and 14. so we
didn't really drop anything but it's
always good to check there's other ways
you can also do are there let's see any
Nas you can detect Nas in here no values
infinite values that's another one you
got to watch out for we're working with
data that we're going to do something
with here in a minute so you got to be a
little careful also in the conversions
are you going to convert something where
people typed in weird characters to
describe the data a certain way so now
we've got to this point where we have
all our different columns we have our
different data and at this point be here
asking or maybe the shareholder the
company is asking hey can we look at the
based on the chip time here's our chip
time can we plot that versus gender how
does gender versus chip time compare and
so we can do that we can take that and
the first thing we look at is we say hey
well chip time came in as a string and
that's going to be an issue now there's
a number of ways you can change this one
of them is we could go all the way back
up here where we created the data and
find a way to tag it and say Hey
whenever this cell text maybe instead of
appending this I notice that anytime
there's two colons in it that's probably
a time signature and let's convert all
the time signatures to date time filled
or whatever A lot of times you don't get
that you don't get that option and
that's always a question in bringing in
data whether you convert the data coming
in at the beginning or do you wait till
you have it open it up and then convert
it when you go to use it we're going to
go ahead and convert it after we got it
into our data frame so we have our df2
here we've dropped all of our Nas we
dropped our we have a shape in fact
let's do this since there's no
difference between DF and df2 well we'll
just go ahead and use df2 so let's go
ahead and take our df2 and we want to
take those that specific field and
convert it into some kind of numerical
value we can use and let's add another
column A lot of times this is something
you want to do is where you want to go
ahead and keep all the original columns
and just add a new column in there and
this new column is going to be based on
the if we remember correctly we had chip
time that's what we're going to look at
okay we want chip time versus gender if
we go into our pandas we find out we
have pandas 2 Delta and this is actually
time Delta and then we just want to take
our df2 and we're going to use the chip
time column so this is going to say hey
let's look at let's convert everything
in df2 chip time into a Time Delta
format that's the data type we're going
to put it as and we can go ahead and
just run this and if we go in here and
we do info df2 and we'll keep our we're
going to look at this particular column
but we want to keep it as a data frame
so
of all the columns
we went we'll just do that info on here
and run that and we do an info on that
you can see it's now a Time Delta 64
nanoseconds uh well we really don't want
nanoseconds we actually probably want to
do it in minutes uh so let's take a look
at that and let's take this whole thing
TF2 let's just set the df2 we have our
df2 this is a column we're working with
here and we can use the as type property
in pandas and so we can set this equal
to df2 we'll take our same column in
here and we'll set it as type time Delta
second so it's still a Delta time here
so if I run this you'll see that it
still comes up as where is it well it
turns it into a float so we're now at a
float 64 so it's the number of seconds
in that Delta time and then finally we
want to go ahead and turn that from
seconds to say minutes and you know
there's 60 seconds in minutes and so now
we divide by 60 we still have a float we
have our info it shows us it's a float
and then we can go ahead and just do a
print df2 and let's just keep it small
we don't want to look at all our data we
just wouldn't do the head of it and we
run this and you can see right here
where we go to it'll be the last one
here's our chip time in minutes and a
lot of times just to make life easy for
viewing since we're only looking at this
particular element we can do chip time
in minutes and now we just see that oops
we take off we'll go ahead and take off
our info done with that and we'll run
this and you can see we have our minutes
16.8 minutes 17.51 minutes and so on
it's a float number now keep in mind
this is 0.8 that's not a 16 minutes and
80 Seconds that they can always throw
you if you're going through so many
numbers you forget it's important to
remember that and we're also going to
look at the other one we're looking at
is what gender you want to look at
gender to chip time in minutes and so we
can see here under the head we get mail
mail mail and a number of different
setups and let's switch this to tell
real quick and just look at the end of
it and here we have female female male
female female so we have two different
genders and we have our chip time in
minutes and if you remember we brought
in our PLT if you haven't used the plot
Library the matplot library you have a
drawing place you're putting stuff on so
we have our PLT we're going to do a bar
graph and we just want to Simply use our
df2 gender and df2 Chip time in minutes
so that's going to plot the two bars and
to make it pretty we'll go ahead and
give it the X label gender the white
label chip time minutes and that simply
is remember it always plots X and it
plots Y and we have our gender our chip
time give it a nice title a comparison
of average minutes run by male and
female and if we go ahead and run this
with the correct titles in here and
everything matches uh you'll get a nice
graph we can see here the comparison of
average minutes run by male and female
here's our chip time in minutes the men
seem to be Slackers in this particular
case and it's actually uh there's a
number of studies that show that women
tend to have as far as doing
cross-country there's a lot of women who
have a longer endurance than men so it's
not too surprising uh we can see here
the average chip time around 70 and for
women over a hundred minutes and then
another really cool thing we can do is
we can describe the data so
df2.describe this again is a pandas
function just like info is we're going
to include NP number the numpy number
and if we run that you'll see here it
comes up and it says chip time in
minutes a count it gives you the average
or the means standard deviation the
minimum the maximum all the different
descriptive information you're going to
want from your data set on there and
just because there's all kinds of fun
ways let's do a box plot to display your
information we can do a box plot where
the column equals chip time in minutes
and let's go ahead and run that keep
mistyping my chip time in minutes you
can see it puts out a nice box plot
showing you the information we have our
different values and floaters and this
is always interesting because this is a
nice way of seeing where we have these
floaters one up here and there's two up
above and of course you're you're a nice
spread on the box plot and we can also
modify this a little bit and we can add
in buy equals gender and then we'll give
it a we'll just give it a blank title I
don't know why we're giving it a blank
title uh we'll just add a y plot y label
on there for runtime and if we run this
you can see here box plot Group by
gender chip time in minutes and now we
have our female and male two different
areas and you can see how they vary you
have your two different uh your outlier
up here and you can also see how there's
such an overlap between the two
different values so if I was looking at
this I'd be like wow you know I I really
could not draw a conclusive thing on
this saying that women's runtime more in
general because they overlap too much
that would be one of the conclusions I'd
have to come up with done here and then
we get to maybe the partners come in
from the company and say hey we'd like
to know the age versus chip time in
minutes that'd be something worth
knowing on the statistics on this and
the first thought is we can simply plot
it and we can do this we can actually
plot the scatter plot chip time versus a
df2 of H those are X Y coordinates but
if you remember from df2 we did the info
let's go way back up here we're looking
at a data object as far as our chip time
on this and our age now we converted the
chip time but we also need to convert
the age and if we do it right here we
just plot it and it'll actually let us
plot it it shouldn't it should give us
an error but it does let us plot it
you'll see the Ages come up a mess over
here because they're converting it to
weird float numbers and all kinds of
things so what we want to do is we want
to take our age and we'll just call this
ah underscore I so we're going to take
our age and we're going to create
another column for df2 age underscore I
and the I is just going to stand for
integers our own choice of values there
is a number of ways to do this but we're
going to do pandas to Merrick is the
best way in pandas and the reason we're
doing this is that numeric creates a
float value so right off the bat we want
all our stuff converted converts it to
the least common denominator so if
they're all integers already you'll get
integers as you can see from here it's
doing some kind of conversion that
converts it to a float value the other
thing that numeric does is if there is a
null value or they put in like a blank
line or a dash to represent no
information it'll convert it to a null
so it goes from like a string to a null
versus just having some kind of made up
number that python somehow created for
the graph we have below and then we want
to add our df2h because that's what
we're converting to numeric and then we
want to coerce it and there's a couple
different options on this like you can
have it where it just doesn't process it
in pandas but coerce means that if it
gets a weird value that is a null value
now and since we're dealing with errors
this is what happens when you get an
error converting it we want to coerce it
there we go and put the end bracket on
there and then finally we want to go and
round this off so I'll put brackets
around all the way around it and this
rounds off everything in this series so
we've done here is I've taken df2h which
is a d-type object which in this case is
mostly strings with a couple blank ones
in there and we're going to convert it
to a numeric which will automatically go
to float and then we're going to take
wherever there's an error wherever it
says hey this doesn't convert and
usually that's a blank screen like I
said I've worked with so many databases
where they somehow puts down none
someone types in space sometimes in a
dash to mean none and you get this
really weird conversion coming up this
covers all of that in pandas so it's
really a nice way of just coercing it
and saying hey if we don't have a number
in there let's make it a null value and
then we're going to round it off and
then finally let's go ahead and take our
DF sign it to df2 so df2 now has rounded
out so it's rounded to the manager we
didn't do any places the age and it's
going to be age I and then we've dropped
all our null values that way we're not
going to get any errors and we try to
plot a null value and it also makes sure
that data by deleting out the rows
because that's what this does it
automatically does axis0 which is your
rows axes one is your column by doing
this it automatically removes all the
rows with null values so it just cleans
out the roles and then when we go ahead
and plot this we see we have a nice
clean data and we have age all the way
up to 70. so we have our chip time set
and then our age going across and it
makes a nice plot that you can easily
show for display and for the the and you
can easily show that to your
shareholders or whatever group you're
working on it makes it really nice and
quick easy display and we're going to
cover the scikit learn tutorial which
has a lot of features and all kinds of
API in it to explore data and do your
data science with effects is probably
one of the top data science packages out
there so what is the site kit learn it's
simple and efficient tool tool for data
mining and data analysis it's built on
numpy scipy and matplot Library so it
interfaces very well with these other
modules and it's an open source
commercially usable BSD license BSD
originally stood for Berkeley software
distribution license but it means it's
open source with very few restrictions
as far as what you can do with it
another reason to really like the site
kit learn setup so you don't have to pay
for it as a commercial license versus
many other copyrighted platforms out
there what we can achieve using the
scikit learn we use class the two main
things or classification and regression
models classification identifying which
category an object belongs to for one
application very commonly used is Spam
detection so is it a Spam or is it not a
Spam yes no in banking it might be is
this a good loan bad loan today we'll be
looking at wine is it going to be a good
wine or a bad wine and regression is
predicting an attribute associated with
an object one example is stock prices
prediction what is going to be the next
value
stock today sold for 23 dollars and five
cents a share what do you think it's
going to sell for tomorrow and the next
day and the next day so that would be a
regression model same thing with weather
weather forecasting any of these are
regression models where we're looking at
one specific prediction I want to
Tribute today we'll be doing
classification like I said we're gonna
be looking at whether a wine is good or
bad but certainly the regression model
which is in many cases more useful
because you're looking for an actual
value is also a little harder to follow
sometimes so classification is a really
good place to start we can also do
clustering and model selection
clustering is taking an automatic
grouping of similar objects into sets
customer segmentation is an example so
we have these customers like this
they'll probably also like this or if
you like this particular kind of
features on your objects maybe you like
these other objects so it's a referral
is a good one especially on Amazon.com
or any of your shopping networks model
selection comparing validating and
choosing parameters in mod now this is
actually a little bit deeper as far as a
site kit learned we're looking at
different models for predicting the
right course or the best course or
what's the best solution today like I
said we're looking at Wines it's going
to be well how do you get the best wine
out of this so we can compare different
models and we'll look a little bit at
that and improve the model's accuracy
via different parameters and fine-tuning
now this is only part one so we're not
going to do too much tuning on the
models we're looking at but I'll Point
them out as we go two other features
dimensionality reduction and
pre-processing dimensionality reduction
is we're reducing the number of random
variables to consider this increases the
model efficiency we won't touch that in
today's tutorial but be aware if you
have you know thousands of columns of
data coming in thousands of features
some of those are going to be duplicated
or some of them you can combine to form
a new column and by reducing all those
different features into a smaller amount
you can have a you can increase the
efficiency of your model it can process
faster and in some cases you'll be less
biased because if you're weighing it on
the same feature over and over again
it's going to be biased to that feature
and pre-processing these are both
pre-processing but pre-processing is
feature extraction and normalization so
we're going to be transforming input
data such as text for use with machine
learning algorithms we'll be doing a
simple scaling in this one for our
pre-processing and I'll point that out
when we get to that and we can discuss
pre-processing at that point with that
let's go ahead and roll up our sleeves
and dive in and see what we got here now
I like to use the Jupiter notebook and I
use it out of the Anaconda Navigator so
if you install the Anaconda Navigator by
default it will come with the jupyter
notebook or you can install the jupyter
notebook by itself this code will work
in any of your python setups I believe
I'm running an environment of 3.7 setup
on there I'd have to go in here in
environments and look it up for the
python setup but it's one of the three
x's and we go ahead and launch this and
this will open it up in a web browser so
it's kind of nice it keeps everything
separate and in this Anaconda you can
actually have different environments
different versions of python different
modules installed in each environment so
it's a very powerful tool if you're
doing a lot of development and Jupiter
notebook is just a wonderful visual
display certainly you can use I know
spider is another one which is installed
with the Anaconda I actually use a
simple notepad plus plus when I'm doing
some of my python script any of your
Ides will work fine jupyter notebook is
iron python because it's designed for
the interface but it's good to be aware
of these different tools and when I
launched the Jupiter notebook I'll open
up like I said a web page in here and
we'll go over here to New and create a
new python setup like I said I believe
this is python37 but any of the three
this the site kit learn works with any
of the three X's there's even two seven
versions so it's been around a long time
so it's very big on the development side
and then the guys in the back guys and
gals developed they went ahead and put
this together for me and let's go ahead
and import our different packages now if
you've been reading some of our other
tutorials you'll recognize pandas as PD
pandas library is pretty widely used
it's a data frame setup so it's just
like columns in rows and a spreadsheet
with a lot of different features for
looking stuff up Seaborn sits on top of
matplot libraries this is for a graphing
and we'll see that how quick it is to
throw a graph out there to view in the
Jupiter notebook for demos and showing
people what's going on and then we're
going to use the random Forest the SVC
or support vector classifier and also
the neural network so we're going to
look at this we're actually going to go
through and look at three different
classifiers that are most common some of
the most common classifiers and let's
show how those work in the scikit-learn
setup and how they're different and then
if you're going to do your setup on here
you'll want to go ahead and import some
metrics so the sklearn.metrix on here
and we're going to use the confusion
metrics and the classification report
out of that and then we're going to use
from the sklearn pre-processing the
standard scalar and label encoder
standard scale is probably the most
commonly used pre-processing there's a
lot of different pre-processing packages
in the sklearn and then model selection
for splitting our data up it's one of
the many ways we can split data into
different sections and the last line
here is our percentage matplot library
in line some of the Seaboard and matplot
Library will go ahead and display
perfectly in line without this and some
won't it's good always include this when
you're in the Jupiter notebook this is
Jupiter notebook so if you're in ide
when you run this it will actually open
up a new window and display the graphics
that way so you only need this if you're
running it in a editor like this one
with the specifically jupyter notebook
I'm not even familiar with other editors
that are like this but I'm sure they're
out there I'm sure there's a Firefox
version or something jupyter notebook
just happens to be the most widely used
out there and we can go ahead and hit
the Run button and this now has saved
all this underneath the packages so my
packages are now all loaded I've run
them whether you read it on top we run
it to the left and all the packages are
up there so we now have them all
available to us for our project we're
working on and I'm just going to make a
little side note on that when you're
playing with these and you delete
something out and add something in even
if I went back and deleted this cell and
just hit the scissors up here these are
still loaded in this kernel so until I
go under kernel and restart or restart
and clear or restart and run all I'll
still have access to pandas important to
know because I've done that before I've
loaded up maybe not a module here but
I've loaded up my own code and then
changed my mind and wondering why does
it keep putting out the wrong output and
then I realize it's still loaded in the
kernel and you have to restart the
kernel just a quick side note for
working with a jupyter notebook and one
of the troubleshooting things that comes
up and we're going to go ahead and load
up our data set we're using the pandas
so if you haven't yet go look at our
pandas tutorial a simple read the CSV
with the separation on here so let me go
ahead and run that and that's now loaded
into the variable wine and let's take a
quick look at the actual file I always
like to look at the actual data I'm
working with in this case we have wine
quality Dash red I'll just open that up
I have it in my open Office setup
separated by semicolons that's important
to notice
and we open that up you'll see we have
go all the way down here it looks like
1600 lines of data minus the first one
so 15
1599 lines and we have a number of
features going across the last one is
quality and right off the bat we see the
quality is uh has different numbers in
it five six seven it's not really I'm
not sure how how high of a level it goes
but I don't see anything over a seven so
it's kind of five through seven is what
I see here five six and seven four five
six and seven looking to see if there's
any other values in there looking
through the demo to begin with I didn't
realize the setup on this so you can see
there's a different quality values in
there alcohol sulfates pH density total
sulfur dioxide and so on those are all
the features we're going to be looking
at and since this is a pandas we'll just
do wine head and that prints the first
five rolls rows of data that's of course
a panda's command and we can see that
looks very similar to what we're looking
at before we have everything across here
it's automatically assigned an index on
the left
this does if you don't give it an index
and for the column names it has assigned
the first row so we have our first row
of data pulled off the our comma
separated variable file in this case
semicolon separated and it shows the
different features going across and we
have what one two three four five six
seven eight nine ten eleven features 12
including quality but that's the one we
want to work on and understand and then
because we're in Panda's data frame we
can also do wine dot info and let's go
ahead and run that this tells us a lot
about our variables we're working with
you'll see here that there is
1599 that's what I said from the
spreadsheet so that looks correct
non-null float 64. this is a very
important information especially the
non-nullness there's no null values in
here that can really trip us up in
pre-processing and there's a number of
ways to process non-null values one is
just to delete that data out of there so
if you have enough data in there you
might just delete your non-null values
another one is to fill that information
in with like the average or the most
common values or other such means but
we're not going to have to worry about
that but we'll look at another way
because we can also do wine is null and
sum it up and this will give us a
similar it won't tell us that these are
float values but it will give us a
summation oops there we go that it'll
give us a summation on here how many
null values in each one so if you wanted
to you know from here you would be able
to say okay this is a null value but she
doesn't tell you how many are null
values this one would clearly tell you
that you have maybe five null values
here two null values here and you might
just if he had only seven null values
and all that different data you'd
probably just delete them out where if
ninety percent of the data was null
values you might rethink either a
different data collection setup or find
a different way to deal with the null
values we'll talk about that just a
little bit in the models too because the
models themselves have some built-in
features especially the forest model
which we're going to look at at this
point we need to make a choice and to
keep it simple we're going to do a
little pre-processing of the data and
we're going to create some bins and bins
we're going to do is 2 comma 6.5 comma
8. what this means is that we're going
to take those values if you remember up
here let me just scroll back up here we
had our quality the quality it comes out
between two and eight basically or one
and eight we have five five five six you
can see just in the just in the first
five lines of variation in quality we're
going to separate that into just two
bins of quality and so we've decided to
create two bins and we have bad and good
it's going to be the labels on those two
bins we have a spread of 6.5 and an
exact index of eight the exact index is
because we're doing zero to eight on
there the 6.5 we can change we could
actually make this smaller or greater
but we're only looking for the really
good wind we're not looking for the zero
one two three four five six we're
looking for wines with seven or eight on
us so high quality you know like this is
what I want to put on my dinner table at
night
I'm going to taste the good wine not the
semi-good wine or mediocre wine and then
this is a panda so PD remember stands
for pandas pandas cut means we're
cutting out the wine quality and we're
replacing it and then we have our bins
equals bins that's the command bins is
the actual command and then our variable
bends to comma 6.58 so two different
bins and our labels bad and good and we
can also do let me just do it this way
wine quality since that's what we're
working on and let's look at unique
another pandas command and we'll run
this and I get this lovely error why did
I get an error well because I replaced
wine quality and I did this cut here
which changes things on here so I
literally altered one of the variables
is saved in the memory so we'll go up
here to the kernel restart and run all
it starts it from the very beginning and
we can see here that that fixes the
error because I'm not cutting something
that's already been cut we have our wine
quality unique and the wine quality
unique is a bad or good so we have two
qualities objects bad is less than good
meaning bad is going to be zero and good
is going to be one and to make that
happen we need to actually encode it so
we'll use the label quality equals label
encoder and the label encoder let me
just go back there so this is part of
sklearned that was one of the things we
imported it was a label encoder you can
see that right here from the
sklearn.processing import standard
scalar which we're going to use in a
minute and label encoder and that's what
tells it to use that equals 0 and good
equals one and we'll go ahead and run
that and then we need to apply it to the
data and when we do that we take our
wine quality that we had before and
we're going to set that equal to label
quality which is our encoder and let's
look at this line right here we have dot
fit transform and you'll see this in the
pre-processing these are the most common
used is fit transform and fit transform
because they're so often that you're
also transforming the data when you fit
it they just combine them into one
command and we're just going to take the
wine quality feed it back into there and
put that back in our wine quality setup
and run that and now when we do the wine
and the head of the first five values
and we go ahead and run this you can see
right here underneath quality zero zero
zero I have to go down a little further
to look at the better wines let's see if
we have some that are ones yeah there we
go there's some ones down here so when
you look at 10 of them you can see all
the way down to zero or one that's our
quality and again we're looking at high
quality we're looking at the seven and
the eights or six point five and up and
let's go ahead and grab our or was it
here we go wine quality and let's take
another look at what else more
information about the wine quality
itself and we can do a simple pandas
thing value counts hopefully I type that
in there correctly and we can see that
we only have 217 of our wines which are
going to be the higher quality so 217
and the rest of them fall into the bad
bucket the zero which is uh 1382 so
again we're just looking for the top
percentage of these the top what is that
it's probably about a little a little
under 20 percent on there so we're
looking for our top wines our seven and
eights and let's use our let's plot this
on a graph so we take a look at this and
the SNS if you remember correctly that
is let me just go back to the top that's
our Seaborn Seabourn sits on top of
matplot Library it has a lot of added
features plus all the features of the
matplot library and also makes it quick
and easy to put out a graph we'll do a
simple bar graph and they actually call
it count plot and then we want to just
do count plot the wine quality so let's
put our wine quality in there and let's
go ahead and run this and see what that
looks like and nice in line remember
this is why we did the inline so make
sure it appears in here and you can see
the blue space or the first space
represents low quality wine and our
second bar is a high quality line and
you can see that we're just looking at
the top quality wine here Mimosa wine we
want to just give it away to the
neighbors now maybe if you don't like
your neighbors maybe give them the good
quality wine and I don't know what you
do with the bad quality wine I guess use
it for cooking there we go but you can
see here it forms a nice little graph
for us with the Seaborn on there and you
can see our setup on that so now we've
looked we've done some pre-processing
we've described our data a little bit we
have a picture of how much of the wine
what we expect it to be high quality low
quality checked out the fact that
there's none we don't have any null
values to contend with or any odd values
some of the other things you sometimes
look at these is if you have like some
values that are just way off the chart
so the measurement might be off or
miscalibrated equipment fear in the
scientific field so the next step we
want to go ahead and do is we want to go
ahead and separate our data set or
reformat our data set and we usually use
capital x and that denotes the features
we're working with and we usually use a
lowercase y that denotes what uh in this
case quality what we're looking for and
we can take this we can go wine it's
going to be our full thing of wine
dropping what are we dropping we're
dropping the quality so these are all
the features minus quality I'll make
sure we have our axes equals one if you
left it out it would still come out
correctly just because of the way it
processes on the defaults and then our y
if we're going to remove quality for our
X that's just going to be 1 and it is
just the quality that we're looking at
for y so we put that in there and we'll
go ahead and run this so now we've
separated the features that we want to
use to predict the quality of the wine
and the quality itself the next step is
if you're going to create a data set in
a model we got to know how good our
model is so we're going to split the
data train and test splitting data and
this is one of the packages we imported
from sklearn and the actual package was
train test split and we're going to do X
Y test size 0.2 random State 42 and this
returns four variables and most common
you'll see is capital x train so we're
going to train our set with capital X
test that's the data we're going to keep
on the side to test it with why train
why remember stands for the quality or
the answer we're looking for so when we
train it we're going to use x train and
Y train and then why test to see how
good our X test does and the train test
split let me just go back up to the top
that was part of the sklearn model
selection import train test split there
is a lot of ways to split data up this
is when you're first starting you do
your first model you probably start with
the basics on here you have one test for
training one for test our test size is
0.2 or 20 percent and random State just
means we just start with a it's like a
random seed number so it's not too
important back there we're randomly
selecting which ones we're going to use
since this is the most common way this
is what we're going to use today there
is and it's not even an sklearn package
yet so someone's still putting it in
there one of the new things they do is
they split the data into thirds and then
they'll run the model on each of they
combine each of those thirds into two
thirds for training and one for testing
and so you actually go through all the
data and you come up with three
different test results from it which is
pretty cool that's a pretty cool way of
doing it you could actually do that with
this by just splitting this into thirds
and then or you have a test site one
test set third and then split the
training set also into thirds and also
do that and get three different data
sets this works fine for most projects
especially when you're starting out it
works great so we have our X train our X
test our y train and our y test and then
we need to go ahead and do the scalar
and let's talk about this because this
is really important some models do not
need to have scaling going on most
models do and so we create our scalar
variable we'll call it SC standard
scalar
and if you remember correctly we
imported that here wrong with the label
encoder the standard scalar setup so
there's our scalar and this is going to
convert the values instead of having
some values that go from zero if you
remember up here we had some values are
54 60 40 59 102. so our total sulfur
dioxide would have these huge values
coming into our model and some models
would look at that and they'd become
very biased to sulfur dioxide it'd have
the hugest impact and then a value that
had
0.076.098 our chlorides would have very
little impact because it's such a small
number so when we take the scalar we
kind of level the playing field and
depending on our scalar it sets it up
between 0 and 1 a lot of times is what
it does let's go ahead and take a look
at that and we'll go ahead and start
with our X train and our X train equals
SC fit transform we talked about that
earlier that's an sklearn setup it's
going to both fit and transform our X
train into our X train variable and if
we have an X train we also need to do
that to our test and this is important
because you need to note that you don't
want to refit the data we want to use
the same fit we used on the training as
on the testing otherwise you get
different results and so we'll do just
oops not fit
transform we're only going to transform
the test side of the data so here's our
X test that we want to transform and
let's go ahead and run that and just so
we have an idea let's go ahead and take
and just print out our X train oh let's
do first 10 variables very similar to
the way you do with the head on a data
frame you can see here our variables are
now much more uniform and they've scaled
them to the same scale so they're
between certain numbers and with the
basic scalar you can fine tune it I just
let it do its defaults on this and
that's fine for what we're doing in most
cases you don't really need to mess with
it too much it does look like it goes
between like minus probably minus two to
two or something like that that's just
looking at the train variable I'm going
to cut that one out of there so before
we actually build the models and start
discussing the SK learn models we're
going to use we covered a lot of ground
here most of when you're working with
these models you put a lot of work into
pre prepping the data so we looked at
the data notice that it's separated and
loaded it up we went in there we found
out there's no null values it's hard to
say no nodal values we have there's none
there's none nobody I can't say it
and of course we sum it up if you had a
lot of null values this would be really
important coming in here so is there a
null summary we looked at pre-processing
the data as far as the quality and we're
looking at the bins so this would be
something you might start playing with
maybe you don't want super fine wine you
don't want the seven and eights maybe
you want to split this differently so
certainly you can play with the bins and
get different values and make the bins
smaller or lean more towards the lower
quality so you then have like medium to
high quality and we went ahead and gave
it labels again this is all pandas we're
doing in here setting up with unique
labels and group names bad good bad is
less than good that could be so
important you don't know how many times
people go through these models and they
have them reversed or something and then
they go back and they're like why is
this data not looking correct so it's
important to remember what you're doing
up here and double check it and we used
our label encoder so that was to set
that up as equality zero one good in
this case we have bad good zero one and
we just double check that to make sure
that's what came up in the quality there
and then we threw it into a graph
because people like to see graphs I
don't know about you but you start
looking at all these numbers and all
this text and you get down here and you
say oh yes you know here this is how
much of the wine we're going to label as
subpar not good this is how much we're
going to label as good and then we got
down here to finally separating out our
data so it's ready to go into the models
and the models take X and A Y in this
case X is all of our features minus the
one we're looking for and then Y is the
features we're looking for so in this
case we dropped quality and in the Y
case we added quality and then because
we need to have a training set and a
test set so we can see how good our
models do we went ahead and split the
models up X train X test y train y test
and that's using the train test split
which is part of the sklearn package and
we did as far as our testing size 0.2 or
20 percent the default is 25 so if you
leave that out it'll do default setup
and we did a random State equals 42. if
you leave that out it'll use a random
State I believe it's default one I'd
have to look that back up and then
finally we scaled the data this is so
important to scale the data going back
up to here if you have something that's
coming out as a hundred it's going to
really outweigh something that's 0.071
that's not in all the models different
models handle it differently and as we
look at the different models I'll talk a
little bit about that we're going to
only look at three models today three of
the top models used for this and see how
they compare and how the numbers come
out between them so we're going to look
at three different setups change my cell
here to mark down there we go and we're
going to start with the random Forest
classifier so the three steps we're
looking at is the random Forest
classifier support vector classifier and
then a neural network now we start with
the random Forest classifier because it
has the least amount of Parts moving
parts to fine-tune and let's go ahead
and put this in here so we're going to
call it RFC for random force classifier
and if you remember we imported that so
let me go back up here to the top real
quick and we did an import of the random
fourth classifier from SK learn Ensemble
and then we'll all we also let me just
point this out here's our svm where we
inputted our support Vector classifier
so svm is support Vector model support
vector classifier and then we also have
our neural network and we're going to
from there the multi-layered precipitron
classifier kind of a mouthful for the P
precipitron don't worry too much about
that name it's just it's a neural
there's a lot of different options on
there and setups which was where they
came up with the precipitron but so we
have our three different models we're
going to go through on here and then
we're going to weigh them here's our
metric so we're going to use a confusion
metrics also from the SK learn package
to see how good our model does with our
split so let's go back down there and
take a look at that and we have our RFS
equals random forest classifier and we
have n estimators equals 200. this is
the only value you play with with a
random Force classifier how many forests
do you need or how many trees in the
forest so how many models are in here
that makes it pretty good as a startup
model because you're only playing with
one number and it's pretty clear what it
is and you can lower this number or
raise it usually start up with a higher
number and then bring it down to see if
it keeps the same value so you have less
you know the smaller the model the
better the fit and it's easier to send
out to somebody else if you're going to
distribute it now the random 4 is
classified fire everything I read says
it's used for kind of a medium-sized
data set so you can run it in on Big
Data you can run it on smaller data
obviously but tends to work best in the
mid-range and we'll go ahead and take
our RFC and I just copied this from the
other side dot fit X train comma y train
so we're sending it our features and
then the quality in the Y train what we
want to predict in there and we just do
a simple fit now remember this is SK
learn so everything is fit or transform
another one is predict which we'll do in
just a second here let's do that now
predict
C equals and it's our RFC model predict
and what are we predicting on well we
trained it with our train values so now
we need our test our X test so this has
done it this is going to do this is the
three lanes of code we need to create
our random Forest variable fit our
training data to it so we're programming
it to fit in this case it's got 200
different trees it's going to build and
then we're going to predict on here let
me go ahead and just run that and we can
actually do something like oh let's do
predict
RFC just real quick we'll look at the
first 20 variables of it let's go ahead
and run that and in our first 20
variables we have three wines that make
the cut and the other 17 don't so the
other 17 are bad quality and three of
them are good quality in our predicted
values and if you can remember correctly
we'll go ahead and take this out of here
this is based on our test so these are
the first 20 values in our test and this
has as you can see all the different
features listed in there and they've
been scaled so when you look at these
they're a little bit confusing to look
at and hard to read but we have there's
a minus 01 so this is 0.36 minus 01 so
0.164 minus
0.09 or no it's still minus one so minus
0.9 all between 0 and 1 on here I think
I was confused earlier and I said 0
between 2 and negative 2. what's between
minus one and one which is what it
should be in the scale and we'll go
ahead and just cut that out of there run
this we have our setup button here so
now that we've run the prediction and we
have predicted values well one you could
publish them but what do we do with them
well we want to do with them is we want
to see how well our model model
performed that's the whole reason for
splitting it between a training and
testing model and for that remember we
imported the classification report
that was again from the SK learn there's
our confusion Matrix and classification
report and the classification report
actually sits on the confusion Matrix so
it uses that information and our
classification report we want to know
how good our y test that's the actual
values versus our predicted RFC so we'll
go ahead and print this report out and
let's take a look and we can see here we
have a Precision out of the zero we had
about 0.92 that were labeled as a bad
that were actually bad and out of
precision for the quality wines we're
running about 78 percent so you kind of
give us an overall 90 percent and you
can see our F1 score our support set up
on there a recall you could also do the
confusion Matrix sign here which gives
you a little bit more information but
for this this is going to be good enough
for right now we're just going to look
at how good this model was because we
want to compare the random fourth
classifier with the other two models and
you know what let's go ahead and put in
the confusion Matrix just so you can see
that on there with Y test and prediction
RFC so in the confusion Matrix we can
see here that we had
266 correct and seven wrong these are
the missed labels for bad wine and we
had a lot of mislabels for good wine so
our quality labels aren't that good
we're good at predicting bad wine not so
good at predicting whether it's a good
quality wine important to note on there
so that is our basic random forest
classifier and let me go ahead of cell
change cell type to mark down and run
that so we have a nice label let's look
at our svm classifier our support Vector
model and this would look familiar we
have our clf we're going to create
what's it we'll call it just like we
call this an RFC and then we'll have our
clf DOT fit and this should be identical
to up above X
train comma y train and just like we did
before let's go ahead and do the
prediction and here is our clf predict
and it's going to equal the clf dot
predict and we want to go ahead and use
x underscore test and right about now
you can realize that you can create
these different models and actually just
create a loop to go through your
different models and put the data in and
that's how they designed it they
designed it to have that ability let's
go ahead and run this and then let's go
ahead and do our classification report
and I'm just going to copy this right
off of here
they say you shouldn't copy and paste
your code and the reason is is when you
go in here and edit it
you invariably will miss something we
only have two lines I think I'm safe to
do it today and let's go ahead and run
this
and let's take a look how the svm
classifier came out so up here we had a
90 percent and down here we're running
about an 86 percent so it's not doing as
good now remember we randomly split the
data so if I run this a bunch of times
you'll see some changes down here so
these numbers this size of data if I
read it 100 times it would probably be
within plus or minus three or four on
here in fact if I ran this 100 times
you'd probably see these come out almost
the same as far as how well they do in
classification and then on the confusion
Matrix let's take a look at this one
this had 22 by 25 this one has 35 by 12.
so it's doing not quite as good that
shows up here 71 percent versus 78
percent and then if we're going to do a
svm classifier we also want to show you
one more and before I do that it kind of
tease you a little bit here before we
jump into neural networks the big save
all deep learning because everything
else must be shallow learning that's a
joke let's just talk a little bit about
out the svm versus the random Forest
classifier the svm tends to work better
on smaller numbers it also works really
good on because a lot of times you
convert things into numbers and bins and
things like that the random Forest tends
to do better with those at least that's
my brief experience with it where if you
have just a lot of raw data coming in
the svm is usually the fastest and
easiest to apply model on there so they
each have their own benefits you'll find
though again that when you run these
like a hundred times difference between
these two on a data set like this is
going to just go away there's Randomness
involved depending on which data we took
and how they classify them the big one
is the neural networks and this is what
makes the neural networks nice is they
can do they can look into huge amounts
of data so for a project like this you
probably don't need a neural network on
this but it's important to see how they
work differently and how they come up
differently so you can work with huge
amounts of data you can also many
respects they work really good with text
analysis especially if it's time
sensitive more and more you have an
order of text and they've just come out
with different ways of feeding that data
in where the series and the Order of the
words is really important same thing
with starting to predict in the stock
market if you have tons of data coming
in from different sources the neural
network can really process that in a
powerful way to pull up things that
aren't seen before when I say lots of
data coming in I'm not talking about
just the high lows that you can run an
svm on real easily I'm talking about the
data that comes in where you have maybe
you pulled off the Twitter feeds and
have word counts going on and you've
pulled off the the different news feeds
the business are looking at and the
different releases when they release the
different reports so you have all this
different data coming in and the neural
network does really good with that
pictures picture processing Now is
really moving heavily into the neural
network if you have a pixel 2 or pixel 3
phone put out by Google it has a neural
network for doing it's kind of goofy but
you can put Little Star Wars Androids
dancing around your pictures and things
things like that that's all done with
the neural network so it has a lot of
different uses but it's also requires a
lot of data and is a little heavy-handed
for something like this and this should
now look familiar because we've done it
twice before we have our multi-layered
precipitron classifier we'll call it an
mlpc and it's this is what we imported
mlpc classifier there's a lot of
settings in here the first one is the
hidden layers you have to have the
hidden layers in there we're going to do
three layers of 11 each so that's how
many nodes are in each layer as it comes
in and that was based on the fact we
have 11 features coming in then I went
ahead and just did three layers probably
get by with a lot less on this but yeah
I did want to sit and play with it all
afternoon again this is one of those
things you play with a lot because the
more hidden layers you have the more
resources you're using you can also run
into problems with over fitting with too
many layers and you also have to run
higher iterations the max iteration we
have is set to 500. the default's 200
because I use three layers of 11 each
which is by the way kind of a default I
use I realized it usually have about
three layers going down and the number
of features going across you'll see that
it's pretty common for the first
classifier when you're working in neural
networks but it also means you have to
do higher iterations so we up the
iterations to 500 so that means it's
going through the data 500 times to
program those different layers and
carefully adjust them and we do have a
full tutorials you can go look up on
neural networks and understand the
neural network settings a lot more and
of course we have you're looking over
here where we had our previous model
where we fit it same thing here mlpc fit
X train y train and then we're going to
create our prediction so let's do our
predict and mlpc and it's going to equal
the mlpc and we'll just take the same
thing here predict X test
put that down here dot predict a test
and if I run that we've now programmed
it we now have our prediction here same
as before and we'll go ahead and do the
copy print again always be careful with
the copy paste now because you always
run the chance of missing one of these
variables so if you're doing a lot of
coding you might want to skip that copy
and paste and just type it in let's go
ahead and run this and see what that
looks like and we came up with an 88
percent we're going to compare that with
the 86 from our tree our svm classifier
and our 90 from the random forest
classifier and keep in mind random
Forest classifiers they do good on
mid-size data the svm on smaller amounts
of data although to be honest I don't
think that's initially the split between
the two and these things will actually
come together if you random number of
times and we can see down here the noun
of good wines mislabeled with the setup
on there it's on par with our random
Forest so it had 22 25 should be a
surprise it's identical it just didn't
do as good with the bad wines labeling
what's a bad one and what's not so yeah
because they had 266 and seven we head
down here 260 and 13. so mislabeled a
couple of the bad wines as good wines so
we've explored three of these basic
classifiers these are probably the three
most widely used right now I might even
throw in the random tree if we open up
their website we go under supervised
learning there's a linear model we
didn't do that almost most of the data
usually just start with a linear model
because it's going to process the
quickest I'm going to use the least
amount of resources but you can see they
have linear quadratic they have kernel
Ridge there's our support Vector of
stochastic gradient nearest neighbors
nearest neighbors is another common one
that's used a lot very similar to the
svm gaussian process cross decomposition
naive Bayes this is more of an
intellectual one that I don't see used a
lot but it's like the basis of a lot of
other things decision tree there's
another one that's used a lot Ensemble
methods not as much multi-class and
multi-label algorithms feature selection
neural networks that's the other one we
use down here and of course the forest
so you can see there's a in sklearn
there are so many different options and
they've just developed them over the
years we covered three of the most
commonly used ones in here and went over
a little bit over why they're different
neural network just because it's fun to
work in deep learning and not in Shallow
learning as I told you that doesn't mean
that the svm is actually shallow it does
a lot of it covers a lot of things and
same thing with the decision for the
random forest classifier and we notice
that there's a number of other different
classifier options in there these are
just the three most common ones and I'd
probably throw the nearest neighbor in
there and the decision tree which is
usually part of the decision for us
depending on what the back end you're
using and since as human beings if I was
in the shareholders office I wouldn't
want to leave them with a confusion
Matrix they need that information for
making decisions but we want to give
them just one particular score and so I
would go ahead and we have our okay
learn metrics we're going to import the
accuracy score and I'm just going to do
this on the random 4 since that was our
best model and we have our CM accuracy
score and I forgot to print it if you
remember in Jupiter notebook we can just
do the last variable we leave out there
it'll print and so our CM accurate score
we get is 90 percent and that matches up
here we should already see that up here
in Precision so you can either quote
that but a lot of times people like to
see it highlighted at the very end this
is our Precision on this model and then
the final stage is we would like to use
this for future so let's go ahead and
take our wine if you remember correctly
we'll do one head of 10. we'll run that
remember our original data set we've
gone through so many steps then we're
going to go back to the original data
and we can see here we have our top 10
our top 10 on the list only two of them
make it as having high enough quality
wine for us to be interested in them and
then let's go ahead and create some data
here we'll call it X Nu equals and this
is important this data has to be we just
kind of randomly selected some data
looks an awful lot like some of the
other numbers on here which is what it
should look like and so we have our X Nu
equals
7.3.58 and so on and then it is so
important this is where people forget
this step X new equals SC remember SC
that was our standard scalar variable we
created if we go right back up here
before we did anything else we created
an sc we fit it and we transformed it
and then we need to do what transform
the data we're going to feed in
so we're going to go back down here and
we're going to transform our X Nu and
then we're going to go ahead and use the
where are we at here we go our random
forest and if you remember all it is is
our RFC predict model right there let's
go ahead and just grab that down here
and so our y Nu equals here's our rafc
predict and we're going to do our X new
in and then it's kind of nice to know
what it actually puts out so according
to this it should print out what our
prediction is for this wine and oh it's
a bad wine okay so we didn't pick out a
good wine for our X new and that should
be expected most of wine if you remember
correctly only a small percentage of the
wine met our quality requirements so we
can look at this and say oh we'll have
to try another wine out which is fine by
me because I like to try out new wines
and I certainly have a collection of old
wine bottles and very few of them match
but you can see here we've gone through
the whole process just a quick rehash we
had our Imports we touched a lot on the
sklearn our random Forest our svm and
our MLP classifier so we had our support
vector classifier we had our random
forests and we have our neural network
three of the top used classifiers in the
sklearn system and we also have our
confusion metric Matrix and our
classification report which we used our
standard scalar for scaling it and our
label encoder and of course we need to
go ahead and split our data up in our
implot line train and we explored the
data in here for null values we set up
our quality into bins we took a look at
the data and what we actually have and
put a nice little plot to show our
quality what we're looking at and then
we went through our three different
models and it's always interesting
because you spend so much time getting
to these models and then you kind of go
through the models and play with them
until you get the best training on there
without becoming biased that's always a
challenge is to not over train your data
to the point where you're training it to
fit the test value and finally we went
ahead and actually used it and applied
it to a new wine which unfortunately
didn't make the cut it's going to be the
one that we drink a glass out of and
save the rest for cooking of course
that's according to the random Forest on
there because we use the best model that
it came up with and this was all for
this tutorial hope you guys found it
informative and helpful if you like this
session then like share and subscribe if
you have any questions then you can drop
them in the comment section below thanks
for watching and stay tuned for more
from Simply learn
staying ahead in your career requires
continuous learning and upskilling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in Cutting Edge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know more
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign