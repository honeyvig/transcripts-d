welcome to the complete AI full course
by simply we will explore the
fascinating world of the artificial
intelligence what is transforming how we
live and work every day whether you are
new to AI or already have some knowledge
this course is designed for everyone we
will explain AI Concept in simple terms
starting with the basics and moving on
to the advanced topics like machine
learning neural networks and deep
learning you will also see how AI is
used in real life situation and so solve
problems and create new opportunities
with the growing demand for AI in 2024
learning these skills will open doors to
many highp paying career opportunities
Industries ranging from Tech to
healthcare by the end of this course you
will have a strong understanding of AI
and be ready to apply these skills to
your projects ofer let's start with the
exciting AI journey together craving a
career upgrade subscribe like and
comment
below dive into the link in the
description to FasTrack your ambition i
s whether you're making a switch or
aiming higher simply learn has your
back before we commence if you want to
enhance your career in AI here is a
quick info for you check out Simply
postgraduate program in machine learning
and AI with P University and IBM this
program is perfect for aspiring AI
Enthusiast and professional looking to
switch careers gain expertise in
generative AI prompt engineering charity
explainable AI machine learning
algorithms and many more a year of
experience is prefer hurry up P Now find
the co link from the description box
below and the pin comment so without any
further Ado let's get started and today
we're going to go over what is
artificial intelligence one example of
today's artificial intelligence is a
smart home so welcome to my Smart Home
Smart Homes are run by artificial
intelligence let's have a look at some
of the key features of a smart housee
home appliances are voice controlled
sensors adjust lights and air coolers
according to the climate security
systems can detect movement outside and
warn the residents all the appliances
are interconnected to each other and can
detect vehicles in the driveway and let
the owner know home appliances can be
controlled remotely through the phone
now this is just one example in today's
world let us start with a brief history
of artificial intelligence let's start
with John McCarthy he was the first one
to coin the word artificial intelligence
and in 1956 had the first artificial
intelligence conference let's jump
forward to 1969 with shaky shaky was the
first general purpose mobile robot built
and although by today's standard he was
very simple he did Mark a milestone and
that we were now processing data
differently he was now able to do things
with a purpose versus just a list of
instructions I'll be it the list
included things like turning on and off
lights and pushing boxes around the room
we're going to move forward to
1997 in 1997 the supercomputer deep blue
was designed which defeated the world
chest champion in game this is the first
time when we're actually seeing the
computer using logic to beat a human
doing some kind of logical uh in this
case a game so it was a huge milestone
by IBM to create this large computer
that was able to do that in a timely
fashion let's jump up to 2002 when we
have our first commercially successful
robotic vacuum cleaner nowadays you can
go down to the Target and buy one but
back in 2002 that was the first Model
that was put out and finally 2005 to
2018 today in the last more than a
decade we have speech recognition RPA
dancing robots Smart Homes and many more
to come what I want you to notice about
this brief history of artificial
intelligence is the compression of time
in 1956 it was just an idea and more
than 10 years later we have our shaky
the first one who's able to flip light
switches and then we go all the way to
97 where we have deep blue and there's a
lot of Little Steps between 69 and 97
but it's actually able to take on a
human in chess and beat them regularly
now they did have a lot of ties so don't
think that it won every game and then we
go only four years later we have our
first commercially successful robotic
vacuum cleaner and as we go from 2005 to
2018 we go from simple speech
recognition to very complicated able to
really register with your Google Voice
and your Siri we have our RPA we have
dancing robots Smart Homes and many more
things that are just coming out almost
monthly in the world of artificial
intelligence so what is is artificial
intelligence AI is a form of computer
science used to create intelligent
machines that can recognize human speech
objects can learn plan and solve
problems like humans and I'd like you to
focus just on that last one can solve
problems like humans as we saw earlier
we compare the computer that can beat
the chess champion it's able to the one
before that able to turn on and off
lights in the house so right now our
concept of artificial intelligence
is based primarily on our understanding
of how it interacts with humans and how
we can compare it to humans so speech
recognition that's a big one today
object detection solve problems and
learn from the given data and plan an
approach for future tasks to be done
these are all very human things that we
do to plan and solve for the future
types of artificial intelligence hi I
heard you want to know the types of AI
you can imagine a little robot taking us
on a tour the first type is purely
reactive the second type is limited
memory the third type theory of mind
which is still in the process of being
invented and definitely the question of
self-awareness being the fourth which is
still not invented yet or is it let's
look at these a little closer here we
have our purely reactive he does not
have any past memory or data to work
with he takes actions reacts based on
what he sees okay it's not that tough
observes every move you can see we have
a nice game of chess going on here and
wins takes the best possible decision
purely reactive machines specialize in
one field of work only in this case we
see a chess game where they figure out
the best moves calculating all the
different moves maybe it's calculating
the next one in a row in a linear
regression model so you can figure out
the best marketing but they're very
reaction of they don't have a lot of
data they just have what's in front of
them to look at next let's look at
limited memory let's try to understand
the limited memory setup these machines
use previous data and keep adding it to
their memory sogg just me a good
restaurant so maybe we're looking to go
out for dinner tonight and we're not
sure what we want to eat H I see you
have been to KFC quite a lot in the past
week there is a KFC nearby so it goes
through the previous location data and
where they ate prior and says M this is
probably a good suggestion for you
tonight thanks it has enough memory or
experience to make proper decisions but
the memory is very limited this isn't
trying to uh guess what a new location
would be or anything like that it just
takes what it has in front of it and
goes okay this is where you've been this
is what you liked you gave it two thumbs
up one thumb down whatever and then it
goes from these suggestions this is
where I think you should go theory of
mind this kind of AI has the capacity to
understand thoughts and emotions and
interact socially a machine based on
this type is yet to be built that human
looks lonely hey human you want to be my
friend huh okay theory of mind is the
thing of the future certainly we see a
lot of the industry poking a little bit
at this as it tries to guess how you
feel about things but most of that is
still based on previous data you know
yes no two thumbs up one thumb down
theory of mind would take it one step
further in understanding the emotions
behind it and finally as we get deep
into the Sci-Fi futuristic we have
self-aware self-aware machines are
future generation of machines they will
be super intelligent Cent sentient and
conscious like the Terminator the good
guy in the thing or Ultron or vision
from Avengers for right now these are
mostly movie characters or cartoon
characters but we certainly are getting
closer to seeing them in the real world
hopefully they'll be of the type of the
good guys and not the bad guys we see
also in the artificial intelligent
Sci-Fi movies applications of artificial
intelligence let's take a look at some
of today's commercial and business uses
for the AI we have banking fraud
detection from a large data consisting
of fraudulent and non-f fraudulent
transactions the AI learns to predict if
a new transaction is fraud or not online
customer support most of the customer
support is now being automated by
artificial intelligence cyber security
using machine learning algorithms and a
lot of sample data AI can be used to
detect anomalies adapt and respond to
threats virtual assistance Siri Cortana
Alexa and Google Now use voice
recognition to follow the user's
commands these are all wonderful
examples of current AIS that are in the
commercial and business world and these
ones in particular have matured over the
last half a decade for instance very few
large banks in today world would not use
banking for fraud and detection or for
deciding whether it's a good person to
give a loan to or not based on their
credit scores and where they're from and
their income same thing with cyber
security detecting anomalies or online
customer support could you imagine in uh
say HP who has over
70,000 help Pages across 17 different
languages now they have to figure out
how to do online customer support to
cover new problems that come up and
track them so they can build new pages
if they had one person doing that that
would take them a year just to do what
they need to have posted yesterday and
of course our virtual assistants I don't
know about you but I love mine kind of
like having a private secretary without
having a private secretary so let's talk
about is AI is a good career or not you
have probably heard a lot about
artificial intelligence or AI it's
everywhere and it's shaking up
Industries all over the world but here's
the big question is AI a good career
choice yes absolutely it is take Elon
Musk for example we all know him as the
guy behind Tesla and SpaceX but did you
know he also co-founded open AI even a
la diving into Ai and that just shows
how massive this field is becoming and
guess what AI isn't just for Tech
Geniuses there's room for everyone let's
talk about numbers AI jobs are growing
like crazy up to 32% in recent years and
the pay is pretty sweet with roles
offering over $100,000 a year so whether
you're into engineering research or even
the ethical side of the things AI has
something for you plus the skills you
pick up in AI can be used in all sorts
of Industries making it a super flexible
career choice now ai is a big field and
there are tons of different jobs you can
go for let's break down some of the key
roles first up we have machine learning
Engineers these folks are like the
backbone of AI they build models that
can analyze huge amounts of data in real
time if you've got a background in data
science or software engineering this
could be your thing the average salary
is around
$131,000 in the US then there's data
scientist the detectives of the AI World
they dig into Data to find patterns that
help businesses make smart decisions if
you're good with programming and stats
this is a great option and you can make
about
$105,000 a year next we've got business
intelligence developers they are the
ones to process and analyze data to
sport trends that guide business
strategies if you enjoy working with
data and have a background in computer
science this role might be for you the
average salary here is around
$87,000 per year then we've got research
scientist these these are the ones
pushing AI to new heights by asking
Innovative questions and exploring new
possibilities it's a bit more academic
ofer needing Advanced degrees but it's
super rewarding with salaries around
$100,000 next up we have big data
engineers and Architects these are the
folks who make sure all the different
parts of business's technology talk to
each other smoothly they work with tools
like Hadoop and Spark and they need
strong programming and data
visualization skills and get this the
average salary is one of the highest in
AI around
$151,000 a year then we have ai software
engineer these engineers build a
software that powers AI application they
need to be really good at coding and
have a solid understanding of both
software engineering and AI if you enjoy
developing software and want to be a
part of the air Revolution this could be
your role the average salary is around
$108,000 now if you're more into
designing systems you might want to look
at becoming a software architect these
guys design and maintain entire AI
system making sure everything is
scalable and efficient with expertise in
Ai and Cloud platforms software
Architects can earn Hefty salary about
$150,000 a year let's not forget about
the data analyst they have been around
for a while but their role has evolved
big time with AI now they prepare data
for machine learning models and create
super insightful reports if you're
skilled in SQL Python and data
visualization tools like Tabu this could
be a great fit for you the average
salary is around
$65,000 but it can go much higher in
tech companies another exciting rules is
robotics engineer these Engineers design
and maintain AI powered robots from
Factory robots to robots that help in
healthcare they usually need Advanced
degrees in engineering and strong skills
in AI machine learning and iot Internet
of Things the average salary of Robotics
engineer is around
$887,000 with experience it can go up to
even more last but not the least we have
got NLP Engineers NLP stands for natural
language processing and these Engineers
specialize in teaching machines to
understand human language think voice
assistants like Siri or Alexa to get
into this role you'll need a background
in computational linguistics and
programming skills the average salary of
an NLP engineer is around
$78,000 and it can go even higher as you
gain more experience so you can see the
world of AI is full of exciting
opportunities whether you're into coding
designing systems working with data or
even building robots there's a role for
you in this fastest growing field so
what skills do you actually need to
learn to land an entry-level AI position
first first off you need to have a good
understanding of AI and machine learning
Concepts you'll need programming skills
like python Java R and knowing your way
around tools like tensor flow and Pie
torch will help you give an edge to and
do not forget about SQL pandas and big
Technologies like Hadoop and Spark which
are Super valuable plus experience with
AWS and Google cloud is often required
so which Industries are hiring AI
professionals AI professionals are in
high Dem demand across a wide range of
Industries here are some of the top
sectors that hire AI Talent technology
companies like Microsoft Apple Google
and Facebook are leading the charge in
AI Innovation consulting firms like PWC
KPMG and accenta looking for AI experts
to help businesses transform then we
have Healthcare organizations are using
AI to revolutionize patient with
treatment then we have got retail giants
like Walmart and Amazon leverage AI to
improve customer experiences then we
have got media companies like Warner and
Bloomberg are using AI to analyze and
predict Trends in this media industry AI
is not just the future it's the present
with right skills and determination you
can carve out a rewarding career in this
exciting field whether you're drawn to a
technical challenges or strategic
possibilities there's a role in AI
that's perfect for you so start building
your skills stay curious and and get
ready to be a part of the air Revolution
the term generative AI has emerged
seemingly out of nowhere in recent
months with a notable search in interest
according to Google Trends even within
the past year the spike in curiosity can
be attributed to the introduction of
generative models such as d 2 B and
chgb however what does generative AI
entail as a part of our introductory
series on generative AI this video will
provide a comprehensive overview of a
subject starting from the basics the
explanation Will C to all levels of
familiarity ensuring that viewers gain a
better understanding of how this
technology operates and its growing
integration to our daily lives
generative AI is after all a tool that
is based on artificial intelligence what
is generative AI generative AI is a form
of artificial intelligence possesses the
capability of to generate a wide range
of content including text visual audio
and synthetic data the recent excitement
surrounding generative AI stems from the
userfriendly interfaces that that allow
users to effortlessly create high
quality text graphics and video within a
seconds now moving forward let's see how
does generative AI Works generative AI
begin a prompt which can take form of
text image video design audio musical
notes or any input that AI system can
process various AI algorithm that
generate new content in response to the
given prompt this content can range from
essay and problem solution to realistic
created using images of or audio of a
person in the early stages of generative
AI utilizing the technology involved
submitting data through an API or a
complex process developers need to
acquaint themselves with a specialized
tool and writing application using
programming language like python some of
the recent and fully operational
generative AIS are Google Bart Dal open
AI chgb Microsoft Bing and many more so
now let's discuss chat GPT Dal and B
which are the most popular generative AI
in faes so first is d 2 which was
developed using open as GPT
implementation in 2021 exemplify a
multimodel AI application it has been
trained on a v data set of images and
their corresponding textual description
Dal is capable of establishing
connection between various media forms
such as Vision text audio it
specifically links the meaning of words
to visual elements open a introduced an
enhanced version called d 2 in 22 which
empowers user to generate imagery in
multiple Styles based on their prompts
and the next one is chity in November
2022 chat GPT and AI power chatboard
built on open AI GPT 3.5
implementation gained immense popularity
worldwide open AI enabled user to
interact with and fine-tune the chatbot
text response through a chat interface
with interactive feedback unlike earlier
version of GPT that was solely
accessible via an API CH GPT brought a
more interactive experience on March 14
2023 open a released GPT 4 CH GPT
integrated the conversational history
with a user making a genuine dialogue
Microsoft impressed by the success of
new chgb interface announced a
substantial investment in open Ai and
integrated a version of GPT into its
Bing search engine and the next one is
Bard Google bard Google was also an
earlier Fortuner in advancing
Transformer techniques for language
processing protein analysis and other
content types it made some of these
model open source for researchers but
were not made available through a public
interface in response to Microsoft
integration of GPT into Bing Google
hardly launched a public facing chat
about name Google b b debut was met by
an error when the language model
incorrectly claimed that the web
telescope was the first to discover a
planet in a foreign solar system as a
consequences Google stock priz suffered
a significant and decline meanwhile
Microsoft implementation of chat GPT and
GPT power system also face criticism for
producing inaccurate result and
displaying eratic behavior in their
early iritation so moving forward let's
see what are the use cases of generative
AI generative AI has broad applicability
and can be employed across a wide range
of use cases to generate diverse form of
content recent advancement like GPT have
made this technology more accessible and
customizable for various application
some notable use cases for generative AI
are as follows chatbot implementation
generative AI can be utilized to develop
chatbot for customer service and
Technical Support enhancing interaction
with users and providing efficient
assistance the second one is language
dubbing announcement in the real in the
realm of movies and educational
accountant generative AI can contribute
to improving dubbing in different
languages ensuring accurate and high
quality translation and the third one is
content writing generative AI can assist
in writing email responses dating
profiles resum√©s and term papers
offering valuable support and generating
customized content tailor to specific
requirement and the fourth one is Art
generation leveraging generative AI
artists can create photo realistic
artwork in various Styles enabling the
exploration of new artistic expression
and enhancing creativity the fifth one
is product demonstration videos
generative AI can hun to enhance protect
demonstration video making them more
engaging visually appealing and
effective in showcasing product features
and benefits so generative AI
versatility allow it to employed in many
other application making it a valuable
tool for Content creation and enhancing
user experience across diverse domains
so after seeing use cases of generative
AI let's see what are the benefits of
generative AI so generative AI offers
extensive application across various
business domains simplifying the
interpretation and comprehension of
existing content while also enabling the
automated creation of of a new content
developers are actively exploring ways
to leverage generative AI in order to
enhance the optimize existing workflow
and even to reshave workflows entirely
to harness the potential of Technology
fully implementing generative AI can
bring numerous benefits including
automated content creation generative AI
can automate the manual process of
writing content saving time and effort
by generating text or other form of
content the next one is efficient email
response responding to emails can be
made more efficient with generative AI
reducing the effort required and
improving response time and the third
one is enhanced technical support
generative AI can improve responses to
specific technical queries providing
accurate and helpful information to
users or customers and the fourth one is
realistic person Generation by
leveraging generative AI it becomes
possible to create realistic
representation of people enabling
applications like virtual characters or
avatars and the fifth one is cerent
information summarization generative AI
can summarize complex information into a
coherent narrative distilling key points
and making it easier to understand and
communicate complex concept the
implementation of generative AI offers a
range of potential benefits steamingly
processed and enhancing content Creation
in various areas of business operation
so after seeing advantages of generative
AI let's move forward and see what are
the limitations of generative AI
early implementation of generative AI
serve as Vivid examples highlighting the
numerous limitation associated with this
technology several challenges arise from
the specific approaches employed to
implement various use gies for instance
while a summary of a complex topic May
more reader friendly than explanation
incorporating multiple supporting
sources the ease of readability comes at
the expense of transparent identifying
the information sources so the first one
is when implementing or utilizing a
generative AI application it is
important to consider the following
limitation I repeat the first one is
lack of source identification generative
AI does not always provide clear
identification of content Source making
it difficult to trace and verify origin
of the information the second one is
assessment of bias assessing the bias of
original sources used generative AI can
be challenging as it may be difficult to
determine the underlying perspective or
agenda of the data utilized in the
training process the third one is
difficulty in identifying inaccurate
information generative AI can generate
realistic content making identifying
inaccuracy or falsehoods within the
generated output harder and the fourth
one is adaptability to a new
circumstances understanding how to
fine-tune generative AI for a new
circumstances or specific context can be
complex requiring careful consideration
and expertise to achieve desired result
and the fifth one is glossing over bias
Prejudice and hatred generative AI
results May amplify or preate biases
prejudices or hateful content present in
the training data requiring Vigilant
scrutiny to prevent such issues so
awareness of these limitation is crucial
when the implementing of utilizing
generative AI as it helps users and
developers critically evaluate and
mitigate potential risk and challenges
associated with the technology so future
of generative AI furthermore advances in
a development platforms will contribute
to the accelerated progress of research
and development in the realm of
generative AI the development will
Encompass various domains such as text
images videos 3D contact drugs Supply
chains logistic and business processes
while the current stand loan tools are
impressive the true transformative
impact generative AI will realize while
these capabilities are seemingly into
the existing tools with regular use
today we'll take you through the
exciting road map of becoming an AI
engineer if our content picks your
interest and helps feel your curiosity
don't forget to subscribe to our Channel
hit that Bell icon so you never miss an
update now let's embark on this AI
journey together as artificial
intelligence continues to revolutionize
various Industries AI Engineers stand at
the Forefront of this technological wave
these professionals are essential in
crafting intelligent systems that
address complex business challenges AI
projects often stumble due to poor
planning Saar architecture or
scalability issues AI Engineers p
crocher role in overcoming these hurdles
by merging Cutting Edge AI Technologies
with strategic mless insights so in this
video we guide you to the essentials of
becoming an AI engineer let's start with
the basics what does an AI engineer do
an AI engineer builds AI models using
machine learning algorithms and deep
learning neural networks these models
are pivotal in generating business
insights that influence organizational
decision making from developing
applications that leverage sentiment
analysis for contextual advertising in
to creating systems for visual
recognition and language translation the
scope of an AI engineer's work is vast
and impactful so to succeed as an AI
engineer you need a blend of technical
progress and soft skills so now let's
break down this 8th month plan month one
computer science fundamentals and
beginners python so before we delve into
AI it's crucial to establish a strong
foundation in computer science this
month you should focus on the following
topics data representation understanding
bits and bytes how text and numbers are
and the binary number system is
foundational for everything in Computing
this knowledge helps in comprehending
how computers interpret and process data
now next comes computer networks learn
the basics of computer networks
including IP addresses and internet
routing protocols it's essential to
understand how data travels across
networks using UDP TCP and HTTP which
form the backbone of the internet and
the worldwide web next comes programming
Basics begin with the basics of
programming like variables strings
numbers conditionals loops and algorithm
Basics these fundamentals will allow you
to write and understand simple programs
simultaneously you'll also start with
python the preferred language for AI so
learn about variables numbers strings
lists dictionaries sets tuples and
control structures like if conditionals
and for loops and then move on to
functions and modules understand how to
create functions including Lambda
functions and work with modules by using
pip install to add functionality to your
projects next comes file handling and
exceptions you should also practice
reading from and writing to files as
well as handling exceptions to make your
programs more robust finally graas the
basics of classes and objects which are
crucial for writing organized and
efficient code so this comprehensive
overview sets the stage for more complex
programming tasks that you'll encounter
in the following months now in month two
you'll move on to data structure
algorithms and advanced python so
building on the foundations from month
one we'll now delve into data structure
and algorithm so familiarize yourself
with the concept of big notation to
understand the efficienc of different
algorithms and data structures learn
about arrays link list hash tables tacks
cues trees and graphs mastering these
structures will allow you to store and
manipulate data effectively now next
comes algorithms you should explore
algorithms such as binary search bubble
sort quick sort merge sort and recursion
these are essential for optimizing your
code and parall you'll Advance your
python skills so you can dive into
inheritance generators iterations list
comprehensions decorators
multi-threading and multi processing
these topics will enable you to write
more efficient and scalable code so this
month's learning prepares you to handle
complex data operations and enhance your
coding efficiency now in month three
you'll move on to Version Control SQL
and data manipulation so in the third
month the focus shifts to collaboration
and data management number one Version
Control so understand the importance of
Version Control Systems especially git
and GitHub so learn basic commands such
as ADD commit and push you should also
learn how to handle branches reward
changes and assent Concepts like head
diff and merch so these skills are
invaluable for tracking changes and
collaborating with other developers next
pool requests Master the art of creating
and managing pool requests to contribute
to collaborative projects next we'll
dive into SQL for managing databases so
first we'll start with SQL Basics so
learn about relational databases and how
to perform basic queries and then you'll
move on to Advanced queries understand
complex query techniques such as CT
subqueries and window functions and then
comes joins and database manag so study
different types of joints like Left
Right inner and full joint you should
also learn how to create databases
manage indexes and write stored
procedures Additionally you will use
numai and pandas for data manipulation
and learn basic data visualization
techniques this comprehensive skill set
will be crucial As you move into more
advanced data science topics so now in
month four you'll deal with maths and
statistics for AI so mathematics and
statistics are the backbone of AI and
this month is dedicated to these
critical subjects so first learn about
descriptive versus inferential
statistics continuous versus discrete
data nominal versus ordinal data
measures of central tendency like mean
median mode and measures of dispersion
like variance and standard deviation
after that understand the basis of
probability and delve into normal
distribution correlation and cience
after which you should move on to
advanced concepts so you can study the
central limit theorem hypothesis testing
P values confidence intervals and so on
in parallel you should also study linear
algebra and calculus so in linear
algebra learn about vectors metrices
Egan values and Egan vectors and in
calculus cover the basics of integral
and differential calculus so this
mathematical Foundation is essential for
developing and understanding AI models
setting you up for success as you
transition into machine learning now in
month five comes exploratory data
analysis which is Eda and machine
learning so with a solid foundation in
math and statistics you are now ready to
delve into machine learning number one
pre-processing learn how to handle na
values treat out layers perform data nor
I ization and conduct fature engineering
you should also understand encoding
techniques such as one hard and label
encoding you'll also explore supervised
and unsupervised learning with a focus
on regression and classification and
learn about linear models like linear
and logistical regression and nonlinear
models like decision tree random Forest
Etc and then understand how to evaluate
models using metrics such as mean square
error mean absolute error me for
regression and accuracy precision recall
Etc then comes hyperparameter tuning
learn about techniques like search CV
and random search CV for optimizing your
models after which we'll move on to
unsupervised learning here you can study
clustering techniques like K means and
hierarchical clustering and delve into
dimensionality reduction with PCA so
this month's focus on Eda and model
building will prepare you for more
complex AI applications transitioning to
the next phase you'll begin to work on
deploying these models and real world
scenarios so in month six comes mlops
and machine learning projects so this
month we'll cover the operational
aspects of machine learning and work on
practical projects s so in mlops Basics
learn about apis particularly using fast
API for Python and server development
understand devops fundamentals including
cicd pipelines and containerization with
Docker and Cub needs you should also
gain familiarity with at least one Cloud
platform like AWS or aure now in month 7
comes deep learning so in this month we
delve into the world of deep learning so
number one comes noodle Network so learn
about noodle networks including forward
and backward propagation and build
multi-layer per R after which you move
on to Advanced architectures so here
explore convolutional neural networks
which are CNN for image data and
sequence models like rnms and lsdm so
this deep learning knowledge will be
crucial as we move into specialized
areas of AI in the final month now in
the final month the eighth month comes
NLP or computer vision so the final
month you have the option to specialize
in either natural language processing
NLP or computer vision so first we leave
with NLP track so here you should learn
about rejects text representation
methods like count vectorizing tfidf b
word TUC embeddings and text
classification with Nave base and
familiarize yourself with the
fundamentals of libraries like Spacey
and nltk and work on end to end NLP
project and talking about computer
vision track focus on basic image
processing techniques like filtering
Edge detection image scaling and
rotation utilize libraries like open CV
and build upon the CNN Knowledge from
the previous month practice data
preprocessing and augmentation so by the
end of this month you should have a
solid foundation in your chosen
specialization ready to embark on your
AI engineering career so in conclusion
adopting AI is more than just a trend
it's a strategic move that can transform
your organization's approach to machine
learning so that's all for today's video
on becoming an AI engineer we hope you
found this guide valuable and
informative so what is deep learning
deep learning is a subset of machine
learning which itself is a branch of
artificial intelligence unlike
traditional machine learning models
which require manual feature extraction
deep learning models automatically
discovers representation from raw data
so this is made possible through neural
networks particularly deep neural
networks which consist of multiple
layers of interconnected nodes so these
neural network are inspired by the
structure and the function of human
brain each layer in the network
transform the input data into more
abstract and composite representation
for instance in image recognition the
initial layer might detect simple
features like edges and textures while
the deeper layer recognizes more complex
structure like shap shapes and objects
so one of the key advantage of deep
learning is its ability to handle large
amount of unstructured data such as
images audios and text making it
extremely powerful for various
application so stay tuned as we delve
deeper into how these neural networks
are trained the types of deep learning
models and some exciting application
that are shaping our future types of
deep learning deep learning AI can be
applied supervised unsupervised and
reinforcement machine machine learning
using various methods for each the first
one supervised machine learning in
supervised learning the neural network
learns to make prediction or classify
that data using label data sets both
input features and Target variables are
provided and the network learns by
minimizing the error between its
prediction and the actual targets a
process called B propagation CNN and RNN
are the common deep learning algorithms
used for tasks like image classification
sentiment analysis and language
translation the second one unsupervised
machine learning in unsupervised machine
learning the neural network discovers Ms
or cluster in unlabelled data sets
without Target variables it identifies
hidden pattern or relationship within
the data algorithms like Auto encoders
and generative models are used for tasks
such as clustering dimensionality
reduction and anomaly detection the
third one reinforcement machine learning
in this an agent learns to make decision
in an environment to maximize a reward
signal the agent takes action observes
the records and learns policies to
maximize cumulative rewards over time
deep reinforement learning algorithms
like deep Q networks and deep
deterministic poly gradient are used for
tasks such as Robotics and gameplay
moving forward let's see what are the
artificial neural networks artificial
neural networks Ann's inspired by the
structure and the function of human
neurons consist of interconnected layers
of artificial neurals or units the input
layer receives data from the external
resources and it passes to one or more
hidden layers each neuron in these
layers computes a weighted sum of inputs
and transfers the result to the next
layer during training the weight of
these connection are adjusted to
optimize the Network's performance a
fully connected artificial neural
network includes an input layer or more
hidden layers and an output layer each
neuron in a hidden layer receives input
from the previous layer and sends its
out outut to the next layer so this
process continues until the final output
layer produce the network response so
moving forward let's see types of neural
networks so deep learning models can
automatically learn feature from data
making them ideal to tasks like image
recognition speech recognition and
natural language processing so the most
common architecture and deep learnings
are the first one feed foral neural
network fnn so these are the simplest
type of neural network where information
flows line LLY from the input to the
output they are widely used for tasks
such as image classification speech
recognition and natural langage
processing NLP the second one
convolutional neural network designed
specifically for image and video
recognition CNN automatically learn
feature from images making them ideal
for image classification object
detection and image segmentation the
third one recurrent neural networks RNN
are specialized for processing
sequential data time ser and natural
language they maintain and internal
state to capture information from
previous input making them suitable for
task such as a spe recognition NLP and
language translation so now let's move
forward and see some deep learning
application the first one is autonomous
vle deep learning is changing the
development of self-driving car
algorithms like CNS process data from
sensors and cameras to detect object
recognize traffic signs and make driving
decision in real time enhancing safety
and efficiency on the road the second
one is Healthcare diagnostic deep
learning models are being used to
analyze medical images such as x-rays
MRIs and CT scans with high accuracy
they help in early detection and
diagnosis of diseases like cancer
improving treatment outcomes and saving
lives the third one is NLP recent
advancement in NLP powered by Deep
learning models like Transformer chat
GPD have led to more sophisticated and
humanik text generation translation and
sentiment analysis so application
include virtual assistant chat Bots and
automated customer service the fourth
one def technology so deep learning
techniques are used to create highly
realistic synthetic media known as def
fix while this technology has
entertainment and creative application
it also raises ethical concern regarding
misinformation and digital manipulation
the fifth one predictive maintenance in
Industries like manufacturing and an
aviation deep learning models predict
equipment failures before they occur by
analyzing sensor data the proactive
approach reduces downtime lowers
maintenance cost and improves
operational efficiency so now let's move
forward and see some advantages and
disadvantages of deep learning so first
one is high computational requirements
so deep learning requires significant
data and computational resources for
training whereas Advantage is high
accuracy achieves a state-of-the-art
performance in tasks like image
recognition and natural Lang processing
whereas deep learning needs large label
data sets often require extensive label
data set for training which can be
costly and time consuming together so
second advantage of deep learning is
automated feature engineering
automatically discovers and learn
relevant features from data without
manual intervention the third
disadvantage is
overfitting so deep planning can overfit
to training data leading to poor
performance on new unseen data whereas
the third deep learning Advantage is
scalability so deep learning can handle
large complex data set and learn from
massive amount of data so in conclusion
deep learning is a transformative leap
in AI mimicking human neural networks it
has changed Health Care Finance
autonomous vehicles and NLP AI will
pretty much touch everything we do it's
more likely to be correct and grounded
in reality talk to the AI about how to
do better it's a very deep philosophical
conversation it's a bit above my pay
grade I'm going to say something and it
it's it's going to sound completely
opposite um of what people feel uh you
you you probably recall uh over the
course of the last 10 years 15 years um
almost everybody who sits on a stage
like this would tell you it is vital
that your children learn computer
science um everybody should learn how to
program and in fact it's almost exactly
the opposite it is our job to create
Computing technology such that
nobody has to
program and that the programming
language is
human everybody in the world is now a
programmer this is the
miracle artificial intelligence or AI
from its humble beginnings in 1950s AI
has evolved from the simple problem
solving and symbolic reasoning to the
advanced machine learning and deep
learning techniques that power some of
the most Innovative application we see
today so AI is not just a bus word it is
a revolutionary Force reshaping
Industries enhancing daily life and
creating unmatch opportunities across
various sector AI is changing numerous
fields in healthare it aids in early
disease diagnosis and personalized
treatment plans in finance it transform
money management with the robo advisors
and fraud detection system the
automotive industry is seeing the rise
of autonomous vehicles that navigate
traffic and recognized obstacle while
retail and e-commerce benefit from
personalized shopping experience and
optimize Supply change management so one
of the most exciting developments in the
AI is the rise of advanced CI tools like
chgb 40 Google Gemini and generative AI
models so these tools represent The
Pinacle of conversational AI capable of
understanding and generating humanik tax
with remarkable accuracy chgb 40 can
assist in writing brainstorming ideas
and even tutoring make its valuable
resource for student professional and
creatives similarly Google Gemini take
AI integration to the next level
enhancing search capabilities providing
insightful responses and inating
seamlessly into our digital lives
generative AI is a subset of AI is also
making views by creating new content
from scratch tools like Dal which
generates images from textual reception
and gpt3 which can write coherent and
creative text are just the beginnings so
these Technologies are changing Fields
like art design and content creation
enabling the generation of unique and
personal outputs that were previously
unimaginable so beyond specific
Industries AI application extend to
everyday's life voice activated
assistant like Siri and Alexa and smart
home devices learn our preferences and
adjust our environments accordingly
so AI is embedded in the technology we
use daily making our lives more
convenient connected and efficient so
join us as we explore the future of AI
examining the breakthroughs the
challenges and the endless possibilities
that lies ahead so whether you are a
tech Enthusiast a professional in the
field or simply curious about worst next
so this video will provide you with a
comprehensive look at how AI is shaping
our world and what we can exper in the
years to come
so before we move forward as we know chb
Gemini generi TOS is an AI based and if
you want to learn how the school AI
develop and want to create your own so
how AI will impact the future the first
is enhanced business automation AI is
transforming business automation with
55% of organization adopting AI
technology chatbots and digital
assistant handle customer interaction
and basic employee inquiries speed leing
of decision making the second thing is
job disruption automation May displace
job with a one3 to takes potentially
automated while roles like Securities
are at risk demand for machine learning
specialist is rising AI is more likely
to augment skilled and creative
positions emphasizing the need for up
Skilling data privacy issues training AI
model requires large data set raising
privacy concern the FTC is invest
investigating open AI for potential
violation and the Biden Harris
Administration introduced an AI bill of
right to promote data
transparency the fourth one is increased
regulation AI impact on intellectual
property and ethical concerns is leading
to increase regulation lawsuits and
government guidelines on responsible AI
use could reshape the industry climate
change concern AI optimize Supply chains
and reduce emission but the energy
needed for the AI model
may increase carbon emission potentially
negating environmental benefits so
understanding these impacts help us to
prepare for ai's future challenges and
opportunities so now let's see what
industries will AI impact the most the
first one is manufacturing AI enhances
manufacturing with robotic arm and
predictive sensors improving tasks like
assembly and equipment and maintenance
the second is healthare AI changes
healthare by quickly identifying
diseases and streaming drug Discovery
and monitoring patients through virtual
nursing assistant The Third One Finance
AI has bank and financial institution
detect fraud conduct Audits and assess
loan applications while Trader use AI
for risk assessment and smart investment
decision the fourth one education AI
personalizes education by digitizing
textbook deducting plagerism and
analyzing student emotions to tailor
learning experience the fifth one
customer service AI our chatbots and
virtual assistant provide data div
insights enhancing customer services
interaction so these industries are
experiencing significant changes due to
AI driving Innovation and efficiency
across various sector so now let's move
forward and see some risk and dangers of
AI so AI offers many benefits but also
possess significant risk the first one
job loss from 2023 to 2028 44% of worker
skills will be disrupted without
upskilling AI could lead to high
unemployment and fewer opportunities for
marginalized groups the second one is
human biases AI often reflect the biases
of its trainer such as facial
recognition favoring lighter skin tones
unchecked biases can perpetuate social
inequalities the third one defects and
misinformation defects plus reality
spreading misinformation with dangerous
consequences they can be used for
political propaganda financial fraud and
compromising reputation the fourth one
data privacy AI training on public data
risk breaches that expose a personal
information a 2024 Cisco survey found
48% of businesses use non-public
information in AI tools with 69
concerned about intellectual property
and legal rights breaches could expose
million of consumers data the fifth one
automated weapons AI in automated weapon
fails to distinguish between Soldier and
civilization posing savior threats Miss
use could lead endangered large
population understanding these risk is
crucial for responsible AI development
and the use so as we explore the future
of AI it's clear that impact will be
profound and far-reaching AI will change
Industries enhance efficiency and drive
Innovation however it also brings
significant challenges including job
displacement biases privacy concern
misinformation and the ethical
implication of automated weapons so to
Hest AI potential responsibility we must
invest in upscaling our Workforce
address biases in AI system protect data
privacy and develop regulations that
ensure ethical AI use hey everyone
welcome to Simply learn today's video
will compare and contrast artificial
intelligence deep learning machine
learning and data science but before we
get started consider subscribing to
Simply learns YouTube channel and hit
the Bell icon that way you'll be the
first to get notified when when we post
similar content before moving on let me
ask you two interesting queries which
among the following is not a branch of
artificial intelligence data analysis
machine learning deep learning neural
networks and the second query is what is
the main difference between machine
learning and deep learning please leave
your answer in the comments section
below and stay tuned to get the answer
first we will unwrap deep learning deep
learning was first introduced in the
1940s deep learning did not develop
suddenly it developed slowly and
steadily over seven decades many thesis
and discoveries were made on deep
learning from the 1940s to 2000 thanks
to companies like Facebook and Google
the term deep learning has gained
popularity and may give the perception
that it is a relatively New Concept deep
learning can be considered as a type of
machine learning and artificial
intelligence or AI that that imitates
how humans gain certain types of
knowledge deep learning includes
statistics and predictive modeling deep
learning makes processes quicker and
simpler which is advantageous to data
scientists to gather analyze and
interpret massive amounts of data having
the fundamentals discussed let's move
into the different types of deep
learning neural networks are the main
component of deep learning but neural
networks comprise three main types which
contain artificial neural networks orn
convolution neural networks or CNN and
recurrent neural networks or RNN
artificial neural networks are inspired
biologically by the animal brain
convolutional neural networks surpass
other neural networks when given inputs
such as images Voice or audio it
analyzes images by processing data
recurrent neural networks uses
sequential data or series of data
convolutional neural networks and
current neural networks are used in
natural language processes speech
recognition image recognition and many
more machine learning the evolution of
ml started with the mathematical
modeling of neural networks that served
as the basis for the invention of
machine learning in 1943 neuroscientist
Warren mccullock and logician Walter
pittz attempted to quantitatively map
out how humans make decisions and carry
out thinking processes therefore the
term machine learning is not new machine
learning is a branch of artificial
intelligence and computer science that
uses data and algorithms to imitate how
humans learn gradually increasing the
systems accuracy there are three types
of machine learning which include
supervised learning what is supervised
learning well here machines are trained
using label data machines predict output
based on this data now coming to
unsupervised learning models are not
supervised using a train tring data set
it is comparable to the learning process
that occurs in the human brain while
learning something new and the third
type of machine learning is
reinforcement learning here the agent
learns from feedback it learns to behave
in a given environment based on actions
and the result of the action this
feature can be observed in
robotics now coming to the evolution of
AI the potential of artificial
intelligence wasn't explored until the
1950s although the idea has been known
for centuries
the term artificial intelligence has
been around for a decade still it wasn't
until British polymath Allan Turing
posed the question of why machines
couldn't use knowledge like humans do to
solve problems and make decisions we can
Define artificial intelligence as a
technique of turning a computer-based
robot to work and act like humans now
let's have a glance at the types of
artificial intelligence weak AI performs
only specific tasks like Apple Siri
Google assistant and Amazon's Alexa you
might have used all of these
Technologies but the types I am
mentioning after this are under
experiment General AI can also be
addressed as artificial general
intelligence it is equivalent to human
intelligence hence an AGI system is
capable of carrying out any task that a
human can strong AI aspires to build
machines that are indistinguishable from
the human mind both General and strong
strong AI are hypothetical right now
rigorous research is going on on this
matter there are many branches of
artificial intelligence which include
machine learning deep learning natural
language processing robotics expert
systems fuzzy logic therefore the
correct answer for which is not a branch
of artificial intelligence is option a
data
analysis now that we have covered deep
learning machine learning and artificial
intelligence the final topic is data
science
Concepts like deep learning machine
learning and artificial intelligence can
be considered a subset of data science
let us cover the evolution of data
science the phrase data science was
coined in the early 1960s to
characterize a new profession that would
enable the comprehension and Analysis of
the massive volumes of data being
gathered at the time since its
Beginnings data science has expanded to
incorporate ideas and methods from other
fields including artificial intelligence
machine learning deep learning and so
forth data science can be defined as the
domain of study that handles vast
volumes of data using modern tools and
techniques to find unseen patterns
derive meaningful information and make
business decisions therefore data
science comprises machine learning
artificial intelligence and deep
learning where we dive into the
fascinating world of large language
models llms if you ever wondered how
machine learning can Now understand and
generate humanlike text you are in the
right place from chat boards like chat
GPT to AI assistant that power search
engines llms are transforming how we
interact with technology one of the most
exciting advancement in this space is
Google Gemini or open LGBT a cutting as
large language model designed to push
the boundaries of what AI can achieve in
this video we will explore what llms are
how they work and why models like Gemini
are critical for the future of AI Google
Gemini is part of a new wave of AI
models that are smarter faster and more
efficient it is designed to understand
context better offer more accurate
responses and integrate deeply into
service like Google search and Google
Assistant providing more humanik
interactions so we will break down the
science behind llms including their
massive training data sets Transformer
architecture and how models like Gemini
use deep learning Innovation to change
Industries plus we will compare Google
Gemini to other popular llm such as open
LGB models showing how each of these
Technologies is used to power chatboard
virtual assistants and other AI driven
application by end of this video you
will have a clear understanding of how
large language models like chamini work
their key features and what they mean
for their future AI don't forget to like
subscribe and hit the Bell icon to never
miss any update from Simply learn so
what are the large language models large
language models like CH GPD 4 Genera
pre-trained Transformer 4 o and Google
Gemini are sophisticated AI system
designed to comprehend and generate
humanik text these models are built
using deep learning techniques and are
trained on was data set collected from
the internet they leverage self
attention mechanism to analyze
relationship between words or tokens
allowing them to capture context and
produce coherent relevant responses llms
have significant application including
powering virtual assistant chat boards
content creation language translation
and supporting research and decision
making their ability to generate fluent
and contextually appropriate text has
advanced natural language processing and
improved human computer interaction so
now let's see what our large language
model used for large language models are
utilized in scenarios with limited or no
domain specific data available for
training these scenarios include both
few short and zero short training
approaches which rely on the model's
strong inductive bias and its capability
to derive meaningful representation from
a small amount of data or even no data
at all so now let's see how are large
language model trained large language
models typically undergrow pre-training
on a boat all encompassing data set that
shares statical similarities with the
data set is specific to the Target task
the objective of pre-tuning is to enable
the model toire high level feature that
can later be applied during the fine
tuning phase for specific task so there
are some training processes of llm which
involves several steps the first one is
text pre-processing the textual data is
transformed into a numerical
representation that the llm model can
effectively process this conversion may
be involved techniques like tokenization
and coding and creating input sequences
the second one is random parameter
initialization the models parameter are
initialized randomly before the training
process begins the third one is input
numerical data the numerical
representation of the text data is fed
into the model of processing the models
architecture typically based on
Transformers allows it to capture the
conceptual relationship between the
words or tokens in the next the fourth
one is loss function calculation a loss
function calculation measure the
discrepancy between the model's
prediction and the actual next word or
token in a SX the llm model aims to
minimize this loss during training the
fifth one is parameter optimization the
models parameter are registed through
optimization technique this involves
calculating gradient and updating the
parameters accordingly gradually
improving the model's performance the
last one is itative training the
training process is repeated over
multiple itation or EPO until the models
output achieve a satisfactory level of
accuracy on the given task or data set
by following this training process large
language model learn to capture
linguistic patterns understand context
and generate coherent responses enabling
them to excel at various language
related tasks the next topic is how do
large language models work so large
language models leverage deep neural
network to generate output based on
patterns learn from the training data
typically a large language model adopts
a Transformer architecture which enables
the model to identify relationship
between words in a sentence irrespective
of their position in the sequence in
contrast to RNs that rely on recurrence
to C capture token relationship
Transformer neural network employs self
attention as their primary mechanism
self attention calculates attention
scores that determine the importance of
each token with respect to the other
token in the text sequence facilitating
the modeling of integrate relationship
within the data next let's see
application of large language models
large language models have a wide range
of application across various domains so
here are some notable application the
first one is natural language processing
NLP large language models are used to
improve natural language understanding
tasks such as sentiment analysis named
entity recognition text classification
and language modeling the second one is
chatbot and virtual assistant LGE
language models power conversational
agents chatbots and virtual assistant
providing more interactive and human
like user interaction the third one is
machine translation L language models
have been used for automatic language
translation enabling text translation
between different languages with
improved accuracy the fourth one is
sentiment analysis llms can analyze and
classify the sentiment or emotion
expressed in a piece of text which is
valuable for market research brend
monitoring and social media analysis the
fifth one is content recommendation
these models can be employed to provide
personalized content recommendations
enhancing user experience and engagement
on platforms such as News website or
streaming services so these application
highlight the potential impact of large
language models in various domains for
improving language understanding
automation today we will discuss about
booming topic of this era multimodel AI
let's understand with an example imagine
you are showing a friend your vacation
photos you might describe the site you
saw the sounds you heard and even your
emotions this is how humans naturally
understand the World by combining
information from different sources
multimodel AI aims to do the same thing
let's break the model AI first
multimodel refers to two different ways
of communicating information like text
speech images and videos where AI stands
for artificial intelligence which are
systems that can learn and make decision
so multimodel AI is a type of AI that
can process and understand information
from multiple sources just like you do
when you look at your vacation photos
before delving more into multimodel AI
so here is a just quick info for you now
that we have understood what is
multimodel AI let's now go a bit further
it is obvious that multimodel AI is not
the only AI out there but what is big
deal about multimodel AI that everyone
is talking about that is what we will
discuss in this segment so now let's
understand the difference between
multimodel Ai and the generative AI
while both multimodel Ai and generative
AI are exciting advancement in AI they
defer in their approach to data and
functionality so generative AI Focus
creates new data similar to the data
it's stained down and in multimodel AI
focus is to understand and processes
information from multiple sources that
is text speech images and videos data
types of generative AI primarily works
with a single data type like text
writing poems or images that is
generating realistic portraits whereas
in multimodel AI data types works with
diverse data types enabling a more
comprehensive understanding of the world
the third one is examples like chat
boards text generation models image
editing tools whereas multimodel AI
example covers virtual assistance
medical diagnosis system and autonomous
vehicles strength are can produce
creative and Innovative content
automated repetitive task and
personalize your experience whereas in
multimodel AI strength are provides a
more humanlike understanding of the
world and improve accuracy in essence
generative AI excels at creating new
data while multimodel AI excels at
understanding and utilizing exising data
from diers sources they can be
complimentary with generative models
being used to create new data for
multimodel AI systems to learn more from
and improve their understanding to the
work next let's understand what are the
benefits of multimodel AI the benefits
of multimodel AI is that it offers
developers and users an AI with more
advanced reasoning problem solving and
generation capabilities these
advancement offers endless possibilities
for how Next Generation application can
change the way we work and live for
developers looking to start building Vex
AI Gemini API offers features such as
Enterprise security data residency
performance and technical support if
you're existing Google Cloud customers
can start prompting with Gemini AI in
Vex AI right now next let's see what are
the multimodel AI big challenges
multimodel AI is powerful but faces
hurdles the first one is data overload
managing and Stor massive diverse data
is expensive and complex the second one
is meaning mystery teaching AI to
understand subtle difference in between
meaning like sarcasm is tricky the third
one is data alignment ensuring data
points from different sources saying in
tune is challenging the fourth one is
data scarcity limited and potentially
biased data sets hinder effective
training the fifth one is missing data
Blues what happens when data is missing
like the sorted audio the last one is
Black Box Blues understanding how AI
makes decision can be difficult so these
challenges must be addressed to unlock
the full potential of model AI next
let's see what is the future of
multimodel AI and why is it important
multimodel Ai and multimodels are
represent a Leap Forward in how
developers build and expand the
functionality of AI in the next
generation of application for example
Gemini can understand explain and
generate high quality code in the
world's most popular programming
languages like python Java C++ and go
freeing developers who work on building
more featured fill application
multimodels AI potential also bring the
world closer to AI That's less like
smart software and more like an expert
helper or assistant so here's the open a
documentation and you could see the new
features introduced with the CH GPD 40
so these are the improvements uh one is
the updated and interactive bar graphs
or pie charts that you can create and
these are the features that you could
see here you could change the color you
could download it and what we have is
you could update the latest file
versions directly from Google Drive and
Microsoft One drive and we have the
interaction with tables and charts in a
new expandable view that I showed you
here that is here you can expand it in
the new window and you can customize and
download charts for presentations and
documents moreover you can create the
presentation also
that we'll see in
further and here we have how data
analysis Works in chat
jbt you could directly upload the files
from Google Drive and Microsoft One
Drive I'll will show you guys how we can
do that and where this option is and we
can work on tables in real
time and there we have customized
presentation ready charts that is you
can create a presentation with all the
charts based on a data provided by by
you and moreover a comprehensive
security and privacy feature so with
that guys we'll move to chat JB and here
we have the chat GPT 40 version so
before commencing guys there's a quick
info for you if you're one of the
aspiring data analyst looking for online
training and graduating from the best
universities or a professional who
elicits to switch careers with data
analytics by learning from the experts
then try giving a short to Simply learn
spue postgraduate program in data
analytics in collaboration with IBM you
can find the link in the description box
and pin command so let's get started
with data analysis part so this is the
PIN section or the insert section where
you can have the options to connect to
Google Drive connect to Microsoft One
drive and you can upload it from the
computer this option was already there
that is upload from computer and you can
upload at least or at Max the 10 files
that could be around Excel files or
documents so the max limit is 10 and if
you have connected to Google Drive I'll
show you guys uh I'm not connecting
you but you guys can connect it too and
you could upload it from there also and
there's another cool update that is
ability to quot directly in your chat uh
so while chatting with chat gbt I'll
show you guys how we can do that and you
could find some new changes that is in
the layout so this is the the profile
section it used to be at the left bottom
but now it's moved to the top right and
making it more accessible than ever so
let's start with the data analysis part
and the first thing we need is data so
you can find it on kager or you could
ask chat GT4 to provide the data I'll
will show you guys so this is the kagle
website you can sign in here and click
on data sets you can find all the data
sets here that would be around computer
Science Education classification
computer vision or else you could move
back to chat
jpt and you could ask the chat GPT for
model to generate a data and provide it
in Excel format so we'll ask him we'll
not ask him can you we'll just ask him
provide a data
set that I can use for data
analysis
and provide in CSV format
so you could see that it has responded
that I can provide a sample data set and
he has started generating the data set
here so you could see that he has
provided only 10 rows and he is saying
that I will now generate this data set
in CSV format first he has provided the
visual presentation on the screen and
now he generating the CSV format so if
you want more data like if you want 100
rows or thousand rows you could specify
in the prompt and chat jpt will generate
that for
you so we already have the data I will
import that data you could import it
from here or else you can import it from
your Google Drive so we have a sales
data here we will open
it so we have the sales data here so the
first step we need to do is data
cleaning so this is the crucial step to
ensure that the accuracy of file
analysis is at its best so we can do
that by handling missing values that is
missing values can distort our analysis
and here chat gb4 can suggest methods to
impute these values such as using the
mean median or a sophisticated approach
Based on data patterns and after
handling the missing values we will
remove duplicates and outlier detection
so we'll ask chat
jpt clean the
data if
needed so we can just write a simple
prompt that would be clean the data if
needed and this is also a new feature
you can see the visual presentation of
the data here that we have 100 rows here
and the columns provided that is sales
ID date product category quantity and
price per unit and total sales so this
is also a new feature that
okay uh we just head it
back we'll move back to our chat GPT
chat here
okay so here we are so you could see
that CHT has cleaned the data and he has
provided that it has checked for missing
values checked for duplicates and ensure
consistent formatting and he's
saying okay okay so now we will ask him
that execute
these
steps
and provide the clean
data as chj has provided that these
would the steps to clean the data and
let's
see so he has provided a new CSV file
with the clean sales data we will
download
it and ask him to use the same file only
use this new
cleaned sales data CSV
file for further
analysis so you could see that he is
providing what analysis we can do
further but once our data is clean the
next step is visualization so
visualizations help us understand the
data better by providing a graphical
representation so the first thing we
will do is we will create a prompt for
generating the histograms and we'll do
that for the age distribution part so
we'll write a prompt that generate a
histogram generator histogram to
visualize the distribution of customer
ages to
visualize the
distribution of customer ages
and what I was telling you guys is this
code button if you just select the text
and you would find this reply section
just click on that and you could see
that it has selected the text or what
you want to get all the prompts started
with chat jpd so
we'll make it cross and you could see
that it has provided the histogram here
and these are the new features
here and we could see that he's
providing a notification that
interactive charts of this type are not
yet supported that is histogram don't
have the color Change option I will show
you the color Change option in the bar
chart section so these features are also
new you can download the chart from here
only and this is the expand chart if you
click on that you could see that you
would expand the chart here and continue
chat with chat GPT here so this is the
interactive section
so you could see that he has provided
the histogram that is showing the
distribution of customer ages and the
age range are from 18 to 70 years with
the distribution visualized in 15 bins
that he has created 15 bins Here and Now
moving to another visualization that we
will do by sales by region so before
that I will open the CSV file that is
provided by the chat GPT so you guys can
also see what data he has
provided so this is the clean sales data
and you could see that we have columns
sales ID date product category quantity
price per item total sales region and
sales
person so now moving back to chat jity
so now we will create a bar chart
showing total sales by region so we'll
enter this prompt that create a bar
chart
showing total
sales by
region so what we are doing here is we
are creating bar charts or histogram
charts but we can do that for only two
columns if we want to create these data
visualization charts we need two columns
to do so so you could see that he has
provided the response and created the
bar chart here and this is the
interactive section you could see that
here's an option to switch to static
chart if we click on that we can't like
we are not getting any information we
scroll on that and if I enable this
option you could see that I can visually
see how many numbers this bar is
indicating and after that we have the
change color
section you can change the color of the
data set
provided so we can change it to any
color that is provided here or you could
just write the color code
here and similarly have have other two
options that is download and is the
expand chart
section and if you need uh what code it
has done to figure out this bar graph so
this is the code you could use any ID to
do so if you don't want the
presentations or the visualizations of
the bar charts here you could use your
ID and use the Python language and he
will provide the code for you just take
your data set and it through pandas and
generate the bar
jarts so moving to next section that is
category wise sales section so here we
will generate a pie chart showing the
proportion of sales for each product
category so for that we'll write a
prompt generate a pie
chart showing the proportion of
Ts for each product
category so you could see that it has
started generating the pie chart and
this is also an interactive
section if you click on that you would
be seeing a static pie chart and if you
want to change the color you can change
for any section that could be clothing
Electronics furniture our
kitchen and similarly we have the
download section and the expand chart
section so this is how this new chat jpd
4 model is better than chat jpd
4 that you could use a more interactive
pie charts you could change the colors
for that and you can just ho over these
bar charts and found all the information
according to them so after this data
visualization now we'll move to
statistical analysis so this will help
uncover patterns and relationships in
the data so the first thing we'll do is
correlation analysis and for that we'll
write the prompt analyze the correlation
between age and purchase amount so this
correlation analysis help us understand
the relationship between two variables
so this can indicate if older customers
tend to spend more or less so we will
find out that by analyzing the data and
we provide a prom to chat jyy that
analyze the
correlation between age and purchase
amount so let's see what it
provides uh so here's the response by CH
gbt you could see a scatter plot that
shows the relationship between customer
age and total sales that is with a
calculated correlation coefficient of
approximately 0.6
so this indicates a weak positive
correlation between age and purchase
amount suggesting that as customer age
increases there's a slight tendency for
total sales to increase as well so you
could just see the scatter plot here
that if the age increases so it is not
correlated to sales as you would see an
empty graph here so till 40 to 50 years
of age or the 70 years of age you would
find what amount they have spent here
that is the total sales accumulated by
these
ages so now I'm mov to sales Trend so
here we will perform a Time series
analysis of purchase amount or the given
dates so what does this do is time
series analysis allows us to examine how
sales amount changes over time helping
us identify Trends and seasonal patterns
so for that we'll write a prompt
performer time series
analysis of purchase
amount or given
dates so you could see that chat gbt has
provided us the response and here is the
time series plot showing total sales or
the given dates and each point on the
plot represents the total sales for a
particular
day so through this you can find out and
the businesses find out which is the
seasonal part of the year and we to
stock up their stocks for these kind of
dates and after that you could also do
customer segmentation so what does this
do is so we can use clustering here to
segment customers based on age income
and purchase amount so clustering groups
customers into segments based on
similarities this is useful for targeted
marketing and personalized
services and after that we have the
advanced usage for data analysis here we
can draw a predictive modeling table and
do the Market Basket analysis and
perform a customer lifetime value
analysis so we will see one of those and
what we'll do is we'll perform a Market
Basket analysis and and performance
Association rule mining to find
frequently both together
products so the theory behind this is
the association rule mining helps
identify patterns of products that are
often purchased together aiding an
inventry management and cross selling
strategies so for that we'll write a
prompt that so perform an association
rule mining to find frequently bought to
gather products so for that we'll write
a prompt here perform an association rle
mining to find frequently
bought products
together so let's see for this prompt
what does CH j4 respond to
us uh so you could see that he is
providing a code here but we don't need
a code here we need the analysis
don't provide
code do the market pket
analysis and provide
visualizations so you could see that uh
Chad has
provided the response that given the
limitations in this environment so he is
not able to do the Market Basket allenes
here so but he can help us how we can
perform this in an ID so he's providing
you can install the required libraries
then prepare the data and here is
providing the example code so you could
see there are some limitations to chat
GPT 4 also that he can't do Advanced
Data
analysis so you could use the code in
your ID and do the Market Basket
analysis there so there are some
limitations to CH GT4 also and now we
will ask chat GPT can you create a
presentation based on the data set and
we'll provide a data set to it
also so we'll provide a sample sales
data and we'll ask him
can you create a
presentation or PowerPoint
presentation based on this data
set and only
provide data
visualization
graphs so you can see that jjt 4 has
started analyzing the data
and he is stating that and he will start
by creating a data visualization from
the provided data set and compile them
into PowerPoint
presentation so you could see that CH4
has provided us the response and these
are all the presentations or the
paragraphs that he has created and now
we have downloaded the presentation here
we will open
that and here's the presentation that is
created by J jpt for Welcome to our
video about Transformers in Ai and no we
don't mean the robot toys from the
movies we are diving into something even
cooler in the world of computers and AI
have you ever wondered how your phone
knows what word you might type next or
how Google translate works so well
that's where Transformers come in they
are like super smart computer brains
that can understand and create humanlike
text here's a fun example I asked a
Transformer to tell me a joke and it
said why did the computer go to the art
school because it wanted to improve its
draw speed okay that's a bit cheesy but
it shows how these computer programs can
come up with new ideas on their own
Transformers are changing how we use
technology every day they help us with
things like translating languages
summarizing long articles writing emails
and stories and even playing games like
chess in this video we will explore how
Transformers work why they are so
special and what cool things they might
do in the future so let's talk about
what exactly are Transformers
Transformers are an artificial
intelligence model used to process and
generate natural languages they can read
and understand huge amount of text and
then use that knowledge to answer
questions translate languages summarize
information and even create stories or
write code the magic behind Transformers
is their ability to focus on different
text Parts with attention mechanisms
this means that they can understand
context better than older models making
their outputs more accurate and natural
sounding the basic structure of a
transformer includes two main parts the
encoder and the decoder think of the
encoder as a translator that understands
and processes the input and the decoder
as the one that takes the pro processed
information and turns it into the output
for example if we are translating a
sentence from English to French the
encoder reads the English sentence and
converts it into a form that AI can
understand the decoder then takes this
form and generates the French sentence a
great example of a transformer in action
is ch gbt ch gbt uses Transformers to
understand and generate humanik text
when you ask a question it processes
your input with its encoder and
generates a response with its decoder
this lets it have conversations write
essays and even tell jokes for instance
if you ask Chad GP what's the weather
like today it uses its Transformer model
to understand your question and respond
it with its Sunny with a chance of rain
in the afternoon this ability to
understand and generate text makes
Transformers incredibly powerful so
let's talk about how Transformers work
Transformers are especially good at
sequence to sequence learning task like
translating a sentence from one language
to another
here's how they work first there's the
attention mechanism this allows the
Transformer to focus on different parts
of input data for example if it's
translating the sentence the cat sat on
the mat it can pay attention to each
word's context to understand the meaning
better so it knows cat is related to sat
and mat helping it produce an accurate
translation in another language
Transformers also use something called
positional encoding since they process
all words at once they need a way to
understand the order of the words
positional encoding adds information
about the position of each word to the
input helping the Transformers
understand the sequence another key
feature is like parallel processing
unlike older models like record and
neural network which is RN NS the
process takes words by word Transformers
can process the entire sentence at once
this makes them much faster and more
efficient let's compare Transformers
with Recine neural networks but but
first let's understand what are rnns so
RNs is a type of neural network designed
to handle sequential data they process
data one step at a time maintaining a
memory of previous steps this makes them
good for task where order matters like
speech recognition or time series
prediction however RNs have a problem
called The Vanishing gradient which
means that they can forget information
from earlier in the sequence imagine
trying to understand the sentence Alice
went to the park and then to the store
and RNN might struggle to remember Alice
by the time it gets to the store but a
Transformer can easily keep track of
Alice throughout the sentence so why are
Transformers better unlike rnns
Transformers process the entire sentence
at once keeping the context intact this
solves the vanishing greent problem and
makes Transformers faster and more
accurate to task like language
translation and text generation so let's
talk about the applications of
Transformers at first we have language
translation they are used by services
like Google Translate to convert text
from one language to another for example
translating hello how are you to Spanish
as hola then we have document
summarization they can take long
articles and summarize them into shorter
more concise versions for instance
summarizing a 10-page report into a few
key points making it easier to
understand the main ideas without
reading the whole document then we have
content generation transform can write
articles stories and even quote they can
create new content based on what they
have learned for example you could ask a
Transformer to write a short story about
a space adventure and then it would come
up with a unique narrative then we have
game playing Transformers can learn and
play complex games like chess making
strategic decisions just like a human
player they analyze the entire board and
make moves considering all possible
outcomes let's talk about image
processing they are used in task like
image classification and object
detection helping computers understand
visual data for example identifying
objects in a photo like recognizing a
cat tree or a car now let's understand
the training process the training
Transformers involves two main steps
semi-supervised learning they can learn
from both labeled data where the answer
is known and unlabeled data where the
answer is not provided this makes them
very versatile for example a Transformer
could be trained on a mix of articles
with and without summaries to learn how
to summarize text effectively
pre-training and fine-tuning
Transformers are pre-trained on a large
data set to learn General patterns then
they are fine tuned with specific task
making them highly versatile for
instance a Transformer might be
pre-trained on a large collection of
books to understand language and then
fine tune to generate marketing copy for
a specific brand the future potential of
Transformers is huge researchers are
continuously improving in them making
them even more powerful we can expect
more advanced applications in areas like
healthcare finance and more
sophisticated AI systems that interact
with humans in more natural ways imagine
having an AI that can provide
personalized medical advice or one that
can help you write a novel in conclusion
we can say that Transformers are a
revolutionary architecture in AI they
offer Speed efficiency and versatility
changing how we interact with the
technology the future looks bright for
transform and we can't wait to see what
they'll do next we've looked at a lot of
examples of machine learning so let's
see if we can give a little bit more of
a concrete definition what is machine
learning machine learning is the science
of making computers learn and act like
humans by feeding data and information
without being explicitly programmed we
see here we have a nice little diagram
where we have our ordinary system and
your computer nowadays you can even run
a lot of the stuff on a cell phone
because cell phones advance so much and
then with artificial intelligence and
machine learning it now takes the data
and it learns from what happened before
and then it predicts what's going to
come next and then really the biggest
part right now in machine learning
that's going on is it improves on that
how do we find a new solution so we go
from descriptive where it's learning
about stuff and understanding how it
fits together to predicting what it's
going to do to post scripting coming up
with a new solution and when we're
working on machine learning there's a
number number of different diagrams that
people have posted for what steps to go
through a lot of it might be very domain
specific so if you're working on Photo
identification versus language versus
medical or physics some of these are
switched around a little bit or new
things are put in they're very specific
to The Domain this is kind of a very
general diagram first you want to Define
your objective very important to know
what it is you're wanting to predict
then you're going to be collecting the
data so once you've defined an objective
you need to collect the data that
matches you spend a lot of time in data
science collecting data and the next
step preparing the data you got to make
sure that your data is clean going in
there's the old saying bad data in bad
answer out or bad data out and then once
you've gone through and we've cleaned
all this stuff coming in then you're
going to select the algorithm which
algorithm are you going to use you're
going to train that algorithm in this
case I think we're going to be working
with svm the support Vector machine then
you have to test the model does this
model work is this a valid model for
what we're doing and then once you've
tested it you want to run your
prediction you want to run your
prediction or your choice or whatever
output it's going to come up with and
then once everything is set and you've
done lots of testing then you want to go
ahead and deploy the model and remember
I said domain specific this is very
general as far as the scope of doing
something a lot of models you get
halfway through and you realize that
your data is missing something and you
have to go collect new data because
you've run a test in here someplace
along the line you're saying hey I'm not
really getting the answers I need so
there's a lot of things that are domain
specific that become part of this model
this is a very general model but it's a
very good model to start with and we do
have some basic divisions of what
machine learning does that's important
to know for instance do you want to
predict a category well if you're
categorizing thing that's classification
for instance whether the stock price
will increase or decrease so in other
words I'm looking for a yes no answer is
it going up or is it going down and in
that cas we'd actually say is it going
up true if it's not going up it's false
meaning it's going down this way it's a
yes no 01 do you want to predict a
quantity that's regression so remember
we just did classification now we're
looking at regression these are the two
major divisions and what data is doing
for instance predicting the age of a
person based on the height weight health
and other factors So based on these
different factors you might guess how
old a person is and then there are a lot
of domain specific things like do you
want to detect an anomaly that's anomaly
detection this is actually very popular
right now for instance you want to
detect money withdrawal anomalies you
want to know when someone's making a
withdrawal that might not be their own
account we've actually brought this up
because this is really big right now if
you're predicting the stock whether to
buy stock or not you want to be able to
know if what's going on in the stock
market is an anomaly use a different
prediction model because something else
is going on you got to pull out new
information in there or is this just the
norm I'm going to get my normal return
on my money invested so being able to
detect anomalies is very big in data
science these days another question that
comes up which is on what we call
untrained data is do you want to
discover structure in unexplored data
and that's called clustering for
instance finding groups of customers
with similar Behavior given a large
database of customer data containing
their demographics and past buying
records and in this case we might notice
that anybody who's wearing certain set
of shoes go shopping at certain stores
or whatever it is they're going to make
certain purchases by having that
information it helps us to Market or
group people together so then we can now
explore that group and find out what it
is we want to Market to them if you're
in the marketing world and that might
also work in just about any Arena you
might want to group people together
whether they're uh based on their
different areas and Investments and
financial background whether you're
going to give them a loan or not before
you even start looking at whether
they're valid customer for the bank you
might want to look at all these
different areas and group them together
based on unknown data so you're not you
don't know what the data is going to
tell you but you want to Cluster people
together that come together let's take a
quick DeTour for quiz time oh my
favorite so we're going to have a couple
questions here under our quiz time and
um we'll be posting the answers in the
part two of this tutorial so let's go
ahead and take a look at these quiz
times questions and hopefully you'll get
them all right it'll get you thinking
about how to to process data and what's
going on can you tell what's happening
in the following cases of course you're
sitting there with your cup of coffee
and you have your check boox and your
pen trying to figure out what's your
next step in your data science analysis
so the first one is grouping documents
into different categories based on the
topic and content of each document very
big these days you know you have legal
documents you have uh maybe it's a
Sports Group documents maybe you're
analyzing newspaper postings
but certainly having that automated is a
huge thing in today's world B
identifying handwritten digits and
images correctly so we want to know
whether uh they're writing an A or
capital A B C what are they writing out
in their hand digit their handwriting C
behavior of a website indicating that
the site is not working as designed D
predicting salary of an individual based
on his or her years of experience HR
hire ing uh setup there so stay tuned
for part two we'll go ahead and answer
these questions when we get to the part
two of this tutorial or you can just
simply write at the bottom and send a
note to Simply learn and they'll follow
up with you on it back to our regular
content and these last few bring us into
the next topic which is another way of
dividing our types of machine learning
and that is with supervised
unsupervised and reinforcement learning
supervised learning is a method used to
en enable machines to classify predict
objects problems or situations based on
labeled data fed to the machine and in
here you see we have a jumble of data
with circles triangles and squares and
we label them we have what's a circle
what's a triangle what's a square we
have our model training and it trains it
so we know the answer very important
when you're doing supervised learning
you already know the answer to a lot of
your information coming in so you have a
huge group of data coming in and then
you have new data coming in so we've
trained our model the model now knows
the difference between a circle a square
a triangle and now that we've trained it
we can send in in this case a square and
a circle goes in and it predicts that
the top one's a square and the next
one's a circle and you can see that this
is uh being able to predict whether
someone's going to default on a loan
because I was talking about Banks
earlier supervised learning on stock
market whether you're going to make
money or not that's always important and
if you are looking to make a fortune in
the stock market keep in mind it is very
difficult to get all the data correct on
the stock market it is very it
fluctuates in ways you really hard to
predict so it's quite a roller coaster
ride if you're running machine learning
on the stock market you start realizing
you really have to dig for new data so
we have supervised learning and if you
have supervised we should need
unsupervised learning in unsupervised
learning machine learning model finds
the hidden pattern in an unlabeled data
so in this case instead of telling it
what the circle is and what a triangle
is and what a square is it goes in there
looks at them and says for whatever
reason it groups them together maybe
it'll group it by the number of corners
and it notices that a number of them all
have three corners a number of them all
have four corners and a number of them
all have no corners and it's able to
filter those through and group them
together we talked about that earlier
with looking at a group of people who
are out shopping we want to group them
together to find out what they have in
common and of course once you understand
what people have in common maybe you
have one of them who's a customer at
your store or you have five of them are
customer at your store and they have a
lot in common with five others who are
not customers at your store how do you
Market to those five who aren't
customers at your store yet they fit the
demograph of who's going to shop there
and you'd like them to shop at your
store not the one next door of course
this is a simplified version you can see
very easily the difference between a
triangle and a circle which is might not
be so easy in marketing reinforcement
learning reinforcement learning is an
important type of machine learning where
an agent learns how to behave in an
environment by performing actions and
seeing the result and we have here where
the in this case a baby it's actually
great that they used an infant for this
slide because the reinforcement learning
is very much in its infant stages but
it's also probably the biggest machine
learning demand out there right now or
in the future it's going to be coming up
over the next few years is reinforcement
learning and how to make that work for
us and you can see here where we have
our action in the action in this one it
goes into the fire hopefully the baby
didn't it was just a little candle not a
giant fire pit like it looks like here
when the baby comes out and the new
state is the baby is sad and crying
because they got burned on the fire and
then maybe they take another action the
baby's called the agent CU it's the one
taking the actions and in this case they
didn't go into the fire they went a
different direction and now the baby's
happy and laughing and playing
reinforcement learning is very easy to
understand because that's how as humans
that's one of the ways we learn we learn
whether it is you know you burn yourself
on the stove don't do that anymore don't
touch the stove in the big picture being
able to have a machine learning program
on an AG I be able to do this is huge
because now we're starting to learn how
to learn that's a big jump in the world
of computer and machine learning and
we're going to go back and just kind of
go back over supervised versus
unsupervised learning understanding this
is huge because this is going to come up
in any project you're working on we have
in supervised learning we have labeled
data we have direct feedback so
someone's already gone in there and said
yes that's a triangle no that's not a
triangle and then you predict outcome so
you have a nice prediction this is this
this new set of data is coming in and we
know what it's going to be and then with
unsupervised trading it's not labeled so
we really don't know what it is there's
no feedback so we're not telling it
whether it's right or wrong we're not
telling it whether it's a triangle or a
square we're not telling it to go left
or right all we do is we're finding
hidden structure in the data grouping
the data together to find out what
connects to each other and then you can
use these together so imagine you have
an image and you're not sure what you're
looking for so you go in and you have
the unstructured data find all these
things that are connected together and
then somebody looks at those and labels
them now you can take that label data
and program something to predict what's
in the picture so you can see how they
go back and forth and you can start
connecting all these different tools
together to make a bigger picture there
are many interesting machine learning
algorithms let's have a look at a few of
them hopefully this give you a little
flavor of what's out there and these are
some of the most important ones that are
currently being used we'll take a look
at linear regression decision tree and
the support Vector machine let's start
with a closer look at linear regression
linear regression is perhaps one of the
most well-known and well understood
algorithms in statistics and machine
learning linear regression is a linear
model for example a model that assumes a
linear relationship between the input
variables X and the single output
variable Y and you'll see this if you
remember from your algebra CL classes y
= mx + C imagine we are predicting
distance traveled y from speed X our
linear regression model representation
for this problem would be y = m * x + C
or distance = M * speed plus C where m
is the coefficient and C is the Y
intercept and we're going to look at two
different variations of this first we're
going to start with time is constant and
you can see we have a bicyclist he's got
a safety gear on thank goodness speed
equals 10 m/ second and so over a
certain amount of time his distance
equals 36 kilometers we have a second
bicyclist is's going twice the speed or
20 m/ second and you can guess if he's
going twice the speed and time is a
constant then he's going to go twice the
distance and that's easily to compute 36
* 2 you get 72 km and so if you had the
question of how fast with somebody's
going three times that speed or 30 m/
second is you can easily compute the
distance in our head we can do that
without needing a computer but we want
to do this for more complicated data so
it's kind of nice to compare the two but
let's just take a look at that and what
that looks like in a graph so in a
linear regression model we have our
distance to the speed and we have our m
equals the ve slope of the line and
we'll notice that the line has a plus
slope and as speed increases distance
also increases hence the variables have
a positive relationship and so your
speed of the person which equals yal m X
plus C distance traveled in a fixed
interval of time and we could very
easily compute either following the line
or just knowing it's three times 10 m/s
that this is roughly 102 kilm distance
that this third bicep has traveled one
of the key definitions on here is
positive relationship so the slope of
the line is positive as distance
increase so does speed increase let's
take a look at our second example where
we put distance is a constant so we have
speed equals 10 m/s second they have a
certain distance to go and it takes them
100 seconds to travel that distance and
we have our second bicyclist who's still
doing 20 m per second since he's going
twice the speed we can guess that he'll
cover the distance in about half the
time 50 seconds and of course you could
probably guess on the third one 100
divided by 30 since he's going three
times the speed you could easily guess
that this is
33.33 seconds time we put that into a
linear regression model or a graph if
the distance is assumed to be constant
let's see the relationship between speed
and time and as time goes up the amount
of speed to go that same distance goes
down so now your m equals a minus ve
slope of the line as the speed increases
time decreases hence the variable has a
negative relationship again there's our
definition positive relationship and
negative relationship dependent on the
slope of the line and with a simple
formula like this um and even a
significant amount of data let's uh see
with the mathematical implementation of
regression and we'll take this data so
suppose we have this data set where we
have xyx = 1 2 3 4 5 standard series and
the Y value is 3 22 43 when we take that
and we go ahead and plot these points on
a graph you can see there's kind of a
nice scattering and you could probably
eyeball a line through the middle of it
but we're going to calculate that exact
line for linear regression and the first
thing we do is we come up here and we
have the mean of XI and remember mean is
basically the average so we added 5 + 4
+ 3 + 2 + 1 and divide by five that
simply comes out as three and then we'll
do the same for y we'll go ahead and add
up all those numbers and divide by five
and we end up with the mean value of y
of I equals 2.8 where the XI references
it's an average or means value and the
Yi also equals a means value of y and
when we plot that you'll see that we can
put in the Y = 2.8 and the X = 3 in
there on our graph we kind of gave it a
little different color so you can sort
it out with the dash lines on it and
it's important to note that when we do
the linear regression the linear
regression model should go through that
dot now let's find our regression
equation to find the best fit line
remember we go ahead and take our yal MX
plus C so we're looking for M and C so
to find this equation for our data we
need to find our slope of M and our
coefficient of c and we have y = mx + C
where m equals the sum of x - x average
* y - y average or y means and X means
over the sum of x minus X means squared
that's how we get the slope of the value
of the line and we can easily do that by
creating some columns here we have XY
computers are really good about
iterating through data and so we can
easily compute this and fill in a graph
of data and in our graph you can easily
see that if we have our x value of one
and if you remember the um X x i or the
means value is 3 1 - 3 = a -2 and 2 - 3
= a -1 so on and so forth and we can
easily fill in the column of x - x i y -
Yi and then from those we can compute x
- x i^ 2 and x - x i * y - Yi and you
can guess it that the next step is to go
ahead and sum the different columns for
the answers we need so we get a total of
10 for our x - x i s and a total of 2
for x - x i * y - y i and we plug those
in we get 210 which equals .2 so now we
know the slope of our line equals 0. 2
so we can calculate the value of c
that'd be the next step is we need to
know where crosses the y axis and if you
remember I mentioned earlier that the
linear regression line has to pass
through the means value the one that we
showed earlier we can just flip back up
there to that graph and you can see
right here there's our means value which
is 3 x = 3 and Y = 2.8 and since we know
that value we can simply plug that into
our formula y = 2x + C so we plug that
in we get 2.8 = 2 * 3 + C and you can
just solve for C so now we know that our
coefficient equals 2.2 and once we have
all that we can go ahead and plot our
regression line Y = 2 * x + 2.2 and then
from this equation we can compute new
values so let's predict the values of Y
using xal 1 2 3 4 5 and plot the points
remember the 1 2 3 4 5 was our original
X values so now we're going to see what
y thinks they are not what they actually
are and when we plug those in we get y
of designated with Y of P you can see
that x = 1 = 2.4 x = 2 = 2.6 and so on
and so on so we have our y predicted
value
of what we think it's going to be when
we plug those numbers in and when we
plot the predicted values along with the
actual values we can see the difference
and this is one of the things that's
very important with linear aggression in
any of these models is to understand the
error and so we can calculate the error
on all of our different values and you
can see over here we plotted um X and Y
and Y predict and we drawn a little line
so you can sort of see what the eror
looks like there between the different
points so our goal is to reduce this
error we want to minimize that error
value on our linear regression model
minimizing the distance there are lots
of ways to minimize the distance between
the line and the data points like sum of
squared errors sum of absolute errors
root mean square error Etc we keep
moving this line through the data points
to make sure the best fit line has the
least Square distance between the data
points and the regression line so to
recap with a very simple linear
regression model we first figure out the
formula of our line through the middle
and then we slowly adjust the line to
minimize the error keep in mind this is
a very simple formula the math gets even
though the math is very much the same it
gets much more complex as we add in
different dimensions so this is only two
Dimensions y = mx plus C but you can
take that out to X Z ijq all the
different features in there and they can
plot a linear regression model on all of
those using the different formulas to
minimize the error let's go ahead and
take a look at decision trees a very
different way to solve problems in the
linear regression model decision tree is
a tree-shaped algorithm used to
determine a course of action each branch
of a tree represents a possible decision
occurrence or reaction we have data
which tells us if it is a good day to
play golf and if we were to open this
data up in a general spreadsheet you can
see we have the Outlook whether it's a
rainy overcast Sunny temperature hot
mild cool humidity windy and did I like
to play golf that day yes or no so we're
taking a ensus and certainly I wouldn't
want a computer telling me when I should
go play golf or not but you can imagine
if you got up in the night before you're
trying to plan your day and it comes up
and says tomorrow would be a good day
for golf for you in the morning and not
a good day in the afternoon or something
like that this becomes very beneficial
and we see this in a lot of applications
coming out now where it gives you
suggestions and lets you know what what
would uh fit the match for you for the
next day or the next purchase or the
next uh whatever you know next mail out
in this case is is tomorrow a good day
for playing golf based on the weather
coming in and so we come up and let's uh
determine if you should play golf when
the day is sunny and windy so we found
out the forecast tomorrow is going to be
sunny and windy and suppose we draw our
tree like this we're going to have our
humidity and then we have our normal
which is if it's if you have a normal
humidity you're going to go play golf
and if the humidity is really high then
we look at the Outlook and if the
Outlook is sunny overcast or rainy it's
going to change what you choose to do so
so if you know that it's a very high
humidity and it's sunny you're probably
not going to play golf cuz you're going
to be out there miserable fighting off
the mosquitoes that are out joining you
to play golf with you maybe if it's
rainy you probably don't want to play in
the rain but if it's slightly overcast
and you get just the right Shadow that's
a good day to play golf and be outside
out on the green now in this example you
can probably make your own tree pretty
easily cuz it's a very simple set of
data going in but the question is how do
you know what to split where do you
split your data what if this is much
more complicated data where it's not
something that you would particularly
understand like studying cancer they
take about 36 measurements of the
cancerous cells and then each one of
those measurements represents how
bulbous it is how extended it is how
sharp the edges are something that as a
human we would have no understanding of
so how do we decide how to split that
data up and is that the right decision
tree but so that's a question is going
to come up is this the right decision
tree for that we should calculate
entropy and Information Gain two
important vocabulary words there are the
entropy and the Information Gain entropy
entropy is a measure of Randomness or
impurity in the data set entropy should
be low so we want the chaos to be as low
as possible we don't want to look at it
and be confused by the images or what's
going on there with mixed data and the
Information Gain it is the measure of
decrease in entropy after the data set
is split also known as in
reduction Information Gain should be
high so we want our information that we
get out of the split to be as high as
possible let's take a look at entropy
from the mathematical side in this case
we're going to denote entropy as I of P
of and N where p is the probability that
you're going to play a game of golf and
N is the probability where you're not
going to play the game of golf now you
don't really have to memorize these
formulas there's a few of them out there
depending on what you're working with
but it's important to note that this is
where this formula is coming from so
when you see it you're not lost when
you're running your programming unless
you're building your own decision tree
code in the back and we simply have a
log s of P Over p+ N minus n / p+ n *
the log s of n of p plus n but let's
break that down and see what actually
looks like when we're Computing that
from the computer script side enter of a
target class of the data set is the
whole entropy so we have entropy play
golf and we look at at this if we go
back to the data you can simply count
how many yeses and no in our complete
data set for playing golf days in our
complete set we find we have five days
we did play golf and nine days we did
not play golf and so our I equals if you
add those together 9 + 5 is 14 and so
our I equals 5 over 14 and 9 over 14
that's our P andn values that we plug
into that formula and you can go 5 over
14 = 36 9 over 14 = 64 and when you do
the whole equation you get the minus. 36
logun 2ar of 36 -64 log s < TK of 64 and
we get a set value we get
.94 so we now have a full entropy value
for the whole set of data that we're
working with and we want to make that
entropy go down and just like we
calculated the entropy out for the whole
set we can also calculate entropy for
playing golf and the Outlook is it going
to be overcast or rainy or sunny and so
we look at the entropy we have P of
Sunny time e of 3 of two and that just
comes out how many sunny days yes and
how many sunny days no over the total
which is five don't forget to put the
we'll divide that five out later on
equals P overcast = 4 comma 0 plus rainy
equal 2A 3 and then when you do the
whole setup we have 5 over 14 remember I
said there was a total of five 5 over 14
* the I of 3 of 2 + 4 over 14 * the 4
comma 0 and 514 over I of 23 and so we
can now compute the entropy of just the
part it has to do with the forecast and
we get 693 similarly we can calculate
the entropy of other predictors like
temperature humidity and wind and so we
look at the gain Outlook how much are we
get a gain from this entropy play golf
minus entropy play golf Outlook and we
can take the original .94 for the whole
set minus the entropy of just the um
rainy day in temperature and we end up
with a gain of. 247 so this is our
Information Gain remember we Define
entropy and we Define Information Gain
the higher the information gain the
lower the entropy the better the
information gain of the other three
attributes can be calculated in the same
way so we have our gain for temperature
equals
029 we have our gain for humidity equals
.1 52 and our gain for a windy day
equals
0048 and if you do a quick comparison
you'll see the. 247 is the greatest gain
of information so that's the split we
want now let's build the decision tree
so we have the Outlook is it going to be
sunny overcast or rainy that's our first
split because that gives us the most
Information Gain and we can continue to
go down the tree using the different
information gains with the largest
information we can continue down the
nodes of the tree where we choose the
attribute with the largest Information
Gain as the root node and then continue
to split each sub node with the largest
Information Gain that we can compute and
although it's a little bit of a tongue
twister to say all that you can see that
it's a very easy to view visual model we
have our Outlook we split it three
different directions if the Outlook is
overcast we're going to play and then we
can split those further down if we want
so if the over Outlook is sunny but then
it's also windy if it's uh windy we're
not going to play if it's not windy
we'll play so we can easily build a nice
decision treat to guess what we would
like to do tomorrow and give us a nice
recommendation for the day so we want to
know if it's a good day to play golf
when it's sunny and windy remember the
original question that came out
tomorrow's weather report is sunny and
windy you can see by going down the tree
we go out look sunny out look windy
we're not going to play golf tomorrow so
our little Smartwatch pops up and says
I'm sorry tomorrow is not a good day for
golf it's going to be sunny and windy
and if you're a huge golf fan you might
go uhoh it's not a good day to play golf
we can go in and watch a golf game at
home so we'll sit in front of the TV
instead of being out playing golf in the
wind now that we looked at our decision
tree let's look at the third one of our
algorithms we're investigating support
Vector machine support Vector machine is
a widely used classification algorithm
the idea of support Vector machine is
simple the algorithm creates a
separation line which divides the
classes in the best possible manner for
example dog or cat disease or no disease
suppose we have a label built sample
data which tells height and weight of
males and females a new data point
arrives and we want to know whether it's
going to be a male or a female so we
start by drawing a line we draw decision
lines but if we consider decision line
one then we will classify the individual
as a male and if we consider decision
line two then it will be a female so you
can see this person kind of lies in the
middle of the two groups so it's a
little confusing trying to figure out
which line they should be under we need
to know which line divides the classes
correctly but how the goal is to choose
a hyperplane and that is one of the key
words they use when we talk about
support Vector machines choose a
hyperplane with the greatest possible
margin between the decision line and the
nearest Point within the training set so
you can see here we have our support
Vector we have the two nearest points to
it and we draw a line between those two
points and the distance margin is the
distance between the hyperplane and the
nearest data point from either set so we
actually have a value and it should be
equally distant between the two um
points that we're comparing it to when
we draw the hyperplanes we observe that
line one has a maximum distance so we
observe that line one has a maximum
distance margin so we'll classify the
new data point correctly and our result
on this one is going to be that the new
data point is Mel one of the reasons we
call it a hyperplane versus a line is
that a lot of times we're not looking at
just weight and height we might be
looking at 36 different features or
dimensions and so when we cut it with a
hyper plane it's more of a
three-dimensional cut in the data
multi-dimensional it cuts the data a
certain way and each plane continues to
cut it down until we get the best fit or
match let's understand this with the
help of an example problem statement I
always start with a problem statement
when you're going to put some code
together we're going to do some coding
now classifying muffin and cupcake
recipes using support Vector machines so
the cupcake versus the muffin let's have
a look at our dat set and we have the
different recipes here we have a muffin
recipe that has so much flour I'm not
sure what measurement 55 is in but it
has 55 maybe it's
ounces but uh it has certain amount of
flour certain amount of milk sugar
butter egg baking powder vanilla and
salt and So based on these measurements
we want to guess whether we're making a
muffin or a cupcake and you can see in
this one we don't have just two features
we don't just have height and weight as
we did before between the mail female in
here we have a number of features in
fact in this we're looking at eight
different features to guess whether it's
a muffin or a cupcake what's the
difference between a muffin and a
cupcake turns out muffins have more
flour while cupcakes have more butter
and sugar so basically the cupcakes a
little bit more of a dessert where the
muffins a little bit more of a fancy
bread but how do we do that in Python
how do we code that to go through
recipes and figure out what the recipe
is and I really just want to say
cupcakes versus muffins like some big
professional wrestling thing before we
start in our cupcakes versus muffins we
are going to be working in Python
there's many versions of python many
different editors that is one of the
strengths and weaknesses of python is it
just has so much stuff attached to it
and it's one of the more popular data
science programming packages you can use
in this case we're going to go ahead and
use anaconda and Jupiter notebook the
Anaconda Navigator has all kinds of fun
tools once you're into the anacon
Navigator you can change environments I
actually have a number of environments
on here we'll be using python 36
environment so this is in Python version
36 although it doesn't matter too much
which version you use I usually try to
stay with the 3x because they're current
unless you have a project that's very
specifically in version 2x 27 I think is
usually what most people use in the
version two and then once we're in our
um Jupiter notebook editor I can go up
and create a new file and we'll just
jump in here in this case we're doing
spvm muffin versus Cupcake and then
let's start with our packages for data
analysis and we almost always use a coup
there's a few very standard packages we
use we use import oops import
import
numpy that's for number python they
usually denote it as NP that's very
comma that's very common and then we're
going to import pandas as
PD and numpy deals with number arrays
there's a lot of cool things you can do
with the numpy uh setup as far as
multiplying all the values in an array
in a numpy array data array pandas I
can't remember if we're using it
actually in this data set I think we do
as an import it makes a nice data frame
and the difference between a data frame
and a numpy array is that a frame is
more like your Excel spreadsheet you
have columns you have indexes so you
have different ways of referencing it
easily viewing it and there's additional
features you can run on a data frame and
pandas kind of sits on numpy so they you
need them both in there and then finally
we're working with the support Vector
machine so from sklearn we're going to
use the sklearn model import svm support
Vector
machine and then
as a data scientist you should always
try to visualize your data some data
obviously is too complicated or doesn't
make any sense to the human but if it's
possible it's good to take a second look
at it so you can actually see what
you're doing and for that we're going to
use two packages we're going to import
matplot library. pyplot as
PLT again very common and we're going to
import cbor as SNS and we'll go ahead
and set the font scale in the SNS right
in our import line that's with this um
semicolon followed by a line of data
we're going to set the SNS and these are
great because the the caborn sits on top
of matap plot Library just like Panda
sits on numpy so it adds a lot more
features and uses and control we're
obviously not going to get into matplot
library and Seaborn it' be its own
tutorial we're really just focusing on
the svm the support Vector machine from
sklearn and since we're in Jupiter
notebook uh we have to add a special
line in here for our matplot library and
that's your percentage sign or Amber
sign matplot library in line now if
you're doing this in just a straight
code Project A lot of times I use like
notepad++ and I'll run it from there you
don't have to have that line in there
because it'll just pop up as its own
window on your computer depending on how
your computer set up because we're
running this in the Jupiter notebook as
a browser setup this tells it to display
all our Graphics right below on the page
so that's what that line is for remember
the first time I ran this I didn't know
that and I had to go look that up years
ago it's quite a headache so map plot
library in line is just because we're
running this on the web setup and we can
go ahead and run this make sure all our
modules are in they're all imported
which is great if you don't have them
import you'll need to go ahead and pip
use the PIP or however you do it there's
a lot of other install packages out
there although pip is the most common
and you have to make sure these are are
all installed on your python setup the
next step of course is we got to look at
the data can't run a model for
predicting data if you don't have actual
data so to do that let me go ahead and
open this up and take a look and we have
our uh cupcakes versus muffins and it's
a CSV file or CSV meaning that it's
comma separated
variable and it's going to open it up in
a nice uh spreadsheet for me and you can
see up here we have the type we have
muffin muffin muffin cupcake cupcake
cupcake and then it's broken up into
flour milk sugar butter egg baking
powder vanilla and salt so we can do is
we can go ahead and look at this data
also in our
python let us create a variable recipes
equals we're going to use our pandas
module. read CSV remember is a comma
separated
variable and the file name happened to
be cupcakes versus muffins oops I got
double brackets there
do it this
way there we go cupcakes versus
muffins because the program I loaded or
the the place I saved this particular
Python program is in the same folder we
can get by with just the file name but
remember if you're storing it in a
different location you have to also put
down the full path on
there and then because we're in pandas
we're going to go ahead and you can
actually in line you can do this but let
me do the full print you can just type
in recipes. head in the Jupiter notebook
but if you're running in code in a
different script youd need to go ahead
and type out the whole print recipes.
head and Panda's knows is that's going
to do the first five lines of data and
if we flip back on over to the
spreadsheet where we opened up our CSV
file uh you can see where it starts on
line two this one calls it zero and then
2 3 four five six is going to match go
and close that out because we don't need
that anymore and it always starts at
zero and these are it automatically
indexes it since we didn't tell it to
use an index in here so that's the index
number for the leftand side and it
automatically took the top row as uh
labels so Panda's using it to read a CSV
is just really slick and fast one of the
reasons we love our pandas not just
because they're cute and cuddly teddy
bears
and let's go ahead and plot our data and
I'm not going to plot all of it I'm just
going to plot the uh sugar and
flour now obviously you can see where
they get really complicated if we have
tons of different features and so you'll
break them up and maybe look at just two
of them at a time to see how they
connect and to plot them we're going to
go ahead and use
caborn so that's our SNS and the command
for that is SNS dolm plot and then the
two different variables I'm going to
plot is flour and
sugar data equals recipes the Hue equals
type and this is a lot of fun because it
knows that this is pandas coming in so
this is one of the powerful things about
pandas mixed with Seaborn and
doing
graphing and then we're going to use a
pallet set one there's a lot of
different sets in there you can go look
them up for C born we do a regular a fit
regular equals false so we're not really
trying to fit anything and it's a
scatter
kws a lot of these settings you can look
up in Seaborn half of these you could
probably leave off when you run them
somebody played with this and found out
that these were the best settings for
doing a Seaborn plot let's go ahead and
run that and because it does it in line
it just puts it right on the
page and you can see right here that
just based on sugar and flour alone
there's a definite split and we use
these models because you can actually
look at it and say hey if I drew a line
right between the middle of the blue
dots and the red dots we'd be able to do
an svm and and a hyperplane right there
in the
middle then the next step is to format
or
pre process our
data and we're going to break that up
into two
parts we need a type label and remember
we're going to decide whether it's a
muffin or a cupcake well a computer
doesn't know muffin or cupcake it knows
zero and one so what we're going to do
is we're going to create a type label
and from this we'll create a numpy array
andp where and this is where we can do
some logic we take our recipes from our
Panda and wherever type equals muffin
it's going to be zero
and then if it doesn't equal muffin
which is cupcakes it's going to be one
so we create our type label this is the
answer so when we're doing our training
model remember we have to have a a
training data this is what we're going
to train it with is that it's zero or
one it's a muffin or it's
not and then we're going to create our
recipe
features and if you remember correctly
from right up here the First Column is
type so we really don't need the type
column that's our muffin or cup cake and
in pandas we can easily sort that
out we take our value
recipes dot columns that's a pandas
function built into
pandas do values converting them to
values so it's just the column titles
going across the top and we don't want
the first one so what we do is since it
always starts at zero we want
one colon till the end
and then we want to go ahead and make
this a list and this converts it to a
list of
strings and then we can go ahead and
just take a look and see what we're
looking at for the features make sure it
looks right me go ahead and run
that and I forgot the S on recipes so
we'll go ahead and add the s in there
and then run that and we can see we have
flour milk sugar butter egg baking
powder vanilla and salt and that matches
what we have up here right where we
printed out everything but the type so
we have our features and we have our
label Now the recipe features is just
the titles of the columns and we
actually need the
ingredients and at this point we have a
couple options one we could run it over
all the
ingredients and when you're doing this
usually you do but for our example we
want to limit it so you can easily see
what's going on CU if we did all the
ingredients we have you know that's what
um seven eight different hyperplanes
that would be built into it we only want
to look at one so you can see what the
svm is
doing and so we'll take our recipes and
we'll do just flour and sugar again you
can replace that with your recipe
features and do all of them but we're
going to do just flour and sugar and
we're going to convert that to values we
don't need to make a list out of it
because it's not string values these are
actual values on there and we can go
ahead and just
print ingredients and you can see what
that looks
like uh and so we have just the N of
flour and sugar just the two sets of
plots and just for fun let's go ahead
and take this over here and take our
recipe
features and so if we decided to use all
the recipe features you'll see that it
makes a nice column of different data so
it just strips out all the labels and
everything we just have just the values
but because we want to be able to view
this easily in a plot later on we'll go
ahead and take that and just do flour
and
sugar and we'll run that and you'll see
it's just the two
columns so the next step is to go ahead
and fit our
model we'll go and just call it model
and it's a svm we're using a package
called
SVC and this case we're going to go
ahead and set the kernel equals linear
so it's using a specific setup on there
and if we go to the reference on their
website for the
svm you'll see that there's about
there's eight of them here three of them
are for
regression three are for classification
the SVC support Vector classification is
probably one of the most commonly used
and then there's also one for detecting
outliers and another one that has to do
with something a little bit more
specific on the model model but SBC and
SBR are the two most commonly used
standing for support vector classifier
and support Vector regression remember
regression is an actual value a float
value or whatever you're trying to work
on and SBC is a classifier so it's a yes
no true
false but for this we want to know 01
muffin cupcake we go ahead and create
our model and once we have our model
created we're going to do model. fit and
this is very common especially in the
sklearn all their models are followed
with the fit
command and what we put into the fit
what we're training with it is we're
putting in the ingredients which in this
case we limited to just flour and sugar
and the type label is it a muffin or a
cupcake now in more complicated data
science series you'd want to split into
we won't get into that today we split it
into uh training data and test data and
they even do something where they split
it into thirds where a third is used for
where you switch between which one's
training and test there's all kinds of
things go into that and gets very
complicated when you get to the higher
end not overly complicated just an extra
step which we're not going to do today
because this is a very simple set of
data and let's go ahead and run this and
now we have our model fit and I got a
error here so let me fix that real quick
it's Capital SVC it turns
out I did it
lowercase support
vector class classifier there we go
let's go ahead and run that and you'll
see it comes up with all this
information that it prints out
automatically these are the defaults of
the model you notice that we changed the
kernel to linear and there's our kernel
linear on the printout and there's other
different settings you can mess
with we're going to just leave that
alone for right now for this we don't
really need to mess with any of
those so next we're going to dig a
little bit into our newly trained
model and we're going to do this so we
can show you on a
graph and let's go ahead and get the
separating and we're going to say we're
going to use a W for our variable on
here we're going to do model.
coefficient 0 so what the heck is that
again we're digging into the model so
we've already got a prediction and a
train this is a math behind it that
we're looking at right now and
so the W is going to represent two
different coefficients and if you
remember we had yal MX plus C so these
coefficients are connected to that but
in two-dimensional it's a
plane we don't want to spend too much
time on this because you can get lost in
the confusion of the math so if you're a
math Wiz this is great you can go
through here and you'll see that we have
a = minus w0 over W of 1 remember
there's two different values there and
that's basically the slope that we're
generating and then we're going to build
an XX what is XX we're going to set it
up to a numpy array there's our np.
linespace so we're creating a
line of values between 30 and 60 so it
just creates a set of numbers for
x and then if you remember correctly we
have our formula y equals the slope X
X Plus The Intercept well to make this
work we can do this as
YY equals the slope times each value in
that array that's the neat thing about
numpy so when I do a * XX which is a
whole numpy array of values it
multiplies a across all of them and then
it takes those same values and we
subtract the model intercept that's your
uh we had MX plus c so that'd be the C
from the formula y = mx plus
C and that's where all these numbers
come from a little bit confusing because
it's digging out of these different
arrays and then we want to do is we're
going to take this and we're going to go
ahead and plot it so plot the parallels
to separating hyper plane that pass
through the support vectors and so we're
going to create b equals a model support
vectors pulling our support vectors out
there here's our y y which we now know
is a set of data and we have uh we're
going to create YY down equal a * XX +
B1 - A * B 0 and then model support
Vector B is going to be set that to a
new value the minus one setup and YY up
equal a * XX + B1 - A * B 0 and we can
go ahead and just run this to load these
variables up if you want to know
understand a little bit more of what's
going on you can see if we print
y y we just run that you can see it's an
array it's this is a line it's going to
have in this case between 30 and 60 so
it's going to be 30 variables in here
and the same thing with y y up y y down
and we'll we'll plot those in just a
minute on a graph so you can see what
those look
like just go ahead and delete that out
of here and run that so it loads up the
variables nice clean slate I'm just
going to copy this from before remember
this our SNS our caborn plot LM plot
flow
sugar and I'll just go and run that real
quick so you can see remember what that
looks like it's just a straight graph on
there and then one of the new things is
because Seaborn sits on top of P plot we
can do the P plot for the line going
through and that is simply PLT do
plot and that's our xx and y y our two
corresponding values XY and then
somebody played with this to figure out
that the line width equals two and the
color black would look nice so let's go
ahead and run this whole thing with the
PIP plot on there and you can see when
we do this it's just doing flour and
sugar on
here coresponding line between the sugar
and the flour and the muffin versus
Cupcake um and then we generated the um
support vectors the YY down and y y up
so let's take a look and see what that
looks
like so we'll do our PL
plot and again this is all against
XX our x value but this time we have
YY
down and let's do something a little fun
with this we can put in a k dash dash
that just tells it to make it a dotted
line and if we're going to do the down
one we also want to do the up one so
here's our y y
up and when we run that it adds both
sets of line and so here's our support
and this is what you expect you expect
these two lines to go through the
nearest data point so the dash lines go
through the nearest muffin and the
nearest cupcake when it's plotting it
and then your svm goes right down the
middle so it gives it a nice split in
our data and you can see how easy it is
to see based just on sugar and flour
which one's a muffin or a cupcake
let's go ahead and create a
function to
predict muffin or
cupcake I've got my um recipes I've
pulled off the um internet and I want to
see the difference between a muffin or a
cupcake and so we need a function to
push that through and create a function
with DEA and let's call it muffin or
cupcake and remember we're just doing
flour and sugar today not doing all the
ingredients and that actually is a
pretty good split you really don't need
all the ingredients to know it's flour
and
sugar and let's go ahead and do an IFL
statement so if model
predict is a flower and sugar equals
zero so we take our model and we do run
a predict it's very common in sklearn
where you have a DOT predict you put the
data in and it's going to return a value
in this case if it equals zero then
print you're looking at a muffin recipe
else if it's not zero that means it's
one then you're looking at a cupcake
recipe that's pretty straightforward
for function or def for definition DF is
how you do that in Python and of course
you're going to create a function you
should run something in it and so let's
run a cupcake and we're going to send it
values 50 and 20 a muffin or a cupcake I
don't know what it is and let's run this
and just see what it gives us and it
says oh it's a muffin you're looking at
a muffin recipe so it very easily
predicts whether we're looking at a
muffin or a cupcake recipe let's plot
this there we go plot this on the graph
so we can see what that actually looks
like and I'm just going to copy and
paste it from below we we plotting all
the points in
there so this is nothing different than
what we did before if I run it you'll
see it has all the points and the lines
on there and what we want to do is we
want to add another point and we'll do
PLT plot
and if I remember correctly we did for
our test we did 50 and
20 and then somebody went in here and
decided we'll do yo for yellow or it's
kind of a orangish yellow color is going
to come out marker size nine those are
settings you can play with somebody else
played with them to come up with the
right setup so it looks good and you can
see there it is graph um clearly a
muffin in this case in cupcakes versus
muffins the muffin has won and if you'd
like to do your own muffin cupcake
Contender series you certainly can send
a note down below and the team at simply
learn will send you over the data they
use for the muffin and cupcake and
that's true of any of the data um we
didn't actually run a plot on it earlier
we had men versus women you can also
request that information to run it on
your data setup so you can test that
out so to go back over our setup we went
ahead for our support Vector machine
code we did to predict 40 Parts flour 20
Parts sugar I think it was different
than the one we did whether it's a
muffin or a cupcake hence we have built
a classifier using spvm which is able to
classify if a recipe is of a cupcake or
a muffin which wraps up our cupcake
versus muffin what's in it for you we're
going to cover clustering what is
clustering K means clustering which is
one of the most common used clustering
tools out there including a flowchart to
understand K means clustering and how it
functions and then we'll do an actual
python live demo on clustering of cars
based on Brands then we're going to
cover legistic regression what is
logistic regression logistic regression
curve and sigmoid function and then
we'll do another python code demo to
classify a tumor as malignant or benign
based on features and let's start with
clustering suppose we have a pile of
books of different genres now we divide
them into different groups like fiction
horror
education and as we can see from this
young lady she definitely is into heavy
 you can just tell by those eyes in
the maple Canadian leaf on her shirt but
we have fiction horror and education and
we want to go ahead and divide our books
up well organizing objects into groups
based on similarity is clustering and in
this case as we're looking at the books
we're talking about clustering things
with no one categories but you can also
use it to explore data so you might not
know the categories you just know that
you need to divide it up in some way to
conquer the data and to organize it
better but in this case we're going to
be looking at clustering in specific
categories and let's just take a deeper
look at that we're going to use K means
clustering K means clustering is
probably the most commonly used
clustering tool in the machine learning
library K means clustering is an example
of unsupervised learning if you remember
from our previous thing it is used when
you have unlabeled data so we don't know
the answer yet we have a bunch of data
that we want to Cluster into different
groups Define clusters in the data based
on feature similarity so we've
introduced a couple terms here we've
already talked about unsupervised
learning and unlabeled data so we don't
know the answer yet we're just going to
group stuff together and see if we can
find un answer of how things connect
we've also introduced feature similarity
features being different features of the
data now with books we can easily see
fiction and horror and history books but
a lot of times with data some of that
information isn't so easy to see right
when we first look at it and so K means
is one of those tools where we can start
finding things that connect that match
with each other suppose we have these
data points and want to assign them into
a cluster now when I look at these data
points I would probably group them into
two clusters just by looking at them I'd
say two of these group of data kind of
come together but in K means we pick K
clusters and assign random centroids to
clusters where the K clusters represents
two different clusters we pick K
clusters inside random centroids to the
Clusters then we compute distance from
objects to the centroids now we form new
clusters based on minimum distances and
calculate the centroids so we figure out
what the best distance is for the
centroid then we move the centroid and
recalculate those distances repeat
previous two steps iteratively till the
cluster centroids stop changing their
positions and become Static repeat
previous two steps iteratively till the
cluster centroid sto changing and the
position become Static once the Clusters
become Static then K means clustering
algorithm is said to be converged and
there's another term we see throughout
machine learning is converged that means
whatever math we're using to figure out
the answer has come to a solution or
it's converged on an answer shall we see
the flowchart to understand make a
little bit more sense by putting it into
a nice easy step by step so we start we
choose K we'll look at the elbow method
in just a moment we assign random
centroids to clusters and sometimes you
pick the centroids because you might
look at the data in in a graph and say
oh these are probably the central points
then we compute the distance from the
objects to the centroids we take that
and we form new clusters based on
minimum distance and calculate their
centroids then we compute the distance
from objects to the new centroids and
then we go back and repeat those last
two steps we calculate the distances so
as we're doing it it brings into the new
centroid and then we move the centroid
around and we figure figure out what the
best which objects are closest to each
centroid so the objects can switch from
one centroid to the other as the
centroids are moved around and we
continue that until it is converged
let's see an example of this suppose we
have this data set of seven individuals
and their score on two topics A and B so
here's our subject in this case
referring to the person taking the test
and then we have subject a where we see
what they've scored on their first
subject and we have subject B and we can
see see what they score on the second
subject now let's take two farthest
apart points as initial cluster
centroids now remember we talked about
selecting them randomly or we can also
just put them in different points and
pick the furthest one apart so they move
together either one works okay depending
on what kind of data you're working on
and what you know about it so we took
the two furthest points one and one and
five and seven and now let's take the
two farthest apart points as initial
cluster centroids each point is then
assigned to the closest cluster with
respect to the distance from the
centroids so we take each one of these
points in there we measure that distance
and you can see that if we measure each
of those distances and you use the
Pythagorean theorem for a triangle in
this case because you know the X and the
Y and you can figure out the diagonal
line from that or you just take a ruler
and put it on your monitor that'd be
kind of silly but it would work if
you're just eyeballing it you can see
how they naturally come together in
certain areas now we again calculate the
centroids of each cluster so cluster one
and then cluster two and we look at each
individual dot there's one two three
we're in one cluster uh the centroid
then moves over it becomes 1.8 comma 2.3
so remember it was that one and one well
the very center of the data we're
looking at would put it at the one point
roughly 22 but 1.8 and 2.3 and the
second one if we wanted to make the
overall mean Vector the average Vector
of all the different distances to that
centroid we come up with 4 comma 1 and
54 four so we've now moved the centroids
We compare each individual's distance to
its own cluster mean and to that of the
opposite cluster and we find build a
nice chart on here that the as we move
that Centro around we now have a new
different kind of clustering of groups
and using ukian distance between the
points and the mean we get the same
formul you see new formulas coming up so
we have our individual dots distance to
the mean cent of the cluster and
distance to the mean cent of the cluster
only individual three is nearer to the
mean of the opposite cluster cluster 2
than its own cluster one and you can see
here in the diagram where we've kind of
circled that one in the middle so when
we've moved the clust the centroids of
the Clusters over one of the points
shifted to the other cluster because
it's closer to that group of individuals
thus individual 3 is relocated to
Cluster 2 resulting in a new Partition
and we regenerate all those numbers of
how close they are to the different
clusters for the new clusters we will
find the actual cluster centroids so now
we move move the centroids over and you
can see that we've now formed two very
distinct clusters on here on comparing
the distance of each individual's
distance to its own cluster mean and to
that of the opposite cluster we find
that the data points are stable hence we
have our final clusters now if you
remember I brought up a concept earlier
k mean on the K means algorithm choosing
the right value of K will help in less
number of iterations and to find the
appropriate number of clusters in a data
set we use the elbow method and within
sum of squares WSS is defined as the sum
of the squared distance between each
member of the cluster and its centroid
and so you see what we've done here is
we have the number of clusters and as
you do the same K means algorithm over
the different clusters and you calculate
what that centroid looks like and you
find the optimal you can actually find
the optimal number of clusters using the
elbow the graph is called as the elbow
method and on this we guessed at two
just by looking at the data but as you
can see the slope you actually just look
for right there where the elbow is in
the slope and you have a clear answer
that we want two different to start with
k means equals 2 A lot of times people
end up Computing K means equals 2 3 4 5
until they find the value which fits on
the elbow joint sometimes you can just
look at the data and if you're really
good with that specific domain remember
domain I mentioned that last time you'll
know that that where to pick those
numbers or where to start guessing at
what that K value is so let's take this
and we're going to use a use case using
K means clustering to Cluster cars into
Brands using parameters such as
horsepower cubic inches make year Etc so
we're going to use the data set cars
data having information about three
brands of cars Toyota Honda and Nissan
we'll go back to my favorite tool the
Anaconda Navigator with the Jupiter
notebook and let's go ahead and flip
over to our Jupiter notebook and in our
Jupiter notebook I'm going to go ahead
and just paste the uh basic code that we
usually start a lot of these off with
we're not going to go too much into this
code because we've already discussed
numpy we've already discussed matplot
library and pandas that being the number
array pandas being the pandas data frame
and mat plot for the graphing and don't
forget uh since if you're using the
Jupiter notebook you do need the map
plot library in line so that it plots
everything on the screen if you're using
a different python editor then you
probably don't need that because it'll
have a popup window on your computer
computer and we'll go ahead and run this
just to load our libraries and our setup
into here the next step is of course to
look at our data which I've already
opened up in a spreadsheet and you can
see here we have the miles per gallon
cylinders cubic inches horsepower weight
pounds how you know how heavy it is time
it takes to get to 60 my cart is
probably on this one at about 80 or 90
what year it is so this is you can
actually see this is kind of older cars
and then the brand Toyota Honda Nissan
so the different cars are coming from
all the way from 1971 if we scroll down
to uh the 80s we have between the 70s
and 80s a number of cars that they've
put out and let's uh when we come back
here we're going to do importing the
data so we'll go ahead and do data set
equals and we'll use pandas to read this
in and it's uh from a CSV file remember
you can always post this in the comments
and request the data files for these
either in the comments here on the
YouTube video or go to Simply learn.com
and request that the car CSV I put it in
the same folder as the code that I've
stored so my python code is stored in
the same folder so I don't have to put
the full path if you store them in
different folders you do have to change
this and double check your name
variables and we'll go ahead and run
this and uh We've chosen data set
arbitrarily because you know it's a data
set we're importing and we've now
imported our car CSV into the data set
as you know you have to prep the data so
we're going to create the X data this is
the one that we're going to try to f
figure out what's going on with and then
there is a number of ways to do this but
we'll do it in a simple Loop so you can
actually see what's going on so we'll do
for i n x. columns so we're going to go
through each of the columns and a lot of
times it's important I I'll make lists
of the columns and do this because I
might remove certain columns or there
might be columns that I want to be
processed differently but for this we
can go ahead and take X of I and we want
to go fill na a and that's a panda
command but the question is what are we
going to fill the missing data with we
definitely don't want to just put in a
number that doesn't actually mean
something and so one of the tricks you
can do with this is we can take X of I
and then addition to that we want to go
ahead and turn this into an integer
because a lot of these are integers so
we'll go ahe and keep it integers and we
add the bracket here and a lot of
editors will do this they'll think that
you're closing one bracket make sure you
get that second bracket in there if it's
a double bracket ET that's always
something that happens regularly so once
we have our integer of X of Y this is
going to fill in any missing data with
the average and I was so busy closing
one set of brackets I forgot that the
mean is also has brackets in there for
the pandas so we can see here we're
going to fill in all the data with the
average value for that column so if
there's missing data in the average of
the data it does have then once we've
done that we'll go ahead and loop
through it
again and just check and see to make
sure everything is filled in correct
corly and we'll print and then we take X
is null and this returns a set of the
null value or the how many lines are
null and we'll just sum that up to see
what that looks like and so when I run
this and so with the X what we want to
do is we want to remove the last column
because that had the models that's what
we're trying to see if we can cluster
these things and figure out the models
there is so many different ways to sort
the X out for one we could take the X
and we could go data set our variable
we're using and use the iocation one of
the features that's in
pandas and we could take that and then
take all the rows and all but the last
column of the data set and at this time
we could do values we just convert it to
values so that's one way to do this and
if I let me just put this down here and
print X it's a capital x we chose and I
run this you can see it's just the
values we could also take out the values
and not going to return anything because
there's no values connected to it what I
like to do with this is instead of doing
the iocation which does integers more
common is to come in here and we have
our data set and we're going to do data
set dot or data set. columns and
remember that lists all the columns so
if I come in here let me just Mark that
as red and I print data set. columns
you can see that I have my index here I
have my MPG cylinders everything
including the brand which we don't want
so the way to get rid of the brand would
be to do data Columns of Everything But
the last one minus one so now if I print
this you'll see the brand disappears and
so I can actually just take data set
columns minus one and I'll put it right
in here for the columns we're going to
look
at and uh let's unmark this
and unmark
this and now if I do an x. head I know
have a new data frame and you can see
right here we have all the different
columns except for the brand at the end
of the year and it turns out when you
start playing with the data set you're
going to get an error later on and it'll
say cannot convert string to float value
and that's because for some reason these
things the way they recorded them must
have been recorded as strings so we have
a neat feature in here on pandas to
convert and it is simply convert
objects and for this we're going to do
convert oops convert
underscore numeric numeric equals true
and yes I did have to go look that up I
don't have it memorized the convert
numeric in there if I'm working with a
lot of these things I remember them but
um depending on where I'm at what I'm
doing I usually have to look it up and
we run that oops I must have missed
something in here let me double check my
spelling and when I double check my
spelling you'll see I missed the first
underscore in the convert objects and
when I run this it now has everything
converted into a numeric value because
that's what we're going to be working
with his numeric values down
here and the next part is that we need
to go through the data and eliminate
null values most people when they're
doing small amounts working with small
data pools discover afterwards that they
have have a null value and they have to
go back and do this so you know be aware
whenever we're formatting this data
things are going to pop up and sometimes
you go backwards to fix it and that's
fine that's just part of exploring the
data and understanding what you
have and I should done this earlier but
let me go ahead and increase the size of
my window one
notch there we go easier to
see so we'll do 4 I in working with X do
columns we'll page through all the
columns and we want to take X of I and
we're going to change that we're going
to alter it and so with this we want to
go ahead and fill in X of I pandis Has
the fill in a and that just fills in any
non-existent missing data I we'll put my
brackets up and there's a lot of
different ways to fill this data if you
have a really large data set some people
just void out that data because if and
then look at it later in a separate uh
exploration of data one of the tricks we
can do is we can take our column and we
can find the
means and the means is in our quotation
marks so when we take the columns we're
going to fill in the the non-existing
one with the means the problem is that
returns a decimal float so some of these
aren't decimals certainly need to be a
little careful of doing this but for
this example we're just going to fill it
in with the integer version of this
keeps it on par with the other data that
isn't a decimal
point and then what we also want to do
is we want to double check A lot of
times you do this first part first to
double check then you do the fill and
then you do it again just to make sure
you did it right so we're going to go
through and test for missing data and
one of the re ways you can do that is
simply go in here and take our X of I
column so it's going to go through the X
ofi column it says is null so it's going
to return any any place there's a null
value it actually goes through all the
rows of each column is null and then we
want to go ahead and sum that so we take
that we add the sum value and these are
all pandas so is null is a panda command
and so is sum and if we go through that
and we go ahead and run
it and we go ahead and take and run that
you'll see that all the columns have
zero null values so we've now tested and
double checked and our data is nice and
clean we have no null values everything
is now a number value we turned it into
numeric and we've removed the last
column in our data and at this point
we're actually going to start using the
elbow method to find the optimal number
of clusters so we're now actually
getting into the SK learn part uh the K
means clustering on here well I guess
we'll go ahead and zoom it up one more
notot so you can see what I'm typing in
here and then from SK learn going to or
sklearn cluster we're going to import K
means I always forget to capitalize the
K and the M when I do this so it's
capital K capital M K
means and we'll go and create a um aray
wcss equals let me get an empty array if
you remember from the elbow method from
our slide
within the sums of squares WSS is
defined as the sum of square distance
between each member of the cluster and a
centroid so we're looking at that change
in differences as far as a squar
distance and we're going to run this
over a number of K mean
values in fact let's go for I in range
we'll do 11 of
them range Z
11 and the first thing we're going to do
is we're going to create the actual
we'll do it all lower
case and so we're going to create this
object from the K means that we just
imported and the variable that we want
to put into this is in clusters and
we're going to set that equals to I
that's the most important one because
we're looking at how increasing the
number of clusters changes our answer
there are a lot of settings to the K
mean
our guys in the back did a great job
just kind of playing with some of them
the most common ones that you see in a
lot of stuff is how you init your K
means so we have K means plus plus plus
this is just a tool to let the model
itself be smart how it picks it
centroids to start with its initial
centroids we only want to iterate no
more than 300 times we have a Max
iteration we put in there we have an the
inth the knit the random State equals
zero really don't need to worry too much
about these when you're first learning
this as you start digging in deeper you
start finding that these are shortcuts
that will speed up the
process as far as a setup but the big
one that we're working with is the in
clusters equals I so we're going to
literally train our K means 11 times
we're going to do this process 11 times
and if you're working with a big data
you know the first thing you do is you
run a small sample of the data so you
can test all your stuff on it and you
can already see the problem that if I'm
going to iterate through a terabyte of
data 11 times and then the K means
itself is iterating through the data
multiple times that's a heck of a
process so you got to be a little
careful with this a lot of times though
you can find your elbow using the elbow
method find your optimal number on a
sample of data especially if you're
working with larger data sources so we
want to go ahead and take our K means
and we're just going to fit it if you're
looking at any of the SK learn very
common you fit your model and if you
remember correctly our variable we're
using is the capital x and once we fit
this value we go back to the um array we
made and we want to go just toin that
value on the
end and it's not the actual fitware
pinning in there it's when it generates
it it generates the value you're looking
for is inertia so K means. inertia will
pull that specific value out that we
need and let's get a visual on this
we'll do our PLT plot and what we're
plotting
here is first the xaxis which is range
01 so that will generate a nice little
plot there and the wcss for our Y
axis it's always nice to give our plot a
title and let's see we'll just give it
the elbow method for the title and let's
get some labels so let's go ahead and do
PLT X
label and what we'll do we'll do number
of clusters for that and PLT y
label and for that we can do oops there
we go wcss since that's what we're doing
on the plot on there and finally we want
to go ahead and display our graph which
is simply PLT do
oops. show there we go and because we
have it set to inline it'll appear
inline hopefully I didn't make a type
error on
there and you can see we get a very nice
graph you you can see a very nice elbow
joint there at uh two and again right
around three and four and then after
that there's not very much now as a data
scientist if I was looking at this I
would do either three or four and I'd
actually try both of them to see what
the um output look like and they've
already tried this in the back so we're
just going to use three as a setup on
here and let's go ahead and see what
that looks like when we actually use
this to show the different kinds of
cars and so let's go ahead and apply the
K means to the cars data set and
basically we're going to copy the code
that we looped through up above where K
means equals K means number of clusters
and we're just going to set that number
of clusters to three since that's what
we're going to look for and you could do
three and four on this and graph them
just to see how they come up
differently' be kind of curious to look
at that but for this we're just going to
set it to three go ahead and create our
own variable y k means for our answers
and we're going to set that equal to
whoops I double equal there to K means
but we're not going to do a fit we're
going to do a fit predict is the setup
you want to use and when you're using
untrained models you'll see um a
slightly different usually you see fit
and then you see just the predict but we
want to both fit and predict the K means
on this and that's fitcore predict and
then our capital x is the data we're
working
with and before we plot this data we're
going to do a little pandas trick we're
going to take our x value and we're
going to set XS Matrix so we're
converting this into a nice rows and
columns kind of set up but we want the
we're going to have columns equals none
so it's just going to be a matrix of
data in here and let's go ahead and run
that a little warning you'll see These
Warnings pop up because things are
always being updated so there's like
minor changes in the versions and future
versions instead of Matrix now that it's
more common to set it values instead of
doing as Matrix but M Matrix works just
fine for right now and you'll want to
update that later on but let's go ahead
and dive in and plot this and see what
that looks like and before we dive into
plotting this data I always like to take
a look and see what I am plotting so
let's take a look at why K means I'm
just going to print that out down here
and we see we have an array of answers
we have 2 1 0 2 one two so it's
clustering these different rows of data
based on the three different spaces it
thinks it's going to
be and then let's go ahead and print X
and see what we have for x and we'll see
that X is an array it's a matrix so we
have our different values in the array
and what we're going to do it's very
hard to plot all the different values in
the array so we're only going to be
looking at the first two or positions
zero and
one and if you were doing a full
presentation in front of the the board
meeting you might actually do a little
different and and dig a little deeper
into the different aspects because this
is all the different columns we looked
at but we only look at columns one and
two for this to make it easy so let's go
ahead and clear this data out of here
and let's bring up our plot and we're
going to do a scatter plot here so PLT
scatter
and this looks a little complicated so
let's explain what's going on with this
we're going to take the X values
and we're only interested in y of K
means equals zero the first cluster okay
and then we're going to take value zero
for the x axis and then we're going to
do the same thing here we're only
interested in K means equals zero but
we're going to take the second column so
we're only looking at the first two
columns in our answer or in the data and
then the guys in the back played with
this a little bit to make it
pretty and they discovered that it looks
good with as a size equal
100 that's the size of the dots we're
going to use red for this one and when
they were looking at the data and what
came out it was definitely the Toyota on
this so we're just going to go ahead and
label it Toyota again that's something
you really have to explore in here as
far as playing with those numbers and
see what looks good we'll go ahead and
hit enter in there and I'm just going to
paste in the next two lines which is the
next two
cars and this is our Nissa and Honda and
you'll see with our scatter plot we're
now looking at where Yore K means equals
1 and we want the zero column and y k
means equals two again we're looking at
just the first two columns zero and one
and each of these rows then corresponds
to Nissan and
Honda and I'll go ahead and hit enter on
there and uh finally let's take a look
and put the centroids on there again
we're going to do a scatter
plot and on the centroids you can just
pull that from our c means the model we
created do cluster centers and we're
going to just do
um all of them in the first number and
all of them in the second number which
is 0 one because you always start with
zero and
one and then they were playing with the
size and everything to make it look good
we'll do a size of 300 we're going to
make the color yellow and we'll label
them so always good to have some good
labels
centroids and then we do want to do a
title PLT
title and pop up there PLT title because
you always make want to make your graphs
look pretty we'll call it clusters of
car
make and one of the features of the plot
library is you can add a legend it'll
automatically bring in it since we've
already labeled the different aspects of
the legend with Toyota Nissan and
Honda and finally we want to go ahead
and show so we can actually see it and
remember it's in line uh so if you using
a different editor this not the Jupiter
notebook you'll get a popup of this and
you should have a nice set of clusters
here so we can look at this and we have
a clusters of Honda and green Toyota and
red Nissan and purple and you can see
where they put the centroids to separate
them now when we're looking at this we
can also plot a lot of other different
data on here as far because we only
looked at the first two columns this is
just column one and two or 0 one as as
you label them in computer script rting
but you can see here we have a nice
clusters of Carm and we' were able to
pull out the data and you can see how
just these two columns form very
distinct clusters of data so if you were
exploring new data you might take a look
and say well what makes these different
almost going in reverse you start
looking at the data and pulling apart
the columns to find out why is the first
group set up the way it is maybe you're
doing loans and you want to go well why
is this group not defaulting on their
loans and why is the last group
defaulting on their loans and why is the
middle group 50% defaulting on their
bank loans and you start finding ways to
manipulate the data and pull out the
answers you
want so now that you've seen how to use
K mean for clustering let's move on to
the next topic now let's look into
logistic regression the logistic
regression algorithm is the simplest
classification algorithm used for binary
or multi-classification problems and we
can see we have our little group girl
from Canada who's into horror books is
back that's actually really scary when
you think about that with those big eyes
in the previous tutorial we learned
about linear regression dependent and
independent variables so to brush up y =
mx + C very basic algebraic function of
uh Y and X the dependent variable is the
target class variable we are going to
predict the independent variables X1 all
the way up to xn are the feature or
attributes we're going to use to predict
the target class we know what a linear
regression looks like but using the
graph we cannot divide the outcome into
categories it's really hard to
categorize 1.5 3.6 9.8 uh for example a
linear regression graph can tell us that
with increase in number of hours studied
the marks of a student will increase but
it will not tell us whether the student
will pass or not in such cases where we
need the output as categorical value we
will use logistic regression and for
that we're going to use the sigmoid
function so you can see here we have our
marks 0 to 100 number of hours studied
that's going to be what they're
comparing it to in this example and we
usually form a line that says y = mx + C
and when we use the sigmoid function we
have P = 1/ 1 + eus y it generates a
sigmoid curve and so you can see right
here when you take the Ln which is the
natural logarithm I always thought it
should be n l not Ln that's just the
inverse of uh e your e to the minus y
and so we do this we get Ln of p over
1us p = m * x + C that's the sigmoid
curve function we're looking for now we
can zoom in on the function and you'll
see that the function as it derives goes
to one or to zero depending on what your
x value is and the probability if it's
greater than 0.5 the value is
automatically rounded off to one
indicating that the student will pass so
if they're doing a certain amount of
studying they will probably pass then
you have a threshold value at the 0.
five it automatically puts that right in
the middle usually and your probability
if it's less than 0.5 the value run it
off to zero indicating the student will
fail so if they're not studying very
hard they're probably going to fail this
of course is ignoring the outliers of
that one student who's just a natural
genius and doesn't need any studying to
memorize everything that's not me
unfortunately have to study hard to
learn new stuff problem statement to
classify whether a tumor is malignant or
benign and this is actually one of my
favorite data sets to play with because
it has so many features and when you
look at them you really are hard to
understand you can't just look at them
and know the answer so it gives you a
chance to kind of dive into what data
looks like when you aren't able to
understand the specific domain of the
data but I also want you to remind you
that in the domain of medicine if I told
you that my probability was really good
it classified things at say 90% or 95%
and I'm classifying whether you're going
to have a malignant or a Bine tumor I'm
guessing that you're going to go get it
tested anyways so you got to remember
the domain we're working with so why
would you want to do that if you know
you're just going to go get a biopsy
because you know it's that serious this
is like an all or nothing just
referencing the domain it's important it
might help the doctor know where to look
just by understanding what kind of tumor
it is so it might help them or Aid them
in something they missed from before so
let's go ahead and dive into the code
and I'll come back to the domain part of
it in just a minute so use case and
we're going to do our noral Imports here
we're importing numpy pandas Seaborn the
matplot library and we're going to do
Matt plot library in line since I'm
going to switch over to Anaconda so
let's go ahead and flip over there and
get this started so I've opened up a new
window in my anaconda Jupiter
notebook by the way jupyter notebook uh
you don't have to use Anaconda for the
Jupiter notebook I just love the
interface and all the tools that
Anaconda brings so we got our import
numpy is in P for our numpy number array
we have our Panda's PD we're going to
bring in caborn to help us with our
graphs as SNS so many really nice Tools
in both caborn and matplot library and
we'll do our matplot library. pyplot as
PLT and then of course we want to let it
know to do it in line and let's go and
just run that so it's all set up
and we're just going to call our data
data not creative today uh equals PD and
this happens to be in a CSV file so
we'll use a pd. read CSV and I happen to
name the file a renamed it data for
p2.png
go any further and let's just see what
it looks like in a
spreadsheet so when I pop it open in a
local spreadsheet and this is just a CSV
file comma separate variables we have an
ID so I guess they um categorizes for
reference of what id which test was done
the diagnosis M for malignant B for B9
so there's two different options on
there and that's what we're going to try
to predict is the m and b and test it
and then we have like the radius mean or
average the texture average perimeter
mean area mean smoothness I don't know
about you but unless you're a doctor in
the field most of the stuff I mean you
can guess what concave means just by the
term concave but I really wouldn't know
what that means in the measurements
they're taking so they have all kinds of
stuff like how smooth it is uh the
Symmetry and these are all float values
we just page through them real quick and
you'll see there's I believe 36 if I
remember correctly in this
one so there's a lot of different values
they take in all these measurements they
take when they go in there and they take
a look at the different growth the
tumorous growth so back in our data and
I put this in the same folder as a code
so I saved this code in that folder
obviously if you have it in a different
location you want to put the full path
in there and we'll just do uh
Panda's first five lines of data with
the data. head and we run that we can
see that we have pretty much what we
just looked at we have an ID we have a
diagnosis
if we go all the way across you'll see
all the different columns coming across
displayed nicely for our
data and while we're exploring the data
our caborn which we referenced as
SNS makes it very easy to go in here and
do a joint plot you'll notice the very
similar to because it is sitting on top
of the um plot Library so the joint plot
does a lot of work for us and we're just
going to look at the first two columns
that we're interested in the radius mean
and the texture mean we'll just look at
those two columns and data equals data
so that tells it which two columns we're
plotting and that we're going to use the
data that we pulled in let's just run
that and it generates a really nice
graph on here and there's all kinds of
cool things on this graph to look at I
mean we have the texture mean and the
radius mean obviously the axes you can
also
see and one of the cool things on here
is you can also see the histogram they
show that for the the radius mean where
is the most common radius mean come up
and where the most common texture is so
we're looking at the tech the on each
growth its average texture and on each
radius its average uh radius on there
gets a little confusing because we're
talking about the individual objects
average and then we can also look over
here and see the the histogram showing
us the median or how common each
measurement is and that's only two
columns so let's dig a little deeper
into to Seaborn they also have a heat
map and if you're not familiar with heat
Maps a heat map just means it's in color
that's all that means heat map I guess
the original ones were plotting heat
density on something and so ever sens
it's just called a heat map and we're
going to take our data and get our
corresponding numbers to put that into
the heat map and that's simply data. C
RR for that that's a pandas expression
remember we're working in a pandas data
frame so that's one of the Cool Tools in
pandas for our dat and this's just pull
that information into a heat map and see
what that looks like and you'll see that
we're now looking at all the different
features we have our ID we have our
texture we have our area our compactness
concave points and if you look down the
middle of this chart diagonal going from
the upper left to bottom right it's all
white that's because when you compare
texture to texture they're identical so
they're 100% or in this case perfect one
in their correspondence
and you'll see that when you look at say
area or right below it it has almost a
black on there when you compare it to
texture so these have almost no
corresponding data They Don't Really
form a linear graph or something that
you can look at and say how connected
they are they're very scattered data
this is really just a really nice graph
to get a quick look at your data doesn't
so much change what you do but it
changes verifying so when you get an
answer or something like that or you
start looking at some of these
individual pieces
you might go hey that doesn't match
according to showing our heat map this
should not correlate with each other and
if it is you're going to have to start
asking well why what's going on what
else is coming in there but it does show
some really cool information on here and
we can see from the ID there's no real
one feature that just says if you go
across the top line that lights up
there's no one feature that says hey if
the area is a certain size then it's
going to be B9 or malignant it says
there's some that sort of add up and
that's a big hint in the data that we're
trying to ID this whether it's malignant
or B9 that's a big hint to us as data
scientists to go okay we can't solve
this with any one feature it's going to
be something that includes all the
features or many of the different
features to come up with the solution
for it and while we're exploring the
data let's explore one more area and
let's look at data do is null we want to
check for null values in in our data if
you remember from earlier in this
tutorial we did it a little differently
where we added stuff up and summ them up
you can actually with pandas do it
really quickly data. isnull and Summit
and it's going to go across all the
columns so when I run
this you're going to see all the columns
come up with no null
data so we've just just to rehash these
last few steps we've done a lot of
exploration we have looked at the first
two columns and seeing how they plot
with the caborn with the joint plot
which shows both the histogram and the
data plotted on the XY coordinates and
obviously you can do that more in detail
with different columns and see how they
plot together and then we took and did
the Seaborn heat map the SNS do heat map
of the data and you can see right here
where it did a nice job showing us some
bright spots where stuff correlates with
each other and forms a very nice
combination or points of scattering
points and you can also see areas that
don't and then finally we went ahead and
check the data is the data null value do
we have any missing data in there very
important step because it'll crash later
on if you forget to do this step it will
remind you when you get that nice error
code that says null values okay so not a
big deal if you miss it but it it's no
fun having to go back when you're you're
in a huge process and you've missed this
step and now you're 10 steps later and
you got to go remember where you were
pulling the data
in so we need to go ahead and pull out
our X and our y so we just put that down
here and we'll set the x equal to and
there's a lot of different options here
certainly we could do x equals all the
columns except for the first two because
if you remember the first two is the ID
and the diagnosis so that certainly
would be an option but what we're going
to do is we're actually going to focus
on the worst the worst r radius the
worst texture parameter area smoothness
compactness and so on one of the reasons
to start dividing your data up when
you're looking at this information is
sometimes the data will be the same data
coming in so if I have two measurements
coming into my model it might overweigh
them it might overpower the other
measurements because it's measur it's
basically taking that information in
twice that's a little bit past the scope
of this tutorial I want you to take away
from this though is that we are dividing
the data up into pieces and our team in
the back went ahead and said hey let's
just look at the worst so I'm going to
create a an array and you'll see this
array radius worst texture worst
perimeter worst we've just taken the
worst of the worst and I'm just going to
put that in my X so this x is still a
panda's data frame but it's just those
columns and our y if you remember
correctly it's going to be oops hold on
one second it's not X it's data there we
go so X equals data and then it's a list
of the different columns the worst of
the worst and if we're going to take
that then we have to have our answer for
our Y for the stuff we know and if you
remember correctly we're just going to
be looking
at the diagnosis that's all we care
about is what is it diagnosed is it Bine
or malignant and since it's a single
column we can just do diagnosis oh I
forgot to put the brackets are the there
we go okay so it's just diagnosis on
there and we can also real quickly do
like x. head if you want to see what
that looks like and Y do head and run
this and you'll see um it only does the
last one I forgot about that if you
don't do print you can see that the the
y. head is just Mmm because the first
ones are all malignant and if I run this
the x. head is just the first five
values of radius worse texture worse
parameter worst area worst and so on
I'll go ahead and take that out so
moving down to the next step we've built
our two data sets our answer and then
the features we want to look
at in data science it's very important
to test your
model so we do that by splitting the
data and from sklearn model selection
we're going to import train test split
so we're going to split it into two
groups there are so many ways to do this
I noticed in one of the more modern ways
they actually split it into three groups
and then you model each group and test
it against the other groups so you have
all kinds of and there's reasons for
that which is pass the scope of this and
for this particular example isn't
necessary for this we're just going to
split it into two groups one to train
our data and one to test our data and
the sklearn uh. model selection we have
train test split you could write your
own quick code to do this we just
randomly divide the data up into two
groups but they do it for us nicely
and we actually can almost we can
actually do it in one statement with
this where we're going to generate four
variables capital x train capital X test
so we have our training data we're going
to use to fit the model and then we need
something to test it and then we have
our y train so we're going to train the
answer and then we have our test so this
is the stuff we want to see how good it
did on our model and we'll go ahead and
take our train test split that we just
imported and we're going to do X and our
y our two different data that's going in
for our split and then the guys in the
back came up and wanted us to go ahead
and use a test size equals three that's
testore size random State it's always
nice to kind of switch a random State
around but not that important what this
means is that the test size is we're
going to take 30% of the data and we're
going to put that into our test
variables our y test and our X test and
we're going to do 70% into the X train
and the Y train so we're going to use
70% of the the data to train our model
and 30% to test it let's go ahead and
run that and load those up so now we
have all our stuff split up and all our
data ready to go now we get to the
actual Logistics part we're actually
going to do our create our model so
let's go ahead and bring that in from
sklearn we're going to bring in our
linear model and we're going to import
logistic regression that's the actual
model we're using and this we'll call it
log
model there we go model and this just
set this equal to our logistic
regression that we just imported so now
we have a variable log model set to that
class for us to use and with most the uh
models in the SK learn we just need to
go ahead and fix it fit do a fit on
there and we use our X train that we
separated out with our y train and let's
go ahead and run this so once we've run
this we'll have a model that fits this
data that 70% of our training data
uh and of course it prints this out that
tells us all the different variables
that you can set on there there's a lot
of different choices you can make but
for word do we're just going to let all
the defaults sit we don't really need to
mess with those on this particular
example and there's nothing in here that
really stands out is super important
until you start fine-tuning it but for
what we're doing the basics will work
just fine and then let's we need to go
ahead and test out our model is it
working so let's create a variable y
predict and this is going going to be
equal to our log model and we want to do
a predict again very standard uh format
for the sklearn library is taking your
model and doing a predict on it and
we're going to test y predict against
the Y test so we want to know what the
model thinks it's going to be that's
what our y predict is and with that we
want the capital x x test so we have our
train set and our test set and now we're
going to do our y predict and let's go
ahead and run that
and if we uh
print y predict let me go ahead and run
that you'll see it comes up and it PRS a
prints a nice array of uh B and M for B9
and
malignant for all the different test
data we put in there so it does pretty
good we're not sure exactly how good it
does but we can see that it actually
works and it's functional was very easy
to create you'll always discover with
our data science that as you explore
this you spend a significant amount of
time prepping your data and making sure
your data coming in is good uh there's a
saying good data in good answers out bad
data in bad answers out that's only half
the thing that's only half of it
selecting your models becomes the next
part as far as how good your models are
and then of course fine tuning it
depending on what model you're using so
we come in here we want to know how good
this came out so we we have our y
predict here log model. predict X
test so for deciding how good our model
is we're going to go from the sklearn
metrics we're going to import
classification report and that just
reports how good our model is doing and
then we're going to feed it the uh model
data and let's just print this out and
we'll take our classification
report and we're going to put into there
our test our actual data so this is what
we actually know is true and our
prediction what our model predicted for
that data on the test side and let's run
that and see what that
does so we pull that up you'll see that
we have um a Precision for B9 and
malignant B&M and we have a Precision of
93 and 91 a total of 92 so it's kind of
the average between these two of 92
there's all kinds of different
information on here your F1
score your recall your support coming
through on this and for this I'll go
ahead and just flip back to our slides
that they put together for describing it
and so here we're going to look at the
Precision using the classification
report and you see this is the same
print out I had up above some of the
numbers might be different because it
does randomly pick out which data we're
using so this model is able to predict
the type of tumor with
91% accuracy that's so when we look back
here that's you will see where we have
uh B9 and Inland it actually is 92
coming up here we're looking about a 92
91% precision and remember I reminded
you about domain so we're talking about
the domain of a medical domain with a
very catastrophic outcome you know at 91
or 92% Precision you're still going to
go in there and have somebody do a
biopsy on it very different than if
you're investing money and there's a 92%
chance you're going to earn 10% and8 %
chance you're going to lose 8% you're
probably going to bet the money because
at that odds it's pretty good that
you'll make some money and in the long
run you do that enough you'll definitely
will make money and also with this
domain I've actually seen them use this
to identify different forms of cancer
that's one of the things that they're
starting to use these models for because
then it helps the doctor know what to
investigate so that wraps up this
section we're finally we're going to go
in there and let's discuss the answer to
the quiz asked in machine learning
tutorial part one can you tell what's
happening in the following cases
grouping documents into different
categories based on the topic and
content of each document this is an
example of clustering where K means
clustering can be used to group the
documents by topics using bag of words
approach so if You' gotten in there that
you're looking for clustering and
hopefully you had at least one or two
examples like K means that are used for
clustering different things then give
yourself a two thumbs up B identifying
handwritten digits and images correctly
this is an example of classification the
traditional approach to solving this
would be to extract digit dependent
features like curvature of different
digits Etc and then use a classifier
like svm to distinguish between images
again if you got the fact that it's a
classification example give yourself a
thumb up and if you're able to go hey
let's use svm or another model for this
give yourself those two thumbs up on it
C behavior of a website indicating that
the site is not working as designed this
is an example of anomaly detection in
this case the algorithm learns what is
normal and what is not normal usually by
observing the logs of the website give
yourself a thumbs up if you got that one
and just for a bonus can you think of
another example of anomaly detection one
of the ones I use for my own business is
detecting anomalies in stock markets
stock markets are very ficked and they
behave very erical so finding those
erratic areas and then finding ways to
track down why they're Ric was something
released in social media was something
released you can see we're knowing where
that anomaly is can help you to figure
out what the answer is to it in another
area D predicting salary of an
individual based on his or her years of
experience this is an example of
regression this problem can be
mathematically defined as a function
between independent years of experience
and dependent variables salary of an
individual and if you guess that this
was a regression model give yourself a
thumbs up and if you're able to remember
that it was between independent and
dependent variables and that terms give
yourself two thumbs up summary so to
wrap it up we went over what is K means
and we went through also the chart of
choosing your elbow method and assigning
a random centroid to the Clusters
Computing the distance and then going in
there and figuring out what the minimum
centroids is and Computing the distance
and going through that Loop until it
gets the perfect centroid and we looked
into the elbow method to choose K based
on running our clusters across a number
of variables and finding the best
location for that we did a nice example
of clustering cars with K means even
though we only looked at the first two
columns to make it simple and easy to
graph can easily extrapolate that and
look at all the different columns and
see how they all fit together and we
looked at what is logistic regression we
discussed the sigmoid function what is
logistic regression and then we went
into an example of classifying tumors
with Logistics think about this you're
about to create something amazing an AI
that can think learn and grow in ways we
only dreamed of and here's the best part
you don't need to be an AI expert to
make it happen what if you could use
Lang chain a tool that connects most
advanced language models to realtime
data allowing you to build AI
applications that are both smart and
flexible it sounds like something out of
Science Fiction but with Lang chain it's
real as large language models quickly
become the backbone of many applications
L chain has emerged as a gamechanging
tool transforming the way we use these
powerful Technologies today we are
diving into Lang chain the ultimate
framework that makes AI development
easier for everyone whether you want to
understand user questions with one llm
create humanik responses with another or
pull in Data Insights L chain makes it
all happen but L chain is more than just
making AI easy to use it's about getting
these models to work together seamless L
lch simplifies what could be a complex
process into simple powerful system from
Smart Chart BS to enhancing data for
machine learning the possibilities with
Lang chain are endless so why has Lang
chain become one of the fastest growing
open source projects ever and how you
can use Lang chain to get ahead in the
world of AI so let's first start by
understanding what is Lang chain Lang
chain is an open source framework
designed to help developers build AI
powered applications using large
language models or llms just like gbd4
but what really sets langin apart is its
ability to link these powerful models
with external data sources and other
components so this allows you to create
sophisticated natural language
processing NLP applications that can do
much more than just understand and
generate text they can interact with
live data databases and other software
tools now you might be asking is Lang
chain a python Library yes it is Lang
chain is available as a python Library
which means you can can easily integrate
into your existing python projects but
it doesn't stop there langin is also
available in JavaScript and typescript
making it accessible to a wide range of
developers whether you're working on a
web app or backend system or a
standalone tool Lang chain fits right in
so why should we use Lang chain so why
is Lang chain such a big deal developing
AI applications typically requires using
multiple tools and writing a lot of
complex code you need to manage data
retrieval processing integration with
language models and many more this can
be time consuming and complicated
especially if you're not deeply familiar
with AI Lang chain simplifies the entire
process allowing you to develop and
deploy and even manage AI applications
more easily and efficiently let's break
down this with an example imagine you're
building a chart board that needs to
provide realtime weather updates without
L chain you would need to manually
connect your AI to weather API fetch
data process it and then format the
response but with Lang chain the process
becomes much more straightforward you
can focus on what matters the most
building the features and
functionalities of your application
while Lang chain handles the complex
Integrations behind the scenes so let's
discuss the key features of Lang chain L
chain is packed with features that make
it incredibly powerful and flexible
let's take a closer look at some of the
key components at first we have model
interaction langin allows you to
interact with any language model
seamlessly it manages the inputs and
outputs to these data models ensuring
that you can integrate them into your
application without a hitch for example
if you want to use gp4 to generate
responses to customer inquiries L chain
makes it easy to plug that model into
your workflow next we have data
connection and retrieval one of the Lang
chain strength is its ability to connect
to external data sources so whether you
need to pull data from a database or web
API or even a file system langing
simplifies this process you can retrieve
transform and use data from almost any
Source making your AI applications more
robust and versatile next we have chains
L gen introduces the concept of chains
where you can link multiple models and
components together to perform complex
task for example you might have a chain
where one component retrieves data and
another processes it and a third
generates a humanlike response this
chaining ability allows you to build
workflows that would otherwise require
extensive coding next we have agents
agents are like the decision makers in L
chain they can create commands deciding
the best course of action based on the
input they receive for example an agent
Cloud determine which language model to
use based on the type of query it's
handling making your application smarter
and more adaptive then we have memory
Lang chain supports both short-term and
long-term memory making that your AI can
remember past interactions this is
particularly useful for applications
like chatbots where maintaining context
over multiple interactions are
significantly improve the user
experience imagine you're building a
virtual assistant the assistant needs to
remember previous interactions to
provide relevant responses now with the
help of flank chain you can easily
Implement memory so that the assistant
knows what you have talked before making
the conversation more natural and
engaging so what are the Integrations
supported by Lang chain well Lang chain
is designed to work seamlessly with a
wide variety of Integrations making it
extremely versatile for different use
cases llm providers Lang chain supports
integration with major llm providers
like openi hugging face and cohort this
means you can easily incorporate the
latest and most powerful language models
into your applications then we have data
sources Lang chain can connect to
variety of data sources such as Google
search Wikipedia and Cloud platforms
like AWS Google cloud and Azure this
makes easy to retrieve and use the most
up-to-date information in your
applications Vector databases are used
for handling large volumes of complex
data such as images or long text so Lang
chain integrates with Vector databases
like pine cone and these databases store
data as high dimensional vectors which
helps in allowing for efficient and
accurate retrieval so this is
particularly useful for applications
that require searching through large
data sets quickly for example let's say
you are building an application that
needs to analyze thousands of documents
to find relevant information with Lang
chain you can integrate a vector
database like pine cones through your
documents as vectors and quickly search
through using them powerful language
models this capability can save you a
lot of time and make your application
much more effective now the question
arises how to create proms in Lang chain
creating proms in Lang chain is much
easier with something called a prompt
template a prompt template acts as a set
of instructions for language model and
these templates can be customized to
varing levels of customizations for
example you might design a prom template
to ask simple questions or you could
create more detailed instructions that
guide the language model to produce high
quality responses let's walk through how
you can create a prompt using Lang chain
in Python Step One is installing Lang
chain first you'll need to have python
installed in your system once that set
up you can install Lang Chain by opening
your python shell or terminal and
running the following command p IP
install Lang chain the next step is
adding Integrations to Lang chain this
often requires at least one integration
to function properly a common choice is
open ai's language model API to use open
ai's API you'll need to create an
account on the openai website and obtain
your API key after that install openi
python package and input your API key
like this so this is the following uh
command which is inserted below you can
look into that next step is importing
and using a prom template now that you
have Lang chain and the necessary
integration set up you can start
creating your prompts Lang chain offers
a pre-made prom template that allows you
to structure your text in a way that the
language model can easily understand
here's how you can do it through this
particular prompt given below so in this
Example The Prompt template asks two
variables which is an adjective and a
Content subject and uses them to
generate a prompt the output might be
something like tell me an interesting
fact about zebras the language model
would then take this prompt and a
relevant fact about zebras based on the
given objective this is simple but
powerful way to generate Dynamic prompt
that can be adapted to a wide range of
task from answering questions to
generating creative content let's now
talk about how to develop applications
with Lang chain so building applications
with Lang chain is straightforward and
involves a few key steps first Define
your application know exactly what
problem it's solving and identify the
necessary components like language
models data sources and user interaction
the next step is to build a
functionality using L chains components
such as prom chains and agents this is
where you can create the logic that
drives your application like processing
user input or retrieving data then we
have customizing your application to
meet specific needs L chains flexibility
allows you to tweak prompts integrate
additional data sources and F tune
models for Optimal Performance before
going live it's crucial to test and
deploy your application testing helps
catch any issues and L chain makes
debugging easy so you can deploy your
application with confidence for example
let's build a chart board using Lang
chain first we have to Define it is a
chart board that answers question about
technology Trends we then create a
functionality by setting a prompt and a
chain to process input next we have
customization we customize it by
integrating a new API to pull in the
latest information and finally we test
and deploy the chatboard to ensure it
responds accurately to users so Lang
chin offers Endless Possibilities across
various Industries let's now look into
the examples and use cases of Lang chain
Lang chain offers Endless Possibilities
across various Industries you can create
customer service chat boards that manage
queries and transaction or coding
assistance that suggest Cod Snippets and
debug issues in healthcare land chain
can assist doctors with diagnosis and
patient data management then we have
marketing and e-commerce it can analyze
consumer Behavior generate product
recommendations and craft compelling
product descriptions so with the help of
this AI assistant it help helps doctors
make quicker more informed decisions so
Lang chain is a powerful framework that
makes EI development accessible and
efficient why reinforcement learning
training a machine learning model
requires a lot of data which might not
always be available to us further the
data provided might not be reliable
learning from a small subset of actions
will not help expand the vast realm of
solutions that may work for a particular
problem you can see here we have the
robot learning to walk um very
complicated setup when you're learning
how to walk and you'll start asking
questions like if I'm taking one step
forward and left what happens if I pick
up a 50 PB object how does that change
how a robot would walk these things are
very difficult to program because
there's no actual information on it
until it's actually tried out learning
from a small subset of actions will not
help expand the vast realm of solutions
that may work for a particular
problem and we'll see here it learned
how to walk this is going to slow the
growth grow that technology is capable
of machines need to learn to perform
actions by themselves and not just learn
off
humans and you see the objective climb a
mountain real interesting point here is
that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it most of the models the
non-reinforcement models in computer uh
machine learning aren't able to do that
very well uh there's a couple of them
that can be used or integrated to see
how it goes is what we're talking about
with reinforcement learning so what is
reinforcement learning reinforcement
learning is a subbranch of machine
learning that trains a model to return
an Optimum solution for a problem by
taking a sequence of decisions by itself
consider a robot learning to go from one
place to another the robot is given a
scenario must arrive at a Solution by
itself the robot can take different
paths to reach the
destination it will know the best path
by the time taken on each path it might
even come up with a unique solution all
by itself and that's really important is
we're looking for Unique Solutions uh we
want the best solution but you can't
find it unless you try it so we're
looking at uh our different systems our
different model we have supervised
versus unsupervised versus reinforcement
learning and with the super vised
learning that is probably the most
controlled environment uh we have a lot
of different supervised learning models
whether it's linear regression neural
networks um there's all kinds of things
in between decision trees the data
provided is labeled data with output
values specified and this is important
because when we talk about supervised
learning you already know the answer for
all this information you already know
the picture has a motorcycle in it so
you're supervised learning you already
know that um the outcome for tomorrow
for you know going back a week you're
looking at stock you can already have
like the graph of what the next day
looks like so you have an answer for
it and you have labeled data which is
used you have an external supervision
and solves Problems by mapping labeled
input to know one output so very
controlled unsupervised learning and
unsupervised learning is really
interesting because it's now taking part
in many other models they start with an
you can actually insert an unsupervised
learning model um in almost either
supervised or reinforcement learning as
part of the system which is really cool
uh data provided is unlabeled data the
outputs are not specified machine makes
its own predictions used to solve
association with clustering problems
unlabeled data is used no supervision
solves Problems by understanding
patterns and discovering
output uh so you can look at this and
you can think um some of these things go
with each other they belong together so
it's looking for what connects in
different ways and there's a lot of
different algorithms that look at this
um when you start getting into those are
some really cool images that come up of
what unsupervised learning is how it can
pick out say uh the area of a donut one
model will see the area of the donut and
the other one will divide it into three
sections based on this location versus
what's next to it so there's a lot of
stuff that goes in with unsupervised
learning and then we're looking at
reinforcement learning probably the
biggest industry in today's market uh in
machine learning or growing Market it's
very in it's very infant stage uh as far
as how it works and what it's going to
be capable of the machine learns from
its environment using rewards and errors
used to solve reward based problems no
predefined data is used no supervision
follows Trail and error problem solving
approach uh so again we have a random
first you start with a random I try this
it works and this is my reward doesn't
work very well maybe or maybe it doesn't
even get you where you're trying to get
it to do and you get your reward back
and then it looks at that and says well
let's try something else and it starts
to play with these different things
finding the best route so let's take a
look at important terms in today's
reinforcement
model and this has become pretty
standardized over the last uh few years
so these are really good to know we have
the agent uh agent is the model that is
being trained via reinforcement learning
so this is your actual uh entity that
has however you're doing it whether
you're using a neural network or a a q
table or whatever combination thereof
this is the actual agent that you're
using this is the
model and you have your environment uh
the training situation that the model
must optimize to is called its
environment uh and you can see here I
guess we have a robot who's trying to
get a chest full of gems or whatever and
that's the output and then you have your
action this is all possible steps that
can be taken by the model
and it picks one action and you can see
here it's picked three different uh
routes to get to the chest of diamonds
and
gems we have a state the current
position condition returned by the
model and you could look at this uh if
you're playing like a video game this is
the screen you're looking at uh so when
you go back here uh the environment is a
whole game board so if you're playing
one of those Mobius games you might have
the whole game board going on uh but
then you have your current position
where are you on that game board what's
around that what's around you um if you
were talking about a robot the
environment might be moving around the
yard where it is in the yard and what it
can see what input it has in that
location that would be the current
position condition returned by the model
and then the reward uh to help the model
move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yay you
did good or if uh didn't do as good
trying to maximize the reward and have
the best reward
possible and then policy policy
determines how an agent will behave at
any time it acts as a mapping between
action and present State this is part of
the model what would what is your action
that you're you're going to take what's
the policy you're using to have an
output from your agent one of the
reasons they separate uh policy as its
own entity is that you usually have a
prediction um of a different options and
then the policy well how am I going to
pick the best based on those predictions
I'm going to guess at different options
and we'll actually weigh those options
in and find the best option we think
will work uh so it's a little tricky but
the policy thing is actually pretty cool
how it works let's go Ahad and take a
look at a reinforcement learning example
and just in looking at this we're going
to take a look uh consider what a dog um
that we want to train
uh so the dog would be like the agent so
you have your your puppy or whatever uh
and then your environment is going to be
the whole house or whatever it is where
you're training them and then you have
an action we want to teach the dog to
fetch so action equals fetching uh and
then we have a little biscuit so we can
get the dog to perform various actions
by offering incentives such as a dog
biscuit as a
reward the dog will follow a policy to
maximize this reward and hence will
follow every commit and might even learn
new actions like begging by itself uh so
you have B you know so we start off with
fetching it goes oh I get a biscuit for
that it tries something else and you get
a handshake or begging or something like
that and it goes oh this is also
reward-based and so it kind of explores
things to find out what will bring it
his biscuit and that's very much like
how a reinforced model goes is it uh
looks for different rewards how do I
find can I try different things and find
a reward that
works the dog also want to run around
and play and explore its environment uh
this quality of model is called
exploration so there's a little
Randomness going on in
Exploration and explores new parts of
the house climbing on the sofa doesn't
get a reward in fact it usually gets
kicked off the
sofa so let's talk a little bit about
markov's decision process uh marov
decision process is a reinforcement
learning policy used to map a current
state to an action where the agent
continuously interacts with the
environment to produce new Solutions and
receive rewards and you'll see here's
all of our different uh uh vocabulary we
just went over we have a reward our
state or agent our environment and our
action and so even though the
environment kind of contains everything
um that you you really when you're
actually writing the program your
environment's going to put out a reward
in state that goes into the agent uh the
agent then looks at this uh state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or it didn't get rewarded and
then it looks at the state then it comes
back and if you remember from policy the
policy comes in um and then we have a
reward the policy is that part that's
connected at the bottom and so it looks
at that policy and it says hey what's a
good action that will probably be
similar to what I did or um sometimes
they're completely random but what's a
good action that's going to bring me a
different
reward so taking the time to just
understand these different pieces as
they go is pretty important in most of
the models today um and so a lot of them
actually have templates based on this
you can pull in and start using um
pretty straightforward as far as once
you start seeing how it works uh you can
see your environment send says hey this
is the agent did this if you're a
character in a game this happened and it
shoots out a reward in a state the agent
looks at the reward looks at the new
state and then takes a little guess and
says I'm going to try this action and
then that action goes back into the
environment it affects the environment
the environment then changes depending
on what the action was and then it has a
new state and a new reward that goes
back to the agent so in the diagram
shown we need to find the shortest path
between note A and D each path has a
reward associated with it and the path
with the maximum reward is what we want
to choose the nodes AB b c d denote the
nodes to travel from node uh A to B is
an action reward is the cost of each
path and policy is each path
taken and you can see here a can go uh
to b or a can go to C right off the bat
or it can go right to D and if you
explored all three of these uh you would
find that a going to D was a zero reward
um a going to C and D would generate a
different reward or you could go AC b d
there's a lot of options here um and so
when we start looking at this diagram
you start to
realize that even though uh today's
reinforced learning models do really
good at um finding an answer they end up
trying almost all the different
directions you see and so they take up a
lot of work uh or a lot of processing
time for reinforcement learning they're
right now in their in stage and they're
really good at solving simple problems
and we'll take a look at one of those in
just a minute in a tic tac toe game uh
but you can see here uh once it's gone
through these and it's explored it's
going to find the
ACD is the best reward it gets a full 30
points for it so let's go ahead and take
a look at a reinforcement learning
demo uh in this demo we're going to use
reinforcement learning to make a tic tac
toe game you will be playing this game
Against the Machine learning model
and we'll go ahead we're doing it in
Python so let's go ahead and go through
I always uh not always actually have a
lot of python tools let's go through um
Anaconda which will open up a Jupiter
notebook seems like a lot of steps but
it's worth it to keep all my stuff
separate and it's also has a nice
display when you're in the Jupiter
notebook for doing
python so here's our Anaconda Navigator
I open up the notebook which is going to
take me to a web page and I've gone in
here and created a new uh pyth folder in
this case I've already done it and
enabled it to change the name to
tic-tac-toe uh and then for this example
uh we're going to go ahead
and import a couple things we're going
to um import nump as NP we'll go ahead
and import pickle nump of course is our
number array and then pickle is just a
nice way sometimes for storing uh
different information uh different
states that we're going to go through on
here uh and so we're going to create a
class called state we're going to start
with
that and there's a lot of uh lines of
code to this uh class that we're going
to put in here don't let that scare you
too much there's not as much here um it
looks like there's going to be a lot
here but there really is just a lot of
setup going on in the in our class date
and so we have up here we're going to
initialize it um we have our board um
it's a Tic Tac Toe board so we're only
dealing with nine spots on the board uh
we have player one player
two uh is in we're going to create a
board hash uh we'll look at that in just
a minute we're just going to stir some
information in there symbol of player
equals one um so there's a few things
going on as far as the
initialization uh then something simple
we're just going to get the hash um of
the board we're going to get the
information from the board on there
which is uh columns and rows we want to
know when a winner occurs uh so if you
get three in a row that's what this
whole section here is for um me go ahead
and scroll up a little bit and you can
get a copy of this code if you send a
note over to Simply learn we'll send you
over U this particular file and you can
play with it yourself and see how it's
put together I don't want to spend a
huge amount of time on this uh because
this is just some real General python
coding uh but you can see here we're
just going through um all the rows and
you add them together and if it equals
three three and a row same thing with
columns um diagonal so you got to check
the diagonal that's what all this stuff
does here is it just goes through the
different areas actually let me go ahead
and
put there we
go um and then it comes down here and we
do our sum and it says true uh minus
three just says did somebody win or is
it a tie so you got to add up all the
numbers on there anyway just in case
they're all filled up and next we also
need to know available positions these
are ones that don't no one's ever used
before this way when you try something
or the computer tries something uh it's
not going to give it an illegal move
that's what the available positions is
doing uh then we want to update our
state and so you have your position
going in we're just sending in the
position that you just chose and you'll
see there's a little user interface we
put in there you can p pick the row and
column in
there and again I mean this is a lot of
code uh so really it's kind of a thing
you'd want to go through and play with a
little bit and just read through it get
a copy of it uh great way to understand
how this works and here is a given
reward um so we're going to give a
reward result equals self winner this is
one of the hearts of what's going on
here uh is we have a result self. winner
so if there's a winner then we have a
result if the result equals one here's
our
feedback uh if it doesn't equal one then
it gets a zero so it only gets a reward
in this particular case if it
wins and that's important to know
because different uh systems of
reinforced learning do rewarding a lot
differently depending on what you're
trying to do this is a very simple
example with a a 3X3 board imagine if
you're playing a video game uh certainly
you only have so many actions but your
environment is huge you have a lot going
on in the environment and suddenly a
reward system like this is going to be
just um is going to have to change a
little bit it's going to have to have
different rewards and different setup
and there's all kinds of advanced ways
to do that as far as weighing you add
weights to it and so they can add the
weights up depending on where the reward
comes in so it might be that you
actually get a reward in this case you
get the reward at the end of the game
and I'm spending just a little bit of
time on this because this is an
important thing to note but there's
different ways to add up those rewards
it might have like if you take a certain
path um the first reward is going to be
weighed a little bit less than the last
reward because the last reward is
actually winning the game or scoring or
whatever it is so this reward system
gets really complicated on some of the
more advanced uh
setups um in this case though you can
see right here that they give a a a 0.1
and a 0. five
reward um just for getting a picking the
right value and something that's
actually valid instead of picking an
invalid value so rewards again that's
like key it's huge how do you feed the
rewards back in uh then we have a board
reset that's pretty straightforward it
just goes back and resets the board to
the beginning because it's going to try
out all these different things while
it's learning it's going to do it by
trial and error so you have to keep
resetting it and then of course there's
the play we want to go ahead and play uh
rounds equals 100 depends on what you
want to do on here here um you can set
this different you obviously set that to
higher level but this is just going to
go through and you'll see in here uh
that we have player one and player two
this is this is the computer playing
itself uh one of the more powerful ways
to learn to play a game or even learn
something that isn't a game is to have
two of these models that are basically
trying to beat each other and so they
they keep finding explore new things
this one works for this one so this one
tries new things it beats this we've
seen this in um chess I think was a big
one where they had the two players in
chess with reinforcement learning uh was
one of the ways they train one of the
top um computer chess playing
algorithms uh so this is just what this
is it's going to choose an action it's
going to try something and the more it
tries stuff um the more we're going to
record the hash we actually have a board
hash where they self get the hash set up
on on here where it stores all the
information and then once you get to a
win one of them wins it gets the reward
uh then we go back and reset and try
again and then kind of the fun part we
actually get down here is uh we're going
to play with a human so we'll get a
chance to come in here and see what that
looks like when you put your own
information in and then it just comes in
here does the same thing it did above it
gives it a reward for its things um or
sees if it wins or ties um looks at
available positions all that kind of fun
stuff and then finally we want to show
the board uh so it's going to print the
board out each
time really um as an integration is not
that exciting what's exciting uh in here
is one looking at this reward system
whoops Play One More up the reward
system is really the heart of this how
do you reward the different uh setup and
the other one is when it's playing it's
got to take an action and so what it
choose es for an action is also the
heart of reinforcement learning how do
we choose that action and those are
really key to right now where
reinforcement learning is uh in today's
uh technology is uh figuring this out
how do we reward it and how do we guess
the next best action so we have our uh
environment and you can see the
environment is we're going to be or the
state uh which is kind of like at what's
going on we're going to return the state
depending on what happens and we want to
go ahead and create our agent uh in this
CL our player so each one is let go and
grab that and so we look at a class
player
um this is where a lot of the magic is
really going on is what how is this
player figuring out how to maneuver
around the board and then the board of
course returns a state uh that it can
look at and a reward uh so we want to
take a look at this we have a name uh
self state this is class player and when
you say class player we're not talking
about a human player we're talking about
U just a uh the computer players and
this is kind of interesting so remember
I told you depending on what you're
doing there's going to be a Decay gamma
um explor rate uh these are what I'm
talking about is how do we train it
um as you try different moves it gets to
the end the first move is important but
it's not as important as the last one
and so you could say that the last one
has the heaviest weight and then as you
as you get there the first one let's see
the first move gives you a five reward
the second gives you a two reward and
the third one gives you a 10 reward
because that's the final ending you got
it the 10's going to count more than the
first step uh and here's our uh we're
going to you know get the board
information coming in and then choose an
action this was the second part that I
was talking about that was so important
uh so once you have your train training
going on we have to do a little
Randomness and you can see right here is
our NP random um uniform so it's picking
out a random number take a random action
this is going to just pick which row and
which column it is um and so choosing
the action this one you can see we're
just doing random States um Choice
length of positions action position and
then it skips in there and takes a look
at the board uh for p and positions you
get storing the different boards each
time you go through so it has a record
of what it did so it can properly weigh
the values and this simply just depins a
hash date what's the last date pin it to
the uh um to our states on here here's
our
feedback reward so the reward comes in
and it's going to take a look at this
and say is it none uh what is the reward
and here is that formula remember I was
telling you about up here um that was
important because it has Decay gamma
times the reward this is where as it
goes through each step and this is
really important this is this is kind of
the heart of this of what I was talking
about earlier uh you have step
one and this might have a a reward of
two you have step two I should probably
should have done ABC this has a step
three uh step
four so on till you get to step in and
this might have a reward of
10 uh so reward of
10 we're going to add that but we're not
adding uh let's say this one right here
uh let's say this reward here right
before 10 was um let's say it's also 10
that just makes the the math easy so we
had 10 and 10 we had 10 this is 10 and
10 n whatever it is but it's time it's
0. n uh so instead of putting a full 10
here we only do nine that's uh 0.9
time
10 and so this
formula um as far as the Decay times the
reward minus the cell State value uh it
basically adds in it says here's one or
here's two I'm sorry I should have done
this ABC it would have been easier uh so
the first move goes in here and it puts
two in
here uh then we have our s uh set up on
here you can see how this gets pretty
complicated in the math but this is
really the key is how do we train our
states and we want the the final State
the win to get the most points if you
win you get most points um and the first
step gets the least amount of points so
you're really training this almost in
Reverse you're training you're training
it from the last place where you have
like it says okay this is now where I
need to sum up my rewards and I want to
sum them up going in reverse and I want
to find the answer in Reverse kind of an
interesting uh uh play on the mind when
you're trying to figure this stuff
out and of course we want to go ahead
and reset the board down here uh save
the policy load
policy these are the different things
that are going in between the agent and
the state to figure out what's going on
let's go ahead and load that up and then
finally we want to go ahead and create a
human player
and the human player is going to be a
little different uh in that uh you
choose an action row and column here's
your action uh if action is if action in
positions meaning positions that are
available uh you return the action if
not it just keeps asking you until you
get an action that actually works and
then we're going to go ahead and append
to the hash state which uh we don't need
to worry about because it Returns the
action up
here and feed forward uh again this is
because it's a
human um at the end of the game bat
propagate and update State values this
part isn't being done because it's not
programming uh the model uh the model is
getting its own rewards so we've gone
ahead and loaded this in here uh so
here's all our pieces and the first
thing we want to
do is set up uh P1 player one uh P2
player two and then then we're going to
send our players to our state so now it
has P1 P2 and it's going to play and
it's going to play 50,000 rounds now we
can probably do a lot less than this and
it's not going to get the full results
in fact you know what uh let's go ahead
and just do five um just to play with it
because I want to show you something
here oops somewhere in there I forgot to
load
something there we go I must have start
forgot to run this run
oops forgot a reference there for the
board rows and columns
3x3 um there is actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning uh so now I've only set this
up with um see where are we going here
I've only set this up to
train five times and the reason I did
that is we're going to uh come in and
actually play it and then I'm going to
change that and we can see how it
differs on
there there we go and I didn't even make
it through a run and we're going to go
ahead and save the
policy um so now we have our player one
and our player two policy uh the way we
set it up it has two separate policies
loaded up in
there and then we're going to come in
here and we're going to do uh player one
is going to be the computer experience
rate zero load policy one human player
human and we're going to go ahead and
play this and I remember I only went
through it um uh just one round of
training in fact minimal training and so
it puts an X there and I'm going to go
ahead and do row zero column one you can
see this is very uh basic on here and so
I put in my zero and then I'm going to
go zero block it zero Z and you can see
right here it let me win uh just like
that I was able to win zero
two and wo human winds so I only trained
it five times we're going to run this
again and this time uh instead of five
let's do 5,000 or 50,000 I think that's
what the guys in the back had and this
takes a while to train it this is where
reinforcement learning really falls
apart look how simple this game is we're
talking about uh 3x3 set of
columns and so for me to train it on
this um I could do a q table which would
take which would go much quicker um you
could build a quick Q table with almost
all the different options on there and
uh you would probably get a the same
result much quicker we're just using
this as an example so when we look at
reinforcement learning you need to be
very careful what you apply it to it
sounds like a good deal until you do
like a large neural network where you're
doing um you set the neural network to a
learning increment of one so every time
it goes through it
learns and then you do your action so
you pick from the learning uh setup and
you actually try actions on the learning
setup until you get the what you think
is going to be the best action so you
actually feed what you think is right
back through the neural network there's
a whole layer there which is really fun
to play
with and then it has an output well
think of all those process Tes I mean
that is just a huge amount of work it's
going to do uh let's go ahead and Skip
ahead here give it a moment it's going
to take a a minute or two to go ahead
and
run now to train it uh we went ahead and
let it run and it took a while this this
took um I got a pretty powerful
processor and it took about five minutes
plus to run it and we'll go ahead and
uh run our player setup on here oops
brought in the last whoops I brought in
the last round so give me just a moment
to redo the policy save there we go I
forgot to save the policy back in
there and then go ahead and run our
player again so we' we've saved the
policy then we want to go ahead and load
the policy for P1 as the computer and we
can see the computer's gone in the
bottom right corner I'm going to go
ahead and go uh one one which is the
center and and it's gone right up the
top and if you have ever played Tic Tac
Toe you know the computer has me uh but
we'll go ahead and play it out row
zero column
two there it is and then it's gone here
and so I'm going to go ahead and go row
01 two no 01 there we go and column zero
that's where I wanted oh and it says I
okay you your action there we go boom uh
so you can see here we've got a didn't
catch the win on this it said Tai um
kind of funny that didn't catch the win
on
there but if we play this a bunch of
times you'll find it's going to win more
and more the more we train it the more
the reinforcement
happens this lengthy training process uh
is really the stopper on reinforcement
learning as this changes reinforcement
learning will be one of the more
powerful uh packages evolving over the
next decade or two in fact I would even
go as far as to say it is the most
important uh machine learning tool and
artificial intelligence tool out there
as it learns not only a simple Tic Tac
Toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
it's in what's the environments it's in
and so being able to attach environment
and context and all those things
together is going to require
reinforcement learning to do
so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with uh run it you can test it
out you can do U you know test it for
different uh uh values you can switch
from P1
computer um where we loaded the policy
one to load the policy2 and just see how
it varies there's all kinds of things
you can do on there so what is Q
learning Q learning is reinforcement
learning policy which will fill the next
best action given a current state it
chooses this action at random and aims
to Max maximize the reward and so you
can see here's our standard
reinforcement learning graph um by now
if you're doing any reinforcement
learning you should be familiar with
this where you have your agent your
agent takes an action the action affects
the environment and then the environment
sends back the reward or the feedback
and the state it's the new state the
agents in where is it at on the
chessboard where is it at in the video
game um if your robots out there picking
trash up off the side of the road where
is it at on the road consider an ad
recommendation system usually when you
look up a product
online you get ads which will suggest
the same product over and over
again using Q learning we can make an ad
recommendation system which will suggest
related products to our previous
purchase the reward will be if user
clicks on the suggested
product and again you can see um you
might have a lot of products um on uh
your web advertisement or your pages but
it's still not a float number it's still
a set number and that's something to be
aware of when you're using Q learning
and you can see here that if you have a
100 people clicking on ads and you click
on one of the ads it might go in there
and say okay this person clicked on this
ad what is the best set of ads based on
clicking on this ad or these two ads
afterwards based on where they are
browsing so let's go a and take a look
at some important terms when we talk
about Q learning uh we have States the
state s represents the current position
of an agent in an
environment um the action the action a
is the step taken by the agent when it
is particular State rewards for every
action the agent will get a positive or
negative
[Music]
reward and again uh when we talk about
States we're usually not with when
you're using a q table you're not
usually talking about float variables
you're talking about true false uh and
we'll take a closer look at that in a
second and episodes when an agent ends
up in a terminating State and can't take
a new
action uh this might be if you're
playing a video game your character
stepped in and is now dead or whatever
uh Q values used to determine how good
an action a taken at a particular State
s is QA of s and temporal difference a
formula used to find the Q value by
using the value of the current state and
acttion action and previous state and
action and very I mean there's bellman's
equation which basically is the equation
that kind of uh covers what we just
looked at in all those different terms
the Bellman equation is used to
determine the values of a particular
State and deduce how good it is to be in
take that state the optimal the optimal
state will give us the highest optimal
value Factor influencing Q values the
current state and action that's your
saay so your current state your
action uh then you have your previous
date in action which is your s um I
guess Prime I'm not sure how they how
they reference that S Prime a prime so
this is what happened before uh then you
have a reward for Action so you have
your R reward and you have your maximum
expected future
reward and you can see there's also a
learning rate put in there and a
discount rate uh so we're looking at
these just like any other model we don't
want to have an absolute um final value
on here we don't want it to if you do
absolute values instead of taking
smaller steps you don't really have that
approach to the solution you just have
it jump and then pretty soon if you jump
one solution out that's what's going to
be the new solution whichever one jumps
up really high first um kind of ruining
the whole idea of doing a random
selection and I'll go into the random
selection in just a second steps in Q
learning
step one create an initial Q table with
all values initialized to zero again
we're looking at
01 uh so are you you know here's our
action we start we're an idol we took a
wrong action we took a correct action in
int and then we have our um actions
fetching sitting and running of course
we're just using the dog example and
choose an action and perform it update
values in the table and of course when
we're choosing an action we're going to
kind of do something random and just
randomly pick one so you start out and
you sit and you have then a um then
depending on that um um action you took
you can now update the value for sitting
after you start from start to
sitting get the value of the reward and
calculate the Val the value Q value
using the Bellman equation and so now we
attach a reward to
sitting and when we attach all those
rewards we continue the same until the
table's filled with or an episode
ends and and my M I was going to come
back to the random side of this and
there's a few different formulas they
use for the random um setup to pick it I
usually let whatever Q model I'm using
do their standard one because someone's
usually gone in and done the math uh for
the optimal uh spread uh but you can
look at this if I have running has a
reward of 10 sitting has a reward of
seven fetching has a reward of five um
just kind of without doing like a a a
means you know using the bell curve for
the means value and like I said there's
some math you can put in there to pick
um so that you're more like so that
running has even a higher chance um but
even if you were just going to do an
average on this you could do an average
a random number by adding them all
together uh so you get 10 plus 7 + 5 is
22 you could do 0 to 22 and or 0 to 21
but 1 to 22 one to five would be
fetching uh and so forth you know the
last 10 so you can just look at this as
what percentage are you going to go for
that particular
option um and then that gets your random
setup in there and then as you slowly
increment these up uh you see that uh uh
if you're idle um where's one here we go
sitting at the end if you're at the end
of wherever you're at sitting gets a
reward of one um where's the good one on
here oh wrong action running for a wrong
action gets almost no reward so that
becomes very very less likely to happen
but it still might happen it still might
have a percentage of coming up and
that's where the random programming and
Q learning comes in the below table
gives us an idea of how many times an
action has been taken and how positively
correct action or negatively wrong
action it is going to affect the next
state so let's go ahead and dive in and
pull up a little piece of code and see
what this looks like um in
Python uh and in this demo we will use Q
learning to find the shortest path
between two given points if getting your
learning started is half the battle what
if you could do that for free visit
scaleup by simply learn click on the
link in the description to know more if
you've seen my videos before um I like
to do it in the uh Anaconda Jupiter
notebook um setup just because it's
really easy to see and it's a nice demo
uh and so here's my anaconda this one
I'm actually using uh python 36
environment that I set up in here and
we'll go ahead and launch the Jupiter
Notebook on this and once we're in our
Jupiter notebook uh which has the kernel
loaded with Python 3 we'll go ahead and
create a new Python 3 uh folder in
here and we'll call this uh
Q
learning and to start this
demo let's go ahead and import our numpy
array we'll just run that so it's
imported and like a lot of these uh
model programs when you're building them
you spend a lot of time putting it all
together um and then you end up with
this really short answer at the
end uh and we'll we'll take a look at
that as we come into it so we we go
ahead and start with our location to
State uh so we have um L1 L2 these are
our nine locations one to nine and then
of course the state is going to be 0 1 2
3 4 it's just a mapping of our location
to a integer on there and then we have
our actions our actions are simply uh
moving
from uh one
location to another so I can go to I can
go to location zero I can go to location
1 2 3 4 5 6 78 uh so these are my
actions I can choose these are the
locations of our
state and if you remember earlier I
mentioned uh um that the limitation is
that you you don't want to put in
um a continually growing table because
you can actually create a dynamic Q
table where you continually add in new
values as they
arise because um if you have float
values this just becomes infinite and
then your memory on your computer's gone
or you know does it's not going to work
at the same time you might think well
that kind of really limits the the Q uh
T learning setup but there are ways to
use it in conjunction with other systems
and so you might look
at uh well I do um been doing some work
in stock um and one of the questions
that comes out is to buy or sell the
stock and the stake coming in might be
um you might take it in create what
called
buckets um where anything that you
predict is going to return more than a
certain amount of money um the error for
that stock that you've had in the past
you put those in buckets and suddenly as
you start putting the creating these
buckets you realize you do have a
limited amount of information coming in
you no longer have a float number you
now have um bucket one two three and
four and then you can take those buckets
put them through a q learning table and
come up with the best action which stock
should I buy it's like gambling stock is
pretty much gambling if you're doing day
trading you're not doing long-term um
Investments and so you can start looking
at it like that a lot of the um current
feeds say that the best algorithms used
for day Traders where you're doing it on
your own is really to ask the question
do I want to trade the stock yes or no
and now you have it in a q learning
table and now you can take it to that
next level and you can see where that
can be a really powerful tool at the end
of doing a basic linear regression model
or something um what is the best
investment and you start getting the
best reward on
there uh and so if we're going to have
rewards these Rewards we just create um
it says uh if basically if you're um
this should match our Q table because
it's going to be uh you have your state
and you have your action across the top
if you remember from the dog and so we
have whatever state we're in going down
and then the next action and what the
reward is for it um and of course if you
were actually doing a um something more
connected your reward would be based on
U the actual environment it's in and
then we want to go ahead head and create
a state to location uh so we can map the
indexes so just like we defined our
rewards uh we're going to go and do
state to location um and you can see
here it's a a dictionary setup for
location State and location to state
with
items and we also need to
um Define what we want for learning
rates uh you remember we had our two
different rates uh
as far as like learning from the past
and learning from the current so we'll
go ahead and set those to0 75 and the
alpha set to 09 and we'll see that when
we do the formula and of course any of
this code uh send a note to our simply
learn team they'll get you a copy of
this code on here let's go ahead and
pull there we
go the new next two
sections um since we're going to keep it
short and
sweet here we go so let's go ahead and
create our agent um so our agent is
going to have our initialization where
we sended all the information uh we'll
Define our self gamma equals gamma we
could have just set the gamma rate down
here instead of uh submitting it it's
kind of nice to keep them separate
because you can play with these numbers
uh our self Alpha um then we have our
location State we'll set that in here um
we have our choice of actions um we're
going to go ahead and just embed the
rewards right into the agent so
obviously this would be coming from
somewhere else uh instead of from uh
self-generated and then a self state to
location equals our state to location uh
dictionary and we go ahead and create a
q learning table and I went ahead and
just set the Q learning table up to um
uh 0 to 0o what what what the setup is
uh location to State how many of them
are there uh this just czy an array of
zero to zero setup on
there and then the big part is the
training we have our rewards new equals
a copy of self.
rewards ending State equals the self
location state in location so this is
whatever we end up at rewards new equals
ending State plus ending State equals
999 just kind of goes to a dead end and
we start going through iterations
and we'll go ahead um let's do this uh
so this we're going to come back and
we're going to call call it on here uh
let me just erase that switch it to an
arrow there we go uh so what we're doing
is we're going to send in here to train
it we're going to say hey um I want to
iterate through this a thousand times
and see what happens now this part would
actually be instead of iterating you
might have your external environment and
they're going back back and forth and
you iterate through outside of here uh
but just for ease of use our agent's
going to come in here and iterate
through this sometimes I'll put this
iteration in here and I'll have it call
the environment and say hey this is what
I did what's the next state and the
environment does this thing right in
here as I iterate through
it uh and then we want to go ahead and
pick a random state to start with that's
what's going on here you have to start
somewhere um and then you have your
playable actions we're going to start
with just an empty thing for playable
actions and we'll fill that up so that's
what choices I have and so we're going
to iterate through the rewards Matrix to
get the states uh directly reachable
from the randomly chosen current state
assign those states to a list named
playable
actions and so you can see here we have
uh range nine I usually use length of
whatever I'm looking at uh which is our
locations or States as they are uh we
have a reward so we want to look at the
current the rewards uh the new
reward is our uh is in our chart here of
rewards uncore new uh current state um
plus J uh J being what is the next date
we want to try and so we go and do our
playable actions and we append
J and so we're doing is we're randomly
trying different things in here to see
what's going to generate a better reward
and then of course we go ahead and
choose our next State uh so we have our
random Choice playable actions and if
you remember I mentioned on this let me
just go ahead and uh oops let's do a
free form when we were talking about the
next State uh this right here just does
a random selection instead of a random
uh selection you might do something
where uh whatever the best selection is
which might be option three here and
then so you can see that it might use a
bell curve and then option two over here
might have a bell curve like this oops
and we start looking at these averages
and these spreads um or we can just add
them all together and pick the one that
kind of goes in all of those uh so those
are some of the options we have in here
we just go with a random Choice uh
that's usually where you start play with
it um and then we have our reward
section down here and so we want to go
ahead and find well in this case the
temporal difference uh so you have your
rewards new plus the self gamma and this
is the formula we were looking at this
is bellman's equation here uh so we have
our current value our learning rate our
discount rate involved in there the
reward system coming in for that um and
we can add it all together this is of
course our U maximum expected future
setup in here uh so this is all of our
our bellman's equation that we're
looking at here and then we come up in
here and we update our Q table that's
all this is on this one that's right
here we have um self Q current state
next state and we add in our um Alpha
because we don't want to we don't want
to train all of it at once in case
there's slight differ inchas coming in
there we want to slowly approach the
answer uh and then we have our route
equals a start
location and next location equals start
location so we're just incrementing we
took a step forward
and then finally remember I was telling
you how uh we're going to do all this
and just have some simple thing at the
end or just generates a simple path
we're going to go ahead and and get the
optimal route we want to find the best
route in here and so we've created a
definition for the optimal route down
here just scroll down for that and we
get the optimal route we go ahead and
put the information and including the Q
table self uh start location in location
next location route Q
and it says while next location is not
equal to in location so while we can
still go our start location equals self
location to State start location so we
already have our best value for the
start
location uh the next state looks at the
Q table and says hey what's uh the next
one with the best value and then the
next location we go ahead and pull that
in and we just depend it that's what's
going on down
here and then our start location equals
the next location and we just go through
all the steps and we'll go ahead and run
this and now that we have our Q table
our um Q agent loaded we're going to go
ahead and uh take our Q agent load them
up with our Alpha Gamma that we set up
above um along with the location step
action reward state to
location and uh our goal is to plot a
course between L9 and
L1 and we're going to go through a 100 a
thousand iterations on here here and so
when I run that it runs pretty quick uh
why is this so fast um if you've been
running neural networks and you've been
doing all these other models you sit
here and wait a long time well we're a
very small amount of data these are all
integers these aren't float values
there's not a the math is not heavy on
the on the processing end and this is
where Q tables are so powerful if you
have a small amount of information
coming in you very quickly uh get an
answer off of this even though we went
through it a thousand times
to train it and you'll see here we have
l985 2 and one and that's based on our
reward table we had set up on there and
this is the shortest path going between
these different uh setups in here and if
you remember on our reward table uh you
can see that if you start here you can
go to here there's places you can't go
that's how this reward table was set up
so I can only go to certain
places uh so kind of a little maze setup
in there there and you can play with it
this is really fun uh setup to play with
uh and you can see how you can take this
whole code and you can like I was saying
earlier you can embed it into another
setup in model and predictions where you
put things into buckets and you're
trying to guess the best investment the
best course of action long as you can
take that course of action and and uh uh
reduce it down to a yes no um or if
you're using text you can use a one hot
encoder which word is next there's all
kinds of things you can do with a Q
table uh depending on just how much
information you're putting in there so
that wraps up our demo in this demo
we've uh found the shortest distance
between two paths based on whatever
rules or state rewards we have to get
from point A to pointb and what
available actions there are hello and
welcome to this tutorial on deep
learning my name is Mohan and in the
next about 1 one and a half hours I will
take you through what is deep learning
and into tensorflow environment to show
you an example of deep learning now
there are several applications of deep
learning really very interesting and
Innovative applications and one of them
is identifying the geographic location
based on a picture and how does this
work the way it works is pretty much we
train an artificial neural network with
millions of images which are tagged
their geolocation is tagged and then
when we feed a new picture it will be
able to identify the geolocation of this
new image for example you have all these
images especially with maybe some
significant monuments or or U
significant locations and you train with
millions of such images and then when
you feed another image it need not be
exactly one of those that you have
claimed it can be completely different
that is the whole idea of training it
will be able to recognize for example
that this is a picture from Paris
because it is able to recognize the iil
so the way it works internally if we
have to look a little bit under the H is
these images are nothing but this is
digital information in the form of
pixels so each image could be a certain
size it can be 256 x 256 pixel kind of a
resolution and then each pixel is either
having a certain grade of color and all
that is fed into the neural network and
it then gets trained in and it's able to
based on these pixels pixel information
it is able to get trained and able to
recognize the features and extract the
features and thereby it is able to
identify these images and the location
of these images and then when you feed a
new image it kind of based on the
training it will be able to figure out
where this image is from so that's way a
little bit under the hood how it works
so what are we going to do in this
tutorial we will see what is deep
learning and what do we need for deep
learning and one of the main components
of deep learning is neural network so we
will see what is neural network what is
a perceptron and how to implement logic
gates like and or nor and so on using
perceptrons the different types of
neural networks and then applications of
deep learning and we will also see how
neural
networks works so how do we do the
training of neural networks and at the
end we will end up with a small demo
code which will take you through intens
of flow now in order to implement deep
learning code there are multiple
libraries or development environments
that are available and tensorflow is one
of them so the focus at the end of this
would be on how to use tensor flow to
write a piece of code using python as a
programming language and we will take up
a an example which is a very common one
which is like the hollow world of deep
learning the handwriting number
recognition which is a mnist commonly
known as mnist database so we will take
a look at amness database and how we can
train a neural network to recognize
handwritten numbers so that's what you
will see in this particular video so
let's get started what is deep learning
deep learning is like a subset of what
is known as a highlevel concept called
artificial intelligence you must be
already familiar must have heard about
this term artificial intelligence so
artificial intelligence is like the high
level concept if you will and in order
to implement artificial intelligence
applications we use what is known as
machine learning and within machine
learning a subset of machine learning is
deep learning machine learning is a
little bit more generic concept and deep
learning is one type of machine learning
if you will and we will see a little
later in maybe the following slides a
little bit more in detail how deep
learning is different from traditional
machine learning but to start with we
can mention here that deep learning uses
one of the differentiators between deep
learning and traditional machine
learning is that deep learning uses
neural networks and we will talk about
what are neural networks and how we can
Implement neural networks and so on and
so forth as a part of this tutorial so a
little deeper into deep learning deep
learning primarily involves working with
complicated unstructured data compared
to traditional machine learning with
where we normally use structured data in
deep learning the data would be
primarily images or Voice or maybe text
file so and it is large amount of data
as well and deep learning can handle
complex operations it involves complex
operations and the other difference
between traditional machine learning and
deep learning is that the feature
extraction happens pretty much
automatically in traditional machine
learning feature engineering is done
manually the data scientists we data
scientists have to do feature
engineering feature extraction but in
deep learning that happens automatically
and of course deep learning for large
amounts of data complicated unstructured
data deep learning gives very good
performance now as I mentioned one of
the secret sources of deep learning is
neural networks let's see what neural
networks is neural networks is based on
our biological neurons the whole concept
of deep learning and artificial
intelligence is based on human brain and
human brain consists of billions of tiny
stuff called neurons and this is how a
biological
neuron looks and this is how an
artificial neuron looks so neural
networks is like a simulation of our
human brain human brain has billions of
biological neurons and we are trying to
simulate the human brain using
artificial neurons this is how a
biological neuron looks it has dendrites
and the corresponding component with an
artificial neural network is or an
artificial neuron are the inputs they
receive the inputs through dendrites and
then there is the cell nucleus which is
basically the processing unit in a way
so in artificial neuron also there is a
piece which is an equivalent of this
cell nucleus and based on the weights
and biases we will see what exactly
weights and biases are as we move the
input gets processed and that results in
an output in a biological neuron the
output is sent through a synapse and in
an artificial neuron there is an
equivalent of that in the form of an
output and biological neurons are also
interconnected so there are billions of
neurons which are interconnected in the
same way artificial neurons are also
interconnected so this output of this
neuron will be fed as an input to
another neuron and so on now in neural
network one of the very basic units is a
perceptron so what is a perceptron a
Peron can be considered as one of the
fundamental units of neural networks it
can consist at least one neuron but
sometimes it can be more than one neuron
but you can create a perceptron with a
single neuron and it can be used to
perform certain functions it can be used
as a basic binary classifier it can be
trained to do some basic binary
classification and this is how a basic
perceptron looks like and this is
nothing but a neuron you have inputs X1
X2 X to xn and there is a summation
function and then there is what is known
as an activation function and based on
this input what is known as the weighted
sum the activation function either gets
gives an output like a zero or a one so
we say the neuron is either activated or
not so that's the way it works so you
get the inputs these inputs are each of
the inputs are multiplied by a weight
and there is a bias that gets added and
that whole thing is fed to an activation
function and then that results in an
output and if the output is correct it
is accepted if it is wrong if there is
an error then that error is fed back and
the neuron then adjust the weights and
biases to give a new output and so on
and so forth so that's what is known as
the training process of a neuron or a
neural network there's a concept called
perceptron learning so perceptron
learning is again one of the very basic
learning processes the way it works is
somewhat like this so you have all these
inputs like X1 to xn and each of these
inputs is multiplied by a weight and
then that sum this is the formula of the
equation so that sum W
ixi Sigma of that which is the sum of
all these product of X and W is added up
and then a bias is added to that the
bias is not dependent on the input but
or the input values but the bias is
common for one neuron however the bias
value keeps changing during the training
process once the training is completed
the values of these weights W1 W2 and so
on and the value of the bias gets fixed
so that is basically the whole training
process and that is what is known as the
perceptron training so the weights and
biases keep changing till you get the
accurate output and the summation is of
course passed through the activation
function as you see here this wixi
summation plus b is passed through
activation function and then the neuron
gets either fired or not and based on
that there will be an output that output
is compared with the actual or expected
value which is also known as labeled
information so this is the process of
super wise learning so the output is
already known and um that is compared
and thereby we know if there is an error
or not and if there is an error the
error is fed back and the weights and
biases are updated accordingly till the
error is reduced to the minimum so this
iterative process is known as perceptron
learning or perceptron learning Rule and
this error needs to be minimized so till
the error is minimized this iteratively
the weights and biases keep changing and
that is what is the training process so
the whole idea is to update the weights
and the bias of the perceptron till the
error is minimized the error need not be
zero error may not ever reach zero but
the idea is to keep changing these
weights and bias so that the error is
minimum the minimum possible that it can
have so this whole process is an
iterative process and this is the
iteration continues till either the
error is zero which is uh unlikely
situation or it is the minimum possible
Within These given conditions now in
1943 two scientists Warren mik and
Walter Pitts came up with an experiment
where they were able to implement the
logical functions like and or and nor
using NE
and that was a significant breakthrough
in a sense so they were able to come up
with the most common logical Gates they
were able to implement some of the most
common logical Gates which could take
two inputs Like A and B and then give a
corresponding result so for example in
case of an and gate A and B and then the
output is a in case of an orgate it is a
plus b and so on and so forth and they
were able to do this using a single
layer perceptron now most of these gats
it was possible to use single layer
perceptron except for XR and we will see
why that is in a little bit so this is
how an endgate works the inputs A and B
the output should be fired or the neuron
should be fired only when both the
inputs are one so if you have 0 0 the
output should be zero for 0 1 it is
again 0 1 0 again 0 and 1 1 one the
output should be one so how do we
implement this with the neuron so it was
found that by changing the values of
Weights it is possible to achieve this
logic so for example if we have equal
weights like 7 7 and then if we take the
sum of the weighted product so for
example 7 into 0 and then 7 into 0 will
give you zero and so on and so forth and
in the last case when both the inputs
are one you get a value which is greater
than one which is the threshold so only
in this case the neuron gets activated
and the output is there is an output in
all the other cases there is no output
because the threshold value is one so
this is implementation of an and gate
using a single perceptron or a single
neuron similarly an orgate in order to
implement an orgate in case of an orgate
the output will be one if either of
these inputs is one so for example 01
will result in one or other in all the
cases it is one except for 0 0 so how do
we implement this using a perceptron
once again if you have a perceptron with
weights for example 1.2 now if you see
here if in the first case when both are
zero the output is zero in the second
case when it is 0 and 1 1.2 into 0 is 0
and then 1.2 into 1 is 1 and in the
second case similarly the output is 1.2
in this last case when both the inputs
are one the output is 2.4 so during the
training process these weights will keep
changing and then at one point where the
weights are equal to W1 is equal to 1.2
and W2 is equal to 1.2 the system learns
that it gives the correct output so that
is implementation of orgate using a
single neuron or a single layer
perceptron now exor gate this was one of
the challenging ones they try to
implement an XR gate with a single level
perceptron but it was not possible and
therefore in order to implement an XR so
this was like a a roadblock in the
progress of U neural network however
subsequently they realize that this can
be implemented an XR gate can be
implemented using a multi-level
perceptron or MLP so in this case there
are two layers instead of a single layer
and this is how you can Implement an XR
gate so you will see that X1 and X2 are
the input puts and there is a hidden
layer and that's why it is denoted as H3
and H4 and then you take the output of
that and feed it to the output at 05 and
provide a threshold here so we will see
here that this is the numerical
calculation so the weights are in this
case for X1 it is 20 and minus 20 and
once again 20 and -20 so these inputs
are fed into H3 and H4 so you'll see
here for H3 the input is 011 1 and for
H4 it is 1 1 1 and if you now look at
the output final output where the
threshold is taken as one if you use a
sigmoid with the threshold one you will
see that in these two cases it is zero
and in the last two cases it is one so
this is a implementation of XR in case
of XR only when one of the inputs is one
you will get an output so that is what
we are seeing here if we have either
both the inputs are one or both the
inputs are zero then the output should
be zero so that is what is an exclusive
or gate so it is exclusive because only
one of the input should be one and then
only you'll get an output of one which
is Satisfied by this condition so this
is a special implementation XR gate is a
special implementation of perceptron now
that we got a good idea about perceptron
let's take a look at what is the neural
network so we have seen what is a
perceptron we have seen what is a neuron
so we will see what exactly is a neural
network so neural network is nothing but
a network of these neurons and there are
different types of neural networks there
are about five of them these are
artificial neural network convolutional
neural network then recursive neural
network or recurrent neural network deep
neural network and deep belief Network
so and each of these types of neural
networks have a special so you know they
can solve special kind of problems for
example convolutional neural networks
are very good at performing image
processing and image recognition and so
on whereas RNN are very good for speech
recognition and also text analysis and
so on so each type has some special
characteristics and they can they're
good at performing certain special kind
of tasks what are some of the
applications of deep learning deep
learning is today used extensively in
gaming you must have heard about alphao
which is a game created by a startup
called Deep Mind which got acquired by
Google and alphao is an AI which
defeated the human world champion lead
all in this game of Go so gaming is an
area where deep learning is being
extensively used and a lot of research
happens in the area of gaming as as well
in addition to that nowadays there are
neural networks or special type called
generative adversarial networks which
can be used for synthesizing either
images or music or text and so on and
they can be used to compose music so the
neural network can be trained to compose
a certain kind of music and autonomous
cars you must be familiar with Google
Google's self-driving car and today a
lot of Automotive comp companies are
investing in this space and uh deep
learning is a core component of this
autonomous Cars the cars are trained to
recognize for example the road the the
lane markings on the road signals any
objects that are in front any
obstruction and so on and so forth so
all this involves deep learning so
that's another major application and uh
robots we have seen several robots
including Sofia may be familiar with
Sophia who was given a citizenship by
Saudi Arabia and there are several such
robots which are very humanlike and the
underlying technology in many of these
robots is deep learning medical
Diagnostics and Health Care is another
major area where deep learning is being
used and within Healthcare Diagnostics
again there are multiple areas where
deep learning and image recognition
image processing can be used for example
for cancer detection as you may be aware
if cancer is detected early on it can be
cured and one of the challenges is in
the availability of Specialists who can
diagnose cancer using these diagnostic
images and various scans and and so on
and so forth so the idea is to train
neural network to perform some of these
activities so that the load on the
cancer specialist doctors or oncologist
comes down and there is a lot of
research happening here and there are
already quite a few applications that
are claimed to be performing better than
human beings in this space can be lung
cancer it can be breast cancer and so on
and so forth so Healthcare is a major
area where deep learning is being
applied let's take a look at the inner
working of a neural network so how does
an artificial neural network let's say
identify can we train a neural network
to identify the shapes like squares and
circles and triangles when these images
are fed so this is how it works any
image is nothing but it is a digital
information of the pixels so in this
particular case let's say this is an
image of 28x 28 pixel and this is an
image of a square there's a certain way
in which the pixels are lit up and so
these pixels have a certain value Maybe
maybe from 0 to 256 and 0 indicates that
it is black or it is dark and 256
indicates it is completely it is white
or lit up so that is like an indication
or a measure of the how the pixels are
lit up and so this is an image is let's
say consisting of information of 784
pixels so all the information what is
inside this image can be kind of comp
pressed into the 784 pixels the way each
of these pixels is lit up provides
information about what exactly is the
image so we can train neural networks to
use that information and identify the
images so let's take a look how this
works so each neuron the value if it is
close to one that means it is white
whereas if it is close to zero that
means it is black now this is a an
animation of how this whole thing works
so these pixels one of the ways of doing
it is we can flatten this image and take
this complete 784 pixels and feed that
as input to our neural network the
neural network can consist of probably
several layers there can be a few hidden
layers and then there is an input layer
and an output layer now the input layer
take the 78 84 pixels as input the
values of each of these pixels and then
you get an output which can be of three
types or three classes one can be a
square a circle or a triangle now during
the training process there will be
initially obviously you feed this image
and it will probably say it's a circle
or it will say it's a triangle so as a
part of the training process we then
send that error back and the weights and
the biases of these neurons are adjusted
till it correctly identifies that this
is a square that is the whole training
mechanism that happens out
here now let's take a look at a circle
same way so you feed these 784 pixels
there is a certain pattern in which the
pixels are lit up and the neural network
is trained to identify that pattern and
during the training process once again
it would probably initially identify it
incorrectly saying this is a square or a
triangle and then that error is fed back
and the weights and biases are adjusted
finally till it finally gets the image
correct so that is the training process
so now we will take a look at same way a
triangle so now if you feed another
image which is consisting of triangles
so this is the training process now we
have trained our neural network to
classify these images into a triangle or
a circle and a square so now this neural
network can identify these three types
of objects now if you feed another image
and it will be able to identify whether
it's a square or a triangle or a circle
now what is important to be observed is
that when you feed a new image it is not
necessary that the image or the the
triangle is exactly in this position now
the neural network actually identifies
the patterns so even if the triangle is
let's say positioned here not exactly in
the middle but maybe at the corner or in
the side it would still identify that it
is a triangle and that is the whole idea
behind pattern recognition so how does
this straining process work this is a
quick view of how the training process
works so we have seen that a neuron
consists of inputs it receives inputs
and then there is a weighted sum which
is nothing but this XI wi summation of
that plus the bias and this is then fed
to the activation function and that in
turn gives us a output now during the
training process initially obviously
when you feed these images when you send
maybe a square it will identify it as a
triangle and when you maybe feed a
triangle it will identify as a square
and so on so that error information is
fed back and initially these weights can
be random maybe all of them have zero
values and then it will slowly keep
changing so the as a part of the
training process the values of these
weights W1 W2 up to WN keep changing in
such a way that towards the end of the
training process it should be able to
identify these images correctly so till
then the weights are adjusted and that
is known as the training process so and
these weights are numeric values could
be
0.525 35 and so on it could be positive
or it could be negative and the value
that is coming here is the pixel value
as we have seen it can be anything
between 0 to 1 you can scale it between
0 to 1 or 0 to 256 whichever way 0 being
black and 256 being white and then all
the other colors in between so that is
the input so these are numerical values
this multiplication or the product w ixi
is a numerical value and the bias is
also a numerical value we need to keep
in mind that the bias is fixed for a
neuron it doesn't change with the inputs
whereas the weights are one per input so
that is one important point to be noted
so but the bias also keeps changing
initially it will again have a random
value but as a part of the training
process the weights the values of the
weights W1 W2 WN and the value of B will
change and ultimately once the training
process is complete these values are
fixed for this particular neuron W1 W2
up to WN and plus the value of the B is
also fixed for this particular neuron
and in this way there will be multiple
neurons and each there may be multiple
levels of neurons here and that's the
way the training process works so this
is another example of multi-layer so
there are two hidden layers in between
and then you have the input layer values
coming from the input layer then it goes
through multiple layers hidden layers
and then there is an output layer and as
you can see there are weights and biases
for each of these neurons in each layer
and all of them gets keeps changing
during the training process and at the
end of the training process all these
weights have a certain value and that is
a trained model and those values will be
fixed once the training is completed all
right then there is something known as
activation function neural network
consists of one of the components in
neural networks is activation function
and every neuron has an activation
function and there are different types
of activation functions that are used it
could be a reu it could be sigmoid and
so on and so forth and the activation
function is what decides whether a
neuron should be fired or not so whether
the output should be zero or one is
decided by the activation function and
the activation function in turn takes
the input which is the weighted sum
remember we talked about wixi + B that
weighted sum is fed as a input to the
activation function and then the output
can be either a zero or a one and there
are different types of activation
functions which are covered in an
earlier video you might want to watch
all right so as a part of the training
process we feed the inputs the labeled
data or the training data and then it
gives an output which is the predicted
output by the network which we indicate
as y hat and then there is a labeled
data because we for supervised learning
we already know what should be the
output so that is the actual output and
in the initial process before the
training is complete obviously there
will be error so that is measured by
what is known as a cost function so the
difference between the predicted output
and the actual output is the error and U
the cost function can be defined in
different ways there are different types
of cost functions so in this case it is
like the average of the squares of the
error so and then all the errors are
added which can sometimes be called as
some of squares some of square errors or
ssse and that is then fed as a feedback
in what is known as backward propagation
or back propagation and that helps in
the network adjusting the weights and
biases and so the weights and biases get
updated till this value the error value
or the cost cost function is minimum now
there is a optimization technique which
is used here called gradient descent
optimization and this algorithm Works in
a way that the error which is the cost
function needs to be minimized so
there's a lot of mathematics that goes
behind this for example they find the uh
local Minima the global Minima using the
differentiation and so on and so forth
but the idea is this so as a training
process as a as a part of training the
whole idea is to bring down the error
which is like let's say this is the
function the cost function at certain
levels it is very high the cost value of
the cost function the output of the cost
function is very high so the weights
have to be adjusted in such a way and
also the bias of course that the cost
function is minimized so there is this
optimization technique called gradient
descent that is used and this is known
as the learning rate now gradient
descent you need to specify what should
be the learning rate and the learning
rate should be optimal because if you
have a very high learning rate then the
optimization will not converge because
at some point it will cross over to the
site on the other hand if you have very
low learning rate then it might take
forever to convert so you need to come
up with the optimum value of the
learning rate and once that is done
using the gradient descent optimization
the error function is reduced and that's
like the end of the training process all
right so this is another view of
gradient descent so this is how it looks
this is your cost function the output of
the cost function and that has to be
minimized using grent descent algorithm
and these are like the parameters and
weight could be one of them so initially
we start with certain random values so
cost will be high and then the weights
keep changing and in such a way that the
cost function needs to come down and at
some point it may reach the minimum
value and then it may increase so that
is where the gradi indescent algorithm
decides that okay it has reached the
minimum value and it will kind of try to
stay here this is known as the global
Minima now sometimes these curves may
not be just for EXP ation purpose this
has been drawn in a nice way but
sometimes these curves can be pretty
erratic there can be some local Minima
here and then there is a peak and then
and so on so the whole idea of gradient
descent optimization is to identify the
global Minima and to find the weights
and the bias at that particular point so
that's what is gradient descent and then
this is another example so you can have
these multiple local Minima so as you
can can see at this point when it is
coming down it may appear like this is a
minimum value but then it is not this is
actually the global minimum value and
the gradient desent algorithm will make
an effort to reach this level and not
get stuck at this point so the algorithm
is already there and it knows how to
identify this Global minimum and that's
what it does during the training process
now in order to implement deep learning
there are multiple platforms and
languages that are available but the
most common platform nowadays is tensor
flow and so that's the reason we have uh
this tutorial we created this tutorial
for tensorflow so we will take you
through a quick demo of how to write a
tensorflow code using Python and
tensorflow is uh an open source platform
created by Google so let's just take a
look at the details of tensor flow and
so this is a a library a python Library
so you can use python or any other
languages it's also supported in other
languages like Java and R and so on but
python is the most common language that
is used so it is a library for
developing deep learning applications
especially using neural networks and it
consists of primarily two parts if you
will so one is the tensors and then the
other is the graphs or the flow that's
the way the name that's the reason for
this kind of a name called tensor flow
so what are tensors tensors are like
multi-dimensional arrays if you will
that's one way of looking at it so
usually you have a one-dimensional array
so first of all you can have what is
known as a scalar which means a number
and then you have a one-dimensional
array something like this which means
this is like a set of numbers so that is
a one-dimension array then you can have
a two-dimensional array which is like a
matrix and beyond that sometimes it gets
difficult so this is a three-dimensional
array but tensor flow can handle many
more Dimensions so it can have
multi-dimensional arrays that is the
strength of tensor flow and which makes
computation deep learning computation
much faster and that's the reason why
tensor flow is used for developing deep
learning applications so tensor flow is
a deep learning tool and this is the way
it works so the data basically flows in
the form of tensors and the way the
programming works as well is that you
first create a graph of how to execute
it and then you actually execute that
particular graph in the form of what is
known as a session we will see this in
the tensor flow code as we move forward
so all the data is managed or
manipulated in tensors and then the
processing happens using this graphs
there are certain terms called like for
example ranks of a tensor the rank of a
tensor is like a dimensional
dimensionality in a way way so for
example if it is scalar so there is just
a number just one number the rank is
supposed to be zero and then it can be a
one-dimensional vector in which case the
rank is supposed to be one and then you
can have a
two-dimensional Vector typically like a
matrix then in that case we say the rank
is two and then if it is a
three-dimensional array then it rank is
three and so on so it can have more than
three as well so it is possible that you
can store multi-dimensional arrays in
the form of tensors so what are some of
the properties of tensorflow I think
today it is one of the most popular
platform tensorflow is the most popular
deep learning platform or Library it is
open source it's developed by Google
developed and maintained by Google but
it is open source one of the most
important things about tensorflow is
that it can run on CPUs as well as G
gpus GPU is a graphical Processing Unit
just like CPU is central processing unit
now in earlier days GPU was used for
primarily for graphics and that's how
the name has come and one of the reasons
is that it cannot perform generic
activities very efficiently like CPU but
it can perform iterative actions or
computations extremely fast and much
faster than a CPU so they are really
good for computational activities and in
deep learning there is a lot of
iterative computation that happens so in
the form of matrix multiplication and so
on so gpus are very well suited for this
kind of computation and tensorflow
supports both GPU as well as CPU and
there's a certain way of writing code in
tensorflow we will see as we go into the
code and of course tensorflow can be
used for traditional machine learning as
well but then that would be an Overkill
but just for understanding it may be a
good idea to start writing code for a
normal machine learning use case so that
you get a hang of how tensorflow code
works and then you can move into neural
networks so that is um just a suggestion
but if you're already familiar with how
tensorflow works then probably yeah you
can go straight into the neural networks
part so in this tutorial we will take
the use case of recognizing handwritten
digits this is like a hollow world of
deep learning and this is a nice little
Ms database is a nice little database
that has images of handwritten digits
nicely formatted because very often in
deep learning and neural networks we end
up spending a lot of time in preparing
the data for training and with MES
database we can avoid that you already
have the data in the right format which
can be directly used for for training
and mnist also offers a bunch of
built-in utility functions that we can
straightway use and call those functions
without worrying about writing our own
functions and that's one of the reasons
why amness database is very popular for
training purposes initially when people
want to learn about deep learning and
tensor flow this is the database that is
used and it has a collection of 70,000
handwritten digits and a large part of
them are for training then you have test
just like in any machine learning
process and then you have validation and
all of them are labeled so you have the
images and they're label and these
images they look somewhat like this so
they are handwritten images collected
from a lot of individuals people have
these are samples written by human
beings they have handwritten these
numbers these numbers going from 0 to 9
so people have written these numbers and
then the images of those have been taken
and formatted in such a way that it is
very easy to handle so that is amness
database and the way we are going to
implement this in our tensor flow is we
will feed this data especially the
training data along with the label
information and uh the data is basically
these images are stored in the form of
the pixel information as we have seen in
one of the the previous slides all the
images are nothing but these are excels
so an image is nothing but an
arrangement of pixels and the value of
the pixel either it is lit up or it is
not or in somewhere in between that's
how the images are stored and that is
how they are fed into the neural network
and for training once the network is
trained when you provide a new image it
will be able to identify within a
certain error of course and for this we
will use one of the simpler neural
network configurations called softmax
and for Simplicity what we will do is we
will flatten these pixels so instead of
taking them in a two-dimensional
arrangement we just flatten them out so
for example it starts from here it is a
28x 28 so there are
7484 pixels so pixel number one starts
here it goes all the way up to 28 then
29 starts here and goes up to 56 and so
on and the pixel number 784 is here so
we take all these pixels flatten them
out and feed them like one single line
into our neural network and this is a
what is known as a softmax layer what it
does is once it is trained it will be
able to identify what digit this is so
there are in this output layer there are
10 neurons each signifying a digit and
at any given point of time when you feed
an image only one of these 10 neurons
gets activated so for example if this is
strained properly and if you feed a
number nine like this then this
particular neuron gets activated so you
get an output from this neuron let me
just use uh a pen or a laser to show you
here okay so your feeding a number nine
let's say this has been trained and now
if you're feeding a number nine this
will get activated now let's say you
feed one to the trained Network then
this neuron will get activated if you
feed two this neuron will get activated
and so on I hope you get the idea so
this is one type of a neural network or
an activation function known as soft Max
layer so that's what we will be using
here this is one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and uh
we will show you there directly but very
quickly this is how the code looks and
uh let me run you through briefly here
and then we will go into the Jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using python here
and that's why the syntax of the
language is Python and and the first
step is to import the tensorflow library
so and we do this by using this line of
code saying import tensor flow as TF TF
is just for convenience so you can name
give any name and once you do this TF is
tensor flow is available as an object in
the name of TF and then you can run its
uh methods and accesses its attributes
and so on and so forth and mess database
is actually an integral part of
tensorflow and that's again another
reason why we as a first step we always
use this example Mist database example
so you just simply import mest database
as well using this line of code and you
slightly modify this so that the labels
are in this format what is known as one
hot true which means that the label
information is stored like an array and
uh let me just uh use the pen to show
what exactly it is so when you do the
one hot true what happens is each label
is stored in the form of an array of 10
digits and let's say the number is uh 8
okay so in this case all the remaining
values there will be a bunch of zeros so
this is like array at position zero this
is at position one position two and so
on and so forth let's say this is
position seven then this is position 8
that will be one because our input is
eight and again position 9 will be zero
okay so one hot encoding this one hot
encoding true will kind of load the data
in such a way that the labels are in
such a way that only one of the digits
has a value of one and that indicates So
based on which digit is one we know what
is the label so in this case the eighth
position is one therefore we know this
sample data the value is eight similarly
if you have a two here let's say then
the labeled information will be somewhat
like this so you have your labels so you
have this as zero the zeroth position
the first position is also zero the
second position is one because this
indicates number two and then you have
third as zero and so on okay so that is
the significance of this one hot true
all right and then we can check how the
data is uh looking by displaying the the
data and as I mentioned earlier this is
pretty much in the form of digital form
like numbers so all these are like pixel
values so you will not really see an
image in this format but there is a way
to visualize that image I will show you
in a bit and uh this tells you how many
images are there in each set so the
training there are 55,000 images in
training and in the test set there are
10,000 and then validation there are
5,000 so altogether there are 70,000
images all right so let's uh move on and
we can view the actual image by uh using
the matplot clip library and this is how
you can view this is the code for
viewing the images and you can view them
in color or you can view them in Gray
scale so the cmap is what tells in what
way we want to view it and what are the
maximum values and the minimum values of
the pixel values so these are the Max
and minimum values so of the pixel
values so maximum is one because this is
a scaled value so one means it is uh
White and zero means it is black and in
between is it can be anywhere in between
black and white and the way to train the
model there is a certain way in which
you write your tsor flow code and um the
first step is to create some
placeholders and then you create a model
in this case we will use the softmax
model one of the simplest ones and um
placeholders are primarily to get the
data from outside into the neural
network so this is a very common
mechanism that is used and uh then of
course you will have variables which are
your remember these are your weights and
biases so for in our case there are 10
neurons and uh each neuron actually has
784 four because each neuron takes all
the inputs if we go back to our slide
here actually every neuron takes all the
784 inputs right this is the first
neuron it has it receives all the 784
this is the second neuron this also
receives all the 78 so each of these
inputs needs to be multiplied with a
weight and that's what we are talking
about here so these are this is a a
matrix of
784 values for each of the neurons and
uh so it is like a 10x 784 Matrix
because there are 10 neurons and uh
similarly there are biases now remember
I mentioned biases only one per neuron
so it is not one per input unlike the
weights so therefore there are only 10
biases because there are only 10 neurons
in this case so that is what we are
creating a variable for biases so this
is something little new intensive flow
you will see unlike our regular
programming languages where everything
is a variable here the variables can be
of three different types you have
placeholders which are primarily used
for feeding data you have variables
which can change during the course of
computation and then a third type which
is not shown here are constants so these
are like fixed numbers all right so in a
regular programming language you may
have everything as variables or at the
most variables and con constants but in
tens of flow you have three different
types placeholders variables and
constants and then you create what is
known as a graph so tensorflow
programming consists of graphs and
tensors as I mentioned earlier so this
can be considered ultimately as a tensor
and then the graph tells how to execute
the whole implementation so that the
execution is stored in the form of a
graph and in this case what we are doing
is we are doing a multiplication TF you
remember remember this TF was created as
a tensor flow object here one more level
one more so TF is available here now
tensor flow has what is known as a
matrix multiplication or matal function
so that is what is being used here in
this case so we are using the matrix
multiplication of tensor flow so that
you multiply your input values x with W
right this is what we were doing x w + B
you're just adding B and this is in very
similar to one of the earlier slides
where we saw Sigma XI wi so that's what
we are doing here matrix multiplication
is multiplying all the input values with
the corresponding weights and then
adding the bias so that is the graph we
created and then we need to Define what
is our loss function and what is our
Optimizer so in this case we again use
the tensor flows apis so tf. nn soft Max
cross entropy with logits is the uh API
that we will use and reduce mean is what
is like the mechanism whereby which says
that you reduce the error and Optimizer
for doing deduction of the error what
Optimizer are we using so we are using
gradient descent Optimizer we discussed
about this in couple of slides uh
earlier and for that you need to specify
the learning rate you remember we saw
that there was a a slide somewhat like
this and then you define what should be
the learning rate how fast you need to
come down that is the learning rate and
this again needs to be tested and tried
and to find out the optimum level of
this learning rate it shouldn't be very
high in which case it will not converge
or shouldn't be very low because it will
in that case it will take very long so
you define the optimizer and then you
call the method minimize for the that
Optimizer and that will Kickstart the
training process and so far we've been
creating the graph and in order to
actually execute that graph we create
what is known as a session and then we
run that session and once the training
is completed we specify how many times
how many iterations we want it to run so
for example in this case we are saying
Thousand Steps so that is a exit
strategy in a way so you specify the
exit condition so a training will run
for th000 iterations and once that is
done we can then evaluate the model
using some of the techniques shown here
so let us get into the code quickly and
see how it works so this is our Cloud
environment now you can install
tensorflow on your local machine as well
I'm showing this demo on our existing
Cloud but you can also install denslow
on your local machine and uh there is a
separate video on how to set up your
tensorflow environment you can watch
that if you want to install your local
environment or you can go for other any
cloud service like for example Google
Cloud Amazon or Cloud Labs any of these
you can use and um run and try the code
okay so it has got
started we will
login all right right so this is our
deep learning tutorial uh code and uh
this is our tensorflow
environment and uh so let's get started
the first we have seen a little bit of a
code walk through uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensorflow and then we will
import the data and we need to adjust
the data in such a way that the one hot
is encoding is set to True one hot
encoding right as I explained earlier so
in this case the label values will be
shown appropriately and if we just check
what is the type of the data so you can
see that this is a data sets python data
sets and if we check the number of
images the way it looks so this is how
it looks it is an array of type float 32
similarly the number if you want to see
what is the number
of training images there are 55,000 then
there are test images 10,000 and then
validation images 5,000 now let's take a
quick look at the data itself
visualization so we will use um matte
plot clip for this and um if we take a
look at the shape now shape gives us
like the dimension of the tensors or or
or the arrays if you will so in this
case is the training data set if we SE
the size of the training data set using
the method shape it says there are
55,000 and 55,000 by 784 so remember the
784 is nothing but the 28 by 28 28 into
28 so that is equal to 784 so that's
what it is uh showing now we can take
just uh one image and just see what is
the the first image and see what is the
shape so again size obviously it is only
784 similarly you can look at the image
itself the data of the first image
itself so this is how it it shows so
large part of it will probably be zeros
because as you can imagine in the image
only certain areas are written rest is U
black so that's why you will mostly see
zeros either it is black or white but
then there are these values are so the
values are actually they are scaled so
the values are between zero and one okay
so this is what you're seeing so certain
locations there are some values and then
other locations there are zeros so that
is how the data is stored and loaded if
we want to actually see what is the
value of the hand written image if you
want to view it this is how you view it
so you create like do this reshape and
um mat plot lib has this um feature to
show you these images so we will
actually use the function called um IM
show and then if you pass this
parameters appropriately you will be
able to see the different images now I
can change the values in this position
so which image we are looking at right
so we can say if I want to see what is
there in maybe
5,000 right
so 5,000 has three similarly you can
just say five what is in five five as
eight what is in
50 again eight so basically by the way
if you're wondering uh how I'm executing
this code shift enter in case you're not
familiar with jupyter notebooks shift
enter is how you execute each cell
individual cell and if you want to
execute the entire program you can go
here and say run all so that is
how this code gets executed and and um
here again we can check what is the
maximum value and what is the minimum
value of this pixel values as I
mentioned this is it is scaled so
therefore it is between the values lie
between 1 and zero now this is where we
create our
model the first thing is to create the
required placeholders and variables and
that's what we are doing here as we have
seen in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by 10
values okay so one for this 10 is for
each neuron there are 10 neurons and 784
is for the pixel values inputs that are
given which is 28 into 28 and the biases
as I mentioned one for each neuron so
there will be 10 biases they are stored
in a variable by the name b and this is
the graph which is basically the
multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute I think
this code is executed then we Define
what is our the Y value is basically the
label value so this is another
placeholder we had X as one placeholder
and Yore true as a second placeholder
and this will have values in the form of
uh 10 digigit 10 digit uh arrays and uh
since we said one hot encoded the
position which has a one value indicates
what is the label for that particular
number all right then we have cross
entropy which is nothing but the loss
loss function
and we have the optimizer we have chosen
gradient descent as our Optimizer then
the training process itself so the
training process is nothing but to
minimize the cross entropy which is
again nothing but the loss function so
we Define all of this in the form of a
graph so the up to here remember what we
have done is we have not exactly
executed any tensorflow code till now we
are just preparing the graph the
execution plan that's how the tensorflow
code works so the whole structure and
format of this code will be completely
different from how we normally do
programming so even with people with
programming experience may find this a
little difficult to understand it and it
needs quite a bit of practice so you may
want to view this uh video also maybe a
couple of times to understand this flow
because the way tensorflow programming
is done is slightly different from the
normal programming some of you who let's
say have done uh maybe spark programming
to some extent we'll be able to easily
understand this uh but even in spark the
the programming the code itself is
pretty straightforward behind the scenes
the execution happens slightly
differently but in tens oflow even the
code has to be written in a completely
different way so the code doesn't get
executed uh in the same way as you have
written so that that's something you
need to understand and a little bit of
practice is needed for this so so far
what we have done up to here is creating
the variables and feeding the variables
and um or rather not feeding but setting
up the variables and uh the graph that's
all defining maybe the uh what kind of a
network you want to use for example we
want to use softmax and so on so you
have created the variables have to load
the data loaded the data viewed the data
and prepared everything but you have not
yet executed anything in tens of flow
now the next step is the execution in
tens of flow so the first step for doing
any execution in tensor flow is to
initialize the variables so anytime you
have any variables defined in your code
you have to run this piece of code
always so you need to basically create
what is known as a a node for
initializing so this is a node you still
are not yet executing anything here you
just created a node for the
initialization so let us go ahead and
create that and here onwards is where
you will actually execute your code uh
in tensorflow and in order to execute
the code what you will need is a session
tensorflow session so tf. session will
give you a session and there are a
couple of different ways in which you
can do this but one of the most common
methods of doing this is with what is
known as a with Loop so you have a with
tf. session as SS and with a colon here
and this is like a block starting of the
block and these indentations tell how
far this block goes and this session is
valid till this block gets executed so
that is the purpose of creating this
width block this is known as a width
block so with tf. session as SS you say
cs. run in it now cs. run will execute a
node that is specified here so for
example here we are saying cs. run cess
is basically an instance of the session
right so here we are saying tf. session
so an instance of the session gets
created and we are calling that sess and
then we run a node within that one of
the nodes in the graph so one of the
nodes here is in it so we say run that
particular node and that is when the
initialization of the variables happens
now what this does is if you have any
variables in your code in our case we
have W is a variable and B is a variable
so any variables that we created you
have to run this code you have to run
the initialization of these variables
otherwise you will get an error okay so
that is the that's what this is doing
then we within this width block we
specify a for Loop and we are saying we
want the system to iterate for, steps
and perform the training that's what
this for Loop does run training for
thousand
iterations and what it is doing
basically is it it is fetching the data
or these images remember there are about
50,000 images but it cannot get all the
images in one shot because it will take
up a lot of memory and performance
issues will be there so this is a very
common way of Performing deep learning
training you always do in batches so we
have maybe 50,000 images but you always
do it in batches of 100 or maybe 500
depending on the size of your system and
so on and so forth so in this case we
are saying okay get me 100 uh images at
a time and get me only the training
images remember we use only the training
data for training purpose and then we
use test data for test purpose you must
be familiar with machine learning so you
must be aware of this but in case you
are not in machine learning also not
this is not specific to deep learning
but in machine learning in general you
have what is known as training data set
and test data set your available data
typically you will be splitting into two
parts and using the training data set
for training purpose and then to see how
well the model has been trained you use
the test data set to check or test the
validity or the accuracy of the model so
that's what we are doing here and You
observe here that we are actually
calling an mest function here so we are
saying mnist train. nextt batch right so
this is the advantage of using this
database because they have provided some
very nice helper functions which are
readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a a lengthy
exercise so we can avoid all that if we
are using amness database and that's why
we use this for the initial learning
phase okay so when we say fetch what it
will do is it will fetch the images into
X and the labels into y and then you use
this batch of 100 images and you run the
training so cs. run basically what we
are doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously the
initially it will be wrong so all that
feedback is given back to the neural
network and thereby all the W's and B's
get updated till it reaches thousand
iterations in this case the exit
criteria is th000 but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neuron and that is run for
thousand iterations and typically by the
end of this thousand iterations the
model would have learned to recognize
these handwritten images obviously it
will not be 100% accurate okay so once
that is
done after so this happens for,
iterations once that is done you then
test the accuracy of these models by
using the test data set right so this is
what we are trying to do here the code
may appear a little complicated because
if you're seeing this for the first time
you need to understand uh the various
methods of tensor flow and so on but it
is basically comparing the output with
what has been what is actually there
that's all it is doing so you have your
test data and uh you are trying to find
out what is the actual value and what is
the predicted value and seeing whether
they are equal or not TF do equal right
and how many of them are correct and so
on and so forth and based on that the
accuracy is calculated as well so this
is the accuracy and that is what we are
trying to see how accurate the model is
in predicting these uh numbers or these
digits okay so let us run this this
entire thing is in one cell so we will
have to just run it in one shot it may
take a little while let us see and uh
not bad so it has finished the th000
iterations and what we see here as an
output is the accuracy so we we see that
the accuracy of this model is around
91% okay now which is pretty good for
such a short exercise within such a
short time we got 90% accuracy however
in real life this is probably not
sufficient so there are other ways in to
increase the accuracy we will see
probably in some of the later tutorials
how to improve this accuracy how to
change maybe the hyper param
like number of neurons or number of
layers and so on and so forth and uh so
that this accuracy can be increased
Beyond 90% so guys let's start first
with a beginner level project and the
first project that we are going to
encounter that is home value prediction
so guys this project aims to develop a
preductive model to estimate the value
of Residential Properties the model will
analyze various features such as
location Squire footage number of
bedrooms and bathroom age of the
property and other relevant factors by
leveraging historical Property Data the
model will be able to provide accurate
home value predictions which can be
useful for real estate agents buyers and
sellers so guys the programming language
that we are going to use all over here
will be Python and machine learning
libraries that we will be using will be
psychic learn tensorflow kiras and for
data handling libraries we have pandas
numai and for visualization we have to
to use mat plot Li and cbon now what
will be the approach for this one guys
so guys the first one that we have a
data collection so here what is going to
happen guys so first you have to collect
the historical Property Data from the
sources like Zillow retailer.com you can
also get database from the public real
estate databases like kaggle data sets
where you have zero home value
prediction ensure that the data set
include features like location where you
have latitude longitude square footage
number of rooms build property typee and
previous sales The Next Step that comes
is data cleaning you have to handle the
missing values by using imputation
techniques or removing incomplete
records removing outliers that may skew
the models prediction normalize or
standardize the data to ensure
consistency the third one that we have
is feature engineering you have to
create new features such as proximity to
schools crime rates and access to the
public transportation encode categorial
variables example property type using
techniques like one hot encoding
generate interaction features that
capture relationship between existing
features the fourth one that we have is
model selection use regression models
like linear regression random Forest
gradient boosting neural networks
experiment with different models to
identify the best performing mod now in
the next phase all you have to do guys
is model training and evaluation split
the data set into training and test sets
train the model on the training set and
evaluate their performance on the
testing set using metrics like rsme
which means root means squared error you
can use cross validation to ensure the
model's robustness and avoid overfitting
the six month that we have all over is
hyperparameter tuning you can optimize
the model's hyper parameter using
techniques such as grid search or random
search to improve accuracy and if you're
looking forward to deploy your model
then you can develop a web interface
using flask or Jango to allow users to
input property features and get
predictions if you can deploy the model
on the cloud platform like AWS for
scalability now if we talk about the
complexity level of this we all know
that it is a beginner level project now
let us move on to the one more set that
is music genre classification and
generation so guys this is also one of
the most beginner level project this
project aims to develop a system that
can classify music tracks into different
genres and generate new music
composition within specified genre the
goal is to build a model that analyzes
audio features to categorize music and
uses deep learning techniques to create
new music this project introduces
Advanced concept of audio processing
deep learning and generative models so
guys what will be used in this so we'll
have programming language that will be
python for audio processing we'll be
using librosa for machine learning
libraries we'll be using tensorflow
kiras pytorch for data handling
libraries we'll be using pandas numi for
visualization we'll be using matte plot
cbon for the data set guys you can use
GD music gen data set or you can get it
from free music archive so guys in the
first phase we are going to have data
collection you can optate data sets
containing music tracks and their
corresponding genre from the sources
from gtzan music genre data set and the
free music archive ensure that a data
set includes diverse genre and
substantial number of tracks per genre
next we'll go for data pre-processing
use library librosa to load and
pre-process audio files including
feature extractions such as mil
frequency seal coefficients chroma
features and spectral contrast you can
normalize the extracted features to
ensure consistent input for the given
models now if you talk about feature
engineering guys you can extract
additional features from the audio files
such as Tempo beat zero Crossing rate
Etc create a feature Matrix that
represents the extracted audio features
then go for the model selection use
conventional neural network or recurrent
neural networks for the music genre
classification split the data set into
training and testing data set now if we
talk about model training and evaluation
guys then you can train the selected
classification model on the training set
evaluate the models performance on the
testing site using metrics like accuracy
precision recall and F1 score use
confusion matrices to understand the
classification performance across
different genres now if you talk about
model selection and training for the
music generation what you will do guys
you can use the generative adverse
networks or recurrent neural networks
such as lstm long short-term memory for
for music generation train the
generative model on the data set to
create new music sequences next we have
model training and evaluation you can
train the generative model on sequences
of audio features you can evaluate the
generated music by listening tests and
by objective metrics like Inception
score or fret audio distance if I talk
about hyperparameter tuning guys you can
optimize the model you can use the hyper
parameters using techniques like grid
search or random search to improve
performance if I talk about deployment
guys you can deploy these models on
cloud platforms like AWS now let us move
on to our next project so guys the
complexity of this project is at the
beginner level now let us move to the
intermediate level projects next project
that we have all over here is sentiment
analysis of Twitter data this project
aims to develop a sentiment analysis
model that can classify to each as
positive negative or neutral the goal is
to analyze public sentiment on various
topics or events using natural language
techniques so guys what will be used all
over here so in this we will have
programming language like python okay
NLP libraries nltk spacy for machine
learning libraries we can use pych learn
tensorflow kiras for data handling
libraries we have pandas dpai for
visualization we have matplot lip cbon
and we can use the API Twitter for data
collection now how you're going to work
on it guys so guys if I talk about the
data collection use a Twitter API to
collect tweets based on specific
hashtags like Q ke words or topics
extract relevant Fields like tweet text
user information timestamp Etc then if I
talk about data pre-processing guys
clean the Tweet text by removing special
characters links mentions hashtags and
stop wordss tokenize the text and
perform liiz or stemming to reduce the
words to their base form next if we talk
about feature engineering guys you
convert the clean text Data into
numerical representation using tfidf bag
of words or word embedding now if I talk
about model selection guys you can
choose a classification algorithms such
as logistic regression na bias or lstm
split the data set into training and
testing data set now if I talk about
model training and evaluation then you
can train the selected model on the
training set evaluate the model's
performance on the testing side using
metrics like accuracy precision recall
and FN score use cross validation to
ensure the models robustness if I talk
about hyperparameter tuning guys you can
optimize the models hyper parameters
using gr search or random search to
improve the performance for deployment
which can be optional you can deploy
your model on AWS for realtime sentiment
analysis so if I talk about the
complexity level guys its complexity is
intermediate so guys our next project is
customer segmentation using K means
clustering this project aims to segment
customers into distinct groups based on
their purchasing behavior and
demographic information the objective is
to understand customer segments and
tailor marketing strategies accordingly
so guys what programming languages we'll
be using so basically we'll be using
python for machine learning libraries we
have pyit learn for data handing
libraries we'll have pandas numpy for
visualization libraries we'll have M
plot lab cbon and data set Source will
be e-commerce transaction data how we
are going to work on this one for data
collection we can obtain a data set of
e-commerce transactions that include
customer demographics purchase history
and product information next we'll have
data pre-processing for data
pre-processing we are going to do the
cleaning of the data by handling the
missing values and
outliers then for feature engineering we
are going to create features like total
purchase amount purchase frequency and
recency of the
purchases then we are going to proceed
for the model selection we can use C's
clustering to segment the customers into
distinct groups you can determine the
optimal number of clusters using methods
like elbow methods or Sout score now if
I talk about model training and
evaluation you can train the K means
model on the process data set
you can evaluate the quality of clusters
by analyzing intracluster and
intercluster distances next we have the
evaluation you can visualize the
Clusters using techniques like PCA
principal component analysis TSN Etc
next we have the hyperparameter tuning
now now you can tune this model and
interpret the characteristic of each
segment you can develop a Target
marketing strategies for each segment
based on unique behavior and preferences
now deployment is optional you can
develop a dashboard using flask or
Django to visualize customer segments
and track marketing campaigns so guys if
I talk about the complexity of this
project so this is an intermediate level
project so guys for data set you can use
the cagles customer segmentation data
set which is available at the kles
platform now the third intermediate
level project that we have all over here
is building a chatboard with rasa this
project aims to build an intelligent
chatbot using rasa framework the
chatboard will be capable of
understanding user queries and providing
appropriate responses making it useful
for customer support personal assistance
or information retrieval what languages
we are going to use so it will be python
based we'll have the NLP libraries like
rasa nltk spacy for machine learning
libraries we are going to have psyit
learn tensorflow kiras etc for data
handling libraries we are going to use
pandas naai so guys this was what we are
going to do it and how you can work on
this one by collecting the data
collect the conversation data and FAQs
from the target domain annotate the data
to create training examples for the
chatbot next comes is data
pre-processing clean the text data by
removing special characters and
normalizing the text you can tokenize
and limiti the text to prepare for a
training the third one we have the
models training you can use rasa's nlus
component to train a model for intent
recognition and entity extraction you
can Define the dialogue management
policies to handle different
conversation flows next guys you can
perform the feature engineering and
integration you can integrate the rasas
nlu and core components to build
complete chatbot you can connect the
chatboard to messenging platform like
Facebook Messenger etc for model
selection and testing what you can do
guys you can test this chatboard with
various inputs to ensure that it handles
the scenarios appropriately and you can
also select the right model using this
now if I talk about model training guys
what you have to do you have to collect
the user feedback and conversational
logs to continuously work on training
the model next similarly you have to
retrain the model periodically with the
new data to see how it is working so
that will be your evaluation now for the
hyperparameter tuning what you going to
do guys you have to checking those
scenarios where it is able to tune up
with those scenarios where it can handle
the input appropriately and next is
deployment so guys for deploying it you
can use AWS so guys for a data set you
can use rasas open source
so that's a very good data set for you
to proceed so guys if you talk about
difficulty of this project this is an
intermediate level project now let us
move on to the advanced level projects
for advanced level projects the first
one that comes up to my mind is movie
similarity from plot summaries now this
project aims to develop a system that
can find out recommended movies similar
to a given movie based on their plot
summaries by analyzing the textual
content of the movie plot summaries the
model will ident identify similarities
and suggest movies with similar themes
story lines or genres this project
introduces beginers to natural language
processing text similarly measures and
recommendation systems what languages we
are going to use guys we'll be using
python NLP libraries like nltk Spacey
machine learning libraries like psyit
learn data handling libraries like
pandas numpy visualization we can use
numpy and data set Source will be IMDb
or kle so guys this process is also
involving the data collection then you
have to go for data cleaning then
feature engineering next model selection
the similar process as I have discussed
in other projects so you have to also go
through the same one next what do you
have to do guys similarly what do you
have to do you have to train the model
then evaluate the model then hypertune
it and finally proceed for the
deployment so this is overall process of
this project try to research on the
website a lot like how you can extract
it so guys
you can use kagle or towards data
science to research more about this
project now guys if I talk about the
difficulty of this project and this is
an advanced level project now let us
move on to the next one that we have all
over here that is image segmentation
project for brain tumor prognosis this
is a very very amazing project and
definitely you can put up on your
portfolio basically guys this project
aims to develop an image segmentation
model to identify and dealate brain
tumors from m& scans the goal is to
accurately segment the tumor regions
which can Aid in prognosis treatment
planning surgical interventions this
project introduces intermediate level
concepts of computer Visions deep
learning and medical image analysis so
guys what we'll be using all over here
for programming languages we can use
Python for deep learning libraries we
can use tensorflow kiras py for image
processing libraries we can use open CV
psychic image for data handing libraries
we can use pandas numpy for
visualization we can use m left cbon now
if I talk about what is the process of
developing this project the first St
will be the same data collection so next
step you have to go for data
pre-processing third step you have to do
the model selection where you can use
CNN models for image segmentation task
then you go for model selection moving
ahead you're going to have the model
training and evaluation you have to
split the data set into training and
validation and testing data sets next
proceed for the evaluation phas okay
evaluate the model with certain metrics
so here I can give you certain idea like
you can use dice coefficient
intersection over Union or accuracy then
go for hyper parameter tuning where you
have to optimize the models hyper
parameters you can use grid search or
random search as we have discussed and
finally you can deploy this model on AWS
now guys we have come to the final
project this is also very amazing
project guys so guys the complexity
level of this project is advanced level
now let us move on to our final project
that is the impact of climate change on
birds this is a very very amazing
project and definitely you can add it on
your resume this project aims to analyze
the impact of climate change on the bird
population and migration patterns by
examining various climatic factors and
their correlation with bird species data
the project seeks to predict how climate
change might affect bird behavior and
distribution this project will introduce
you some advanced level Concepts like
time series and is environmental data
modeling Etc so guys what programming
languages we'll be using for data
analysis you can see we'll have pandas
numai for machine learning libraries we
are going to have pyic learn tensor flow
for visualization we are going to have
mat plot plotly for G spatel we are
going to have geopandas folium for data
source we're going to have public data
sets on bird observation and climate
data sources from eird now what will the
process flow for this one guys first you
have to proceed for data collection
gather bir observation from the data
like eBird which provides extensive
record of bird sightings okay and for
climate data you can collect it from NOA
including temperature precipitation and
other relevant climatic factors over the
time and similar next process will be
the data pre-processing then you have to
proceed for feature engineering then you
have to go for model selection okay
moving ahead you have to go for model
training then evaluation then hyper
parameter tuning and finally you have to
deploy the model so research about this
project see what models you're going to
use suppose I can give you a hint about
this you can use time series analysis
models like ARA or ml models you can
also use random forest or gradient
boosting for predicting impact on the
bird population so guys use Google
exhaustively to research about this
project this is also a very amazing
project and it's going to give you a lot
of idea now if I talk about the
complexity level of this it is an
advanced level project so what are
confusion metrics and why do we need
them so in machine learning
classification is used to divide data
into different categories but after we
have clean prepared the data and trained
our model how we can tell if the model
is performing well so there is where
confusion matrics come in handy so a
confusion matric is a tool that helps
measure the performance of a classifier
in detail it goes beyond just giving an
overall accuracy score it shows exactly
where the model is making mistake and
how it is classifying each category for
example if there is an imbalance in the
data where one category has many more
instances than other the model might
predict the majority category most of
the time and still show a high accuracy
but this wouldn't be helpful because the
model is not accurately predicting the
minority categories the confusion
matrics help visualize these outcome by
showing a table of all the predicted and
the actual values allowing use to see
where the model is performing well and
where it needs Improvement so moving
forward but let's see confusion matrices
parts or how to create 2x two Matrix so
we can get four different outcomes when
comparing the predict actual values from
a classifier the first one is true
positive this is when the model
correctly predicts a positive outcome
you predicted positive and it was
actually positive the second one is
false positive this happens when the
model incorrectly predicts a positive
outcome for something that is actually
negative or a predictive positive but it
was actually negative the third one true
negative this is when the model
correctly predicts a negative outcome
you predictive negative and it was
actually negative and the fourth one
false negative this occurs when the
model incorrectly predicts a negative
outcome for something that is actually
positive Q predictive negative and but
it was actually positive so moving
forward let's see some matrices of
confusion Matrix so this is a graph so
we know there are four parts true
positive false positive false negative
and true negative so this one is true
positive this one is false positive this
one is false negative this one is true
negative so just consider a confusion
Matrix made for a classifier that
classify People based on whether they
speak English or they speak Spanish okay
from this diagram you can see that fine
so these are some values TP FN FB TN
there is nothing to positive to negative
like this just from looking at the
Matrix the performance of our model is
not very clear so to find out how
accurate our model is we use the
following matrices the first one is
accuracy so this is the formula of
accuracy TP means true positive means
this 86 value so the accuracy is used to
find the portion of correctly classified
values it tells us how often our
classifier is right it is the sum of all
the True Values divided by total values
so in this case this is
88% okay 86 + 79 2 positive + 2 negative
divided by this it will come
8.23 okay and the second one is
precision so Precision is used to
calculate the model ability to classify
positive value correctly it is the true
positive divided by the total number of
predicted positive value so this is the
formula it is true positive div by true
positive plus false positive so in this
case the value will come
87.7% okay and the third one is recall
so recall is used to calculate the
model's ability to predict positive
values how often does the model predict
the current positive values so it is two
positives divided by the total number of
actual positive values so in this case
the value is 88.9 3 okay and the last
one is F1 score
so it is the harmonic mean of recall and
precision it is useful when you need to
take both precision and recall into an
account so in this case the value is
88.7% so now let's jump into the demo
part and see how we can get these values
accuracy precision recall and F1 score
from our data set using handson so I'm
performing this in Google collab so
first I will rename it to confusion
Matrix okay so yes let's start so first
we will import some basic libraries of
python like numai and the pandas so here
I like import numai as
NP and
import pandas as PD why I'm writing this
npn PD because I don't want to write
again and again npay and pandas okay
this is a short form NP so everyone
knows I guess Z and pandas because these
are the basic Li so numai is used for
mostly linear algebra stuff and pandas
is used for the creating data frames
data processing or if you want to read
files CSP files like that okay yeah so
now let's import our data okay so data
equals to pd.
Dore CSV then file name data. CSV so I
will upload this file on the description
box below you guys can check out from
there okay yes so let okay what is
saying no such file or directory on data
or CSV I guess it's
there okay it's not let me import it
okay
yeah okay now file is there yeah it's
working fine so now let's
check okay five
row yeah so this is our data you can see
we have columns with data do
columns okay so we have this 32 columns
in our data set ID diagnosis radius
means texture means parameter means so I
just simply you know to this data there
is nothing related to confusion Matrix
and all okay we have to just we can
perform you know confusion Matrix to you
know any data so head is used for
displaying top five rows of of our data
set okay if I will write data simply you
can see the full data okay with 33
columns and all the rows right so uh now
we don't need this ID
and one is this unnamed data because
there is no
values okay so what I will do I will
write data dot
drop ID comma okay let me copy it
because I don't want any spelling
mistake
yeah copy paste here okay let me write
XIs = to
1 then comma in place equals to
True okay let me run it okay if you will
check data do head
now I guess there will be no ID and all
see there is now no ID and all okay this
we remove that LGE column as well uname
32 okay
so now what I have to
do I will write here data
dot
diagnosis equals to 1 if each go equals
to
M okay
else
zero
for each in
data
dot
diagnosis
diagnosis yeah so if you can see there
is mmm value and there is you know other
values as well so for M I'm putting one
for for the rest I will put
zero okay let's check
again yeah
okay you can see it's done replacement
is done
basically okay so let's check the info
of the
data so these are some basic uh what to
say functions of python so these are the
column these are the nonnal values these
are the data values okay float we have
or int we have okay so we have only two
t type of data sets so now what we will
do we will do normalization
okay yals to data do
diagonosis
diagnosis dot
values okay so here I will write xcore
data equals to data do
drop
diagnosis comma
XIs = to
1 okay here I will write x equals
to
xcore
data minus NP dot minimum value and the
xcore
data and divided by so I put using that
formula if you guys know okay divided
by
NP so I'm basically what I'm finding is
accuracy okay I'm using that formula NP
dot Max of X for
data then minus
NP do main value then xcore data
okay okay some okay access value is
wrong sorry spelling is
wrong now fine yes it's fine so now we
will do train test split so what is
train test split okay so first let me r
underscore
selection
import
train test split okay so what is train
test split so what we do we you know
split our data set into training and the
testing mostly we train and the some
part of data we test okay so here I will
set the ratio so xcore train
comma
xcore
test comma Yore
train comma
Yore test equals to
train test
split then X comma y comma
testore size
equals to
0.15
means 85% we are on the training and 15%
we are on the testing part okay comma
random State equals to 42 okay let me
run it it's working fine yeah it's
working fine so now let's see the
accuracy from the random Forest so I
will write from
skarn
dot
emble
import
random random Forest
classifier okay then RF equals
to random for is classifier and
ncore estimators
equals to 100 and here I will set random
state
to one okay then RF this is a short form
of random Forest do
fit
fit xcore
train comma Yore
train then print
random Forest
score comma
RF do
score then xcore test comma Yore
test okay now let's check how much the
accuracy so accuracy is coming
95% okay which is very good so now what
I will do I will create confus confusion
Matrix now
Yore
prediction = to RF do
predict xcore
test okay and
Yore true equals to
Yore
test okay so here now let's create the
array
skill
learn
dot
matrix
import confusion
Matrix okay then I will write CM short
form of confusion
metric here I will WR y
true comma y
prediction then confusion
Matrix so this is our array okay the
true positive and this is
false negative and this is false
positive like this okay so now let's
create the visualization of confusion
Matrix okay so here I will write import
cbor as
SNS and
import M plot
lip do
pyplot as
PLT PLT means plot okay the short form F
comma ax =
to PLT do
subplots let's give the figure size
equals to 5A
5 then SNS do heat
map then confusion Matrix comma n not
equals to
true then comma line
width = to
0.5 comma line color
equals to let's give
red
then
fmt equals
to I don't need
any you know uh that decimal
values comma ax = to a
x okay then PLT
dot X label
Yore
prediction then PLT
do y
Lael then y underscore
true add the okay forgot to add
this Braes then PLT do
show yeah so as you can see this is the
Y2 values and this is the Y prediction
values okay this is the heat map we have
created so this is how you can visualize
your confusion Matrix fine welcome to
the RNN tutorial that's the recurrent
neural network so we talk about a feed
forward neural network in a feed forward
neural network information flows only in
the forward direction from the input
nodes through the hidden layers if any
and to the output nodes there are no
Cycles or Loops in the network and so
you can see here we have our input layer
I was talking about how it just goes
straightforward into the hidden layers
so each one of those connects and then
connects to the next hidden layer
connects to the output layer and of
course we have a nice simplified version
where it has a predicted output and they
refer to the input as x a lot of times
and the output as y decisions are based
on current input no memory about the
past no future scope why recurrent
neural network issues in feed forward
neural network so one of the biggest
issues is because it doesn't have a
scope of memory or time a feed forward
neural network doesn't know how to
handle sequential data uh it only
considers only the current input so if
you have a series of things and because
three points back affects what's
happening now and what your output
affects what's happening that's very
important so whatever I put as an output
is going to affect the next one um a
feed forward all doesn't look at any of
that it just looks at this is what's
coming in and it cannot memorize
previous inputs so it doesn't have that
list of inputs coming in solution to
feed forward neural network you'll see
here where it says recurrent neural
network and we have our X on the bottom
going to H going to Y that's your feed
forward uh but right in the middle it
has a value C so it's a whole another
process so it's memorizing what's going
on in the hidden layers and the hidden
layers as they produce data feed into
the next one so your hidden layer might
have an output that goes off to Y uh but
that output goes back into the next
prediction coming in what this does is
this allows it to handle sequential data
it considers the current input and also
the previously received inputs and if
we're going to look at General drawings
and um Solutions we should also look at
applications of the RNN image captioning
RNN is used to caption image by
analyzing the activities present in it a
dog catching a ball in midair uh that's
very tough I mean you know we have a lot
of stuff that analyzes images of a dog
and the image of a ball but it's able to
add one more feature in there that's
actually catching the ball in midair
time series prediction any time series
problem like predicting the prices of
stocks in a particular month can be
solved using RNN and we'll dive into
that in our use case and actually take a
look at some stock one of the things you
should know about analyzing stock today
is that it is very difficult and if
you're analyzing the whole stock the
stock market at the New York Stock
Exchange in the US produces somewhere in
the neighborhood if you count all the
individual trades and fluctuations by
the second um it's like three terabytes
a day of data so we're only you look at
one stock just analyzing One stock is
really tricky in here we'll give you a
little jump on that so that's exciting
but don't expect to get rich off of it
immediately another application of the
RNN is natural language processing text
Mining and sentiment analysis can be
carried out using RNN for natural
language processing and you can see
right here the term natural language
processing when you stream those three
words together is very different than I
if I said processing language natural
leave so the time series is very
important when we're analyzing
sentiments it can change the whole value
of a sentence just by switching the
words around or if you're just counting
the words you might get one sentiment
where if you actually look at the order
they're in you get a completely
different sentiment when it rains look
for rainbows when it's dark look for
stars both of these are positive
sentiments and they're based upon the
order of which the sentence is going in
machine translation given an input in
one language RNN can be used to
translate the input into a different
languages as output I myself very
linguistically challenged but if you
study languages and you're good with
languages you know right away that if
you're speaking English you would say
big cat and if you're speaking Spanish
you would say cat big so that
translation is really important to get
the right order to get uh there's all
kinds of parts of speech that are
important to know by the order of the
words here this person is speaking in
English and getting translated and you
can see here a person is speaking in
English in this little diagram I guess
that's denoted by the flags I have a
flag I own it no um but they're speaking
in English and it's getting translated
into Chinese Italian French German and
Spanish languages some of the tools
coming out are just so cool so somebody
like myself who's very linguistically
challenged I can now travel into Worlds
I would never think of because I can
have something translate my English back
and forth readily and I'm not stuck with
a communication gap so let's dive into
what is a recurrent neural network
recurrent neural network works on the
principle of saving the output of a
layer and feeding this back to the input
in order to predict the output of the
layer sounds a little confusing when we
start breaking it down it'll make more
sense and usually we have a propagation
forward neural network with the input
layers the hidden layers the output
layer with the recurrent neural network
we turn that on its side so here it is
and now our X comes up from the bottom
into the hidden layers into Y and they
usually draw very simplified X to H with
c as a loop a to Y where a B and C are
the perimeters a lot of times you'll see
this kind of drawing in here digging
closer and closer into the H and how it
works going from left to right you'll
see that the C goes in and then the X
goes in so the x is going Upward Bound
and C is going to the right a is going
out and C is also going out that's where
it gets a little confusing so here we
have xn uh CN and then we have y out and
C out and C is based on HT minus one so
our value is based on the Y and the H
value are connected to each other
they're not necessarily the same value
because H can be its own thing and
usually we draw this or we represent it
as a function h of T equals a function
of C where H of T minus one that's the
last H output and x a t going in so it's
the last output of H combined with the
new input of x uh where HT is the new
state FC is a function with the
parameter C that's a common way of
denoting it uh HT minus one is the Old
State coming out and then X of T is an
input Vector at time of Step T well we
need to cover types of recurrent neural
networks and so the first one is the
most common one which is a onetoone
single output Ono one neural network is
usually known as a vanilla neural
network used for regular machine
learning problems why because vanilla's
usually considered kind of a just a real
basic flavor but because it's a very
basic a lot of times they'll call it the
vanilla neural network uh which is not
the common term but it is you know like
kind of a slang term people will know
what you're talking about usually if you
say that then we run one to mini so you
have a single input and you might have a
multiple outputs in this case uh image
captioning as we looked at earlier where
we have not just looking at it as a dog
but a dog catching a ball in the air and
then you have Min to1 Network takes in a
sequence of inputs examples sentiment
analysis where a given sentence can be
classified as expressing positive or
negative sentiments and we looked at
that as we were discussing if it rains
look for a rainbow so positive sentiment
where rain might be a negative sentiment
if you were just adding up the words in
there and then of course if you're going
to do a one to one many to one one to
many there's many to many networks takes
in a sequence of inputs and generates a
sequence of outputs example machine
translation so we have a lengthy
sentence coming in in English and then
going out in all the different languages
uh you know just a wonderful tool very
complicated set of computations you know
if you're a translator you realize just
how difficult it is to translate into
different languages one of the biggest
things you need to understand when we're
working with this neural network is
what's called The Vanishing gradient
problem while training an RNN your slope
can be either too small or very large
and this makes training difficult when
the slope is too small the problem is
known as Vanishing gradient and you'll
see here they have a nice U image loss
of information through time so if you're
pushing not enough information forward
that information is lost and then when
you go to train it you start losing the
third word in the sentence or something
like that or it doesn't quite follow
follow the full logic of what you're
working on exploding gradient problem Oh
this is one that runs into everybody
when you're working with this particular
neural network when the slope tends to
grow exponentially instead of decaying
this problem is called exploding
gradient issues in gradient problem Long
training time poor performance bad
accuracy and I'll add one more in there
uh your computer if you're on a a
lower-end computer testing out a model
will lock up and give you the memory
error explaining gradient problem
consider the following two examples to
understand what should be the next word
in the sequence the person who took my
bike and blank a thief the students who
got into engineering with blank from
Asia and you can see in here we have our
x value going in we have the previous
value going forward and then you back
propagate the error like you do with any
neural network and as we're looking for
that missing word maybe we'll have the
person took my bike and blank was a
thief and the student who got into
engineering with a blank were from Asia
consider the following example the
person who took the bike so we'll go
back to the person who took the bike was
blank a thief in order to understand
what would be the next word in the
sequence the RNN must memorize the
previous context whether the subject was
singular noun or a plural noun so was a
thief is singular the student who got
into engineering well in order to
understand what would be the next word
in the sequence the RNN must memorize
the previous context whether the subject
was singular noun or a plural noun and
so you can see here the students who got
into engineering with blank were from
Asia it might be sometimes difficult for
the eror to back propagate to the
beginning of the sequence to predict
what should be the output so when you
run into the gradient problem we need a
solution the solution to the gradient
problem first we're going to look at
exploding gradient where we have three
different solutions depending on what's
going on one is identity initialization
so the first thing we want to do is see
if we can find a way to minimize the
identities coming in instead of having
it identify everything just the
important information we're looking at
next is to truncate the back propagation
so instead of having uh whatever
information it's sending to the next
series we can truncate what it's sending
we can lower that particular uh set of
layers make those smaller and finally is
a gradient clipping so when we're
training it we can clip what that
gradient looks like and narrow the
training model that we're using when you
have a Vanishing gradient the op problem
uh we can take a look at weight
initialization very similar to the
identity but we're going to add more
weights in there so it can identify
different aspects of what's coming in
better choosing the right activation
function that's huge so we might be
activating based on one thing and we
need to limit that we haven't talked too
much about activation functions so we'll
look at that just minimally uh there's a
lot of choices out there and then
finally there's long short-term memory
networks the
lstms and we can make adjustments to
that that so just like we can clip the
gradient as it comes out we can also um
expand on that we can increase the
memory Network the size of it so it
handles more information and one of the
most common problems in today's uh setup
is what they call longterm dependencies
suppose we try to predict the last word
in the text the clouds are in the and
you probably said sky here we do not
need any further context it's pretty
clear that the last word is going to be
Sky suppose we try to predict the last
word in the text text I have been
staying in Spain for the last 10 years I
can speak fluent maybe you said
Portuguese or French no you probably
said Spanish the word we predict will
depend on the previous few words in
context here we need the context of
Spain to predict the last word in the
text it's possible that the gap between
the relevant information and the point
where it is needed to become very large
lstms help us solve this problem so the
lstms are a special kind of recurrent
neural network capable of learning
long-term dependencies remembering
information for long periods of time is
their default Behavior All recurrent
neural networks have the form of a chain
of repeating modules of neural network
connections in standard rnns this
repeating module will have a very simple
structure such as a single tangent H
layer lstm s's also have a chain-like
structure but the repeating module has a
different structure instead of having a
single neural network layer there are
four interacting layers is communicating
in a very special way lstms are a
special kind of recurrent neural network
capable of learning long-term
dependencies remembering information for
long periods of time is their default
Behavior LST tms's also have a
chain-like structure but the repeating
module has a different structure instead
of having a single neural network layer
there are four interacting layers
communicating in a very special way as
you can see the deeper we dig into this
the more complicated the graphs kit in
here I want you to note that you you
have X of T minus1 coming in you have X
of T coming in and you have x a t + one
and you have H of T minus1 and H of T
coming in and H of t plus one going out
and of course on the other side is the
output a um in the middle we have our
tangent H but it occurs in two different
places so not only when we're Computing
the x of t+ one are we getting the
tangent H from X of T but we're also
getting that value coming in from the X
of T minus one so the short of it is as
you look at these layers not only does
it does the propagate through the first
layer goes into the second layer back
into itself but it's also going into the
third layer so now we're kind of
stacking those up and this can get very
complicated as you grow that in size it
also grows in memory too and in the
amount of resources it takes uh but it's
a very powerful tool to help us address
the problem of complicated long
sequential information coming in like we
were just looking at in the sentence and
when we're looking at at our long
shortterm memory network uh there's
three steps of processing processing in
the lstms that we look at the first one
is we want to forget irrelevant parts of
the previous state you know a lot of
times like you know is as in a unless
we're trying to look at whether it's a
plural noun or not they don't really
play a huge part in the language so we
want to get rid of them then selectively
update cell State values so we only want
to update the cell State values that
reflect what we're working on and
finally we want to put only output
certain parts of the cell state so
whatever is coming out we want to limit
what's going out too and let's dig a
little deeper into this let's just see
what this really looks like uh so step
one decides how much of the past it
should remember first step in the lstm
is to decide which information to be
omitted in from the cell in that
particular time step it is decided by
the sigmoid function it looks at the
previous state h of T minus one and the
current input X of T and computes the
function so you can see over here we
have a function of T equals the sigmoid
function of the weight of f the H at T
minus one and then xit t plus of course
you have a bias in there with any of our
neural network so we have a bias
function so F of T equals forget gate
decides which information to delete that
is not important from the previous time
step considering an L STM is fed with
the following inputs from the previous
and present time step Alice is good in
physics John on the other hand is good
in chemistry so previous output John
plays football well he told me yesterday
over the phone that he had served as a
captain of his college football team
that's our current input so as we look
at this the first step is the forget
gate realizes there might be a change in
context after encountering the First
full stop Compares with the current
input sentence of exit so we're looking
at that full stop and then Compares it
with the input of the new sentence the
next sentence talks about John so the
information on Alice is deleted okay
that's important to know so we have this
input coming in and if we're going to
continue on with John then that's going
to be the primary information we're
looking at the position of the subject
is vacated and is assigned to John and
so in this one we've seen that we've
weeded out a whole bunch of information
and we're only passing information on
John since that's now the new topic so
step two is in to decide how much should
this unit add to the current state in
the second layer there are two parts one
is a sigmoid function and the other is a
tangent H in the oid function it decides
which values to let through zero or one
tangent 8 function gives the weightage
to the values which are passed deciding
their level of importance minus one to
one and you can see the two formulas
that come up uh the I of T equals the
sigmoid of the weight of i h of T minus
one X of t plus the bias of I and the C
of T equals the tangent of H of the
weight of C of H of t minus1 x of t plus
the bias of C so our I of T equals the
input gate deter detes which information
to let through based on its significance
in the current time step if this seems a
little complicated don't worry CU a lot
of the programming is already done when
we get to the case study understanding
though that this is part of the program
is important when you're trying to
figure out these what to set your
settings at you should also note when
you're looking at this it should have
some semblance to your forward
propagation neural networks where we
have a value assigned to a weight plus a
bias very important steps than any of
the neural network layers whether we're
propagating into them the information
from one to the next or we're just doing
a straightforward neural network
propagation let's take a quick look at
this what it looks like from the human
standpoint um as I step out in my suit
again consider the current input at X of
John plays football well he told me
yesterday over the phone that he had
served as a captain of his college
football team that's our input input
gate analysis the important information
John plays football and he was a captain
of his college team is important
he told me over the phone yesterday is
less important hence it is forgotten
this process of adding some new
information can be done via the input
gate now this example is as a human form
and we'll look at training this stuff in
just a minute uh but as a human being if
I wanted to get this information from a
conversation maybe it's a Google Voice
listening in on you or something like
that um how do we weed out the
information that he was talking to me on
the phone yesterday well I don't want to
memorize that he talked to me on the
phone yesterday or maybe that is
important but in this case it's not I
want to know that he was the captain of
the football team I want to know that he
served I want to know that John plays
football and he was the captain of the
college football team those are the two
things that I want to take away as a
human being again we measure a lot of
this from the human Viewpoint and that's
also how we try to train them so we can
understand these neural networks finally
we get to step three decides what part
of the current cell State makes it to
the output the third step is to decide
what will be our output first we run a
sigmoid layer which is decides what
parts of the cell State make it to the
output then we put the cell State
through the tangent H to push the values
to be between minus1 and one and
multiply it by the output of the sigmoid
gate so when we talk about the output of
T we set that equal to the sigmoid of
the weight of zero of the H of T minus
one you back One Step in Time by the x
of t plus of course the bias the H of T
equals the out of ttimes the tangent of
the tangent h of C of T so our o T
equals the output gate allows the past
in information to impact the output in
the current time step let's consider the
example to predicting the next word in
the sentence johon played tremendously
well against the opponent and won for
his team for his contributions Brave
blank was awarded player of the match
there could be a lot of choices for the
empty space current input Brave is an
adjective adjectives describe a noun
John could be the best output after
Brave thumbs up for John and where
player of the match and if you were to
pull just the nouns out of the sentence
team doesn't look right because that's
not really the subject we're talking
about contributions you know Brave
contributions or Brave team Brave player
Brave match um so you look at this and
you can start to train this these this
neural network so starts looking at and
goes oh no JN is what we're talking
about so brave is an adjective Jon's
going to be the best output and we give
Jon a big thumbs up and then of course
we jump in to my favorite part the case
study use case implementation of lstm
let's predict the prices of stocks using
the lstm network based on the stock
price data between 2012 2016 we're going
to try to predict the stock prices of
2017 and this will be a narrow set of
data we're not going to do the whole
stock market it turns out that the New
York Stock Exchange generates roughly
three terabytes of data per day that's
all the different trades up and down of
all the different stocks going on and
each individual one uh second to second
or nanc to nanc uh but we're going to
limit that to just some very basic
fundamental information so don't think
you're going to get rich off this today
but at least you can give an a you can
give a step forward in how to start
processing something like stock prices a
very valid use for machine learning in
today's markets use case implementation
of
lstm let's dive in we're going to import
our libraries we're going to import the
training set and uh get the scaling
going um now if you watch any of our
other tutorials a lot of these pieces
just start to look very familiar CU it's
very similar setup uh but let's take a
look at that and um just a reminder
we're going to be using Anaconda the
Jupiter notebook so here I have my
anaconda Navigator when we go under
environments I've actually set up a
carass python 36 I'm in Python 36 and U
nice thing about Anaconda especially the
newer version remember a year ago
messing with Anaconda different versions
of python and different environments um
Anaconda now has a nice interface um and
I have this installed both on a Ubuntu
Linux machine and on windows so it works
fine on there you can go in here and
open a terminal window and then in here
once you're in the terminal window this
is where you're going to start uh
installing using pip to install your
different modules and everything now
we've already pre-installed them so we
don't need to do that in here uh but if
you don't have them installed on your
particular environment you'll need to do
that and of course you don't need to use
the anaconda or the Jupiter you can use
whatever favorite python ID you like I'm
just a big fan of this cuz it keeps all
my stuff separate you can see on this
machine I have specifically installed
one for carass since we're going to be
working with carass under tensor flow
when we go back to home I've gone up
here to application and that's the
environment I've loaded on here and then
we'll click on the launch Jupiter
notebook now I've already in my Jupiter
notebook um have set up a lot of stuff
so that we're ready to go kind of like
Martha Stewarts in the old cooking shows
we want to make sure we have all our
tools for you so you're not waiting for
them to load and if we go up here to
where it says new you can see where you
can um create a new Python 3 that's what
we did here underneath the setup so it
already has all the modules installed on
it and I'm actually renamed this so if
you go under file you can rename it we
I'm calling it RNN stock and let's just
take a look at start diving into the
code let's get into the exciting part
now we've looked at the tool and of
course you might be using a different
tool which is fine uh let's start put
that code in there and seeing what those
Imports and uploading everything looks
like now first half is kind of boring
when we hit the rum button CU we're
going to be importing numpy as NP that's
uh uh the number python which is your
numpy array and the matap plot library
because we're going to do some plotting
at the end and our pandas for our data
set our pandas is PD and when I hit run
uh it really doesn't do anything except
for load those modules just a quick note
let me just do a quick uh draw here oops
shift alt there we go you'll notice when
we're doing this setup if I was to
divide this up oops I'm going to
actually U let's overlap these here we
go uh this first part that we're going
to do
is our
data prep a lot of prepping
involved um in fact depending on what
your system is since we're using carass
I put an overlap here uh but you'll find
that almost maybe even half of the code
we do is all about the data prep and the
reason I overlap this with uh carass me
just put that down because that's what
we're working in uh is because caras has
like their own preset stuff so it's
already pre-built in which is really
nice so there's a couple Steps A lot of
times that are in the Kass setup uh
we'll take a look at that to see what
comes up in our code as we go through
and look at stock and then the last part
is to evaluate and if you're working
with um shareholders or uh you know
classroom whatever it is you're working
with uh the evaluate is the next biggest
piece um so the actual code here crossed
is a little bit more but when you're
working with uh some of the other
packages you might have like three lines
that might be it all your stuff is in
your pre-processing in your data since
carass has is is Cutting Edge and you
load the individual layers you'll see
that there's a few more lines here and
carass is a little bit more robust and
then you spend a lot of times uh like I
said with the evaluate you want to have
something you present to everybody else
to say hey this is what I did this is
what it looks like so let's go through
those steps this is like a kind of just
general overview and let's just take a
look and see what the next set of code
looks like and in here we have a data
set train and it's going to be read
using the PD or pandas read CSV and it's
the Google stock pric train.csv and so
under this we have training set equals
data set train. iocation and we've kind
of sorted out part of that so what's
going on here let's just take a look at
that let's let let's look at the actual
file and see what's going on there now
if we look at this ignore all the extra
files on this um I already have a train
and a test set where it's sorted out
this is important to notice because a
lot of times we do that as part of the
pre-processing of the data we take 20%
of the data out so we can test it and
then we train the rest of it that's what
we use to create our neural network that
way we can find out how good it is uh
but let's go ahead and just take a look
and see what that looks like as far as
the file itself and I went ahead and
just open open this up in a basic word
pad text editor just so we can take a
look at it certainly you can open up in
Excel or any other kind of spread sheeet
um and we note that this is a comma
separated variables we have a date uh
open high low close volume this is the
standard stuff that we import into our
stock one the most basic set of
information you can look at in stock
it's all free to download um in this
case we downloaded it from uh Google
that's why we call it the Google stock
price um and it specifically is Google
this is the Google stock values from uh
as you can see here we started off at 13
20102 so when we look at this first
setup up here uh we have a data set
train equals pdor CSV and if you noticed
on the original frame um let me just go
back there they had it set to home
Ubuntu downloads Google stock price
train I went ahead and change that
because we're in the same file where I'm
running the code so I've saved this
particular python code and I don't need
to go through any special paths or have
the full path on there and then of
course we want to take out um certain
values in here and you're going to
notice that we're using um our data set
and we're now in pandas uh so pandas
basically it looks like a spreadsheet um
and in this case we're going to do I
location which is going to get specific
locations the first value is going to
show us that we're pulling all the rows
in the data and this second one is we're
only going to look at columns one and
two and if you remember here from our
data as we switch back on over columns
we always start with zero which is the
date and we're going to be looking at
open and high which would be one and
two I'll just label that right there so
you can see now when you go back and do
this you certainly can extrapolate and
do this on all the columns um but for
the example let's just limit a little
bit here so that we can focus on just
some key aspects of
stock and then we'll go up here and run
the code and uh again I said the first
half is very boring whenever we hit the
Run button it doesn't do anything
because we're still just loading the
data and setting it up now that we've
loaded our data we want to go ahead and
scale it we want to do what they call
feature scaling and in here we're going
to pull it up from the SK learn or the
SK kit pre-processing import min max
scaler and when you look at this you got
to remember that um biases in our data
we want to get rid of that so if you
have something that's like a really high
value U let's just draw a quick graph
and I have something here like the maybe
the stock has a value One stock has a
value of 100 and another stock has a
value of
five um you start to get a bias between
different stocks and so when we do this
we go ahead and say okay 100's going to
be the Max and five is going to be the
min and then everything else goes and
then we change this so we just squish it
down I like the word squish so it's
between one and zero so 100 equal 1 or 1
equal 100 and 0 equal 5 and you can just
multiply it's usually just a simple
multiplication we're using uh
multiplication so it's going to be uh
minus5 and then 100 divided or 95
divided by one so or whatever value is
is divided by
95 and uh once we've actually created
our scale we've toing it's going to be
from 0 to one we want to take our
training set and we're going to create a
training set scaled and we're going to
use our scaler SC and we're going to fit
we're going to fit and transform the
training Set uh so we can now use the SC
this this particular object we'll use it
later on our testing set because
remember we have to also scale that when
we go to test our uh model and see how
it works and we'll go Ahad and click on
the run again uh it's not going to have
any output yet because we're just
setting up all the variables
okay so we pasted the data in here and
we're going to create the data structure
with the 60 time steps and output first
note we're running 60 time steps and
that is where this value here also comes
in so the first thing we do is we create
our X train and Y train variables we set
them to an empty python array very
important to remember what kind of array
we're in and what we're working with and
then we're going to come in here we're
going to go for I in range 60 to 1258
there's our 60 60 time steps and the
reason we want to do this is as we're
adding the data in there there's nothing
below the 60 so if we're going to use 60
time steps uh we have to start at 60
because it includes everything
underneath of it otherwise you'll get a
pointer error and then we're going to
take our X train and we're going to
append training set scaled this is the
scaled value between zero and one and
then as I is equal to 60 this value is
going to be um 60 - 60 is 0o so this
actually is 0 to I so it's going to be 0
to 60 1 to 61 let me just circle this
part right here 1 to 61 uh 2 to 62 and
so on and so on and if you remember I
said 0 to 60 that's incorrect because it
does not count remember it starts at
zero so this is a count of 60 so it's
actually 59 important to remember that
as we're looking at this and then the
second part of this that we're looking
at so if you remember correctly here we
go we go from uh zero to 59 of I and
then we have a comma a zero right here
and so finally we're just going to look
at the open value now I know we did put
it in there for one to two um if you
remember correctly it doesn't count the
second one so it's just the open value
we're looking at just open um and then
finally we have y train. append training
set I to zero and if you remember
correctly I to or I comma 0o if you
remember correctly this is 0 to 59 so
there 60 values in it uh so when we do I
down here this is number 60 so we're
going to do this is we're creating an
array and we have 0 to
59 and over here we have number 60 which
is going into the Y train it's being
appended on there and then this just
goes all the way up so this is down here
is a 0o to 59 and we'll call it 60 since
that's the value over here and it goes
all the way up to
12 58 that's where this value here comes
in that's the length of the data we're
loading so we've loaded two arrays we
loaded one array that has uh which is
filled with arrays from 0 to 59 and we
loaded one array which is just the value
and what we're looking at you want to
think about this as a Time sequence uh
here's my open open open open open open
what's the next one in the series so
we're looking at the Google stock and
each time it opens we want to know what
the next one uh 0 through 59 what's 60 1
through through 60 what's 61 2 through
62 what's 62 and so on and so on going
up and then once we've loaded those in
our for Loop we go ahead and take XT
train and Y train equals np. array XT
tr. NP array YT train we're just
converting this back into a numpy array
that way we can use all the cool tools
that we get with numpy array including
reshaping so if we take a look and see
what's going on here we're going to take
our X
train we're going to reshape it
wow what the heck does reshape mean uh
that means we have an array if you
remember correctly um so many numbers by
60 that's how wide it is and so we're
when you when you do xtrain do shape
that gets one of the shapes and you get
um XT train. shape of one gets the other
shape and we're just making sure the
data is formatted correctly and so you
use this to pull the fact that it's 60
by um in this case where's that value 60
by
1199 1258 -
60199 and we're making sure that that is
shaped correctly so the data is grouped
into uh
1199 by 60 different arrays and then the
one on the end just means at the end
because this when you're dealing with
shapes and numpy they look at this as
layers and so the in layer needs to be
one value that's like the leaf of a tree
where this is the branch and then it
branches out some more um and then then
you get the Leaf np. reshape comes from
and using the existing shapes to form it
we'll go ahead and run this piece of
code again there's no real output and
then we'll import our different carass
modules that we need so from carass
Models we're going to import the
sequential model we're dealing with
sequential data we have our dense layers
we have actually three layers we're
going to bring in our dense our lstm
which is what we're focusing on and our
Dropout and we'll discuss these three
layers more in just a moment but you do
need the with the lstm you do need the
Dropout and then the final layer will be
the dents but let's go ahead and run
this and that'll bring Port our modules
and you'll see we get an error on here
and if you read it closer it's not
actually an error it's a warning what
does this warning mean these things come
up all the time when you're working with
such Cutting Edge modules that are
completely being updated all the time
we're not going to worry too much about
the warning all it's saying is that the
h5py module which is part of caros is
going to be updated at some point and uh
if you're running new stuff on carass
and you start updating your carass
system you better make sure that your H5
Pi is updated too otherwise you're going
to have an error later on and you can
actually just run an update on the H5 Pi
now if you wanted to not a big deal
we're not going to worry about that
today and I said we were going to jump
in and start looking at what those
layers mean I meant that and uh we're
going to start off with initializing the
RNN and then we'll start adding those
layers in and you'll see that we have
the lstm and then the dropout lstm M
then Dropout lstm then Dropout what the
heck is that doing so let's explore that
we'll start by initializing the RNN
regressor equals sequential because
we're using the sequential model and
we'll run that and load that up and then
we're going to start adding our lstm
layer and some Dropout regularization
and right there should be the que
Dropout regularization and if we go back
here and remember our exploding gradient
well that's what we're talking about the
Dropout drops out unnecessary data so
we're not just shifting huge amounts of
data through um the network so and so we
go in here let's just go ahead and add
this in I'll go ahead and run this and
we had three of them so let me go and
put all three of them in and then we can
go back over them here's the second one
and let's put one more in let's put that
in and we'll go and put two more in I
meant to put I said one more in but it's
actually two more in and then let's add
one more after that and as you can see
each time I run these they don't
actually have an output so let's take a
closer look and see what's going on here
so we're going to add our first lstm
layer in here we're going to have units
50 the units is the positive integer and
it's the dimential of the output space
this is what's going out into the next
layer so we might have 60 coming in but
we have 50 going out we have a return
sequence because it is a sequence data
so we want to keep that true and then
you have to tell it what shape it's in
well we already know the shape by just
going in here and looking at xtrain
shape so input shape equals the XT train
shape of 1 comma 1 makes it really easy
you don't have to remember all the
numbers that put in 60 or whatever else
is in there you just let it tell the
regressor what model to use and so we
follow our STM with a Dropout layer now
understanding the Dropout layer is kind
of exciting because one of the things
that happens is we can overtrain our
Network that means that our neural
network will memorize such specific data
that it has trouble predicting anything
that's not in that specific realm to fix
for that each time we run through the
training mode we're going to take 02 or
20% of our neurons and just turn them
off so we're only going to train on the
other ones and it's going to be random
that way each time we pass through this
we don't overtrain these nodes come back
in in the next training cycle we
randomly pick a different 20 and finally
they see a big difference as we go from
the first to the second and third and
fourth the first thing is we don't have
to input the shape because the shape's
already the output units is 50 here this
Auto The Next Step automatically knows
this layer is putting out 50 and because
it's the next layer it automatically
sets that and says oh 50 is coming out
from our last layer it's coming out you
know goes into the regressor and of
course we have our Dropout and that's
what's coming into this one and so on
and so on and so the next three layers
we don't have to let it know what the
shape is it automatically understands
that and we're going to keep the units
the same we're still going to do 50
units it's still a sequence coming
through 50 units and a sequence now the
next piece of code is what brings it all
together let's go ahead and take a look
look at that and we come in here we put
the output layer the dense layer and if
you remember up here we had the three
layers we had uh lstm drop out and d uh
D just says we're going to bring this
all down into one output instead of
putting out a sequence we just know want
to know the answer at this point and
let's go ahead and run that and so in
here you notice all we're doing is
setting things up one step at a time so
far we've brought in our uh way up here
we brought in our data we brought in our
different modules we formatted the data
for training it we set it up you know we
have our y x train and our y train we
have our source of data and the answers
we we know so far that we're going to
put in there we've reshaped that we've
come in and built our carass we've
imported our different layers and we
have in here if you look we have what uh
five total layers now carass is a little
different than a lot of other systems
because a lot of other systems put this
all in one line and do it automatic but
they don't give you the options of how
those layers interface and they don't
give you the opt options of how the data
comes in carass is Cutting Edge for this
reason so even though there's a lot of
extra steps in building the model this
has a huge impact on the output and what
we can do with this these new models
from carass so we brought in our dents
we have our full model put together a
regressor so we need to go ahead and
compile it and then we're going to go
ahead and fit the data we're going to
compile the pieces so they all come
together and then we're going to run our
training data on there and actually
recreate our regressor so it's ready to
be used so let's go ahead and compile
that and I can goe and run that and uh
if you've been looking at any of our
other tutorials on neural networks
you'll see we're going to use the
optimizer atom atom is optimized for Big
Data there's a couple other optimizers
out there uh beyond the scope of this
tutorial but certainly Adam will work
pretty good for this and loss equals
mean squared value so when we're
training it this is what we want to base
the loss on how bad is our error well
we're going to use the mean squared
value for our error and the atom
Optimizer for its differential equations
you don't have have to know the math
behind them but certainly it helps to
know what they're doing and where they
fit into the bigger models and then
finally we're going to do our fit
fitting the RN to the training set we
have the regressor do fit xtrain y train
epics and batch size so we know where
this is this is our data coming in for
the X train our y train is the answer
we're looking for of our data our
sequential input epex is how many times
we're going to go over the whole data
set we created a whole data set of XT
train so this is each each of those rows
which includes a Time sequence of 60 and
badge size another one of those things
where carass really shines is if you
were pulling this save from a large file
instead of trying to load it all into
RAM it can now pick smaller batches up
and load those indirectly we're not
worried about pulling them off a file
today this isn't big enough to uh cause
the computer too much of a problem to
run not too straining on the resources
but as we run this you can imagine what
would happen if I was doing a lot more
than just one column in one set of stock
in this case Google stock imagine if I
was doing this across all the stocks and
I had instead of just the open I had
open close high low and you can actually
find yourself with about 13 different
variables times 60 because it's a Time
sequence suddenly you find yourself with
a gig of memory you're loading into your
RAM which will just completely you know
if it's just if you're not on multiple
computers or cluster you're going to
start running into resource problems but
for this we don't have to worry about
that so let's go ahead and run this and
this will actually take a little bit on
my computer it's an older laptop and
give it a second to kick in there there
we go all right so we have epic so this
is going to tell me it's running the
first run through all the data and as
it's going through it's batching them in
32 pieces so 32 uh lines each time and
there's 1198 I think I said 1199 earlier
but it's 1198 I was off by one and each
one of these is 13 seconds so you can
imagine this is roughly 20 to 30 minutes
runtime on this computer like I said
it's an older laptop top running at uh9
GHz on a dual processor and that's fine
what we'll do is I'll go ahead and stop
go get a drink of coffee and come back
and let's see what happens at the end
and where this takes us and like any
good cooking show I've kind of gotten my
latte I also had some other stuff
running in the background so you'll see
these numbers jumped up to like 19
seconds 15 seconds which you can scroll
through and you can see we've run it
through 100 steps or 100 epics so the
question is what does all this mean one
of the first things you'll notice is
that our loss can is over here I kind of
stopped at 0.0014 you can see it kind of
goes down until we hit about 0.14 three
times in a row so we guessed our epic
pretty close since our loss has remain
the same on there so to find out what
we're looking at we're going to go ahead
and load up our test data the test data
that we didn't process yet and uh real
stock price data set test iocation this
is the same thing we did when we prepped
the data in the first place so let's go
ahead and go through this code and you
see we've labeled it part three making
the predictions and visualizing the
results results so the first thing we
need to do is go ahead and read the data
in from our test CSV and you see I've
changed the path on it for my computer
and uh then we'll call it the real stock
price and again we're doing just the one
column here and the values from I
location so it's all the rows and just
the values from these that one location
that's the open Stock open let's go
ahead and run that so that's loaded in
there and then let's go ahead and uh
create we have our inputs we're going to
create inputs here and this should all
look familiar this is the same thing we
did before we're going to take our data
set total we're going to do a little
Panda concap from the data State train
now remember the end of the data set
train is part of the data going in and
let's just visualize that just a little
bit here's our train data let me just
put TR for train and it went up to this
value here but each one of these values
generated a bunch of columns it was 60
across and this value here equals this
one and this value here equals this one
and this value here equals this one and
so we need these top 60 to go into our
new data so to find out what we're
looking at we're going to go ahead and
load up our test data the test data that
we didn't process yet and uh real stock
price data set test iocation this is the
same thing we did when we prepped the
data in the first place so let's go
ahead and go through this code and we
can see we've labeled it part three
making the predictions and visualizing
the results so the first thing we need
to do is go ahead and read the data in
from our test CSV and you see I've
changed the path on it for my computer
and uh then we'll call it the real stock
price and again we're doing just the one
column here and the values from I
location so it's all the rows and just
the values from these that one location
that's the open Stock open let's go
ahead and run that so that's loaded in
there and then let's go ahead and uh
create we have our inputs we're going to
create inputs here and this should all
look familiar this is the same thing we
did before we're going to take our data
set total we're going to do a little
Panda concat from the data set train now
remember the end of the data set train
is part of the data going in let's just
visualize that just a little bit here's
our train data let me just put TR for
train and it went up to this value here
but each one of these values generated a
bunch of columns it was 60 across and
this value here equals this one and this
value here equals this one and this
value here equals this one and so we
need these top 60 to go into our new
data because that's part of the next
data or it's actually the top 59 so
that's what this first setup is over
here is we're going in we're doing the
real stock price and we're going to just
take the data set test and we're going
to load that in and then the real stock
price is our data test. test location so
we're just looking at that first uh
column the open price and then our data
set total we're going to take pandas and
we're going to concat and we're going to
take our data set train for the open and
our data set test open and this is one
way you can refence ref these columns
we've referenced them a couple different
ways we've referenced them up here with
the one two but we know it's labeled as
a panda set is open so pandas is great
that way lots of Versatility there and
we'll go ahead and go back up here and
run this there we go and uh you'll
notice this is the same as what we did
before we have our open data set we
pended our two different or concatenated
our two data sets together we have our
inputs equals data set total length data
set total minus length of data set minus
test minus 60 values so we're going to
run this over all of them and you'll see
why this works because normally when
you're running your test set versus your
training set you run them completely
separate but when we graph this you'll
see that we're just going to be we'll be
looking at the part that uh we didn't
train it with to see how well it graphs
and we have our inputs equals inputs.
reshapes or reshaping like we did before
we're Transforming Our inputs so if you
remember from the transform between zero
and one and uh finally we want to go
ahead and take our X test and we're
going to create that X test and for I in
range 60 to 80 so here's our X test and
we're appending our inputs I to 60 which
remember is 0 to 59 and I comma 0 on the
other side so it's just the First Column
which is our open column and uh once
again we take our X test we convert it
to a numpy array we do the same reshape
we did before and uh then we get down to
the final two lines and here we have
something new right here on these last
two lines let me just highlight those or
or mark them predicted stock price
equals regressor do predicts X test so
we're predicting all the stock including
both training and the testing model here
and then we want to take this prediction
and we want to inverse the transform so
remember we put them between zero and
one well that's not going to mean very
much to me to look at a at a float
number between zero one I want the
dollar amounts I want to know what the
cash value is and we'll go ahead and run
this and you'll see it runs much quicker
than the training that's what's so
wonderful about these neural networks
once you put them together it takes just
a second to run the same neural network
that took us what a half hour to train
ahead and plot the data we're going to
plot what we think it's going to be and
we're going to plot it against the real
data what what the Google stock actually
did so let's go ahead and take a look at
that in code and let's uh pull this code
up so we have our PLT that's our uh oh
if you remember from the very beginning
let me just go back up to the top we
have our matplot library. pyplot as PLT
that's where that comes in and we come
down here we're going to plot let me get
my drawing thing out again we're going
to go ahead and PLT is basically kind of
like an object it's one of the things
that always threw me when I'm doing
graphs in python because I always think
you have to create an object and then it
loads that class in there well in this
case PLT is like a canvas you're putting
stuff on so if you've done HTML 5 you'll
have the canvas object this is the
canvas so we're going to plot the real
stock price that's what it actually is
and we're going to give that color red
so it's going to be a bright red we're
going to label it real Google stock
price and then we're going to do our
predicted stock and we're going to do it
in blue and it's going to be labeled
predicted and we'll give it a title CU
it's always nice to give a title to your
uh graph especially if you're going to
present this to somebody you know to
your shareholders in the office and uh
the xlabel is going to be time because
it's a Time series and we didn't
actually put the actual date and times
on here but that's fine we just know
they're incremented by time and then of
course the Y label is the actual stock
price pt. Legend tells us to build the
legend on here so that the color red and
and real Google stock price show up on
there and then the plot shows us that
actual graph so let's go ahead and run
this and see what that looks like and
you can see here we have a nice graph
and let's talk just a little bit about
this graph before we wrap it up here's
our Legend I was telling you about
that's why we have the legend to showed
the prices we have our title and
everything and you'll notice on the
bottom we have a Time sequence we didn't
put the actual time in here now we could
have we could have gone ahead and um
plotted the X since we know what the the
dates are and plotted this to dates but
we also know this only the last piece of
data that we're looking at so last piece
of data which ends somewh where probably
around here on the graph I think it's
like about 20% of the data probably less
than that we have the Google price and
the Google price has this little up jump
and then down and you'll see that the
actual Google instead of a a turn down
here just didn't go up as high and
didn't low go down so our prediction has
the same pattern but the overall value
is pretty far off as far as um stock but
then again we're only looking at one
column we're only looking at the open
price we're not looking at how many
volumes we're Trading like I was
pointing out earlier we talk about stock
just right off the bat there's six
columns there's open high low close
volume then there's WEA uh I mean volume
shares then there's the adjusted open
adjusted High adjusted low adjusted
close they have a special formula to
predict exactly what it would really be
worth based on the value of the stock
and then from there there's all kinds of
other stuff you can put in here so we're
only looking at one small aspect the
opening price of the stock and as you
can see here we did a pretty good job
this curve follows the curve pretty well
it has like a you know little jumps on
it bins they don't quite match up so
this Bend here does not quite match up
with that bend there but it's pretty
darn close we have the basic shape of it
and the prediction isn't too far off and
you can imagine that as we add more data
in and look at different aspects in the
specific domain of stock we should be
able to get a better representation each
time we drill in deeper of course this
took a half hour for my program my
computer to train so you can imagine
that if I was running it across all
those different variables might take a
little bit longer to train the data not
so good for doing a quick tutorial like
this so we're going to dive right into
what is carass we'll also uh go all the
way through this into a couple of
tutorials because that's where you
really learn a lot is when you roll up
your sleeves so we talk about what is
carass carass is a highlevel deep
learning API written in Python for easy
impl implementation of neural networks
uses deep learning Frameworks such as
tensor flow pie torch Etc is backend to
make computation
faster and this is really nice because
as a programmer there is so much stuff
out there and it's evolving so fast it
can get confusing and having some kind
of high level order in there we can
actually view it and easily program
these different neural networks uh is
really powerful it's really powerful to
to um have something out really quick
and also be able to start testing your
models and seeing where you're going so
cross works by using complex deep
learning Frameworks such as tensorflow
pytorch um ml play Etc as a back end for
fast computation while providing a
userfriendly and easy tolearn front end
and you can see here we have the carass
API uh specifications and under that
you'd have like TF carass for tensorflow
thano coros and so on and then you have
your tensorflow workflow that this is
all sitting on top top
of and this is like I said it organizes
everything the heavy lifting is still
done by tensor flow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to um dig as deeply into the
heavy-end stuff while still having a
very robust package you can get up and
running rather quickly and it doesn't
distract from the processing time
because all the heavy lifting is done by
packages like tensor flow this is the
organization on top of it so the working
principle of
carass uh the working principle of
carass is carass uses computational
graphs to express and evaluate
mathematical
Expressions you can see here we put them
in blue they have the expression um
expressing complex problems as a
combination of simple mathematical
operators uh where we have like the
percentage or in this case in Python
that's usually your uh left your um
remainder or multiplication uh you might
have the oper Ator of x uh to the power
of3 and it uses useful for calculating
derivatives by using uh back propagation
so if we're doing with neural networks
when we send the error back up to figure
out how to change it uh this makes it
really easy to do that without really
having not banging your head and having
to handr write everything it's easier to
implement distributed computation and
for solving complex problems uh specify
input and outputs and make sure all
nodes are connected and so this is
really nice as you come in through is
that um as your layers are going in
there you can get some very complicated
uh different setups nowadays which we'll
look at in just a second and this just
makes it really easy to start spinning
this stuff up and trying out the
different models so when we look at
caros models uh carass model we have a
sequential model sequential model is a
linear stack of layers where the
previous layer leads into the next
layer and this if you've done anything
else even like the sklearn with their
neural networks and propagation and any
of these setups this should look
familiar you should have your input
layer it goes into your layer one layer
two and then to the output layer and
it's useful for simple classifier
decoder models and you can see down here
we have the model equals a coros
sequential this is the actual code you
can see how easy it is uh we have a
layer that's dense your layer one has an
activation they're using the ru in this
particular example and then you have
your name layer one layer dense Ru name
Layer Two and so forth uh and they just
feed right into each other so it's
really easy just to stack them as you
can see here and it automatically takes
care of everything else for you and then
there's a functional model and this is
really where things are at this is new
make sure you update your caros or
you'll find yourself running this um
doing the functional model you'll run
into an error code because this is a
fairly new release and he uses
multi-input and multioutput put model
the complex model which Forks into two
or more branches and you can see here we
have our image inputs equals your carass
input shape equals 32x 32 by 3 you have
your dense layers dense 64 activation
railu this should look similar to what
you already saw before uh but if you
look at the graph on the right it's
going to be a lot easier to see what's
going on you have two different
inputs uh and one way you could think of
this is maybe one of those is a small
image and one of those is a full-sized
image and that feedback goes into you
might feed both of them into one Noe
because it's looking for one thing and
then only into one Noe for the other one
and so you can start to get kind of an
idea that there's a lot of use for this
kind of split and this kind of setup uh
where we have multiple information
coming in but the information's very
different even though it overlaps and
you don't want it to send it through the
same neural network um and they're
finding that this train faster and is
also has a better result depending on
how you split the data up and and how
you Fork the models coming
down and so in here we do have the two
complex uh models coming in uh we have
our image inputs which is a 32x 32x 3
your three channels or four if you're
having an alpha channel uh you have your
dense your layers dense is 64 activation
using the ru very common uh x equals
dense inputs X layers dense x64
activation equals Ru X outputs equals
layers dense 10 X model equals coros
model inputs equals inputs outputs
equals outputs name equals n
model uh so we add a little name on
there and again this is this kind of
split here this is setting us up to um
have the input go into different areas
so if you're already looking at corus
you probably already have this answer
what are neural networks uh but it's
always good to get on the same page and
for those people who don't fully
understand neural networks to dive into
them a little bit or do a quick overview
neural networks are deep learning
algorithms modeled after the human brain
they use multiple neurons which are
mathematical operations to break down
and solve complex mathal
problems and so just like the neuron one
neuron fires in and it fires out to all
these other neurons or nodes as we call
them and eventually they all come down
to your output layer and you can see
here we have the really standard graph
input layer here hidden layer and an
output layer one of the biggest parts of
any data processing is your data
pre-processing uh so we always have to
touch base on that with a neural network
like many of these models they're kind
of uh when you first start using them
they're like a black box you put your
data in you train it and you test it and
see how good it was and you have to
pre-process that data because bad data
in is uh bad output puts so in data
pre-processing we will create our own
data examples set with carass the data
consists of a clinical trial conducted
on 2100 patients ranging from ages 13 to
100 with a the patients under 65 and the
other half over 65 years of age we want
to find the possibility of a patient
experiencing side effects due to their
age and you can think of this in today's
world with uh covid uh what's going to
happen on there and we're going to go do
an example of that in our uh live
handson like I said most of this you
really need to have Hands-On to
understand so let's go ahead and bring
up our anaconda and uh open that up and
open up a Jupiter notebook for doing the
python code in now if you're not
familiar with those you can use pretty
much any of your uh setups I just like
those for doing demos and uh showing
people especially share holders it
really helps it's a nice visual so let
me go and flip over to our anaconda and
the Anaconda has a lot of cool to tools
they just added datal lore and IBM
Watson Studio clad into the Anaconda
framework but we'll be in the Jupiter
lab or Jupiter notebook um I'm going to
do jupyter notebook for this because I
use the lab for like large projects with
multiple pieces because it has a
multiple tabs or the notebook will work
fine for what we're doing and this opens
up in our browser window because that's
how Jupiter notbook sorry Jupiter
notebook is set to run and we'll go
under new create a new Python 3 and uh
it creates an Untitled python we'll go
ahead and give this a title and we'll
just call this uh
cross
tutorial and let's change that to
Capital there we go we go and just
rename that and the first thing we want
to go ahead and do is uh get some
pre-processing tools involved and so we
need to go ahead and import some stuff
for that like our numpy do some random
number
Generation Um I mentioned sklearn or
your s kit if you're installing sklearn
the sklearn stuff it's a s kit you want
to look
up that should be a tool of anybody who
is um doing data science if you if
you're not if you're not familiar with
the sklearn
toolkit it's huge uh but there's so many
things in there that we always go back
to and we want to go ahead and create
some train labels and train samples uh
for training our
data and then just a note of what we're
we're actually doing in here uh let me
go ahead and change this this is kind of
a fun thing you can do we can change the
code to
markdown and then markdown code is nice
for doing examples once you've already
built this uh our example data we're
going to do
experimental there we go experimental
drug was tested on 2100 individuals
between 13 to 100 years of age half the
participants are under 65 and 95% of
participants are under 65 experien no
side effects well 95% of participants
over 65 um experience side effects so
that's kind of where we're starting at
um and this is just a real quick example
because we're going to do another one
with a little bit more uh complicated
information uh and so we want to go
ahead and
generate our setup
uh so we want to do for I in range and
we want to go ahead and create if you
look here we have random
integers train the labels a pin so we're
just creating some random
data uh let me go ahead and just run
that and so once we've created our
random data and if you if I mean you can
certainly ask for a copy of the code
from Simply learn they'll send you a
copy of this or you can zoom in on the
video and see how we went ahead and did
our train samples a pin um
and we're just using this I do this kind
of stuff all the time I was running a
thing on uh that had to do with errors
following a bell-shaped curve on uh a
standard distribution error and so what
do I do I generate the data on a
standard distribution error to see what
it looks like and how my code processes
it since that was the Baseline I was
looking for in this we're just doing uh
uh generating random data for our setup
on
here and uh we could actually go in uh
print some of the data up let's just do
this
print um we'll do
[Music]
train
samples and we'll just gen do the first
um five pieces of data in there to see
what that looks like and you can see the
first five pieces of data in our train
samples is 49 85 41 68 19 just random
numbers generated in there that's all
that is uh and we generated
significantly more than that um let's
see 50 up here 1,000 yeah so there's
1,000 here 1,000 numbers we generated
and we could also if we wanted to find
that out we could do a quick uh print
the length of
it and so or you could do a shape kind
of thing and if you're using
numpy although the link for this is just
fine and there we go it's actually 2100
like we said in the demo setup in
there and then we want to go ahead and
take our labels oh that was our train
labels we also did samples didn't
we uh so we could also print do the same
thing oh
labels um let's change this
to
labels and
[Music]
labels and run that just to double check
and sure enough we have 2100 and they're
labeled on 0 1 0 1 0 I guess that's if
they have symptoms or not one symptoms
uh Zer none and so we want to go ahead
and take our train labels and we'll
convert it into a numpy array and the
same thing with our samples and let's go
ahead and run that and we also Shuffle
uh this is just a neat feature you can
do in uh numpy right here put my drawing
thing on which I didn't have on earlier
um I can take the data and I can Shuffle
it uh so we have our so it's it just
randomizes it that's all that's doing um
we've already randomized it so it's kind
of an Overkill it's not really
necessary but if you're doing uh a
larger package where the data is coming
in and a lot of times it's organized
somehow and you want to randomize it
just to make sure that that you know the
input doesn't follow a certain pattern
uh that might create a bias in your
model and we go ahead and create a
scaler uh the scaler range uh min
minimum Max scaler feature range 0 to
one uh then we go ahead and scale the uh
scaled train samples we're going to go
ahead and fit and transform the data uh
so it's nice and scaled and that is the
age uh so you can see up here we have 49
8541 we're just moving that so it's
going to be uh between zero and one and
so this is true with any of your neural
networks you really want to convert the
data uh to zero and one otherwise you
create a bias uh so if you have like a
100 creates a bias
versus the math behind it gets really
complicated um if you actually start
multiplying stuff there a lot of
multiplication addition going on in
there that higher end value will
eventually multiply down and it will
have a huge bias as to how the model
fits it and then it will not fit as well
and then one of the fun things we can do
in Jupiter notebook is that if you have
a variable you're not doing anything
with it it's the last one on the line it
will automatically
print um and we're just going to look at
the first five samples on here it's just
going to print the first five samples
and you can see here we go uh 9 95. 791
so everything's between 0 and one and
that just shows us that we scaled it
properly and it looks good uh it really
helps a lot to do these kind of print
UPS halfway through uh you never know
what's going to go on there
I don't know how many times I've gotten
down and found out that the data sent to
me that I thought was scaled was not and
then I have to go back and track it down
and figure it out on
there uh so let's go ahead and create
our artificial neural
network and for doing that this is where
we start diving into tensor flow and
carass uh tensor
flow if you don't know the history of
tensor
flow it helps to uh jump into we'll just
use
Wikipedia careful don't quote Wikipedia
on these things because you get in
trouble uh but it's a good place to
start uh back in 2011 Google brain built
disbelief as a proprietary machine
learning setup tensor flow became the
open source for it uh so tensorflow was
a Google product and then it became uh
open sourced and now it's just become
probably one of the defacto when it
comes from neural networks as far as
where we're at uh so when you see the
tensor flow
setup it it's got like a huge following
there are some other setups like a um
the S kit under the sklearn has our own
little neural network uh but the
tensorflow is the most robust one out
there right now and carass sitting on
top of it makes it a very powerful tool
so we can leverage both the carass uh
easiness in which we can build a
sequential setup on top of tensor flow
and so in here we're going to go ahead
and do our input of tensor flow uh and
then we have the rest of this is all
carass here from number two down uh
we're going to import from tensor flow
the coros uh connection and then you
have your tensor flow across models
import sequential it's a specific kind
of model we'll look at that in just a
second if you remember from the files
that means it goes from one layer to the
next layer to the next layer there's no
funky splits or anything like that
uh and then we have from tensorflow
Cross layers we're going to import our
activation and our dense
layer and we have our Optimizer atom um
this is a big thing to be aware of how
you optimize uh your data when you first
do it atom's as good as any atom is
usually uh there's a number of Optimizer
out there there's about uh there's a
couple main ons but atom is usually
assigned to bigger data
uh it works fine usually the lower data
does it just fine but atom is probably
the mostly used but there are some more
out there and depending on what you're
doing with your layers your different
layers might have different activations
on them and then finally down here
you'll see um our setup where we want to
go ahead and use the
metrics and we're going to use the
tensorflow cross metrics um for
categorical cross centropy uh so we can
see how everything performs when we're
done that's all that is um a lot of
times you'll see us go back and forth
between tensor flow and then scikit has
a lot of really good metrics also for
measuring these things um again it's the
end of the you know at the end of the
story how good does your model do and
we'll go ahead and load all that and
then comes the fun part um I actually
like to spend hours messing with these
things and uh four lines of code you're
like ah you're going to spend hours on
four lines of code um no we don't spend
hours on four lines of code code that's
not what we're talking about when I say
spend hours on four lines of code uh
what we have here I'm going to explain
that in just a second we have a model
and it's a sequential model if you
remember correctly we mentioned the
sequential up here where it goes from
one layer to the next and our first
layer is going to be your
input it's going to be uh what they call
D which is um you usually it's just D
and then you have your input and your
activation um how many units are coming
in we have 16 uh what's the shape What's
the activation and this is where it gets
interesting uh because we have in here
uh
railu on two of these and softmax
activation on one of these there are so
many different options for what these
mean um and how they function how does
the ru how does the softmax
function and they do a lot of different
things um we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different
activations um and just some of it is
just U um almost like you're playing
with it like an artist you start getting
a fill for like a uh inverse tangent
activation or the tan
activation takes up a huge processing
amount uh so you don't see it a lot yet
it comes up with a better solution
especially when you're doing uh when
you're analyzing Word documents and
you're tokenizing the words and so
you'll see the shift from one to the
other because you're both trying to
build a better model and if you're
working on a huge data set um it'll
crash the system it'll just take too
long to process um and then you see
things like soft Max uh soft Max
generates an interesting um setup where
a lot of these when you talk about rayu
oops let me do this rayu there we go
rayu has um a setup where if it's less
than zero it's zero and then it goes up
um and then you might have what they
call lazy uh setup where it has a slight
negative to it so that the errors can
translate better same thing with softmax
it has a slight laziness to it so that
errors translate better all these little
details make a huge different on your
model um so one of the really cool
things about data science that I like is
you build your uh what they call you
build theil and it's an interesting uh
design setup oops I forgot the end of my
code
here the concept of build a fail is you
want the model as a whole to work so you
can test your model
out so you can do uh you can get to the
end and you can do your let's see where
was it overshot down here you can test
your test out the the quality of your
setup on there and see where did I do my
tensor flow oh here we go I did it was
right above me here we go we start doing
your cross enty and stuff like that is
you need a full functional set of code
so that when you run
it you can then test your model out and
say hey it's either this model works
better than this model and this is why
um and then you can start swapping in
these models and so when I say spend a
huge amount of time on pre-processing
data is probably 80% of your programming
time um well between those two it's like
8020 you'll spend a lot of time on the
models once you get the model down once
you get the whole code and the flow down
uh set depending on your data your
models get more and more robust as you
start experimenting with different
inputs different data streams and all
kinds of things and we can do a simple
model summary
here uh here's our quential here's our
layer output our
parameter this is one of the nice things
about coros is you just you can see
right here here's our sequential one
model boom boom boom boom everything's
set and clear and easy to read so once
we have our model built uh the next
thing we're going to want to do is we're
want to go ahead and train that
model and so the next step is of course
model
training and when we come in here this a
lot of times it's just paired with the
model because it's so straightforward
it's nice to print out the model setup
so you can have a
tracking but here's our model uh the
keyword in Cross is
compile Optimizer atom learning rate
another term right there that we're just
skipping right over that really becomes
the meat of uh the setup is your
learning rate uh so whoops I forgot that
I had an arrow but just underl
it a lot of times the learning rate set
to 0.0 uh set to 0.01 uh depending on
what you're doing this learning rate um
can overfit and underfit uh so you'd
want to look up I know we have a number
tutorials out on overfitting and
underfitting that are really worth
reading once you get to that point in
understanding and we have our loss um
sparse categorical cross entropy so this
is going to tell carass how far to go
until it stops and then we're looking
for metrics of accuracy so we'll go
ahead and run that and now that we've
compiled our model we want to go ahead
and um run it fit it so here's our model
fit um we have our scaled train
samples our train labels our validation
split um in this case we're going to use
10% of the data for
validation uh batch size another another
number you kind of play with not a huge
difference as far as how it works but it
does affect how long it takes to run and
it can also affect the bias a little bit
uh most of the time though a batch size
is between 10 to 100 um depending on
just how much data you're processing in
there we want to go ahead and Shuffle it
uh we're going to go through 30 epics
and uh put a verbose of two let me just
go and run this and you can see right
here here's our epic here's our training
um here's our l Lo now if you remember
correctly up here we set the loss see
where was it um compiled our
data there we go loss uh so it's looking
at the sparse categorical cross entropy
this tells us that as it goes how how
how much um how how much does the um
error go down uh is the best way to look
at that and you can see here the lower
the number the better it just keeps
going down and vice versa accuracy we
want let's see where's my
accuracy value accuracy at the end uh
and you can see 619 69. 74 it's going up
we want the accuracy would be ideal if
it made it all the way to one but we
also the loss is more important because
it's a balance um you can have 100%
accuracy in your model doesn't work
because it's overfitted uh again you
won't look up overfitting and
underfitting model
models and we went ahead and went
through uh 30 epics it's always fun to
kind of watch your code going um to be
honest I usually uh um the first time I
run it I'm like oh that's cool I get to
see what it does and after the second
time of running it I'm like I like to
just not see that and you can repress
those of course in your code uh repress
the warnings in the
printing and so the next step is going
to be building a test set and predi ing
it now uh so here we go we want to go
ahead and build our test set and we have
just like we did our training
set a lot of times you just split your
your initial set up uh but we'll go
ahead and do a separate set on here and
this is just what we did above uh
there's no difference as far as
um the randomness that we're using to
build this set on here uh the only
difference is
that we are already uh did scaler up
here well it doesn't matter because the
the data is going to be across the same
thing but this should just be just
transform down here instead of fit
transform uh because you don't want to
refit your data um on your testing
data there we go now we're just
transforming it because you never want
to transform the test data um easy
mistake to make especially on an example
like this where we're not doing um you
know we're randomizing the data anyway
so it doesn't matter too much because
we're not expecting something
weird and then we go ahead do our
predictions the whole reason we built
the model is we take our model we
predict and we're going to do here's our
xcal data batch size 10 verbose and now
we have our predictions in here and we
could go ahead and do a um oh we'll
print
predictions and then I guess I could
just put down predictions and five so we
can look at the first five of the
predictions and what we have here is we
have our
age and uh the prediction on this age
versus what is what we think it's going
to be what what we think is going to
they going to have uh symptoms or not
and the first thing we notice is that's
hard to read because we really want a
yes no answer uh so we'll go ahead and
just uh round off the
predictions using the argmax um the
numpy argmax uh for prediction so it
just goes to
a01 and if you remember this is a
Jupiter notebook so I don't have to put
the print I can just put in uh rounded
predictions and we'll just do the first
five and you can see here 0 one 0 0 0 so
that's what the predictions are that we
have coming out of this um is no
symptoms symptoms no symptoms symptoms
no symptoms and just as uh we were
talking about at the beginning we want
to go ahead and um take a look at this
there we go confusion matrixes for
accuracy check um most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the S kit um the SK learn metrics uh
pyit being where that comes from import
confusion Matrix uh some iteration tools
and of course a nice map plot library
that makes a big difference so it's
always nice to
um have a nice graph to look at um
picture is worth a a thousand
words um and then we'll go ahead call it
CM for confusion Matrix y true equals
test labels y predict rounded
predictions and we'll go ahead and load
in our
cm and I'm not going to spend too much
time on the plotting um going going over
the different plotting
code um you can spend uh like whole we
have whole tutorials on how to do your
different plotting on there uh but we do
have here is we're going to do a plot
confusion Matrix there's our CM our
classes normalized false title confusion
Matrix cmap is going to be in
blues and you can see here we have uh to
the nearest cmap titles all the
different pieces whether you put tick
marks or not the mark the classes a
color bar um so a lot of different
information on here as far as how we're
doing the printing of the of the
confusion Matrix you can also just dump
the confusion Matrix um into a caborn
and real quick get an output it's worth
knowing how to do all this uh when
you're doing a presentation to the
shareholders you don't want to do this
on the Fly you want to take the time to
make it look really nice uh like our
guys in the back did and uh let's go
ahead and do this forgot to put together
our cm plot labels we'll go and run
that and then we'll go ahead and call
the little the
definition for our
mapping and you can see here plot
confusion Matrix that's our the the
little script we just wrote and we're
going to dump our data into it um so our
confusion Matrix our classes um title
confusion Matrix so let's just go ahead
and run
that and you can see here we have our
basic setup uh no side effects
195 had side effects 200 no side effects
that had side effects so we predicted
the 10 of them who actually had side
effects and that's pretty good I mean I
I don't know about you but you know
that's 5% error on this and this is
because there's 200 here that's where I
get 5% is uh divide these both by by two
and you get five out of 100 uh you can
do the same kind of math up here not as
quick on the flight it's 15 and 195 not
an easily rounded number but you can see
here where they have 15
people who predicted to have no uh with
the no side effects but had side effects
kind of setup on there and these
confusion Matrix are so important at the
end of the day this is really where
where you show uh whatever you're
working on comes up and you can actually
show hey this is how good we are or not
how messed up it is
so this was a uh I spent a lot of time
on some of the parts uh but you can see
here is really simple uh we did the
random generation of data but when we
actually built the model coming up here
uh here's our model
summary and we just have the layers on
here that we built with our model on
this and then we went ahead and trained
it and ran the prediction now we can get
a lot more complicated uh let me flip
back on over here because we're going to
do another uh demo so that was our basic
introduction to it we talked about the
uh oops here we go okay so implementing
a neural network with carass after
creating our samples and labels we need
to create our carass neural network
model we will be working with a
sequential model which has three layers
and this is what we did we had our input
layer our hidden layers and our output
layers and you can see the input layer
uh coming in uh was the age Factor we
had our hidden layer and then we had the
output are you going to have symptoms or
not so we're going to go ahead and go
with something a little bit more
complicated uh um training our model is
a two-step process we first compile our
model and then we train it in our
training data set uh so we have
compiling compiling converts the code
into a form of understandable by
Machine we use the atom in the last
example a gradient descent algorithm to
optimize a model and then we trained our
model which means it let it uh learn on
training data uh and I actually had a
little backwards there but this is what
we just did is we if you remember from
our Cod we just had o let me go back
here
um here's our model that we
created
summarized uh we come down here and we
compile it so it tells it hey we're
ready to build this model and use it uh
and then we train it this is the part
where we go ahead and fit our model and
and put that information in here and it
goes through the training on there and
of course we scaled the data which was
really important to do and then you saw
we did the creating a confusion Matrix
with carass um as we are performing
classifications on our data we need a
confusion Matrix to check the results a
confusion Matrix breaks down the various
misclassifications as well as correct
classifications to get the
accuracy um and so you can see here this
is what we did with the true positive
false positive true negative false
negative and that is what we went over
let me just scroll down
here on the end we printed it out and
you can see we have a nice print out out
of our confusion Matrix uh with the true
positive false positive false negative
true negative and so the blue ones uh we
want those to be the biggest numbers
because those are the better side and
then uh we have our false predictions on
here uh as far as this one so it had no
side effects but we predicted let's see
no side effects predicting side effects
and vice versa we'll start with a very
general concept of what is deep learning
this is where we take large volumes of
data in this case on cats and dogs or
whatever A lot of times you use um a
training setup to train your model
remember it's kind like a magic Black
Box going on there then we use that to
extract features or extract information
and in this case classify the image of a
cat and a dog so the primary takeaway
we're talking about deep learning is it
learns from large volumes of structured
and even unstructured data and uses
complex algorithms to train neural
network it also performs complex
operations to extract hidden patterns
and features and if we're going to
discuss deep learning in this very uh
simplified overview and we also have to
go over what is a neural network this is
a common image you'll see of a drawing
of a forward propagation neural network
and it's it's a human brain inspired
system which replicate the way humans
learn so this is inspired how our own
neurons in our brain fire but at a much
simplified level obviously it's not
ready to take over the human U
population and and be our leader yet not
for many years it's very much in its
infant stage but it's inspired by how
our brains work
um and they use a lot of other
Inspirations you can study brains of
moths and other animals that they've
used to figure out how to improve these
neural networks the most common one
consists of three layers of network and
this is generally how you view these
networks is you have an input you have a
hidden layer and an output and the
neural network is uh broken up into many
pieces but when we focus just on the
neural network it's always on the hidden
layers that we making all the
adjustments and figuring out how to best
set up those hidden layers for their
functions to both train faster and to
function better when we look at this of
course we have our input hidden and
output each layer contains neurons
called as nodes perform various
operations and you can see here we have
the list of the nodes we have both our
input nodes and our output nodes and
then our hidden layer nodes and it's
used in deep learning algorithm like CNN
RNN G Etc we'll address some of these
models a little closer at least the most
common models as we go down the list and
we study the Deep learning and the
neural network framework
let's start with what is a multi-layer
patron or MLP a lot of time is it
referred to and you'll see these
abbreviations I'll be honest I have to
write them down on a piece of paper and
go through them because I never remember
what they all mean even though I play
with them all the time what is a
multi-layer patron well if you look at
the image on the right it's very similar
to what we just looked at you have your
input layer your hidden layer and your
output layer and that's exactly what
this is it has the same structure of a
single layer Perron with one or more
hidden layers except the input layer
each node in the other layers uses a
nonlinear activation function what that
means is your input layers your data
coming in and then your activation
function is based upon all those nodes
and weights being added together and
then it has the output MLP uses
supervised learning method called back
propagation for training the model very
key word there is back propagation
single layer Perron can classify only
linear separable classes with binary
output 01 but the MLP can classify
nonlinear classes so let's break this
down just a little bit the multi-layer
Perron with an input layer and a hid
layer and an output layer as you see
that it comes in there it has adds up
all the numbers and weights depending on
how your setup is that then goes to the
next layer that then goes to the next
hidden layer if you have multiple hidden
layers and finally to the output layer
the back propagation takes the error
that it sees so whatever the output is
it says hey this has an error to it it's
wrong and then sends that error
backwards from where came from and
there's a lot of different functions
used to uh train this based on that
error and how that error goes backwards
in the notes uh so forward is you get
your answers backward is for training
you see this every day even my uh Google
pixel phone has this it they train the
neural network which takes a lot more
data to train than it does to use and
then they load up that neural network
into in this case I have a pixel 2 which
actually has a built-in neural network
for processing pictures and so it's just
the Ford propag ation I use when it
processes my photos but when they were
training it you use a back propagation
to train it with the errors they had
we'll be coming back to different models
that are used for right now though
multi-layer Perron MLP put that down as
your vocabulary word and of course back
propagation what is data normalization
and why do we need it this is so
important we spend so much time in
normalizing our data and getting our
data clean and setting it up uh so we
talk about data there's a pre-processing
step to standardize the data so whatever
we have coming in we don't want it to be
a uh you know 1 gbyte file here a 2 gab
picture here and a 3 kilobyte text there
even as a human I can't process those
all in the same group I have to reformat
them in some way that Loops them
together so that a standardized format
we use this uh data normalization in
pre-processing to reduce it and
eliminate data redundancy a lot of times
the data comes in and you end up with
two of the same images or uh uh the same
information in different formats then we
want to rescale values to fit into a
particular range for achieving better
convergence what this means is with most
neural networks they form a bias we've
seen this in recently in attacks on
neural networks where they light up one
pixel or one piece of the view and it
skews the whole answer so suddenly um
because one pixel is really bright uh it
doesn't know what to do well when we
start rescaling it we put all the values
between say minus one and one and we
change them and refit them to those
values it helps get rid of that bias
helps fix for some of those problems and
then finally we restructure the data and
improve the Integrity we want to make
sure that we're not missing values um or
we don't have partial data coming in one
way to look at this is uh bad data in
bad data out so we want clean data in
and you want good answers coming out one
of the most basic models used is a
boltzman machine so let's address what
is a boltzman machine and if you know we
just did the MLP multi-layer prron so
now we're going to come into almost a
simplified version of that and in this
we have our visible input layer and we
have our hidden layer the boltman
machines are almost always shallow
they're usually just two layer neural
Nets that make stochastic decisions
whether a neuron should be on or off
true false yes no first layer is a
visible layer and second layer is the
hidden layer nodes are connected to each
other across layers but no two nodes of
the same layer are connected hence it is
also known as restricted boltzman
machine now that we've covered a basic
MLP or multi-layer Patron and we've gone
over the boltzman machine also known as
the restricted boltzman machine let's
talk a little bit about activation
formulas and this is a huge topic that
can get really complicated but it also
is automated so it's very simple so you
have both a complicated and a simple at
the same time so what is the role of
activation functions in a neural network
activation function decides whether a
neuron should be fired or not that's the
most basic one and that actually changes
a little bit because it's either whether
fired or not in this case activation
function or what value should come out
when it's fired but in these models
we're looking at just the bolts men
restricted layers so this is what causes
them to fire either they don't or they
do it's a yes or no true false All or
Nothing it accepts the weighted sum of
the inputs to bias as input to any
activation function so whatever our
activation function is it needs to have
the sum of the weight we times the input
so each input if you remember on that
model and let's just go back to that
model real quick and then you always
have to add a bias and you can look at
the bias if you remember from your ukian
geometry you draw a straight line
formula for that line has a y-coordinate
at the end it's always um CX plus M or
something like that where m is where it
crosses the Y coordinates if you're
doing a straight line with these weights
it's very similar but a lot of times we
just add it in as its own weight we take
it as a node of a one value coming in
and then we compute its new weight and
that's how we compute that bias just
like we compute all the other weights
coming in the node which gets fires
depends on the Y value and then we have
a step function and the step function
this is where remember I said it's going
to get complicated and simple all at the
same time we have a lot of different
step functions we have the sigmoid
function we have just the standard step
function we have the um rayu is
pronounced like Ray the ray from the Sun
and L like a name so rayu function
function and we have the tangent H
function and if you look at these they
all have something similar they all
either force it to be um one value or
the other they force it to be in the
case of the first three a zero or one
and in the last one it's either a minus
one or one and you can easily convert
that to a 0 one yes no true false and on
this one of the most common ones is the
step function itself because there is no
middle value there is no um uh
discrepancy that says well I'm not quite
sure but as you get into different
models probably the most commonly used
it used to be the sigmoid was most
commonly used but I see the ru used more
often really depending on what you're
doing you just have to play with these
and find out which one works best
depending on the data in your output the
reason to have a non Z1 answer or
something kind of in the middle is when
you're looking at this and it's coming
out you can actually process that middle
ground as part of the answer into
another neural network so it might be
that The Rao function says hey this is
only a point six not a one and uh even
though the one is what's going into the
next neural network or the next hidden
layer as an input the 6 value might also
be going in there to let you know hey
this is not a straight up one or
straight up zero someplace in the middle
this is a little uncertain what's coming
out here so it's a very powerful tool
but in the basic neural network you
usually just use the step function it's
yes or no let's take a um a big step
back and take a kind of an overview the
next function is what is a cost function
that we're going to cover this is so
important because this is your end
result that you're going to do over and
over again and use to decide whether the
model is working or not whether you need
to try a different step function whether
you need to try a different activation
whether you need to try a fully
different model used uh so what is the
cost function cost function is a measure
to evaluate how good your model's
performance is it is also referred as
loss or error used to compute the error
of the output layer during back
propagation there's our back propagation
where we're training our model that's
one of our key words mean squared error
is an example of a popular cost function
and so here we have the cost function C
equals half of Yus y predicted um and
then you square that so the first thing
is um you know real quick if you haven't
done statistics this is not a percentage
it's not a percentage of how accurate it
is it's just a measurement of the error
and we take that error for training it
and we push that error backwards through
the neural Network and we use that
through the different training functions
depending on what model you're using to
train the neural network so when you
just Ploy the network you're usually
done training it because it takes a lot
of computational force to train it um
this is a very simple model and so you
deploy the trained one uh but we want to
know how your error is and so how do we
do that well you split your data part of
your data is for training and part of
your data is for testing and then we can
also test the error on there so it's
very important and then we're going to
go one more step on this we got to look
look at both the local and the global
setup it might work great to test your
data on what you have on your computer
but that's different than in the field
so when we're talking about all these
different tests and the error test as
far as your loss you don't you want to
make sure that you're in a closed
environment when you do initial testing
but you also want to open that up and
make sure you follow up with the testing
on the larger scale of data because it
will change it might not fit the larger
scale there might be something in there
in the way you brought the data in
specifically or the data group you used
or um any of those could cause an error
so it's very important to remember that
we're looking at both the local and the
global context of our error and just one
other side note on a lot of the newer
models of neural networks by comparing
the error we get on the data our
training data with a portion of the test
data we can actually figure out how good
the model is whether it's overfitted or
not we'll go into that a little bit more
as we go into some of the different
models so we have our output we're able
to um figure out the error on it based
on the Square means usually although
there's other uh functions used so we
want to talk about what is gradient
descent another vocabulary word gradient
descent is an optimation algorithm to
minimize the cost function or to
minimize the error aim is to find the
local or Global Minima of a function
determine the direction the model should
take to reduce the error so as we're
looking at this we have our uh squared
error that we just figured out the co
based on the cost function it says how
bad is my model fitting the data I just
put through it and then we want to
reduce that error so how do you figure
out what direction to do that in well
you could be that you're looking at just
that line of that line of data coming in
so that would be a local Minima we want
to know the error of that particular
setup coming in and then you have your
Global your Global Minima we want to
minimize it based on the overall data
we're putting through it and with this
we can figure out the global minimum
cost we want to take all those local
minimum costs of each piece of data
coming in and figure out the global one
how are we going to adjust this model to
fit all the data we don't want it to be
biased just on three or four lines of
data coming in we want it to kind of
extrapolate a general answer for all the
data coming in at this of course uh we
mentioned it briefly about back
propagation this is where really comes
in handy is training our model neural
network technique to minimize the cost
function helps to improve the
performance of the network back
propagates the error and updates the
weights to reduce the error so as you
can see here is a very nice depiction of
a back prop a we have our U predicted y
coming out and then we have since it's a
training set we already know the answer
and the answer comes back and based on
case of the square means was one of the
functions we looked at uh one of the
activation functions based on cost
function that cost function then
depending on what you choose for your
back propagation method and there's a
number of them will change the weights
it will change the weight going to each
of one of those nodes in the hidden
layer and then based upon the error
that's still being carried back it'll
change the the weight's going to the
next hidden layer and then it computes
an error level on that and sends that
back up and you're going to say well if
it computes the error into the first
hidden layer and fixes it why would it
stop there well remember we don't want
to create a biased neural network so we
only make small adjustments on these
weights we don't make a big adjustment
that changes everything right off the
bat so no matter how far back you go
you're always going to have a small
amount of error and that's still going
to continue to go all the way back up
the hidden layers for right now focus on
the back propagation is taking that
error and moving it backwards on the
neural network to change the weights and
help program it so that it'll have the
correct answers so far we've been
talking about forward propagation naral
networks everything goes forwards goes
left to right uh but let's let's take a
little detep tour and let's see what is
the difference between a feed forward
neural network and a recurrent neural
network now this is in the function not
when we're training it using the back
propagation so you've got new
information coming in and you want to
get the answer and there's a couple
different networks out there and we want
to know we have a feed forward neural
network and we have a new uh vocabulary
term recurrent neural network a feed
forward neural network signals travel in
one direction from input to Output no
feedback loops considers only the
current input cannot memorize previous
inputs one example of one of these feed
forward neural networks and we've
covered a number of them but one of the
ones this has a big highlight nowadays
is the CNN a convolutional neural
network 10 tensor flow the one put out
by Google is probably most known for
their CNN where the information goes
forward it uh first takes a picture
splits it apart goes through the
individual pixels on the picture so it
picks up a different reading then
calculates based on that goes into a
regular feed forward neural network and
then gives your categorization on there
now we're not covering the CNN today but
we do have a video out that you can look
up on YouTube put out by simply learn
the convolutional neural network
wonderful tutorial check that out and
learn a lot more about the convolutional
neural network but you do need to know
that the CNN is a forward propagation
neural network only so it's only moving
in One Direction so we want look at a
recurrent neural network signals travel
in both directions making it a looped
Network considers the current input
along with the previous received inputs
for generating the output of a layer has
the ability to memorize past data due to
its internal memory and you can see they
have a nice uh image here we have our um
input and for some reason they always do
the recurrent neural network um in
Reverse from bottom up in the images
kind of a standard although I'm not sure
why your X goes into your hidden layer
and your hidden layer the answer for
part of the answer from that it
generates feeds back into the hidden
layer so now you have an input of both X
and part of the Hidden layer and then
that feeds into your output now if we go
back to the forward let me just go back
a slide and we're looking at uh our
forward propagation Network one of the
tricks you can do to use just a forward
propag vation network is if you're in a
what they call a Time sequence that's a
good uh term to remember or a Time
series meaning that a sequential data
each term comes after the other you can
trick this by creating your input nodes
as with the history so if you know that
uh you have values one five and seven
going in and you know what the output is
from one what those outputs are you can
expand the input to include the history
input that's one of the ways to trick a
forward propagation Network into looking
at that but when you do with the
recurrent neural network you let the
hidden layer do that for you it sends
that data and reprocesses it back into
itself what are some of the applications
of recurrent neural network the RNN can
be used for sentiment analysis and text
mining getting up early in the morning
is good for health it's a positive
sentiment one of the catches you really
want to look at this when you're looking
at the language is that I could switch
this around and totally negate the
meaning of what I'm doing so it no
longer be positive so when you're
looking at a sentence knowing the order
of the words is as important as the
meaning of the words you can't just
count how many good words there are
versus bad words to get positive
sentiment you now have to know what
they're addressing and there's lots of
other different uses uh kids are playing
football or soccer as we call it in the
US RN can help you caption an image So
based on previous information coming in
it refeeds that back in and you have a
uh image Setter and then time series
problems like predicting the prices of
stocks in a month or quarter or or sell
of product can be solved using an RNN
and this is a really good example you
have whatever your stocks were doing
earlier this month will have a huge
effect of what they're doing today if
you're investing so having an RNN Model
A recurrent neural network feeding into
itself what was happening previously
allows it to take that model and program
in that whole series without having to
put in the whole a month at a time of
data you can only put in one day at a
time but if you keep them in order it
will look back and say oh this because
of what happened yesterday I need some
information from that I'm going to use
that to help predict today and so on and
so on we're going to go back to our
activation functions remember I told you
Ru is one of the most common functions
used uh so let's talk a little bit more
about Ru and also softmax softmax is an
activation function that generates the
output between zero and one it divides
each output such that the total sum of
the outputs is equal to one it is often
used in the output layers soft Max L of
the N equals e to L on the N over the
absolute value of e to the L so what
does this function mean I mean what is
actually going on here so we have our
output nodes and our output nodes are
giving us uh let's say they give us
1.2.9 and4 as a human being I look at
that and I say well the greatest value
is 1.2 so whatever category that is if
you have three different categories
maybe you're not just doing if it's a
cat or it's a dog or um oh let's say
it's a cow we had cats and dogs earlier
why the cats and dogs are hanging out
with a cow I don't know but we have a
value and it might say 1.2 is is a cat
09 is a dog and 04 is a cow uh for some
reason it SS that there's a chance of it
being any one of these three items and
that's how it comes out of the output
layer well as a human I can look at 1.2
and say this is definitely what it is
it's definitely a cat or whatever it is
uh maybe it's looking at different kinds
of cars might be a better whether it's a
car truck or a motorcycle maybe that'd
be a better example well from a computer
standpoint that might be a little
confusing because they're just numbers
waving at us and so with the soft Max we
want all those numbers to always add up
to one so when I add three numbers
together I want the final output to be
one on there and so it goes through this
formula changes each of these numbers in
this case it changes them to 046 34 and
.20 they all add up to one and that's a
lot easier to register because it's very
set it's a set output it's never going
to be more than one it's never going to
be less than zero and so you can see
here that there's probably a pretty high
chance that it's the first one so you're
as a human being we have no problem
knowing that but this output can then
also go into to say another input so it
might be an automated car that's picking
up images and it says that image in
front of us is probably a big truck we
should deal with it like it's a big
truck it's probably not a motorcycle um
or whatever those categories are that's
the softmax part of it but now we have
the railu well what's where's the railu
coming from well the railo is what's
generating the 1.2 and the 0.9 and the
04 and so if you remember our railu
stands for rectified linear unit and is
the most widely used activation function
we we looked at a number of different
activation functions including tangent H
the step function remember I said the
step function is really used if that's
what your actual output is because then
you know it's a zero or one but the
railu if you have that as your output
you know have a discrepancy in there and
if that's going into another neural
network or another process having that
discrepancy is really important and it
gives an output of x if x is positive
and zero otherwise so it says my x value
is going to be somewhere between zero or
one and then the uh usually unless it's
really uncertain the output's usually a
one or zero and then you have that
little piece of uncertainty there that
you can send forward to another Network
or you can look at to know that there's
uncertainty involved and is often used
in the hidden layers this is what's
coming out of the Hidden layers into the
output layer usually or as we referen
the convolution neural network the CNN
you'd have to go to another video to
review the ru is the most common used
for convolutional part of that Network
it has a bunch of little pieces that are
very simplified looking at all the
different images or different sections
of the map and the ru works really good
for that like I said there's other
formulas used but that this is the most
common one and you'll see that in the
hidden layer going maybe between one
layer and the next layer so just a quick
recap we have our soft Max which means
that if you have uh numerous categories
only one of them is going to be picked
but you also want to have some value
attached to it how well it picked it and
you put that between zero one so it's
very uh standardized so we have our soft
Max we looked at that let's go back one
we looked at that here where it
transforms in numbers and then we have
our Ru function which takes the
information and the summation and puts
it between a zero and a one where it's
either clearly a zero or depending on
how confident our model is it'll go
between the zero and one value what are
hyperparameters oh this is a great
interview question hyper parameters when
you are doing neural networks this is
what you're playing with most of the
time once you gotten the data formatted
correctly a hyperparameter is a
parameter whose value is set before the
learning process begins determines how a
network is trained and the structure of
the network this includes things like
the number of hidden units how many
hidden layers are you going to have and
how many nodes in each layer learning
rate learning rate is usually multiplied
once you figured out the error and how
much you want to change the weights we
talked about or I mentioned it early
just briefly you don't want to just make
a huge change otherwise you're going to
have a biased model so you only take
little incremental changes and that's
what the learning rate is is those small
incremental changes epics how many times
are you going to go through all the data
in your training set so one Epic is one
trip through all the data and there's a
lot of other things depending on which
model you're working with and which
programming script you're working with
like the python SK learn package will
have it slightly different than say
Google's tensorflow package which will
be a little bit different than the spark
machine learning package so these are
just some examples of the
hyperparameters and so you see in here
we have have a nice image of our data
coming in and we train our model then we
do a comparison to see how good our
model is and then we go back and we say
hey this this model is pretty good but
it's biased so then we send it back and
we change our hyperparameters to see if
we can get an unbiased model or we can
have a better prediction on it that
matches our data closer what will happen
if learning rate is set too low or too
high we have a nice couple graphs here
we have one over here says a learning
rate set too low you can see that it
slowly Works its way down the curve and
on the right you can see a learning rate
set too high it's just bouncing back and
forth when your learning rate is too low
that's what we studied at two slides
over ask what the learning rate was
training of the model will progress very
slowly as we are making very tiny
updates to the weights we'll take many
updates before reaching the minimum
point so I just mentioned epic going
through all the data you might have to
go through all the data a thousand times
instead of 500 times for it to train
learning rate too high causes
undesirable Divergent Behavior to the
loss function due to drastic updates and
weights at times it may fail to converge
or even diverge so if you have your
learning rate set too high and it's
training too quickly maybe you'll get
lucky and it trains after one epic run
but a lot of times it might never be
able to train because the weights are
changing too fast they they flip back
and forth too easy and you see down here
we've introduced uh two new terms
converge and diverge a converge means
that our model has reached a point where
it's able to give a fairly good answer
for all the data we put in all those
weights have adjusted and it's minimized
the error diverg means that the data is
so chaotic that it can never manage to
to train to that data the data is just
too chaotic for it to train so we have
two new words there converge and diverge
are important to know also what is
Dropout and batch normalization Dropout
is a technique of dropping out hidden
and visible units of a network randomly
to prevent overfitting of data it
doubles the number of iterations needed
to converge the network so here we have
our standard neural network and then
after applying Dropout now it doesn't
mean we actually delete the node the
node is still there and we're still
going to use that node what it means is
that we're only going to work with a few
of the notes um a lot of times I think
the most common one right now used is
20% uh so you'll drop out 20% of the
nodes when you do your training you
reverse propagate your data and then
you'll randomly pick another 20 nodes
the next time you go through an epic
data training so each time you go
through one Epic you will randomly pick
of those nodes not to not to mess with
and this allows for less overfitting of
the data so by randomly doing this you
create some I guess it just kind of pull
some nodes off to the Sid it says we're
going to handle the data later on so we
don't overfit batch normalization is a
technique to improve the performance and
stability of neural network the idea is
to normalize the inputs in every layer
so that they have mean output and
activation of zero in standard deviation
of one this question covers a lot of
different things which is great it's a
great uh interview question because it
pulls in that you have to understand
what the mean value is so a mean output
activation of zero that means our
average activation is zero so when you
normalize it remember usually we're
going between minus one and one on a lot
of these it's a very standard setup so
you have to be very aware that this is
your mean output activation of zero and
then we have our standard deviation of
one so we want to keep our error down to
just a one value the benefits of this
doing a batch normalization is it
provides regular ation it trains faster
Higher Learning rates and weights are
easier to initialize what is the
difference between batch gradient
descent and stochastic gradient descent
batch gradient descent batch gradient
computes the gradient using the entire
data set it takes time to converge
because the volume of data is huge and
weights update slowly so you can look at
the batches a lot of times if you're
using big data batch the data in but you
still go through a full epic you still
go through all the data on there so bash
gradient descent means you're going to
use it to fit all the data and look for
a convergence there stochastic gradient
descent stochastic gradient computes the
gradient using a single sample it
converges much faster than bat gradient
because it updates weight more
frequently explain overfitting and
underfitting and how to combat them
overfitting happens when a model learns
the details and noise and the training
data to the degree that it adversely
impacts the execution of the model on
the new information it is more likely to
occur with nonlinear models that have
more flexibility when learning a Target
function an example of this would be um
if you're looking at say cars and trucks
and motorcycles it might only recognize
trucks that have a certain box-like
shape it might not be able to notice a
flatbed truck unless it's only a
specific kind of flatbed truck or only
Ford trucks because that's what it saw
on the training set this means that your
model performs great on your train data
and great on maybe a small test amount
of data but when you go to use it in the
r or World it leaves out a lot and start
is not very functional outside of your
small area your SL laboratory data
coming in underfitting doing the
opposite when you underfit your data
underfitting alludes to a model that is
neither well trained on training data
nor can generalize to new information
usually happens when there is less and
improper data to train a model has a
performance and accuracy so if you're
using underfitted data and you generate
a model and you distribute that in a
commercial Zone you'll have a lot of
people unhappy with you cuz it's not
going to give very good answers so we've
explained overfitting and underfitting
so now we want to ask how to combat them
combating overfitting and underfitting
resampling the data to estimate the
model accuracy kfold cross validation
having a validation data set to evaluate
the model so when we do the resampling
we're randomly going to be picking out
data and we'll run it a few times to see
how that works depending on our random
data and how we um sample the data to
generate our model and then we want to
go ahead and validate the data set by
having our training data and then
keeping some data on the side uh testing
data to validate it how are weights
initialized in a network initializing
all weights to zero all the weights are
set to zero this makes your model
similar to a linear model so if you have
linear data coming in doing a basic
setup like that might work all the
neurons in every layer perform the same
operation given the same output and
making the Deep net useless right there
is a key word it's going to be useless
if you initialize everything to zero at
that point be looking into some other uh
machine learning tools initializing all
weights randomly here the weights are
assigned randomly by initializing them
very close to zero it gives better
accuracy to the model since every neuron
performs different computations and here
we have the weights are set randomly we
have our input layer the hidden layers
and the output layer and W equals NP
random random n layer size L layer size
L minus one this is the most commonly
used is to randomly generate your
weights what are the different layers in
CNN conv evolutional neural network
first is the convolutional layer that
performs a convolutional operation we
have our other video out if you want to
explore that more so and go into detail
exactly how the C the convolutional
layer works in the CNN as far as
creating a number of smaller uh picture
windows that go over the data uh the
second step is has a Ru layer railu
brings nonlinearity to the network and
converts all the negative pixels to zero
output is rectified feature map so it
goes into a mapping feature there pool
pulling layer pulling is a down sampling
operation that reduces the
dimensionality of the feature map so we
have all our railu layer which is
pulling all these little Maps out of our
convolutional layer is taking that
picture and little creating little tiny
neural networks to look at different
parts of the picture uh then we need to
pull it together and then finally the
fully connected layer so we flatten our
pulling layer out and we have a fully
connected layer recognizes and
classifies the objects in the image and
that's actually your forward propagation
reverse propagation training model
usually I mean there's a number of
different models out there of course
what is pooling in CNN and how does it
work pooling used to reduce the spatial
dimensions of a CNN performs down
sampling operation to reduce the
dimensionality creates a pulled feature
map by sliding a filter Matrix over the
input Matrix I mentioned that briefly on
the previous slide um it's important to
know that you have if you see here they
have a rectified feature map and so each
one of those colors like the yellow
color that might be one of the a smaller
little neural network using the railu
you'll look at it'll just kind of um go
over the main picture and look at all
the different areas on the main picture
so you might step one two three four
spaces um and then you have another one
that's also looking at features and it
has a 2785 each one of those is a map so
it might be the first one might be a map
looking for cat ears and the second one
looking for human eyes when it does this
you then have this rectified feature map
looking at these different features and
the max pooling with a 2X two filters
and a stride of two stride means instead
of skipping every pixel you're going to
go every two pixels you take the maximum
values and you can see over here when we
look at a pulled feature map one of the
features says hey I had a max value of
eight so somewhere in here we saw a
human eye labeled as eight pretty high
label and maybe seven was a human hand
and maybe four was cat whiskers or
something that we thought might be cat
whiskers four is kind of a low number in
this particular case compared to the
other ones so you have your full pull
feature map you can see the process here
is we have our step we look for the max
value and then we create a pulled
feature map of the maxed values how does
a lstm network work and that's long
short-term memory so the first thing to
know is that an lstms are a special kind
of recurrent neural network capable of
learning long-term dependencies
remembering information for long periods
of time is their default Behavior we did
look at the RNN briefly talked about how
the hidden layer feeds back into itself
with the lstm as a much more more
complicated feedback and you can see
here we have um the hidden layer of T
minus one and hidden layer that's what
the H stands for hidden layer of T and
the formula is going in as we can see
here we have the hidden layers we have T
minus one and then h of T where T stands
for time so this is a series remember
working with series and we want to
remember the past and you can see you
have your your input of T and that might
be a frame in a video as a frame comes
in they usually use in this one the
tangent h activation formula but you
also see that it goes through a couple
other formulas the Omega formula and so
when it combines these that thing goes
into the next layer your next hidden
layer that then goes into the data
that's submitted to the next input so
you have your X of t + one so when you
have that coming in then you have your H
value that's coming forward from the
last process and depending on how many
of these um Omega structures you put in
there depends on how longterm the memory
gets so it's important to remember this
is more for your long-term term
recurrent neural networks the three
steps in an lstm step one decides what
to forget and what to remember step two
selectively update cell State values So
based on what we want to remember and
forget we want to update those cell
values and then decides what part of the
current state make it to the output so
now we have to also have an output on
there what are Vanishing and exploding
gradients this is a great question that
affects all our neural networks while
training an RNN your slope can become
either to small or too large and this
makes the training difficult when the
slope is too small the problem is known
as Vanishing gradient so our slope we
have our change in X and our change in y
when the slope decreases gradually to a
very small value sometimes negative and
makes training difficult when the slope
tends to grow exponentially instead of
decaying this problem is called
exploding gradient the slope grows
exponentially you can see nice graph of
that here issues in gradient problem
Long training time poor performance and
low accuracy what is the difference
between epic batch and iteration in deep
learning epic an epic represents one
iteration over the entire data set so
that's everything you're going to go
ahead and put into that training model
batch we cannot pass the entire data set
into the neural network at once so we
divide the data set into a number of
batches and then iteration if we have
10,000 images as data and a batch size
of 200 then the Epic should run 10,000
times over 200 so that means we have our
total number the 200 equals 50 iteration
so in each epic we're running over all
the data set we're going to have 50
iterations and each of those iterations
includes a batch of 200 images in this
case why tensorflow is the most
preferred library in deep learning uh
well first tensorflow provides both C++
and python apis that makes it easier to
work on has a faster compilation time
than other deep learning libraries like
carass and torch tensorflow supports
both CPUs and gpus computing devices so
right now tensor flow is at the top of
the market because it's so easy to use
for both programmer side and for
Hardware side and for the speed of
getting something up and running what do
you mean by tensor and tensor flow
tensor is a mathematical object
represented as arrays of higher
Dimension these arrays of data with
different dimensions and ranks that are
fed as input to the neural network are
called tensors and you can see here we
have a tensor of Dimensions 5 comma 4 so
it's a two-dimensional tensor coming in
um you can look at an image like this
this that each one of those pixels is a
different value if it's a black and
white so it might be zero and ones and
then each one represents a black and
white image in a color photo you might
um either find a different value system
or you might have a tensor value that
has the XY coordinates as we see here
plus a colors so you might have three
more different dimensions for the three
different images the red the blue and
the yellow coming in and even as you go
from one layer or one tensor to the next
these layers might change we might
flatten them might bring in numerous in
the case of the convergence neural
network we have all those smaller
different mappings of features that come
in so each one of those layers coming
through is a tensor if it has multiple
Dimensions coming in and weights
attached to it what are the programming
elements in tensor flow well we have our
constants constants are parameters whose
value does not change to define a
constant we use tf. constant command
example a equal tf. constant 2.0 TF
float 32 so it's a tensor float value of
32 b equals TF constant 3.0 print AB if
we did a print of ab we'd have um tf.
constant and then of course uh B is that
instance of it variables variables allow
us to add new trainable parameters to
graph to Define a variable we use tf.
variable command and initialize them
before running the graph in session
example wal TF variable. 3 dtype TF
float 32 or b equal a TF variable Min -3
comma dtype flat 32 placeholders
placeholders allow us to feed data to a
T flow model from outside a model it
permits a value to be assigned later to
define a placeholder we use TF
placeholder command example AAL TF
placeholder Bal a * 2 with the TF
session as sess result equals session
run B comma feed dictionary equals a 3.0
print result uh so we have a nice
example there of a placeholder session a
session is run to evaluate the nodes
this is called as the tensor flow
runtime so for example you have AAL TF
constant 2.0 B TF constant 4.0 Cal a
plus b and at this point you'd go ahead
and create a session equals TF session
and then you could evaluate the tensor C
print session run C that would input c
as an input into your session what do
you understand by a computational wph
everything in tensorflow is based on
creating a computational graph it has a
network of nodes where each node
performs an operation nodes represent
mathematical operation and edges
represent tensors since data flows in a
form of a graph it is also called a data
flow graph and we have a nice visual of
this graph or graphic image of a
computational graph and you can see here
we have our input nodes our add multiply
nodes and our multiply node at the end
and then we have the edges where the
data flows so we have from a going to C
A going to D you can see we have a two
flowing a four flowing explain
generative adversarial Network along
with an example suppose there's a wine
shop that purchases wine from dealers
which say will resell later so we have
our dealer going to the wine our shop
owner that then sells it for a profit
but there are some malactor dealers who
sell fake wine in this case the shop
owner should be able to distinguish
between fake and authentic wine the
forger will try to different techniques
to sell fake wine and make sure certain
techniques go past the shop owner's
check so here's our forger fake wine
shop owner the shop owner would probably
get some feedback from the wine experts
that some of the wine is not original
the owner would have to improve how he
determines whether a wine is fake or
authentic goal of forger to create wines
that are indistinguishable from the
authentic ones goal of shop owner to
accurately tell if the wine is real or
not there are two main components of
generative adversarial Network and we'll
refer it to as a noise Vector coming in
where we have our forger who's going to
generate fake wine and then we have our
real authentic wine and of course our
shop owner has to figure out whether
it's real or fake the generator is a CNN
that keeps producing images that are
closer in appearance to the real images
while the discrimin Ator tries to
determine the difference between real
and fake images the ultimate aim is to
make the discriminator learn to identify
real and fake images what is an
autoencoder the network is trained to
reconstruct its inputs it is a neural
network that has three layers here the
input neurons are equal to the output
neuron the Network's Target outside is
same as the input it uses dimensionality
reduction to restructure the input input
image comes in we have our Latin space
representation and then goes back out
reconstructing the image it works by
compressing the input to a Latin space
representation and then reconstructing
the output from this representation what
is bagging and boosting bagging and
boosting are Ensemble techniques where
the idea is to train multiple models
using the same learning algorithm and
then take a call so we have in here
where we're bagging we take a data set
and we split it we're going to have our
training data and our test data very
standard thing to do then we're going to
randomly select data into the bags and
train your model separately so we might
have bag one model one bag two model two
bag three model 3 and so on in boosting
the infanist is to select the data
points which give wrong output in order
to improve the accuracy so in boosting
we have our data set again we split it
to test data and train data and we'll
take a bag one and we'll train the model
data points with wrong predictions then
go into Bag two and we then train that
model and repeat so if you are looking
to try your hands on this field then do
check simply learns postgraduate program
in Ai and machine learning so hurry up
and enroll Now find the course Link in
the description box so one of the first
questions that you may face is what are
the different types of machine learning
now what is the best way to respond to
this there are three types of machine
learning if you read any material you
will always be told there are three
types of machine learning but what is
important is you would probably be
better of emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is a third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say past data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you're trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact okay so that is what is
supervised learning when you have
existing labeled data which you then use
to train your model that is known as
supervised learning and unsupervised
learning is when you don't have this
labeled data so you have data it is not
labeled so the system has to figure out
a way to do some analysis on this okay
so that is unsupervised learning and you
can then add a few things like what what
are the ways of Performing uh supervised
learning and unsupervised learning or
what are some of the techniques so
supervised learning we we perform or we
do uh regression and classification and
unsupervised learning uh we do
clustering okay and clustering can be of
different types similarly regression can
be of different types but you don't have
to probably elaborate so much if they
are asking uh for uh just the different
types you can just mention these and
just at a very high level you can but if
they want you to elaborate give examples
then of course I think there is a
different question for that we will see
that later then the third so we have
supervised then we have unsupervised and
then reinforcement you need to provide a
little bit of information around that as
well because it is sometimes a little
difficult to come up with a good
definition for reinforcement learning so
you may have to little bit elaborate on
how reinforcement learning works right
so reinforcement learning works in in
such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and uh every time it is moving
in the direction of the target so the
agent basically has to take some action
and every time it takes an action which
is moving uh the agent towards the
Target right towards a goal a Target is
nothing but a goal okay then it is
rewarded and every time it is going in a
direction where it is away from the goal
then it is punished so that is the way
you can a little bit explain and uh this
is used primarily or very very impact
act f for teaching the system to learn
games and so on examples of this are
basically used in alphao you can throw
that as an example where alphao used
reinforcement learning to actually learn
to play the game of Go and finally it
defeated the co world champion all right
this much of information that would be
good enough okay then there could be a
question on overfitting uh so the
question could be what is overfitting
and and how can you avoid it so what is
overfitting so let's first try to
understand the concept because sometimes
overfitting may be a little difficult to
understand overfitting is a situation
where the model has kind of memorized
the data so this is an equivalent of
memorizing the data so we can draw an
analogy so that it becomes easy to
explain this now let's say you're
teaching a child about some recognizing
some fruits or something like that okay
and you're teaching this child about
recognizing let's say three fruits
apples oranges and pineapples okay so
this is a a small child and for the
first time you're teaching the child to
recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you'll take a basket of fruits
which consists of apples oranges and
pineapples okay and you take this basket
to this child and uh there may be let's
say hundreds of these fruits so you take
this basket to this child and keep
showing each of this root and then first
time obviously the child will not know
what it is so you show an apple and you
say hey this is Apple then you show
maybe an orange and say this is orange
and so on and so and then again you keep
repeating that right so till that basket
is over this is basically how training
work in machine learning also that's how
training works so till the basket is
completed maybe 100 fruits you keep
showing this child and then in the
process what has happened the child has
pretty much memorized these so even
before you finish that basket right by
the time you are halfway through the
child has learned about recognizing the
Apple orange and pineapple now what will
happen after halfway through initially
you remember it made mistakes in
recognizing but halfway through now it
has learned so every time you show a
fruit it will exactly 100% accurately it
will identify it will say the child will
say this is an apple this is an orange
and if you show a pineapple it will say
this is a pineapple right so that means
it has kind of memorized this data now
let's say you bring another basket of
fruits and it will have a mix of maybe
apples which were already there in the
previous set but it will also have in
addition to Apple it will probably have
a banana or maybe another fruit like a
jack fruit right so this is an
equivalent of your test data set which
the child has not seen before some parts
of it it probably has seen like the
apples it has seen but this banana and
jack fruit it has not seen so then what
will happen in the first round which is
an equivalent of your training data set
towards the end it has 100% it was
telling you what the fruits are right
Apple was accurately recognized orange
was accurately recognized and pineapples
were accurately recognized right so that
is like 100% accuracy but now when you
get another a fresh set which were not a
part of the original one what will
happen all the apples maybe it will be
able to recognize correctly but all the
others like the jack fruit or the banana
will not be recognized by the child
right so this is an analogy this is an
equivalent of overfitting so what has
happened during the training process it
is able to recognize or reach 100%
accuracy maybe very high accuracy okay
and we call that as very low loss right
so that is the technical term so the
loss is pretty much zero and accuracy is
pretty much 100% whereas when you use
testing there will be a huge error which
means the loss will be pretty high and
therefore the accuracy will be also low
okay this is known as overfitting this
is basically a process where training is
done training process is it goes very
well almost reaching 100% accuracy but
while testing it really drops down now
how can you avoid it so that is a
extension of this question there are
multiple ways of avoiding overfitting
there are techniques like what do you
call regularization that is the most
common technique that is used uh for uh
avoiding overfitting and within
regularization there can be a few other
subtypes like Dropout in case of neural
networks and a few other examples but I
think if you give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then the will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process
we train the model and then we test the
model and then once we are satisfied
with the test only then we deploy the
model so what happens in the train and
test is that you remember the labeled
data so let's say you have thousand
records with labeling information now
one way of doing it is you use all the
Thousand records for training and then
maybe right which means that you have
exposed all this thousand records during
the training process and then you take a
small set of the same data and then you
say okay I will test it with this okay
and then you probably what will happen
you may get some good results all right
but there is a flaw there what is the
flaw this is very similar to human
beings it is like you are showing this
model the entire data as a part of
training okay so obviously it has become
familiar with the entire data so when
you're taking a part of that again and
you're saying that I want to test it
obviously you will get good results so
that is not a very accurate way of
testing so that is the reason what we do
is we have the label data of this
thousand records or whatever we set
aside before starting the training
process we set aside a portion of that
data and we call that test set and the
remaining we call as training set and we
use only this for training our model now
the training process remember is not
just about passing one round of this
data set so let's say now your training
set has 800 records it is not just one
time you pass this 800 records what you
normally do is you actually as a part of
the training you may pass this data
through the model multiple times so this
thousand records may go through the
model maybe 10 15 20 times till the
training is perfect till the accuracy is
high till the errors are minimized okay
now so which is fine which means that
here that is what is known as the model
has seen your data and gets familiar
with your data and now when you bring
your test data what will happen is this
is like some new data because that is
where the real test is now you have
trained the model and now you are
testing the model with some data which
is kind of new that is like a situation
like like a realistic situation because
when the model is deployed that is what
will happen it will receive some new
data not the data that it has already
seen right so this is a realistic test
so you put some new data so this data
which you have set aside is for the
model is new and if it is able to
accurately predict the values that means
your training has worked okay the model
got drain properly but let's say while
you're testing this with this test data
you're getting a lot of errors that
means you need to probably either change
your model or retrain with more data and
things like that now coming back to the
question of how do you split this what
should be the ratio there is no fixed uh
number again this is like individual
preferences some people split it into 50
50 50% test and 50% training Some people
prefer to have a larger amount for
training and a smaller amount for test
so they can go by either 6040 or 7030 or
some people even go with some odd
numbers like
6535 or uh 6333 and 33 which is like 1/3
and 2/3 so there is no fixed rule that
it has to be something the ratio has to
be this you can go by your individual
preferences all right then you may have
questions around uh data handling data
manipulation or what do you call data
management or Preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there a
whole lot of things this is a very
generic question and if you need to be
little careful about responding to this
as well so probably have to illustrate
this again if you have experience in
doing this kind of work in handling data
you can illustrate with example saying
that I was on one project where I
received this kind of data this were the
columns where data was not filled or
these were the this many rows where the
data was missing that would be in fact a
perfect way to respond to this question
but if you don't have that obviously you
have to provide some good answer I think
it really depends on what exactly the
situation is and there are multiple ways
of handling the missing data or corrup
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing and you have pretty much half of
your data having these missing values in
terms of number of rows okay that could
be one situation another situation could
be that you have records or data missing
but when you do some initial calculation
how many records are corrupt or how many
rows or observations as we call it has
this missing data let's assume it is
very minimal like 10% okay now between
these two cases how do you so let's
assume that this is not a mission
critical situation and in order to fix
this 10% of the data the effort that is
required is much higher and obviously
effort means also time and money right
so it is not so critical and it is okay
to let's say get rid of these records so
obviously one of the easiest ways of
handling the data part or missing data
is remove those records or remove those
observations from your analysis so that
is the easiest way to do but then the
downside is as I said in as in the first
case if let's say 50% of your data is
like that because some column or the
other is missing so it is not like every
in every place in every Row the same
column is missing but you have in maybe
10% of the records column one is missing
and another 10% column 2 is missing
another 10% column 3 is missing and so
on and so forth so it adds up to maybe
half of your data set so you cannot
completely remove half of your data set
then the whole purpose is lost okay so
then how do you handle then you need to
come up with ways of filling up this
data with some meaningful value right
that is one way of handling so when we
say meaningful value what is that
meaningful value let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely we off so you have values
which are not missing first of all so
your system will work number two these
values are not so completely out of
whack that your whole analysis goes for
a toss right there may be situations
where if the missing values instead of
putting mean maybe a good idea to uh
fill it up with the minimum value or
with a zero so or with a maximum value
again as a said there are so many
possibilities so there is no like one
correct answer for this you need to
basically talk around this and
illustrate with your experience as I
said that would be the best otherwise
this is how you need to handle this
question okay so then the next question
can be how can you choose a classifier
based on a training set data size so
again this is one of those questions uh
where you probably do not have like a
one siiz fits-all answer first of all
you may not let's say decide your
classifier based on the training set
size maybe not the best way to decide
the type of the classifier and uh even
if you have to there are probably some
thumb rules which we can use but then
again every time so in my opinion the
best way to respond to this question is
you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just by if somebody defines a a
problem to you and somebody even if if
they show the data to you or tell you
what is the data or even the size of the
data I don't think there is a way to
really say that yes this is the
classifier that will work here no that's
not the right way so you need to still
uh you know test it out get the data try
out a couple of classifiers and then
only you will be in a position to decide
which classifier to use you try out
multiple classifiers see which one gives
the best accuracy and only then you can
decide then you can have a question
around confusion Matrix so the question
can be explain confusion Matrix right so
confusion Matrix I think the best way to
explain it is by taking an example and
drawing like a small diagram otherwise
it can really become tricky so my
suggestion is to take a piece of pen and
paper and uh explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification uh learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and uh
it is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
yes and how many no so you have that
information and you have the predicted
values how many yes and how many no
right so the total actual values the
total yes is 12 + 1 13 and they are
shown here and the actual value NOS are
9 + 3 12 okay so that is what this
information here is so this is about the
actual and this is about the predicted
similarly the predicted values there are
yes are 12 + 3 15 yeses and no are 1 + 9
10 NOS okay so this is the way to look
at this confusion Matrix okay and uh out
of this what is the meaning convey so
there are two or three things that needs
to be explained outright the first thing
is for a model to be accurate the values
across the diagonal should be high like
in this case right that is one number
two the total sum of these values is
equal to the total observations in the
test data set so in this case for
example you have 12 + 3 15 + 10 25 so
that means we have 25 observations in
our test data set okay so these are the
two things you need to First explain
that the total sum in this Matrix the
numbers is equal to the size of the test
data set and the diagonal values
indicate the accuracy so by just by
looking at it you can probably have a
idea about is this uh an accurate model
is the model being accurate if they're
all spread out equally in all these four
boxes that means probably the accuracy
is not very good okay now how do you
calculate the accuracy itself right how
do you calculate the accuracy itself so
it is a very simple mathematical
calculation you take some of the
diagonals right so in this case it is 9
+ 12 21 and divide it by the total so in
this case what will it be let's me uh
take a pen so your your D Al values is
equal to if I say d is equal to 12 + 9
so that is 21 right and the total data
set is equal to right we just calculated
it is 25 so what is your accuracy it is
21 by your accuracy is equal to 21 by 25
and this turns out to be about
85% right so this is 85% so that is our
accuracy okay so this is the way you
need to explain draw diagram given
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you don't have to
calculate those numbers on the fly right
so couple of uh hints are that you take
some numbers which are with which add up
to 100 that is always a good idea so you
don't have to really do this complex
calculations so the total value will be
100 and then diagonal values you divide
once you find the diagonal values that
is equal to your percentage okay all
right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and Pen
otherwise it will be pretty difficult to
to explain this so we use the same
example of the confusion Matrix and uh
we can explain that so A confusion
Matrix looks somewhat like this and um
when we just take yeah it looks somewhat
like this and we continue with the
previous example where this is the
actual value this is the predicted value
and uh in the actual value we have 12 +
1 13 yeses and 3 + 9 12 Nos and the
predicted values there are 12 + 3 15
yeses and uh 1 + 9 10 NOS okay now this
particular case which is the false
positive what is a false positive first
of all the second word which is positive
okay is referring to the predict value
so that means the system has predicted
it as a positive but the real value so
this is what the false comes from but
the real value is not positive okay that
is the way you should understand this
term false positive or even false
negative so false positive so positive
is what your system has predicted so
where is that system predicted this is
the one positive is what yes so you
basically consider this row okay now if
you consider this row so this is this is
is all positive values this entire row
is positive values okay now the false
positive is the one which where the
value actual value is negative predicted
value is positive but the actual value
is negative so this is a false positive
right and here is a true positive so the
predicted value is positive and the
actual value is also positive okay I
hope this is making sense now let's take
a look at what is false negative false
negative so negative is the second term
that means that is the predict value
that we need to look for so which are
the predicted negative values this row
corresponds to predicted negative values
all right so this row corresponds to
predicted negative values and what they
are asking for false so this is the row
for predicted negative values and the
actual value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive you need
to take for the predicted part so
predicted positive is here okay and then
the first term is for the actual so true
positive so true in case of actual is
yes right so true positive is this one
okay and then in case of actual the
negative now we are talking about let's
say true negative true negative negative
is this one and the true comes from here
so this is true negative right nine is
true negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model right so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model
development of the machine learning
model happens is like this so first of
all you try to understand the problem
and try to figure out whether it is a
classification problem or a regression
problem based on that you select a few
algorithms and then you start the
process of training these models okay so
you can either do that or you can after
due diligence you can probably decide
that there is one particular algorithm
that which is most suitable usually it
happens through trial and error process
but at some point you will decide that
okay this is the model we are going to
use okay so in that case we have the
model algorithm and the model decided
and then you need to do the process of
training the model and testing the model
and this is where if it is supervised
learning you split your data the label
data into training data set and test
data set and you use the training data
set to train your model and then you use
the test data set to check the accuracy
whether it is working fine or not so you
test the model before you actually put
it into production right so once you
test the model you're satisfied it's
working fine then you go to the next
level which is putting it for production
and then in production obviously new
data will come and uh the inference
happens so the model is readily
available and only thing that happens is
new data comes and the model predicts
the values whether it is regression or
classification now so this can be an
iterative process so it is not a
straightforward process where you do the
training do the testing and then you
move it to production now so during the
training and test process there may be a
situation where because of either
overfitting or or things like that the
test doesn't go through which means that
you need to put that back into the
training process so that can be an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
what once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so net net this is a continuous
process of um tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then then obviously
the the question would be then what is
the difference right so deep learning
you need to mention there are two key
parts that interviewer will be looking
for when you are defining deep learning
so first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
uh scope and deep learning is one one
part of it so then what exactly is the
difference deep learning is primarily
when we are implementing these our
algorithms or when we are using neural
networks for doing our training and
classification and regression and all
that right so when we use neural network
then it is considered as deep learning
and the term deep comes from the fact
that you can have several layers of
neural networks and these are called
Deep neural networks and therefore the
term deep you know deep learning uh the
other difference between machine
learning and deep learning which the
interviewer may be wanting to hear is
that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and uh this data has several let's say
if it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict the
height weight and so on and so forth so
these are all features of human beings
let's say we have sensus data and we
have all this so those are the features
now there may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to our model right so we as a
data scientist we have to decide whether
we should take all of them all the
features or we should throw away some of
them because again if we take all of
them number one of course your accuracy
will probably get affected but also
there is a computational part so if you
have so many features and then you have
so much data it becomes very tricky so
in case of machine learning we manually
take care of identifying the features
that do not contribute to the learning
process and thereby we eliminate those
features and so on right so this is
known as feature engineering and in
machine learning we do that manually
whereas in deep learning where we use
neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on their own
based on past data so here we are
talking primity of supervised learning
and um it needs only a small amount of
data for training and then works well on
lowend system so you don't need large
machines and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and uh the problem
is divided into parts and solved
individually and then combine so that is
about the machine learning part in deep
learning deep learning basically enables
machines to take decisions with the help
of artificial neural network so here in
deep learning we use neural length so
that is the key differentiator between
machine learning and deep learning and
usually deep learning involves a large
amount of data and therefore the
training also requires usually the
training process requires highend
machines uh because it needs a lot of
computing power and the Machine learning
features are or the featur feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of De planning therefore it is said that
the problem is handled end to end so
this is a quick comparison between
machine learning and deep learning in
case you have that kind of a question
then you might get a question around the
uses of machine learning or some real
life applications of machine learning in
modern business the question may be
worded in different ways but the the
meaning is how exactly is machine
learning used or actually supervised
machine learning it could be a very
specific question around supervised
machine learning so this is like give
examples of supervised machine learning
use of supervised machine learning in
modern business so that could be the
next question so there are quite a few
examples or quite a few use cases if you
will for supervised machine learning the
very common one is email spam detection
so you want to train your application or
your system to detect between spam and
non-spam so this is a very common
business application of supervised
machine learning so how does this work
the way it works is that you obviously
have historical data of your emails and
they are categorized as spam and not
spam so that is what is the labeled
information and then you feed this
information or the all these emails as
an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
the training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use that to train your
model right so this is one example now
there are a few Industries specific
applications for supervised machine
learning one of the very common ones is
in healthcare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning here the way
it works is that existing images it
could be x-ray images it be MRI or any
of these images are available and they
are tacked saying that okay this x-ray
image is defective or the person has an
illness or it could be cancer whichever
illness right so it is tacked as
defective or clear or good image and
defective image something like that so
we come up with the binary or it could
be multiclass as well saying that this
is defective to 10% this is 25% and so
on but let's keep it simple you can give
an example of just a binary
classification that would be good enough
so you can say that in Healthcare
Diagnostics using image we need to
detect whether a person is ill or
whether a person is having cancer or not
so here the way it works is you feed
labeled images and you allow the model
to learn from that so that when New
Image is fed it will be able to predict
whether this person is having that
illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
um unsupervised and so there can be a
question around semi-supervised machine
learning so what is semisupervised
machine learning now semi-supervised
learning as the name suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existence is
that supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
Industries or in many under many
situations getting the label data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of the diagnostic images we
can just let's say take X-ray images now
there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
the images are there but whether it is
defective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
uh then say that okay yes this image is
clean and this image is cancerous and so
on and so forth now that is a huge
effort by itself so this is where semi
supervis Ed learning comes into play so
what happens is there is a large amount
of data maybe a part of it is labeled
then we try some techniques to label the
remaining part of the data so that we
get completely labeled data and then we
train our model so I know this a little
long winding explanation but
unfortunately there is no uh quick and
easy definition for semi-supervised
machine learning this is the only way
probably to explain this concept we may
have another question as um what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding finding
similar objects so their characteristics
can be measured and if they have in most
of the characteristics if they are
similar then they can be put together
this is clustering then Association you
can I think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other so
for example if somebody bought a maybe a
laptop the person has also purchased a
mouse so so this is more in a e-commerce
scenario for example so you can give
this as an example so people who are
buying laptops are also buying Mouse so
that means there is an association
between laptops and mouse or maybe
people who are buying bread are also
buying butter so that is a Association
that can be created so this is
unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and un
supervised machine learning so machine
learning these are the two main types of
machine learning supervised and unised
and in case of supervised and again here
probably the keyword that the person may
be wanting to hear is labeled data now
very often people say yeah we have
historical data and if we run it it is
supervised and if we don't have
historical data yes but you may have
historical data but if it is not labeled
then you cannot use it for supervised
learning so it is it's very key to
understand that we we put in that
keyword labeled okay so when we have
labeled data for training our model then
we can use supervised learning and if we
do not have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these types of uh
trainings so there can be another
question a little bit more theoretical
and conceptual in nature this is about
inductive machine learning and deductive
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
can vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it okay what can
be an example so we can probably tell
the person or show a person person a
video that fire can burn the F burn his
finger or fire can cause damage so what
is happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw
conclusion usion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or if throws
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
uh probably that explanation will be
sufficient the next question is are KNN
and K means clustering similar to one
another or are they same right because
that the letter K is kind of common
between them okay so let us take a
little while to understand what these
two are one is KNN and another is kin
KNN stands for K nearest neighbors and K
means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them K andn is completely
different K means clustering is
completely different KN andn is a
classification process and therefore it
comes under supervised learning whereas
K means clustering is actually a
unsupervised okay when you have KN andn
when you want to implement KN andn which
is basically K nearest neighbors the
value of K is a number so you can say k
is equal to 3 you want to implement K
and with K is equal to 3 so which means
that it performs the classification in
such a way that how does it perform the
classification so it will take three
nearest objects and that's why it's
called nearest neighbor so basically
based on the distance it will try to
find out its nearest objects that are
let's say three of the nearest objects
and then it will check whether the class
they belong to which class right so if
all three belong to one particular class
obviously this new object is also
classified as that particular class but
it is possible that they may be from two
or three different classes okay so let's
say they are from two classes and then
if they are from two classes now usually
you take a odd number you assign a odd
number to so if there are three of them
and two of them belong to one class and
then one belongs to another class so
this new object is assigned to the class
to which the two of them belong now the
value of K is sometimes tricky whether
should you use three should you use five
should you use seven that can be tricky
because the ultimate classification can
also vary so it's possible that if
you're taking K as three the object is
probably in one particular class but if
you take K is equal to 5 maybe the
object will belong to a different class
because when you're taking three of them
probably two of them belong to class one
and one belong to class two whereas when
you take five of them it is possible
that only two of them belong to class
one and three of them belong to class
two so which means that this object will
belong to class two right so you see
that so this the class allocation can
vary depending on the value of K now K
means on the other hand is a clustering
process and it is unsupervised where
what it does is the system will
basically identify how the objects are
how close the objects are with respect
to some of their features okay and but
similarity of course is the the letter K
and in case of K means also we specify
its value value and it could be three or
five or seven there is no technical
limit as such but it can be any number
of clusters that uh you can create okay
so based on the value that you provide
the system will create that many
clusters of similar objects so there is
a similarity to that extent that K is a
number in both the cases but actually
these two are completely different
processes we have what is known as kniv
based classifier and people often get
confused thinking that knif base is the
name of the person who found this uh
classifier or who developed this
classifier which is not 100% true base
is the name of the person b a y s is the
name of the person but naive is not the
name of the person right so naive is
basically an English word and that has
been added here because of the nature of
this particular classifier KN based
classifier is a probability based
classifier and uh it makes some
assumptions that presence of feature of
a class is not related to the presence
of any other feature of maybe other
classes right so which is not very
strong or not a very what do you say
accurate assumption because these
features can be related and so on but
even if you go with this assumption this
whole algorithm works very well even
with this assumption and uh that is the
good side of it but the term comes from
there so that is the explanation that
you can give then there can be question
around reinforcement learning
it can be paraphrased in multiple ways
one could be can you explain how a
system can play a game of chess using
reinforcement learning or it can be any
game so the best way to explain this is
again to talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an agent and the
agent is basically performing some
actions in order to achieve a certain
goal and this goal goals can be anything
either if it is related to game then the
goal could be that you have to score
very high score a high value High number
or it could be that your uh number of
lives should be as high as possible
don't lose lives so these could be some
of them more advanced examples could be
for driving in the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
the deaths for example it is rewarded
and every time it takes a step which
goes against that goal right contrary or
in the reverse Direction it is penalized
okay so it is like a carrot and stick
system now how do you use this to create
a game of chess or to create a system to
play a game of chess now the way this
works is and this could probably go back
to this alphago example where alphao
defeated a human Champion so the way it
works is in reinforcement learning the
system is allowed for example if in this
case we're talking about Chess so we
allow the system to first of all watch
playing a game of chess chess so it
could be with a human being or it could
be the system itself there are computer
games of Chess right so either this new
learning system has to watch that game
or watch a human being play the game
because this is reinforcement uh
learning is pretty much all visual so
when you're teaching the system to play
a game the system will not actually go
behind the scenes to understand the
logic of your software of this game or
anything like that it is just visually
watching the screen and then it learns
okay so reinforcement learning to a
large extent works on that so you need
to create a mechanism whereby your model
will be able to watch somebody playing
the game and then you allow the system
also to start playing the game so it
pretty much starts from scratch okay and
as it moves forward it it it's at right
at the beginning the system really knows
nothing about the game of CH okay so
initially it is a clean slate it just
starts by observing how you playing so
it will make some random moves and keep
losing badly but then what happens is
over a period of time so you need to now
allow this system or you need to play
with the system not just 1 2 3 four or
five times but hundreds of times
thousands of times maybe even hundreds
of thousands of times and that's exactly
how alpha go has done it played Millions
of games between itself and the system
right so for the game of chess also you
need to do something like that you need
to allow the system to play chess and uh
then learn on its own over a period of
repetition so I think you can probably
explain it to this much to this extent
and it should be uh sufficient now this
is another question which is again
somewhat similar but here the size is
not coming into picture so the question
is how will you know which machine
learning algor them to choose for your
classification problem now this is not
only classification problem it could be
a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I'm going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have svm you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is uh no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work so this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are provided
obviously something like uh you know
Amazon website or e-commerce site like
Amazon collects a lot of data around the
customer Behavior who isch puring what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the unsupervised learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age maybe your
agender and where you're located what
you purchased earlier right so all this
is taken and the recommendation engine
basically uses all this information and
comes up with with recommendations for a
particular user so that is how the
recommendation engine work all right
then the question can be something very
basic like when will you go for
classification versus regression right
when do you do classification instead of
regression or when you use
classification instead of regression now
yes so so this is basically going back
to the understanding of the basics of
classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression we use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values and other is the
continuous values so for regression if
you are trying to find some continuous
values you use regression whereas if
you're trying to find some discrete
values you use classification and then
you need to illustrate what are some of
the examples so classification is like
let's say there are images and you need
to put them into classes like cat dog
elephant tiger something like that so
that is a classification problem or it
can be that is a multiclass
classification problem it could be
binary classification problem like for
example whether a customer will buy or
he will not buy that is a classification
binary classification it can be in the
weather forecast area now weather
forecast is again combination of
regression and classification because on
the one hand you want to predict whether
it's going to rain or not that's a
classification problem that's a binary
classification right whether it's going
to rain or not rain however you also
have to predict what is going to be the
temperature tomorrow right now
temperature is a continuous value you
can't answer the temperature in a yes or
no kind of a response right so what will
be the temperature tomorrow so you need
to give a number which can be like 20¬∞
30¬∞ or whatever right so that is where
you use regression one more example is
stock price prediction so that is where
again you will use regression so these
are the various examples so you need to
illustrate with examples and make sure
you include those keywords like discrete
and continuous so the next question is
more about a little bit of a design
related question to understand your
Concepts and things like that so it is
how will you design a spam filter so how
do you basically design or develop a
spam filter so I think the main thing
here is he is looking at probably
understanding your Concepts in terms of
uh what is the algorithm you will use or
what is your understanding about
difference between classification and
regression uh and things like that right
and the process of course the
methodology and the process so the best
way to go about responding to this is we
say that okay this is a classific
problem because we want to find out
whether an email is a spam or not spam
so that we can apply the filter
accordingly so first thing is to
identify what type of a problem it is so
we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector missions for
example SPM so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and uh
then you split that into training and
test data set you use your training data
set to train your model that or your
algorithm that you have used rather the
model actually so and you actually will
have three models let's say you are
trying to test out three algorithms so
you will obviously have three models so
you need to try all three models and
test them out as well see which one
gives the best accuracy and then you
decide that you will go with that model
okay so training and test will be done
and then you zero in on one particular
model and then you say okay this is the
model will we use we will use and then
go ahead and Implement that or put that
in production so that is the way you
design a Spam F the next question is
about random Forest what is random
Forest so this is a very straightforward
question however the response you need
to be again a little careful while we
all know what is random Forest
explaining this can sometimes be tricky
so one thing is random Forest is kind of
in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and uh trees will basically we
will use for doing if it is
classification mostly it is
classification you will use the the
trees for classification and then you
use voting for finding the the final
class so that is the underlying but how
will you explain this how will you
respond to this so first thing obviously
we will say that random Forest is one of
the algorithms and the more important
thing that you need to probably the
interviewer is is waiting to hear is is
Ensemble learner right so this is one
type of Ensemble learner what is
Ensemble learner Ensemble learner is
like a combination of algorithms so it
is a learner which consists of more than
one algorithm or more than one maybe
models okay so in case of random Forest
the algorithm is the same but instead of
using one instance of it we use multiple
instances of it and we use so in a way
that is a a random Forest is an ensemble
there are other types of assemble
Learners where we have like we use
different algorithms itself so you have
one maybe logistic regression and a
decision tree combined together and so
on and so forth or there are other ways
like for example splitting the data in a
certain way and so on so that's all
about Ensemble we will not go into that
but random Forest itself I think the
interviewer will be happy to hear this
word Ensemble Learners and so then you
go and explain how the random Forest
works so if the random Forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work let's say your random
Forest consists of 100 trees okay and
each observation you pass through this
forest and each observation let's say it
is a classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and 10 of the trees say it is a
one you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and and so on
so that is the way random Forest works
for classification if it is a regression
problem it's somewhat similar but only
thing is instead of w what we will do is
so in regression remember what happens
you actually calculate a value right so
for example you're using regression to
predict the temperature and you have 100
trees and each tree obviously will
probably predict a different value of
the temperature they may be close to
each other but they may not be exactly
the same value so these trees so how do
you now find the actual value the output
for the entire Forest right so you have
outputs of individual trees which are a
part of this Forest but then you need to
find the final output of the forest
itself so how do you do that so in case
of regression you take like an average
or the mean of all the 100 G right so
this is also a way of reducing the error
so maybe if you have only one tree and
if that one tree makes a error it is
basically 100% wrong or 100% right right
but if you have on the other hand if you
have a bunch of trees you are basically
mitigating that error or reducing that
error okay so that is the way random
Forest works so the next question is
considering the long list of machine
learning algorithms how will you decide
on which one to use so once again here
there is no way to outright say that
this is the algorithm that we will use
for a given data set this is a very good
question but then the response has to be
like again there will not be a one side
size fits all so we need to first of all
you can probably shorten the list in
terms of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably uh shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithms
right so for example if it is a
classification you cannot use linear
regression algorithm there or if it is a
regression problem you cannot use svm or
maybe you can use svm but maybe a
logistic regression right so to that
extent you can probably shorten the list
but still you will not be able to 100%
decide on saying that this is the exact
algorithm that I'm going to use so the
way to go about is you choose a few
algorithms based on what the problem is
you try out your data you train some
models of these algorithms check which
one gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm okay
all right then there can be questions
around bias and variance so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example bias in machine learning it
occurs when the predicted values are far
away from the actual values so that is a
bias okay and whereas they are all all
the values are probably they are far off
but they are very near to each other
though the predicted values are close to
each other right while they are far off
from the actual value but they are close
to each other you see the difference so
that is bias and then the other part is
your variance now variance is when the
predicted values are all over the place
right so the variance is high that means
it may be close to the Target but it is
kind of very scattered so the point the
predicted values are not close to each
other right in case of bias the
predicted values are close to each other
but they are not close to the Target but
here they may be close to the Target but
they may not be close to each other so
they are a little bit more scattered so
that is what in case of a variance okay
then the next question is about again
related to bias and variance what is the
tradeoff between bias and variance yes I
think this is a interesting question
because these two are heading in
different directions so for example if
you try to minimize the bias variance
will keep going high and if you try to
minimize the variance bias will keep
going high and there is no way you can
minimize both of them so you need to
have a tradeoff saying that okay this is
the level at which I will have my buyers
and this is the level at which I will
have variance so the trade-off is that
pretty much uh that you you decide what
is the level you will tolerate for your
buyers and what is the level you will
tolerate for variance and a combination
of these two in such a way that your
final results are not way off and having
a tradeoff will ensure that the the
results are consistent right so that is
basically the output is consistent and
which means that they are close to each
other and they're also accurate that
means they are as close to the Target as
possible right so if either of these is
high then one of them will go off the
track define precision and Recall now
again here I think uh it would be best
to uh draw a diagram and take a the
confusion Matrix and it is very simple
the definition is like a formula your
Precision is true positive by true
positive plus false positive and your
recall is true positive by true positive
plus false negative okay so that's you
can just show it in a mathematical way
that's pretty much uh you know that can
be shown that's the easiest way to
define so the next question can be about
decision tree what is decision tree
pruning and why is it so basically
decision tree trees are really simple to
implement and understand but one of the
drawbacks of decision trees is that it
can become highly complicated as it
grows right and the rules and the
conditions can become very complicated
and this can also lead to overfitting
which is basically that during training
you will get 100% accuracy but when
you're doing testing you'll get a lot of
Errors so that is the reason pruning
needs to be done so the purpose so the
reason for doing decision tree pruning
is to reduce overfitting or to cut down
on overfitting and what is decision tree
pruning it is basically that you reduce
the number of branches because as you
may be aware a tree consists of the root
node and then there are several internal
nodes and then you have the leaf nodes
now if there are too many of these
internal nodes that is when you face the
problem of overfitting and pruning is
the process of reducing the those
internal nodes all right so the next
question can be what is logistic
regression uh so basically logistic
regression is um one of the techniques
used for performing classification
especially binary classification now
there is something special about
logistic regression and there are a
couple of things you need to be careful
about first of all the name is a little
confusing it is called logistic
regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if if
it's really you know if it is required
and they can also ask this like a trick
question right so that is one part
second thing is the term logistic has
nothing to do with the usual Logistics
that we talk about but it is derived
from log so that the mathematical
derivation involves log and therefore
the name logistic regression so what is
logis listic regression and how is it
used so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between zero
and one and we can set a thresh short
that can vary and typically it is 0.5 so
any value above 0. five is considered as
one and if the probability is below 0.5
it is considered as zero so that is the
way we calculate the probability or the
system calculates the probability and
based on the threshold it sets a value
of zero or one which is like a binary
classification zero or one okay then we
have a question around K nearest
neighbor algorithm so explain K nearest
neighbor algorithm so first of all what
is a k nearest neighbor algorithm this
is a classification algorithm so that is
the first thing we need to mention and
we also need to mention that the K is a
number it is an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you're explaining
I think that'll be the best way so draw
some diagram like this and let's say we
have three clusters or three classes
existing and now you want to find for a
new item that has come you want to find
out which class this belongs to right so
you go about as the name suggests you go
about finding the nearest neighbors
right the points which are closest to
this and how many of them you will find
that is what is defined by K now let's
say our initial value of K was five okay
so you will find the k the five nearest
data points so in this case as it is
Illustrated these are the five nearest
data points but then all five do not
belong to the same class or cluster so
there are one belonging to this cluster
one the second one belonging to this
cluster to three of them belonging to
this third cluster okay so how do you
decide that's exactly the reason we
should as much as possible try to assign
a odd number so that it becomes easier
to assign this so in this case you see
that the majority actually if there are
multiple classes then you go with the
majority so since three of these items
belong to this class we assign which is
basically the in in this case the green
or the tennis or the third cluster as I
was talking about right so we assign it
to this third class so in this case it
is uh that's how it is decided okay so K
nearest neighbor so first thing is to
identify the number of neighbors that
are mentioned as K so in this case it is
K is equal to five so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that okay and and then the uh
new data point is assigned to that class
okay so that's pretty much how K nearest
neighbors work with this we have come to
end of this full course if you have any
question or doubt please feel free to
ask in the comment section below our
team of experts will help you as soon as
possible thank you and keep learning
with simply learn staying ahead in your
career requires continuous learning and
upskilling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
[Music]
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here